<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src\hotspot\os_cpu\windows_x86\atomic_windows_x86.hpp</title>
    <link rel="stylesheet" href="..\..\..\..\style.css" />
    <script type="text/javascript" src="..\..\..\..\navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
 26 #define OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
 27 
<a name="1" id="anc1"></a>
 28 #include &quot;runtime/os.hpp&quot;
 29 
 30 // Note that in MSVC, volatile memory accesses are explicitly
 31 // guaranteed to have acquire release semantics (w.r.t. compiler
 32 // reordering) and therefore does not even need a compiler barrier
 33 // for normal acquire release accesses. And all generalized
 34 // bound calls like release_store go through Atomic::load
 35 // and Atomic::store which do volatile memory accesses.
 36 template&lt;&gt; inline void ScopedFence&lt;X_ACQUIRE&gt;::postfix()       { }
 37 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X&gt;::prefix()        { }
 38 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::prefix()  { }
 39 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::postfix() { OrderAccess::fence(); }
 40 
 41 // The following alternative implementations are needed because
 42 // Windows 95 doesn&#39;t support (some of) the corresponding Windows NT
 43 // calls. Furthermore, these versions allow inlining in the caller.
 44 // (More precisely: The documentation for InterlockedExchange says
 45 // it is supported for Windows 95. However, when single-stepping
 46 // through the assembly code we cannot step into the routine and
 47 // when looking at the routine address we see only garbage code.
 48 // Better safe then sorry!). Was bug 7/31/98 (gri).
 49 //
 50 // Performance note: On uniprocessors, the &#39;lock&#39; prefixes are not
 51 // necessary (and expensive). We should generate separate cases if
 52 // this becomes a performance problem.
 53 
<a name="2" id="anc2"></a><span class="line-removed"> 54 #pragma warning(disable: 4035) // Disables warnings reporting missing return statement</span>
 55 
 56 template&lt;size_t byte_size&gt;
 57 struct Atomic::PlatformAdd
 58   : Atomic::AddAndFetch&lt;Atomic::PlatformAdd&lt;byte_size&gt; &gt;
 59 {
 60   template&lt;typename D, typename I&gt;
 61   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;
 62 };
 63 
<a name="3" id="anc3"></a><span class="line-removed"> 64 #ifdef AMD64</span>
<span class="line-removed"> 65 template&lt;&gt;</span>
<span class="line-removed"> 66 template&lt;typename D, typename I&gt;</span>
<span class="line-removed"> 67 inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="line-removed"> 68                                                atomic_memory_order order) const {</span>
<span class="line-removed"> 69   return add_using_helper&lt;int32_t&gt;(os::atomic_add_func, dest, add_value);</span>
<span class="line-removed"> 70 }</span>
 71 
<a name="4" id="anc4"></a><span class="line-modified"> 72 template&lt;&gt;</span>
<span class="line-modified"> 73 template&lt;typename D, typename I&gt;</span>
<span class="line-modified"> 74 inline D Atomic::PlatformAdd&lt;8&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="line-modified"> 75                                                atomic_memory_order order) const {</span>
<span class="line-modified"> 76   return add_using_helper&lt;int64_t&gt;(os::atomic_add_long_func, dest, add_value);</span>
<span class="line-modified"> 77 }</span>










 78 
 79 #define DEFINE_STUB_XCHG(ByteSize, StubType, StubName)                  \
 80   template&lt;&gt;                                                            \
 81   template&lt;typename T&gt;                                                  \
 82   inline T Atomic::PlatformXchg&lt;ByteSize&gt;::operator()(T volatile* dest, \
 83                                                       T exchange_value, \
 84                                                       atomic_memory_order order) const { \
 85     STATIC_ASSERT(ByteSize == sizeof(T));                               \
<a name="5" id="anc5"></a><span class="line-modified"> 86     return xchg_using_helper&lt;StubType&gt;(StubName, dest, exchange_value); \</span>


 87   }
 88 
<a name="6" id="anc6"></a><span class="line-modified"> 89 DEFINE_STUB_XCHG(4, int32_t, os::atomic_xchg_func)</span>
<span class="line-modified"> 90 DEFINE_STUB_XCHG(8, int64_t, os::atomic_xchg_long_func)</span>
 91 
 92 #undef DEFINE_STUB_XCHG
 93 
 94 #define DEFINE_STUB_CMPXCHG(ByteSize, StubType, StubName)                  \
 95   template&lt;&gt;                                                               \
 96   template&lt;typename T&gt;                                                     \
 97   inline T Atomic::PlatformCmpxchg&lt;ByteSize&gt;::operator()(T volatile* dest, \
 98                                                          T compare_value,  \
 99                                                          T exchange_value, \
100                                                          atomic_memory_order order) const { \
101     STATIC_ASSERT(ByteSize == sizeof(T));                                  \
<a name="7" id="anc7"></a><span class="line-modified">102     return cmpxchg_using_helper&lt;StubType&gt;(StubName, dest, compare_value, exchange_value); \</span>



103   }
104 
<a name="8" id="anc8"></a><span class="line-modified">105 DEFINE_STUB_CMPXCHG(1, int8_t,  os::atomic_cmpxchg_byte_func)</span>
<span class="line-modified">106 DEFINE_STUB_CMPXCHG(4, int32_t, os::atomic_cmpxchg_func)</span>
<span class="line-modified">107 DEFINE_STUB_CMPXCHG(8, int64_t, os::atomic_cmpxchg_long_func)</span>
108 
109 #undef DEFINE_STUB_CMPXCHG
110 
<a name="9" id="anc9"></a><span class="line-removed">111 #else // !AMD64</span>
112 
<a name="10" id="anc10"></a><span class="line-modified">113 template&lt;&gt;</span>
<span class="line-removed">114 template&lt;typename D, typename I&gt;</span>
<span class="line-removed">115 inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="line-removed">116                                                atomic_memory_order order) const {</span>
<span class="line-removed">117   STATIC_ASSERT(4 == sizeof(I));</span>
<span class="line-removed">118   STATIC_ASSERT(4 == sizeof(D));</span>
<span class="line-removed">119   __asm {</span>
<span class="line-removed">120     mov edx, dest;</span>
<span class="line-removed">121     mov eax, add_value;</span>
<span class="line-removed">122     mov ecx, eax;</span>
<span class="line-removed">123     lock xadd dword ptr [edx], eax;</span>
<span class="line-removed">124     add eax, ecx;</span>
<span class="line-removed">125   }</span>
<span class="line-removed">126 }</span>
<span class="line-removed">127 </span>
<span class="line-removed">128 template&lt;&gt;</span>
<span class="line-removed">129 template&lt;typename T&gt;</span>
<span class="line-removed">130 inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">131                                              T exchange_value,</span>
<span class="line-removed">132                                              atomic_memory_order order) const {</span>
<span class="line-removed">133   STATIC_ASSERT(4 == sizeof(T));</span>
<span class="line-removed">134   // alternative for InterlockedExchange</span>
<span class="line-removed">135   __asm {</span>
<span class="line-removed">136     mov eax, exchange_value;</span>
<span class="line-removed">137     mov ecx, dest;</span>
<span class="line-removed">138     xchg eax, dword ptr [ecx];</span>
<span class="line-removed">139   }</span>
<span class="line-removed">140 }</span>
<span class="line-removed">141 </span>
<span class="line-removed">142 template&lt;&gt;</span>
<span class="line-removed">143 template&lt;typename T&gt;</span>
<span class="line-removed">144 inline T Atomic::PlatformCmpxchg&lt;1&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">145                                                 T compare_value,</span>
<span class="line-removed">146                                                 T exchange_value,</span>
<span class="line-removed">147                                                 atomic_memory_order order) const {</span>
<span class="line-removed">148   STATIC_ASSERT(1 == sizeof(T));</span>
<span class="line-removed">149   // alternative for InterlockedCompareExchange</span>
<span class="line-removed">150   __asm {</span>
<span class="line-removed">151     mov edx, dest</span>
<span class="line-removed">152     mov cl, exchange_value</span>
<span class="line-removed">153     mov al, compare_value</span>
<span class="line-removed">154     lock cmpxchg byte ptr [edx], cl</span>
<span class="line-removed">155   }</span>
<span class="line-removed">156 }</span>
<span class="line-removed">157 </span>
<span class="line-removed">158 template&lt;&gt;</span>
<span class="line-removed">159 template&lt;typename T&gt;</span>
<span class="line-removed">160 inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">161                                                 T compare_value,</span>
<span class="line-removed">162                                                 T exchange_value,</span>
<span class="line-removed">163                                                 atomic_memory_order order) const {</span>
<span class="line-removed">164   STATIC_ASSERT(4 == sizeof(T));</span>
<span class="line-removed">165   // alternative for InterlockedCompareExchange</span>
<span class="line-removed">166   __asm {</span>
<span class="line-removed">167     mov edx, dest</span>
<span class="line-removed">168     mov ecx, exchange_value</span>
<span class="line-removed">169     mov eax, compare_value</span>
<span class="line-removed">170     lock cmpxchg dword ptr [edx], ecx</span>
<span class="line-removed">171   }</span>
<span class="line-removed">172 }</span>
173 
<a name="11" id="anc11"></a><span class="line-modified">174 template&lt;&gt;</span>
<span class="line-removed">175 template&lt;typename T&gt;</span>
<span class="line-removed">176 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">177                                                 T compare_value,</span>
<span class="line-removed">178                                                 T exchange_value,</span>
<span class="line-removed">179                                                 atomic_memory_order order) const {</span>
<span class="line-removed">180   STATIC_ASSERT(8 == sizeof(T));</span>
<span class="line-removed">181   int32_t ex_lo  = (int32_t)exchange_value;</span>
<span class="line-removed">182   int32_t ex_hi  = *( ((int32_t*)&amp;exchange_value) + 1 );</span>
<span class="line-removed">183   int32_t cmp_lo = (int32_t)compare_value;</span>
<span class="line-removed">184   int32_t cmp_hi = *( ((int32_t*)&amp;compare_value) + 1 );</span>
<span class="line-removed">185   __asm {</span>
<span class="line-removed">186     push ebx</span>
<span class="line-removed">187     push edi</span>
<span class="line-removed">188     mov eax, cmp_lo</span>
<span class="line-removed">189     mov edx, cmp_hi</span>
<span class="line-removed">190     mov edi, dest</span>
<span class="line-removed">191     mov ebx, ex_lo</span>
<span class="line-removed">192     mov ecx, ex_hi</span>
<span class="line-removed">193     lock cmpxchg8b qword ptr [edi]</span>
<span class="line-removed">194     pop edi</span>
<span class="line-removed">195     pop ebx</span>
<span class="line-removed">196   }</span>
<span class="line-removed">197 }</span>
198 
199 template&lt;&gt;
200 template&lt;typename T&gt;
201 inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
202   STATIC_ASSERT(8 == sizeof(T));
203   volatile T dest;
204   volatile T* pdest = &amp;dest;
205   __asm {
206     mov eax, src
207     fild     qword ptr [eax]
208     mov eax, pdest
209     fistp    qword ptr [eax]
210   }
211   return dest;
212 }
213 
214 template&lt;&gt;
215 template&lt;typename T&gt;
216 inline void Atomic::PlatformStore&lt;8&gt;::operator()(T volatile* dest,
217                                                  T store_value) const {
218   STATIC_ASSERT(8 == sizeof(T));
219   volatile T* src = &amp;store_value;
220   __asm {
221     mov eax, src
222     fild     qword ptr [eax]
223     mov eax, dest
224     fistp    qword ptr [eax]
225   }
226 }
227 
<a name="12" id="anc12"></a><span class="line-removed">228 #endif // AMD64</span>
<span class="line-removed">229 </span>
230 #pragma warning(default: 4035) // Enables warnings reporting missing return statement
231 
<a name="13" id="anc13"></a><span class="line-removed">232 #ifndef AMD64</span>
233 template&lt;&gt;
234 struct Atomic::PlatformOrderedStore&lt;1, RELEASE_X_FENCE&gt;
235 {
236   template &lt;typename T&gt;
237   void operator()(volatile T* p, T v) const {
238     __asm {
239       mov edx, p;
240       mov al, v;
241       xchg al, byte ptr [edx];
242     }
243   }
244 };
245 
246 template&lt;&gt;
247 struct Atomic::PlatformOrderedStore&lt;2, RELEASE_X_FENCE&gt;
248 {
249   template &lt;typename T&gt;
250   void operator()(volatile T* p, T v) const {
251     __asm {
252       mov edx, p;
253       mov ax, v;
254       xchg ax, word ptr [edx];
255     }
256   }
257 };
258 
259 template&lt;&gt;
260 struct Atomic::PlatformOrderedStore&lt;4, RELEASE_X_FENCE&gt;
261 {
262   template &lt;typename T&gt;
263   void operator()(volatile T* p, T v) const {
264     __asm {
265       mov edx, p;
266       mov eax, v;
267       xchg eax, dword ptr [edx];
268     }
269   }
270 };
271 #endif // AMD64
272 
273 #endif // OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
<a name="14" id="anc14"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="14" type="hidden" />
</body>
</html>