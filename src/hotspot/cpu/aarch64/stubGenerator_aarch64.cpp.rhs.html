<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src\hotspot\cpu\aarch64\stubGenerator_aarch64.cpp</title>
    <link rel="stylesheet" href="..\..\..\..\style.css" />
    <script type="text/javascript" src="..\..\..\..\navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 2003, 2019, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.hpp&quot;
  28 #include &quot;asm/macroAssembler.inline.hpp&quot;
  29 #include &quot;gc/shared/barrierSet.hpp&quot;
  30 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  31 #include &quot;interpreter/interpreter.hpp&quot;
  32 #include &quot;memory/universe.hpp&quot;
  33 #include &quot;nativeInst_aarch64.hpp&quot;
  34 #include &quot;oops/instanceOop.hpp&quot;
  35 #include &quot;oops/method.hpp&quot;
  36 #include &quot;oops/objArrayKlass.hpp&quot;
  37 #include &quot;oops/oop.inline.hpp&quot;
  38 #include &quot;prims/methodHandles.hpp&quot;
  39 #include &quot;runtime/frame.inline.hpp&quot;
  40 #include &quot;runtime/handles.inline.hpp&quot;
  41 #include &quot;runtime/sharedRuntime.hpp&quot;
  42 #include &quot;runtime/stubCodeGenerator.hpp&quot;
  43 #include &quot;runtime/stubRoutines.hpp&quot;
  44 #include &quot;runtime/thread.inline.hpp&quot;
  45 #include &quot;utilities/align.hpp&quot;
  46 #ifdef COMPILER2
  47 #include &quot;opto/runtime.hpp&quot;
  48 #endif
  49 #if INCLUDE_ZGC
  50 #include &quot;gc/z/zThreadLocalData.hpp&quot;
  51 #endif
  52 
  53 // Declaration and definition of StubGenerator (no .hpp file).
  54 // For a more detailed description of the stub routine structure
  55 // see the comment in stubRoutines.hpp
  56 
  57 #undef __
  58 #define __ _masm-&gt;
  59 #define TIMES_OOP Address::sxtw(exact_log2(UseCompressedOops ? 4 : 8))
  60 
  61 #ifdef PRODUCT
  62 #define BLOCK_COMMENT(str) /* nothing */
  63 #else
  64 #define BLOCK_COMMENT(str) __ block_comment(str)
  65 #endif
  66 
  67 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  68 
  69 // Stub Code definitions
  70 
  71 class StubGenerator: public StubCodeGenerator {
  72  private:
  73 
  74 #ifdef PRODUCT
  75 #define inc_counter_np(counter) ((void)0)
  76 #else
  77   void inc_counter_np_(int&amp; counter) {
  78     __ lea(rscratch2, ExternalAddress((address)&amp;counter));
  79     __ ldrw(rscratch1, Address(rscratch2));
  80     __ addw(rscratch1, rscratch1, 1);
  81     __ strw(rscratch1, Address(rscratch2));
  82   }
  83 #define inc_counter_np(counter) \
  84   BLOCK_COMMENT(&quot;inc_counter &quot; #counter); \
  85   inc_counter_np_(counter);
  86 #endif
  87 
  88   // Call stubs are used to call Java from C
  89   //
  90   // Arguments:
  91   //    c_rarg0:   call wrapper address                   address
  92   //    c_rarg1:   result                                 address
  93   //    c_rarg2:   result type                            BasicType
  94   //    c_rarg3:   method                                 Method*
  95   //    c_rarg4:   (interpreter) entry point              address
  96   //    c_rarg5:   parameters                             intptr_t*
  97   //    c_rarg6:   parameter size (in words)              int
  98   //    c_rarg7:   thread                                 Thread*
  99   //
 100   // There is no return from the stub itself as any Java result
 101   // is written to result
 102   //
 103   // we save r30 (lr) as the return PC at the base of the frame and
 104   // link r29 (fp) below it as the frame pointer installing sp (r31)
 105   // into fp.
 106   //
 107   // we save r0-r7, which accounts for all the c arguments.
 108   //
 109   // TODO: strictly do we need to save them all? they are treated as
 110   // volatile by C so could we omit saving the ones we are going to
 111   // place in global registers (thread? method?) or those we only use
 112   // during setup of the Java call?
 113   //
 114   // we don&#39;t need to save r8 which C uses as an indirect result location
 115   // return register.
 116   //
 117   // we don&#39;t need to save r9-r15 which both C and Java treat as
 118   // volatile
 119   //
 120   // we don&#39;t need to save r16-18 because Java does not use them
 121   //
 122   // we save r19-r28 which Java uses as scratch registers and C
 123   // expects to be callee-save
 124   //
 125   // we save the bottom 64 bits of each value stored in v8-v15; it is
 126   // the responsibility of the caller to preserve larger values.
 127   //
 128   // so the stub frame looks like this when we enter Java code
 129   //
 130   //     [ return_from_Java     ] &lt;--- sp
 131   //     [ argument word n      ]
 132   //      ...
 133   // -27 [ argument word 1      ]
 134   // -26 [ saved v15            ] &lt;--- sp_after_call
 135   // -25 [ saved v14            ]
 136   // -24 [ saved v13            ]
 137   // -23 [ saved v12            ]
 138   // -22 [ saved v11            ]
 139   // -21 [ saved v10            ]
 140   // -20 [ saved v9             ]
 141   // -19 [ saved v8             ]
 142   // -18 [ saved r28            ]
 143   // -17 [ saved r27            ]
 144   // -16 [ saved r26            ]
 145   // -15 [ saved r25            ]
 146   // -14 [ saved r24            ]
 147   // -13 [ saved r23            ]
 148   // -12 [ saved r22            ]
 149   // -11 [ saved r21            ]
 150   // -10 [ saved r20            ]
 151   //  -9 [ saved r19            ]
 152   //  -8 [ call wrapper    (r0) ]
 153   //  -7 [ result          (r1) ]
 154   //  -6 [ result type     (r2) ]
 155   //  -5 [ method          (r3) ]
 156   //  -4 [ entry point     (r4) ]
 157   //  -3 [ parameters      (r5) ]
 158   //  -2 [ parameter size  (r6) ]
 159   //  -1 [ thread (r7)          ]
 160   //   0 [ saved fp       (r29) ] &lt;--- fp == saved sp (r31)
 161   //   1 [ saved lr       (r30) ]
 162 
 163   // Call stub stack layout word offsets from fp
 164   enum call_stub_layout {
 165     sp_after_call_off = -26,
 166 
 167     d15_off            = -26,
 168     d13_off            = -24,
 169     d11_off            = -22,
 170     d9_off             = -20,
 171 
 172     r28_off            = -18,
 173     r26_off            = -16,
 174     r24_off            = -14,
 175     r22_off            = -12,
 176     r20_off            = -10,
 177     call_wrapper_off   =  -8,
 178     result_off         =  -7,
 179     result_type_off    =  -6,
 180     method_off         =  -5,
 181     entry_point_off    =  -4,
 182     parameter_size_off =  -2,
 183     thread_off         =  -1,
 184     fp_f               =   0,
 185     retaddr_off        =   1,
 186   };
 187 
 188   address generate_call_stub(address&amp; return_address) {
 189     assert((int)frame::entry_frame_after_call_words == -(int)sp_after_call_off + 1 &amp;&amp;
 190            (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,
 191            &quot;adjust this code&quot;);
 192 
 193     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;call_stub&quot;);
 194     address start = __ pc();
 195 
 196     const Address sp_after_call(rfp, sp_after_call_off * wordSize);
 197 
 198     const Address call_wrapper  (rfp, call_wrapper_off   * wordSize);
 199     const Address result        (rfp, result_off         * wordSize);
 200     const Address result_type   (rfp, result_type_off    * wordSize);
 201     const Address method        (rfp, method_off         * wordSize);
 202     const Address entry_point   (rfp, entry_point_off    * wordSize);
 203     const Address parameter_size(rfp, parameter_size_off * wordSize);
 204 
 205     const Address thread        (rfp, thread_off         * wordSize);
 206 
 207     const Address d15_save      (rfp, d15_off * wordSize);
 208     const Address d13_save      (rfp, d13_off * wordSize);
 209     const Address d11_save      (rfp, d11_off * wordSize);
 210     const Address d9_save       (rfp, d9_off * wordSize);
 211 
 212     const Address r28_save      (rfp, r28_off * wordSize);
 213     const Address r26_save      (rfp, r26_off * wordSize);
 214     const Address r24_save      (rfp, r24_off * wordSize);
 215     const Address r22_save      (rfp, r22_off * wordSize);
 216     const Address r20_save      (rfp, r20_off * wordSize);
 217 
 218     // stub code
 219 
 220     address aarch64_entry = __ pc();
 221 
 222     // set up frame and move sp to end of save area
 223     __ enter();
 224     __ sub(sp, rfp, -sp_after_call_off * wordSize);
 225 
 226     // save register parameters and Java scratch/global registers
 227     // n.b. we save thread even though it gets installed in
 228     // rthread because we want to sanity check rthread later
 229     __ str(c_rarg7,  thread);
 230     __ strw(c_rarg6, parameter_size);
 231     __ stp(c_rarg4, c_rarg5,  entry_point);
 232     __ stp(c_rarg2, c_rarg3,  result_type);
 233     __ stp(c_rarg0, c_rarg1,  call_wrapper);
 234 
 235     __ stp(r20, r19,   r20_save);
 236     __ stp(r22, r21,   r22_save);
 237     __ stp(r24, r23,   r24_save);
 238     __ stp(r26, r25,   r26_save);
 239     __ stp(r28, r27,   r28_save);
 240 
 241     __ stpd(v9,  v8,   d9_save);
 242     __ stpd(v11, v10,  d11_save);
 243     __ stpd(v13, v12,  d13_save);
 244     __ stpd(v15, v14,  d15_save);
 245 
 246     // install Java thread in global register now we have saved
 247     // whatever value it held
 248     __ mov(rthread, c_rarg7);
 249     // And method
 250     __ mov(rmethod, c_rarg3);
 251 
 252     // set up the heapbase register
 253     __ reinit_heapbase();
 254 
 255 #ifdef ASSERT
 256     // make sure we have no pending exceptions
 257     {
 258       Label L;
 259       __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));
 260       __ cmp(rscratch1, (u1)NULL_WORD);
 261       __ br(Assembler::EQ, L);
 262       __ stop(&quot;StubRoutines::call_stub: entered with pending exception&quot;);
 263       __ BIND(L);
 264     }
 265 #endif
 266     // pass parameters if any
 267     __ mov(esp, sp);
 268     __ sub(rscratch1, sp, c_rarg6, ext::uxtw, LogBytesPerWord); // Move SP out of the way
 269     __ andr(sp, rscratch1, -2 * wordSize);
 270 
 271     BLOCK_COMMENT(&quot;pass parameters if any&quot;);
 272     Label parameters_done;
 273     // parameter count is still in c_rarg6
 274     // and parameter pointer identifying param 1 is in c_rarg5
 275     __ cbzw(c_rarg6, parameters_done);
 276 
 277     address loop = __ pc();
 278     __ ldr(rscratch1, Address(__ post(c_rarg5, wordSize)));
 279     __ subsw(c_rarg6, c_rarg6, 1);
 280     __ push(rscratch1);
 281     __ br(Assembler::GT, loop);
 282 
 283     __ BIND(parameters_done);
 284 
 285     // call Java entry -- passing methdoOop, and current sp
 286     //      rmethod: Method*
 287     //      r13: sender sp
 288     BLOCK_COMMENT(&quot;call Java function&quot;);
 289     __ mov(r13, sp);
 290     __ blr(c_rarg4);
 291 
 292     // we do this here because the notify will already have been done
 293     // if we get to the next instruction via an exception
 294     //
 295     // n.b. adding this instruction here affects the calculation of
 296     // whether or not a routine returns to the call stub (used when
 297     // doing stack walks) since the normal test is to check the return
 298     // pc against the address saved below. so we may need to allow for
 299     // this extra instruction in the check.
 300 
 301     // save current address for use by exception handling code
 302 
 303     return_address = __ pc();
 304 
 305     // store result depending on type (everything that is not
 306     // T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)
 307     // n.b. this assumes Java returns an integral result in r0
 308     // and a floating result in j_farg0
 309     __ ldr(j_rarg2, result);
 310     Label is_long, is_float, is_double, exit;
 311     __ ldr(j_rarg1, result_type);
 312     __ cmp(j_rarg1, (u1)T_OBJECT);
 313     __ br(Assembler::EQ, is_long);
 314     __ cmp(j_rarg1, (u1)T_LONG);
 315     __ br(Assembler::EQ, is_long);
 316     __ cmp(j_rarg1, (u1)T_FLOAT);
 317     __ br(Assembler::EQ, is_float);
 318     __ cmp(j_rarg1, (u1)T_DOUBLE);
 319     __ br(Assembler::EQ, is_double);
 320 
 321     // handle T_INT case
 322     __ strw(r0, Address(j_rarg2));
 323 
 324     __ BIND(exit);
 325 
 326     // pop parameters
 327     __ sub(esp, rfp, -sp_after_call_off * wordSize);
 328 
 329 #ifdef ASSERT
 330     // verify that threads correspond
 331     {
 332       Label L, S;
 333       __ ldr(rscratch1, thread);
 334       __ cmp(rthread, rscratch1);
 335       __ br(Assembler::NE, S);
 336       __ get_thread(rscratch1);
 337       __ cmp(rthread, rscratch1);
 338       __ br(Assembler::EQ, L);
 339       __ BIND(S);
 340       __ stop(&quot;StubRoutines::call_stub: threads must correspond&quot;);
 341       __ BIND(L);
 342     }
 343 #endif
 344 
 345     // restore callee-save registers
 346     __ ldpd(v15, v14,  d15_save);
 347     __ ldpd(v13, v12,  d13_save);
 348     __ ldpd(v11, v10,  d11_save);
 349     __ ldpd(v9,  v8,   d9_save);
 350 
 351     __ ldp(r28, r27,   r28_save);
 352     __ ldp(r26, r25,   r26_save);
 353     __ ldp(r24, r23,   r24_save);
 354     __ ldp(r22, r21,   r22_save);
 355     __ ldp(r20, r19,   r20_save);
 356 
 357     __ ldp(c_rarg0, c_rarg1,  call_wrapper);
 358     __ ldrw(c_rarg2, result_type);
 359     __ ldr(c_rarg3,  method);
 360     __ ldp(c_rarg4, c_rarg5,  entry_point);
 361     __ ldp(c_rarg6, c_rarg7,  parameter_size);
 362 
 363     // leave frame and return to caller
 364     __ leave();
 365     __ ret(lr);
 366 
 367     // handle return types different from T_INT
 368 
 369     __ BIND(is_long);
 370     __ str(r0, Address(j_rarg2, 0));
 371     __ br(Assembler::AL, exit);
 372 
 373     __ BIND(is_float);
 374     __ strs(j_farg0, Address(j_rarg2, 0));
 375     __ br(Assembler::AL, exit);
 376 
 377     __ BIND(is_double);
 378     __ strd(j_farg0, Address(j_rarg2, 0));
 379     __ br(Assembler::AL, exit);
 380 
 381     return start;
 382   }
 383 
 384   // Return point for a Java call if there&#39;s an exception thrown in
 385   // Java code.  The exception is caught and transformed into a
 386   // pending exception stored in JavaThread that can be tested from
 387   // within the VM.
 388   //
 389   // Note: Usually the parameters are removed by the callee. In case
 390   // of an exception crossing an activation frame boundary, that is
 391   // not the case if the callee is compiled code =&gt; need to setup the
 392   // rsp.
 393   //
 394   // r0: exception oop
 395 
 396   address generate_catch_exception() {
 397     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;catch_exception&quot;);
 398     address start = __ pc();
 399 
 400     // same as in generate_call_stub():
 401     const Address sp_after_call(rfp, sp_after_call_off * wordSize);
 402     const Address thread        (rfp, thread_off         * wordSize);
 403 
 404 #ifdef ASSERT
 405     // verify that threads correspond
 406     {
 407       Label L, S;
 408       __ ldr(rscratch1, thread);
 409       __ cmp(rthread, rscratch1);
 410       __ br(Assembler::NE, S);
 411       __ get_thread(rscratch1);
 412       __ cmp(rthread, rscratch1);
 413       __ br(Assembler::EQ, L);
 414       __ bind(S);
 415       __ stop(&quot;StubRoutines::catch_exception: threads must correspond&quot;);
 416       __ bind(L);
 417     }
 418 #endif
 419 
 420     // set pending exception
 421     __ verify_oop(r0);
 422 
 423     __ str(r0, Address(rthread, Thread::pending_exception_offset()));
 424     __ mov(rscratch1, (address)__FILE__);
 425     __ str(rscratch1, Address(rthread, Thread::exception_file_offset()));
 426     __ movw(rscratch1, (int)__LINE__);
 427     __ strw(rscratch1, Address(rthread, Thread::exception_line_offset()));
 428 
 429     // complete return to VM
 430     assert(StubRoutines::_call_stub_return_address != NULL,
 431            &quot;_call_stub_return_address must have been generated before&quot;);
 432     __ b(StubRoutines::_call_stub_return_address);
 433 
 434     return start;
 435   }
 436 
 437   // Continuation point for runtime calls returning with a pending
 438   // exception.  The pending exception check happened in the runtime
 439   // or native call stub.  The pending exception in Thread is
 440   // converted into a Java-level exception.
 441   //
 442   // Contract with Java-level exception handlers:
 443   // r0: exception
 444   // r3: throwing pc
 445   //
 446   // NOTE: At entry of this stub, exception-pc must be in LR !!
 447 
 448   // NOTE: this is always used as a jump target within generated code
 449   // so it just needs to be generated code wiht no x86 prolog
 450 
 451   address generate_forward_exception() {
 452     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;forward exception&quot;);
 453     address start = __ pc();
 454 
 455     // Upon entry, LR points to the return address returning into
 456     // Java (interpreted or compiled) code; i.e., the return address
 457     // becomes the throwing pc.
 458     //
 459     // Arguments pushed before the runtime call are still on the stack
 460     // but the exception handler will reset the stack pointer -&gt;
 461     // ignore them.  A potential result in registers can be ignored as
 462     // well.
 463 
 464 #ifdef ASSERT
 465     // make sure this code is only executed if there is a pending exception
 466     {
 467       Label L;
 468       __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
 469       __ cbnz(rscratch1, L);
 470       __ stop(&quot;StubRoutines::forward exception: no pending exception (1)&quot;);
 471       __ bind(L);
 472     }
 473 #endif
 474 
 475     // compute exception handler into r19
 476 
 477     // call the VM to find the handler address associated with the
 478     // caller address. pass thread in r0 and caller pc (ret address)
 479     // in r1. n.b. the caller pc is in lr, unlike x86 where it is on
 480     // the stack.
 481     __ mov(c_rarg1, lr);
 482     // lr will be trashed by the VM call so we move it to R19
 483     // (callee-saved) because we also need to pass it to the handler
 484     // returned by this call.
 485     __ mov(r19, lr);
 486     BLOCK_COMMENT(&quot;call exception_handler_for_return_address&quot;);
 487     __ call_VM_leaf(CAST_FROM_FN_PTR(address,
 488                          SharedRuntime::exception_handler_for_return_address),
 489                     rthread, c_rarg1);
 490     // we should not really care that lr is no longer the callee
 491     // address. we saved the value the handler needs in r19 so we can
 492     // just copy it to r3. however, the C2 handler will push its own
 493     // frame and then calls into the VM and the VM code asserts that
 494     // the PC for the frame above the handler belongs to a compiled
 495     // Java method. So, we restore lr here to satisfy that assert.
 496     __ mov(lr, r19);
 497     // setup r0 &amp; r3 &amp; clear pending exception
 498     __ mov(r3, r19);
 499     __ mov(r19, r0);
 500     __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));
 501     __ str(zr, Address(rthread, Thread::pending_exception_offset()));
 502 
 503 #ifdef ASSERT
 504     // make sure exception is set
 505     {
 506       Label L;
 507       __ cbnz(r0, L);
 508       __ stop(&quot;StubRoutines::forward exception: no pending exception (2)&quot;);
 509       __ bind(L);
 510     }
 511 #endif
 512 
 513     // continue at exception handler
 514     // r0: exception
 515     // r3: throwing pc
 516     // r19: exception handler
 517     __ verify_oop(r0);
 518     __ br(r19);
 519 
 520     return start;
 521   }
 522 
 523   // Non-destructive plausibility checks for oops
 524   //
 525   // Arguments:
 526   //    r0: oop to verify
 527   //    rscratch1: error message
 528   //
 529   // Stack after saving c_rarg3:
 530   //    [tos + 0]: saved c_rarg3
 531   //    [tos + 1]: saved c_rarg2
 532   //    [tos + 2]: saved lr
 533   //    [tos + 3]: saved rscratch2
 534   //    [tos + 4]: saved r0
 535   //    [tos + 5]: saved rscratch1
 536   address generate_verify_oop() {
 537 
 538     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;verify_oop&quot;);
 539     address start = __ pc();
 540 
 541     Label exit, error;
 542 
 543     // save c_rarg2 and c_rarg3
 544     __ stp(c_rarg3, c_rarg2, Address(__ pre(sp, -16)));
 545 
 546     // __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()));
 547     __ lea(c_rarg2, ExternalAddress((address) StubRoutines::verify_oop_count_addr()));
 548     __ ldr(c_rarg3, Address(c_rarg2));
 549     __ add(c_rarg3, c_rarg3, 1);
 550     __ str(c_rarg3, Address(c_rarg2));
 551 
 552     // object is in r0
 553     // make sure object is &#39;reasonable&#39;
 554     __ cbz(r0, exit); // if obj is NULL it is OK
 555 
 556 #if INCLUDE_ZGC
 557     if (UseZGC) {
 558       // Check if mask is good.
 559       // verifies that ZAddressBadMask &amp; r0 == 0
 560       __ ldr(c_rarg3, Address(rthread, ZThreadLocalData::address_bad_mask_offset()));
 561       __ andr(c_rarg2, r0, c_rarg3);
 562       __ cbnz(c_rarg2, error);
 563     }
 564 #endif
 565 
 566     // Check if the oop is in the right area of memory
<a name="1" id="anc1"></a><span class="line-modified"> 567     __ mov(c_rarg3, (address) Universe::verify_oop_mask());</span>
 568     __ andr(c_rarg2, r0, c_rarg3);
<a name="2" id="anc2"></a><span class="line-modified"> 569     __ mov(c_rarg3, (address) Universe::verify_oop_bits());</span>
 570 
 571     // Compare c_rarg2 and c_rarg3.  We don&#39;t use a compare
 572     // instruction here because the flags register is live.
 573     __ eor(c_rarg2, c_rarg2, c_rarg3);
 574     __ cbnz(c_rarg2, error);
 575 
 576     // make sure klass is &#39;reasonable&#39;, which is not zero.
 577     __ load_klass(r0, r0);  // get klass
 578     __ cbz(r0, error);      // if klass is NULL it is broken
 579 
 580     // return if everything seems ok
 581     __ bind(exit);
 582 
 583     __ ldp(c_rarg3, c_rarg2, Address(__ post(sp, 16)));
 584     __ ret(lr);
 585 
 586     // handle errors
 587     __ bind(error);
 588     __ ldp(c_rarg3, c_rarg2, Address(__ post(sp, 16)));
 589 
 590     __ push(RegSet::range(r0, r29), sp);
 591     // debug(char* msg, int64_t pc, int64_t regs[])
 592     __ mov(c_rarg0, rscratch1);      // pass address of error message
 593     __ mov(c_rarg1, lr);             // pass return address
 594     __ mov(c_rarg2, sp);             // pass address of regs on stack
 595 #ifndef PRODUCT
 596     assert(frame::arg_reg_save_area_bytes == 0, &quot;not expecting frame reg save area&quot;);
 597 #endif
 598     BLOCK_COMMENT(&quot;call MacroAssembler::debug&quot;);
 599     __ mov(rscratch1, CAST_FROM_FN_PTR(address, MacroAssembler::debug64));
 600     __ blr(rscratch1);
 601     __ hlt(0);
 602 
 603     return start;
 604   }
 605 
 606   void array_overlap_test(Label&amp; L_no_overlap, Address::sxtw sf) { __ b(L_no_overlap); }
 607 
 608   // The inner part of zero_words().  This is the bulk operation,
 609   // zeroing words in blocks, possibly using DC ZVA to do it.  The
 610   // caller is responsible for zeroing the last few words.
 611   //
 612   // Inputs:
 613   // r10: the HeapWord-aligned base address of an array to zero.
 614   // r11: the count in HeapWords, r11 &gt; 0.
 615   //
 616   // Returns r10 and r11, adjusted for the caller to clear.
 617   // r10: the base address of the tail of words left to clear.
 618   // r11: the number of words in the tail.
 619   //      r11 &lt; MacroAssembler::zero_words_block_size.
 620 
 621   address generate_zero_blocks() {
 622     Label done;
 623     Label base_aligned;
 624 
 625     Register base = r10, cnt = r11;
 626 
 627     __ align(CodeEntryAlignment);
 628     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;zero_blocks&quot;);
 629     address start = __ pc();
 630 
 631     if (UseBlockZeroing) {
 632       int zva_length = VM_Version::zva_length();
 633 
 634       // Ensure ZVA length can be divided by 16. This is required by
 635       // the subsequent operations.
 636       assert (zva_length % 16 == 0, &quot;Unexpected ZVA Length&quot;);
 637 
 638       __ tbz(base, 3, base_aligned);
 639       __ str(zr, Address(__ post(base, 8)));
 640       __ sub(cnt, cnt, 1);
 641       __ bind(base_aligned);
 642 
 643       // Ensure count &gt;= zva_length * 2 so that it still deserves a zva after
 644       // alignment.
 645       Label small;
 646       int low_limit = MAX2(zva_length * 2, (int)BlockZeroingLowLimit);
 647       __ subs(rscratch1, cnt, low_limit &gt;&gt; 3);
 648       __ br(Assembler::LT, small);
 649       __ zero_dcache_blocks(base, cnt);
 650       __ bind(small);
 651     }
 652 
 653     {
 654       // Number of stp instructions we&#39;ll unroll
 655       const int unroll =
 656         MacroAssembler::zero_words_block_size / 2;
 657       // Clear the remaining blocks.
 658       Label loop;
 659       __ subs(cnt, cnt, unroll * 2);
 660       __ br(Assembler::LT, done);
 661       __ bind(loop);
 662       for (int i = 0; i &lt; unroll; i++)
 663         __ stp(zr, zr, __ post(base, 16));
 664       __ subs(cnt, cnt, unroll * 2);
 665       __ br(Assembler::GE, loop);
 666       __ bind(done);
 667       __ add(cnt, cnt, unroll * 2);
 668     }
 669 
 670     __ ret(lr);
 671 
 672     return start;
 673   }
 674 
 675 
 676   typedef enum {
 677     copy_forwards = 1,
 678     copy_backwards = -1
 679   } copy_direction;
 680 
 681   // Bulk copy of blocks of 8 words.
 682   //
 683   // count is a count of words.
 684   //
 685   // Precondition: count &gt;= 8
 686   //
 687   // Postconditions:
 688   //
 689   // The least significant bit of count contains the remaining count
 690   // of words to copy.  The rest of count is trash.
 691   //
 692   // s and d are adjusted to point to the remaining words to copy
 693   //
 694   void generate_copy_longs(Label &amp;start, Register s, Register d, Register count,
 695                            copy_direction direction) {
 696     int unit = wordSize * direction;
 697     int bias = (UseSIMDForMemoryOps ? 4:2) * wordSize;
 698 
<a name="3" id="anc3"></a>
 699     const Register t0 = r3, t1 = r4, t2 = r5, t3 = r6,
 700       t4 = r7, t5 = r10, t6 = r11, t7 = r12;
 701     const Register stride = r13;
 702 
 703     assert_different_registers(rscratch1, t0, t1, t2, t3, t4, t5, t6, t7);
 704     assert_different_registers(s, d, count, rscratch1);
 705 
 706     Label again, drain;
 707     const char *stub_name;
 708     if (direction == copy_forwards)
 709       stub_name = &quot;forward_copy_longs&quot;;
 710     else
 711       stub_name = &quot;backward_copy_longs&quot;;
 712 
 713     __ align(CodeEntryAlignment);
 714 
 715     StubCodeMark mark(this, &quot;StubRoutines&quot;, stub_name);
 716 
 717     __ bind(start);
 718 
 719     Label unaligned_copy_long;
 720     if (AvoidUnalignedAccesses) {
 721       __ tbnz(d, 3, unaligned_copy_long);
 722     }
 723 
 724     if (direction == copy_forwards) {
 725       __ sub(s, s, bias);
 726       __ sub(d, d, bias);
 727     }
 728 
 729 #ifdef ASSERT
 730     // Make sure we are never given &lt; 8 words
 731     {
 732       Label L;
 733       __ cmp(count, (u1)8);
 734       __ br(Assembler::GE, L);
 735       __ stop(&quot;genrate_copy_longs called with &lt; 8 words&quot;);
 736       __ bind(L);
 737     }
 738 #endif
 739 
 740     // Fill 8 registers
 741     if (UseSIMDForMemoryOps) {
 742       __ ldpq(v0, v1, Address(s, 4 * unit));
 743       __ ldpq(v2, v3, Address(__ pre(s, 8 * unit)));
 744     } else {
 745       __ ldp(t0, t1, Address(s, 2 * unit));
 746       __ ldp(t2, t3, Address(s, 4 * unit));
 747       __ ldp(t4, t5, Address(s, 6 * unit));
 748       __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 749     }
 750 
 751     __ subs(count, count, 16);
 752     __ br(Assembler::LO, drain);
 753 
 754     int prefetch = PrefetchCopyIntervalInBytes;
 755     bool use_stride = false;
 756     if (direction == copy_backwards) {
 757        use_stride = prefetch &gt; 256;
 758        prefetch = -prefetch;
 759        if (use_stride) __ mov(stride, prefetch);
 760     }
 761 
 762     __ bind(again);
 763 
 764     if (PrefetchCopyIntervalInBytes &gt; 0)
 765       __ prfm(use_stride ? Address(s, stride) : Address(s, prefetch), PLDL1KEEP);
 766 
 767     if (UseSIMDForMemoryOps) {
 768       __ stpq(v0, v1, Address(d, 4 * unit));
 769       __ ldpq(v0, v1, Address(s, 4 * unit));
 770       __ stpq(v2, v3, Address(__ pre(d, 8 * unit)));
 771       __ ldpq(v2, v3, Address(__ pre(s, 8 * unit)));
 772     } else {
 773       __ stp(t0, t1, Address(d, 2 * unit));
 774       __ ldp(t0, t1, Address(s, 2 * unit));
 775       __ stp(t2, t3, Address(d, 4 * unit));
 776       __ ldp(t2, t3, Address(s, 4 * unit));
 777       __ stp(t4, t5, Address(d, 6 * unit));
 778       __ ldp(t4, t5, Address(s, 6 * unit));
 779       __ stp(t6, t7, Address(__ pre(d, 8 * unit)));
 780       __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 781     }
 782 
 783     __ subs(count, count, 8);
 784     __ br(Assembler::HS, again);
 785 
 786     // Drain
 787     __ bind(drain);
 788     if (UseSIMDForMemoryOps) {
 789       __ stpq(v0, v1, Address(d, 4 * unit));
 790       __ stpq(v2, v3, Address(__ pre(d, 8 * unit)));
 791     } else {
 792       __ stp(t0, t1, Address(d, 2 * unit));
 793       __ stp(t2, t3, Address(d, 4 * unit));
 794       __ stp(t4, t5, Address(d, 6 * unit));
 795       __ stp(t6, t7, Address(__ pre(d, 8 * unit)));
 796     }
 797 
 798     {
 799       Label L1, L2;
 800       __ tbz(count, exact_log2(4), L1);
 801       if (UseSIMDForMemoryOps) {
 802         __ ldpq(v0, v1, Address(__ pre(s, 4 * unit)));
 803         __ stpq(v0, v1, Address(__ pre(d, 4 * unit)));
 804       } else {
 805         __ ldp(t0, t1, Address(s, 2 * unit));
 806         __ ldp(t2, t3, Address(__ pre(s, 4 * unit)));
 807         __ stp(t0, t1, Address(d, 2 * unit));
 808         __ stp(t2, t3, Address(__ pre(d, 4 * unit)));
 809       }
 810       __ bind(L1);
 811 
 812       if (direction == copy_forwards) {
 813         __ add(s, s, bias);
 814         __ add(d, d, bias);
 815       }
 816 
 817       __ tbz(count, 1, L2);
 818       __ ldp(t0, t1, Address(__ adjust(s, 2 * unit, direction == copy_backwards)));
 819       __ stp(t0, t1, Address(__ adjust(d, 2 * unit, direction == copy_backwards)));
 820       __ bind(L2);
 821     }
 822 
 823     __ ret(lr);
 824 
 825     if (AvoidUnalignedAccesses) {
 826       Label drain, again;
 827       // Register order for storing. Order is different for backward copy.
 828 
 829       __ bind(unaligned_copy_long);
 830 
 831       // source address is even aligned, target odd aligned
 832       //
 833       // when forward copying word pairs we read long pairs at offsets
 834       // {0, 2, 4, 6} (in long words). when backwards copying we read
 835       // long pairs at offsets {-2, -4, -6, -8}. We adjust the source
 836       // address by -2 in the forwards case so we can compute the
 837       // source offsets for both as {2, 4, 6, 8} * unit where unit = 1
 838       // or -1.
 839       //
 840       // when forward copying we need to store 1 word, 3 pairs and
 841       // then 1 word at offsets {0, 1, 3, 5, 7}. Rather thna use a
 842       // zero offset We adjust the destination by -1 which means we
 843       // have to use offsets { 1, 2, 4, 6, 8} * unit for the stores.
 844       //
 845       // When backwards copyng we need to store 1 word, 3 pairs and
 846       // then 1 word at offsets {-1, -3, -5, -7, -8} i.e. we use
 847       // offsets {1, 3, 5, 7, 8} * unit.
 848 
 849       if (direction == copy_forwards) {
 850         __ sub(s, s, 16);
 851         __ sub(d, d, 8);
 852       }
 853 
 854       // Fill 8 registers
 855       //
 856       // for forwards copy s was offset by -16 from the original input
 857       // value of s so the register contents are at these offsets
 858       // relative to the 64 bit block addressed by that original input
 859       // and so on for each successive 64 byte block when s is updated
 860       //
 861       // t0 at offset 0,  t1 at offset 8
 862       // t2 at offset 16, t3 at offset 24
 863       // t4 at offset 32, t5 at offset 40
 864       // t6 at offset 48, t7 at offset 56
 865 
 866       // for backwards copy s was not offset so the register contents
 867       // are at these offsets into the preceding 64 byte block
 868       // relative to that original input and so on for each successive
 869       // preceding 64 byte block when s is updated. this explains the
 870       // slightly counter-intuitive looking pattern of register usage
 871       // in the stp instructions for backwards copy.
 872       //
 873       // t0 at offset -16, t1 at offset -8
 874       // t2 at offset -32, t3 at offset -24
 875       // t4 at offset -48, t5 at offset -40
 876       // t6 at offset -64, t7 at offset -56
 877 
 878       __ ldp(t0, t1, Address(s, 2 * unit));
 879       __ ldp(t2, t3, Address(s, 4 * unit));
 880       __ ldp(t4, t5, Address(s, 6 * unit));
 881       __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 882 
 883       __ subs(count, count, 16);
 884       __ br(Assembler::LO, drain);
 885 
 886       int prefetch = PrefetchCopyIntervalInBytes;
 887       bool use_stride = false;
 888       if (direction == copy_backwards) {
 889          use_stride = prefetch &gt; 256;
 890          prefetch = -prefetch;
 891          if (use_stride) __ mov(stride, prefetch);
 892       }
 893 
 894       __ bind(again);
 895 
 896       if (PrefetchCopyIntervalInBytes &gt; 0)
 897         __ prfm(use_stride ? Address(s, stride) : Address(s, prefetch), PLDL1KEEP);
 898 
 899       if (direction == copy_forwards) {
 900        // allowing for the offset of -8 the store instructions place
 901        // registers into the target 64 bit block at the following
 902        // offsets
 903        //
 904        // t0 at offset 0
 905        // t1 at offset 8,  t2 at offset 16
 906        // t3 at offset 24, t4 at offset 32
 907        // t5 at offset 40, t6 at offset 48
 908        // t7 at offset 56
 909 
 910         __ str(t0, Address(d, 1 * unit));
 911         __ stp(t1, t2, Address(d, 2 * unit));
 912         __ ldp(t0, t1, Address(s, 2 * unit));
 913         __ stp(t3, t4, Address(d, 4 * unit));
 914         __ ldp(t2, t3, Address(s, 4 * unit));
 915         __ stp(t5, t6, Address(d, 6 * unit));
 916         __ ldp(t4, t5, Address(s, 6 * unit));
 917         __ str(t7, Address(__ pre(d, 8 * unit)));
 918         __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 919       } else {
 920        // d was not offset when we started so the registers are
 921        // written into the 64 bit block preceding d with the following
 922        // offsets
 923        //
 924        // t1 at offset -8
 925        // t3 at offset -24, t0 at offset -16
 926        // t5 at offset -48, t2 at offset -32
 927        // t7 at offset -56, t4 at offset -48
 928        //                   t6 at offset -64
 929        //
 930        // note that this matches the offsets previously noted for the
 931        // loads
 932 
 933         __ str(t1, Address(d, 1 * unit));
 934         __ stp(t3, t0, Address(d, 3 * unit));
 935         __ ldp(t0, t1, Address(s, 2 * unit));
 936         __ stp(t5, t2, Address(d, 5 * unit));
 937         __ ldp(t2, t3, Address(s, 4 * unit));
 938         __ stp(t7, t4, Address(d, 7 * unit));
 939         __ ldp(t4, t5, Address(s, 6 * unit));
 940         __ str(t6, Address(__ pre(d, 8 * unit)));
 941         __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 942       }
 943 
 944       __ subs(count, count, 8);
 945       __ br(Assembler::HS, again);
 946 
 947       // Drain
 948       //
 949       // this uses the same pattern of offsets and register arguments
 950       // as above
 951       __ bind(drain);
 952       if (direction == copy_forwards) {
 953         __ str(t0, Address(d, 1 * unit));
 954         __ stp(t1, t2, Address(d, 2 * unit));
 955         __ stp(t3, t4, Address(d, 4 * unit));
 956         __ stp(t5, t6, Address(d, 6 * unit));
 957         __ str(t7, Address(__ pre(d, 8 * unit)));
 958       } else {
 959         __ str(t1, Address(d, 1 * unit));
 960         __ stp(t3, t0, Address(d, 3 * unit));
 961         __ stp(t5, t2, Address(d, 5 * unit));
 962         __ stp(t7, t4, Address(d, 7 * unit));
 963         __ str(t6, Address(__ pre(d, 8 * unit)));
 964       }
 965       // now we need to copy any remaining part block which may
 966       // include a 4 word block subblock and/or a 2 word subblock.
 967       // bits 2 and 1 in the count are the tell-tale for whetehr we
 968       // have each such subblock
 969       {
 970         Label L1, L2;
 971         __ tbz(count, exact_log2(4), L1);
 972        // this is the same as above but copying only 4 longs hence
 973        // with ony one intervening stp between the str instructions
 974        // but note that the offsets and registers still follow the
 975        // same pattern
 976         __ ldp(t0, t1, Address(s, 2 * unit));
 977         __ ldp(t2, t3, Address(__ pre(s, 4 * unit)));
 978         if (direction == copy_forwards) {
 979           __ str(t0, Address(d, 1 * unit));
 980           __ stp(t1, t2, Address(d, 2 * unit));
 981           __ str(t3, Address(__ pre(d, 4 * unit)));
 982         } else {
 983           __ str(t1, Address(d, 1 * unit));
 984           __ stp(t3, t0, Address(d, 3 * unit));
 985           __ str(t2, Address(__ pre(d, 4 * unit)));
 986         }
 987         __ bind(L1);
 988 
 989         __ tbz(count, 1, L2);
 990        // this is the same as above but copying only 2 longs hence
 991        // there is no intervening stp between the str instructions
 992        // but note that the offset and register patterns are still
 993        // the same
 994         __ ldp(t0, t1, Address(__ pre(s, 2 * unit)));
 995         if (direction == copy_forwards) {
 996           __ str(t0, Address(d, 1 * unit));
 997           __ str(t1, Address(__ pre(d, 2 * unit)));
 998         } else {
 999           __ str(t1, Address(d, 1 * unit));
1000           __ str(t0, Address(__ pre(d, 2 * unit)));
1001         }
1002         __ bind(L2);
1003 
1004        // for forwards copy we need to re-adjust the offsets we
1005        // applied so that s and d are follow the last words written
1006 
1007        if (direction == copy_forwards) {
1008          __ add(s, s, 16);
1009          __ add(d, d, 8);
1010        }
1011 
1012       }
1013 
1014       __ ret(lr);
1015       }
1016   }
1017 
1018   // Small copy: less than 16 bytes.
1019   //
1020   // NB: Ignores all of the bits of count which represent more than 15
1021   // bytes, so a caller doesn&#39;t have to mask them.
1022 
1023   void copy_memory_small(Register s, Register d, Register count, Register tmp, int step) {
1024     bool is_backwards = step &lt; 0;
1025     size_t granularity = uabs(step);
1026     int direction = is_backwards ? -1 : 1;
1027     int unit = wordSize * direction;
1028 
1029     Label Lword, Lint, Lshort, Lbyte;
1030 
1031     assert(granularity
1032            &amp;&amp; granularity &lt;= sizeof (jlong), &quot;Impossible granularity in copy_memory_small&quot;);
1033 
1034     const Register t0 = r3, t1 = r4, t2 = r5, t3 = r6;
1035 
1036     // ??? I don&#39;t know if this bit-test-and-branch is the right thing
1037     // to do.  It does a lot of jumping, resulting in several
1038     // mispredicted branches.  It might make more sense to do this
1039     // with something like Duff&#39;s device with a single computed branch.
1040 
1041     __ tbz(count, 3 - exact_log2(granularity), Lword);
1042     __ ldr(tmp, Address(__ adjust(s, unit, is_backwards)));
1043     __ str(tmp, Address(__ adjust(d, unit, is_backwards)));
1044     __ bind(Lword);
1045 
1046     if (granularity &lt;= sizeof (jint)) {
1047       __ tbz(count, 2 - exact_log2(granularity), Lint);
1048       __ ldrw(tmp, Address(__ adjust(s, sizeof (jint) * direction, is_backwards)));
1049       __ strw(tmp, Address(__ adjust(d, sizeof (jint) * direction, is_backwards)));
1050       __ bind(Lint);
1051     }
1052 
1053     if (granularity &lt;= sizeof (jshort)) {
1054       __ tbz(count, 1 - exact_log2(granularity), Lshort);
1055       __ ldrh(tmp, Address(__ adjust(s, sizeof (jshort) * direction, is_backwards)));
1056       __ strh(tmp, Address(__ adjust(d, sizeof (jshort) * direction, is_backwards)));
1057       __ bind(Lshort);
1058     }
1059 
1060     if (granularity &lt;= sizeof (jbyte)) {
1061       __ tbz(count, 0, Lbyte);
1062       __ ldrb(tmp, Address(__ adjust(s, sizeof (jbyte) * direction, is_backwards)));
1063       __ strb(tmp, Address(__ adjust(d, sizeof (jbyte) * direction, is_backwards)));
1064       __ bind(Lbyte);
1065     }
1066   }
1067 
1068   Label copy_f, copy_b;
1069 
1070   // All-singing all-dancing memory copy.
1071   //
1072   // Copy count units of memory from s to d.  The size of a unit is
1073   // step, which can be positive or negative depending on the direction
1074   // of copy.  If is_aligned is false, we align the source address.
1075   //
1076 
1077   void copy_memory(bool is_aligned, Register s, Register d,
1078                    Register count, Register tmp, int step) {
1079     copy_direction direction = step &lt; 0 ? copy_backwards : copy_forwards;
1080     bool is_backwards = step &lt; 0;
1081     int granularity = uabs(step);
1082     const Register t0 = r3, t1 = r4;
1083 
1084     // &lt;= 96 bytes do inline. Direction doesn&#39;t matter because we always
1085     // load all the data before writing anything
1086     Label copy4, copy8, copy16, copy32, copy80, copy_big, finish;
1087     const Register t2 = r5, t3 = r6, t4 = r7, t5 = r8;
1088     const Register t6 = r9, t7 = r10, t8 = r11, t9 = r12;
<a name="4" id="anc4"></a><span class="line-modified">1089     const Register send = r17, dend = r16;</span>
1090 
1091     if (PrefetchCopyIntervalInBytes &gt; 0)
1092       __ prfm(Address(s, 0), PLDL1KEEP);
1093     __ cmp(count, u1((UseSIMDForMemoryOps ? 96:80)/granularity));
1094     __ br(Assembler::HI, copy_big);
1095 
1096     __ lea(send, Address(s, count, Address::lsl(exact_log2(granularity))));
1097     __ lea(dend, Address(d, count, Address::lsl(exact_log2(granularity))));
1098 
1099     __ cmp(count, u1(16/granularity));
1100     __ br(Assembler::LS, copy16);
1101 
1102     __ cmp(count, u1(64/granularity));
1103     __ br(Assembler::HI, copy80);
1104 
1105     __ cmp(count, u1(32/granularity));
1106     __ br(Assembler::LS, copy32);
1107 
1108     // 33..64 bytes
1109     if (UseSIMDForMemoryOps) {
1110       __ ldpq(v0, v1, Address(s, 0));
1111       __ ldpq(v2, v3, Address(send, -32));
1112       __ stpq(v0, v1, Address(d, 0));
1113       __ stpq(v2, v3, Address(dend, -32));
1114     } else {
1115       __ ldp(t0, t1, Address(s, 0));
1116       __ ldp(t2, t3, Address(s, 16));
1117       __ ldp(t4, t5, Address(send, -32));
1118       __ ldp(t6, t7, Address(send, -16));
1119 
1120       __ stp(t0, t1, Address(d, 0));
1121       __ stp(t2, t3, Address(d, 16));
1122       __ stp(t4, t5, Address(dend, -32));
1123       __ stp(t6, t7, Address(dend, -16));
1124     }
1125     __ b(finish);
1126 
1127     // 17..32 bytes
1128     __ bind(copy32);
1129     __ ldp(t0, t1, Address(s, 0));
1130     __ ldp(t2, t3, Address(send, -16));
1131     __ stp(t0, t1, Address(d, 0));
1132     __ stp(t2, t3, Address(dend, -16));
1133     __ b(finish);
1134 
1135     // 65..80/96 bytes
1136     // (96 bytes if SIMD because we do 32 byes per instruction)
1137     __ bind(copy80);
1138     if (UseSIMDForMemoryOps) {
1139       __ ld4(v0, v1, v2, v3, __ T16B, Address(s, 0));
1140       __ ldpq(v4, v5, Address(send, -32));
1141       __ st4(v0, v1, v2, v3, __ T16B, Address(d, 0));
1142       __ stpq(v4, v5, Address(dend, -32));
1143     } else {
1144       __ ldp(t0, t1, Address(s, 0));
1145       __ ldp(t2, t3, Address(s, 16));
1146       __ ldp(t4, t5, Address(s, 32));
1147       __ ldp(t6, t7, Address(s, 48));
1148       __ ldp(t8, t9, Address(send, -16));
1149 
1150       __ stp(t0, t1, Address(d, 0));
1151       __ stp(t2, t3, Address(d, 16));
1152       __ stp(t4, t5, Address(d, 32));
1153       __ stp(t6, t7, Address(d, 48));
1154       __ stp(t8, t9, Address(dend, -16));
1155     }
1156     __ b(finish);
1157 
1158     // 0..16 bytes
1159     __ bind(copy16);
1160     __ cmp(count, u1(8/granularity));
1161     __ br(Assembler::LO, copy8);
1162 
1163     // 8..16 bytes
1164     __ ldr(t0, Address(s, 0));
1165     __ ldr(t1, Address(send, -8));
1166     __ str(t0, Address(d, 0));
1167     __ str(t1, Address(dend, -8));
1168     __ b(finish);
1169 
1170     if (granularity &lt; 8) {
1171       // 4..7 bytes
1172       __ bind(copy8);
1173       __ tbz(count, 2 - exact_log2(granularity), copy4);
1174       __ ldrw(t0, Address(s, 0));
1175       __ ldrw(t1, Address(send, -4));
1176       __ strw(t0, Address(d, 0));
1177       __ strw(t1, Address(dend, -4));
1178       __ b(finish);
1179       if (granularity &lt; 4) {
1180         // 0..3 bytes
1181         __ bind(copy4);
1182         __ cbz(count, finish); // get rid of 0 case
1183         if (granularity == 2) {
1184           __ ldrh(t0, Address(s, 0));
1185           __ strh(t0, Address(d, 0));
1186         } else { // granularity == 1
1187           // Now 1..3 bytes. Handle the 1 and 2 byte case by copying
1188           // the first and last byte.
1189           // Handle the 3 byte case by loading and storing base + count/2
1190           // (count == 1 (s+0)-&gt;(d+0), count == 2,3 (s+1) -&gt; (d+1))
1191           // This does means in the 1 byte case we load/store the same
1192           // byte 3 times.
1193           __ lsr(count, count, 1);
1194           __ ldrb(t0, Address(s, 0));
1195           __ ldrb(t1, Address(send, -1));
1196           __ ldrb(t2, Address(s, count));
1197           __ strb(t0, Address(d, 0));
1198           __ strb(t1, Address(dend, -1));
1199           __ strb(t2, Address(d, count));
1200         }
1201         __ b(finish);
1202       }
1203     }
1204 
1205     __ bind(copy_big);
1206     if (is_backwards) {
1207       __ lea(s, Address(s, count, Address::lsl(exact_log2(-step))));
1208       __ lea(d, Address(d, count, Address::lsl(exact_log2(-step))));
1209     }
1210 
1211     // Now we&#39;ve got the small case out of the way we can align the
1212     // source address on a 2-word boundary.
1213 
1214     Label aligned;
1215 
1216     if (is_aligned) {
1217       // We may have to adjust by 1 word to get s 2-word-aligned.
1218       __ tbz(s, exact_log2(wordSize), aligned);
1219       __ ldr(tmp, Address(__ adjust(s, direction * wordSize, is_backwards)));
1220       __ str(tmp, Address(__ adjust(d, direction * wordSize, is_backwards)));
1221       __ sub(count, count, wordSize/granularity);
1222     } else {
1223       if (is_backwards) {
1224         __ andr(rscratch2, s, 2 * wordSize - 1);
1225       } else {
1226         __ neg(rscratch2, s);
1227         __ andr(rscratch2, rscratch2, 2 * wordSize - 1);
1228       }
1229       // rscratch2 is the byte adjustment needed to align s.
1230       __ cbz(rscratch2, aligned);
1231       int shift = exact_log2(granularity);
1232       if (shift)  __ lsr(rscratch2, rscratch2, shift);
1233       __ sub(count, count, rscratch2);
1234 
1235 #if 0
1236       // ?? This code is only correct for a disjoint copy.  It may or
1237       // may not make sense to use it in that case.
1238 
1239       // Copy the first pair; s and d may not be aligned.
1240       __ ldp(t0, t1, Address(s, is_backwards ? -2 * wordSize : 0));
1241       __ stp(t0, t1, Address(d, is_backwards ? -2 * wordSize : 0));
1242 
1243       // Align s and d, adjust count
1244       if (is_backwards) {
1245         __ sub(s, s, rscratch2);
1246         __ sub(d, d, rscratch2);
1247       } else {
1248         __ add(s, s, rscratch2);
1249         __ add(d, d, rscratch2);
1250       }
1251 #else
1252       copy_memory_small(s, d, rscratch2, rscratch1, step);
1253 #endif
1254     }
1255 
1256     __ bind(aligned);
1257 
1258     // s is now 2-word-aligned.
1259 
1260     // We have a count of units and some trailing bytes.  Adjust the
1261     // count and do a bulk copy of words.
1262     __ lsr(rscratch2, count, exact_log2(wordSize/granularity));
1263     if (direction == copy_forwards)
1264       __ bl(copy_f);
1265     else
1266       __ bl(copy_b);
1267 
1268     // And the tail.
1269     copy_memory_small(s, d, count, tmp, step);
1270 
1271     if (granularity &gt;= 8) __ bind(copy8);
1272     if (granularity &gt;= 4) __ bind(copy4);
1273     __ bind(finish);
1274   }
1275 
1276 
1277   void clobber_registers() {
1278 #ifdef ASSERT
1279     __ mov(rscratch1, (uint64_t)0xdeadbeef);
1280     __ orr(rscratch1, rscratch1, rscratch1, Assembler::LSL, 32);
<a name="5" id="anc5"></a><span class="line-modified">1281     for (Register r = r3; r &lt;= NOT_WIN64(r18) WIN64_ONLY(r17); r++)</span>
1282       if (r != rscratch1) __ mov(r, rscratch1);
1283 #endif
<a name="6" id="anc6"></a><span class="line-added">1284 </span>
1285   }
1286 
1287   // Scan over array at a for count oops, verifying each one.
1288   // Preserves a and count, clobbers rscratch1 and rscratch2.
<a name="7" id="anc7"></a><span class="line-modified">1289   void verify_oop_array (unsigned int size, Register a, Register count, Register temp) {</span>
1290     Label loop, end;
1291     __ mov(rscratch1, a);
1292     __ mov(rscratch2, zr);
1293     __ bind(loop);
1294     __ cmp(rscratch2, count);
1295     __ br(Assembler::HS, end);
1296     if (size == (size_t)wordSize) {
1297       __ ldr(temp, Address(a, rscratch2, Address::lsl(exact_log2(size))));
1298       __ verify_oop(temp);
1299     } else {
1300       __ ldrw(r16, Address(a, rscratch2, Address::lsl(exact_log2(size))));
1301       __ decode_heap_oop(temp); // calls verify_oop
1302     }
1303     __ add(rscratch2, rscratch2, size);
1304     __ b(loop);
1305     __ bind(end);
1306   }
1307 
1308   // Arguments:
1309   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1310   //             ignored
1311   //   is_oop  - true =&gt; oop array, so generate store check code
1312   //   name    - stub name string
1313   //
1314   // Inputs:
1315   //   c_rarg0   - source array address
1316   //   c_rarg1   - destination array address
1317   //   c_rarg2   - element count, treated as ssize_t, can be zero
1318   //
1319   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1320   // the hardware handle it.  The two dwords within qwords that span
1321   // cache line boundaries will still be loaded and stored atomicly.
1322   //
1323   // Side Effects:
1324   //   disjoint_int_copy_entry is set to the no-overlap entry point
1325   //   used by generate_conjoint_int_oop_copy().
1326   //
<a name="8" id="anc8"></a><span class="line-modified">1327   address generate_disjoint_copy(int size, bool aligned, bool is_oop, address *entry,</span>
1328                                   const char *name, bool dest_uninitialized = false) {
1329     Register s = c_rarg0, d = c_rarg1, count = c_rarg2;
1330     RegSet saved_reg = RegSet::of(s, d, count);
1331     __ align(CodeEntryAlignment);
1332     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1333     address start = __ pc();
1334     __ enter();
1335 
1336     if (entry != NULL) {
1337       *entry = __ pc();
1338       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1339       BLOCK_COMMENT(&quot;Entry:&quot;);
1340     }
1341 
1342     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;
1343     if (dest_uninitialized) {
1344       decorators |= IS_DEST_UNINITIALIZED;
1345     }
1346     if (aligned) {
1347       decorators |= ARRAYCOPY_ALIGNED;
1348     }
1349 
1350     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1351     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, s, d, count, saved_reg);
1352 
1353     if (is_oop) {
1354       // save regs before copy_memory
1355       __ push(RegSet::of(d, count), sp);
1356     }
1357     {
1358       // UnsafeCopyMemory page error: continue after ucm
1359       bool add_entry = !is_oop &amp;&amp; (!aligned || sizeof(jlong) == size);
1360       UnsafeCopyMemoryMark ucmm(this, add_entry, true);
1361       copy_memory(aligned, s, d, count, rscratch1, size);
1362     }
1363 
1364     if (is_oop) {
1365       __ pop(RegSet::of(d, count), sp);
1366       if (VerifyOops)
1367         verify_oop_array(size, d, count, r16);
1368     }
1369 
1370     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1, RegSet());
1371 
1372     __ leave();
1373     __ mov(r0, zr); // return 0
1374     __ ret(lr);
1375     return start;
1376   }
1377 
1378   // Arguments:
1379   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1380   //             ignored
1381   //   is_oop  - true =&gt; oop array, so generate store check code
1382   //   name    - stub name string
1383   //
1384   // Inputs:
1385   //   c_rarg0   - source array address
1386   //   c_rarg1   - destination array address
1387   //   c_rarg2   - element count, treated as ssize_t, can be zero
1388   //
1389   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1390   // the hardware handle it.  The two dwords within qwords that span
1391   // cache line boundaries will still be loaded and stored atomicly.
1392   //
<a name="9" id="anc9"></a><span class="line-modified">1393   address generate_conjoint_copy(int size, bool aligned, bool is_oop, address nooverlap_target,</span>
1394                                  address *entry, const char *name,
1395                                  bool dest_uninitialized = false) {
1396     Register s = c_rarg0, d = c_rarg1, count = c_rarg2;
1397     RegSet saved_regs = RegSet::of(s, d, count);
1398     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1399     address start = __ pc();
1400     __ enter();
1401 
1402     if (entry != NULL) {
1403       *entry = __ pc();
1404       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1405       BLOCK_COMMENT(&quot;Entry:&quot;);
1406     }
1407 
1408     // use fwd copy when (d-s) above_equal (count*size)
1409     __ sub(rscratch1, d, s);
1410     __ cmp(rscratch1, count, Assembler::LSL, exact_log2(size));
1411     __ br(Assembler::HS, nooverlap_target);
1412 
1413     DecoratorSet decorators = IN_HEAP | IS_ARRAY;
1414     if (dest_uninitialized) {
1415       decorators |= IS_DEST_UNINITIALIZED;
1416     }
1417     if (aligned) {
1418       decorators |= ARRAYCOPY_ALIGNED;
1419     }
1420 
1421     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1422     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, s, d, count, saved_regs);
1423 
1424     if (is_oop) {
1425       // save regs before copy_memory
1426       __ push(RegSet::of(d, count), sp);
1427     }
1428     {
1429       // UnsafeCopyMemory page error: continue after ucm
1430       bool add_entry = !is_oop &amp;&amp; (!aligned || sizeof(jlong) == size);
1431       UnsafeCopyMemoryMark ucmm(this, add_entry, true);
1432       copy_memory(aligned, s, d, count, rscratch1, -size);
1433     }
1434     if (is_oop) {
1435       __ pop(RegSet::of(d, count), sp);
1436       if (VerifyOops)
1437         verify_oop_array(size, d, count, r16);
1438     }
1439     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1, RegSet());
1440     __ leave();
1441     __ mov(r0, zr); // return 0
1442     __ ret(lr);
1443     return start;
1444 }
1445 
1446   // Arguments:
1447   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1448   //             ignored
1449   //   name    - stub name string
1450   //
1451   // Inputs:
1452   //   c_rarg0   - source array address
1453   //   c_rarg1   - destination array address
1454   //   c_rarg2   - element count, treated as ssize_t, can be zero
1455   //
1456   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1457   // we let the hardware handle it.  The one to eight bytes within words,
1458   // dwords or qwords that span cache line boundaries will still be loaded
1459   // and stored atomically.
1460   //
1461   // Side Effects:
1462   //   disjoint_byte_copy_entry is set to the no-overlap entry point  //
1463   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1464   // we let the hardware handle it.  The one to eight bytes within words,
1465   // dwords or qwords that span cache line boundaries will still be loaded
1466   // and stored atomically.
1467   //
1468   // Side Effects:
1469   //   disjoint_byte_copy_entry is set to the no-overlap entry point
1470   //   used by generate_conjoint_byte_copy().
1471   //
1472   address generate_disjoint_byte_copy(bool aligned, address* entry, const char *name) {
1473     const bool not_oop = false;
1474     return generate_disjoint_copy(sizeof (jbyte), aligned, not_oop, entry, name);
1475   }
1476 
1477   // Arguments:
1478   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1479   //             ignored
1480   //   name    - stub name string
1481   //
1482   // Inputs:
1483   //   c_rarg0   - source array address
1484   //   c_rarg1   - destination array address
1485   //   c_rarg2   - element count, treated as ssize_t, can be zero
1486   //
1487   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1488   // we let the hardware handle it.  The one to eight bytes within words,
1489   // dwords or qwords that span cache line boundaries will still be loaded
1490   // and stored atomically.
1491   //
1492   address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,
1493                                       address* entry, const char *name) {
1494     const bool not_oop = false;
1495     return generate_conjoint_copy(sizeof (jbyte), aligned, not_oop, nooverlap_target, entry, name);
1496   }
1497 
1498   // Arguments:
1499   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1500   //             ignored
1501   //   name    - stub name string
1502   //
1503   // Inputs:
1504   //   c_rarg0   - source array address
1505   //   c_rarg1   - destination array address
1506   //   c_rarg2   - element count, treated as ssize_t, can be zero
1507   //
1508   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4- or 2-byte boundaries, we
1509   // let the hardware handle it.  The two or four words within dwords
1510   // or qwords that span cache line boundaries will still be loaded
1511   // and stored atomically.
1512   //
1513   // Side Effects:
1514   //   disjoint_short_copy_entry is set to the no-overlap entry point
1515   //   used by generate_conjoint_short_copy().
1516   //
1517   address generate_disjoint_short_copy(bool aligned,
1518                                        address* entry, const char *name) {
1519     const bool not_oop = false;
1520     return generate_disjoint_copy(sizeof (jshort), aligned, not_oop, entry, name);
1521   }
1522 
1523   // Arguments:
1524   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1525   //             ignored
1526   //   name    - stub name string
1527   //
1528   // Inputs:
1529   //   c_rarg0   - source array address
1530   //   c_rarg1   - destination array address
1531   //   c_rarg2   - element count, treated as ssize_t, can be zero
1532   //
1533   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4- or 2-byte boundaries, we
1534   // let the hardware handle it.  The two or four words within dwords
1535   // or qwords that span cache line boundaries will still be loaded
1536   // and stored atomically.
1537   //
1538   address generate_conjoint_short_copy(bool aligned, address nooverlap_target,
1539                                        address *entry, const char *name) {
1540     const bool not_oop = false;
1541     return generate_conjoint_copy(sizeof (jshort), aligned, not_oop, nooverlap_target, entry, name);
1542 
1543   }
1544   // Arguments:
1545   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1546   //             ignored
1547   //   name    - stub name string
1548   //
1549   // Inputs:
1550   //   c_rarg0   - source array address
1551   //   c_rarg1   - destination array address
1552   //   c_rarg2   - element count, treated as ssize_t, can be zero
1553   //
1554   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1555   // the hardware handle it.  The two dwords within qwords that span
1556   // cache line boundaries will still be loaded and stored atomicly.
1557   //
1558   // Side Effects:
1559   //   disjoint_int_copy_entry is set to the no-overlap entry point
1560   //   used by generate_conjoint_int_oop_copy().
1561   //
1562   address generate_disjoint_int_copy(bool aligned, address *entry,
1563                                          const char *name, bool dest_uninitialized = false) {
1564     const bool not_oop = false;
1565     return generate_disjoint_copy(sizeof (jint), aligned, not_oop, entry, name);
1566   }
1567 
1568   // Arguments:
1569   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1570   //             ignored
1571   //   name    - stub name string
1572   //
1573   // Inputs:
1574   //   c_rarg0   - source array address
1575   //   c_rarg1   - destination array address
1576   //   c_rarg2   - element count, treated as ssize_t, can be zero
1577   //
1578   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1579   // the hardware handle it.  The two dwords within qwords that span
1580   // cache line boundaries will still be loaded and stored atomicly.
1581   //
1582   address generate_conjoint_int_copy(bool aligned, address nooverlap_target,
1583                                      address *entry, const char *name,
1584                                      bool dest_uninitialized = false) {
1585     const bool not_oop = false;
1586     return generate_conjoint_copy(sizeof (jint), aligned, not_oop, nooverlap_target, entry, name);
1587   }
1588 
1589 
1590   // Arguments:
1591   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1592   //             ignored
1593   //   name    - stub name string
1594   //
1595   // Inputs:
1596   //   c_rarg0   - source array address
1597   //   c_rarg1   - destination array address
1598   //   c_rarg2   - element count, treated as size_t, can be zero
1599   //
1600   // Side Effects:
1601   //   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the
1602   //   no-overlap entry point used by generate_conjoint_long_oop_copy().
1603   //
1604   address generate_disjoint_long_copy(bool aligned, address *entry,
1605                                           const char *name, bool dest_uninitialized = false) {
1606     const bool not_oop = false;
1607     return generate_disjoint_copy(sizeof (jlong), aligned, not_oop, entry, name);
1608   }
1609 
1610   // Arguments:
1611   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1612   //             ignored
1613   //   name    - stub name string
1614   //
1615   // Inputs:
1616   //   c_rarg0   - source array address
1617   //   c_rarg1   - destination array address
1618   //   c_rarg2   - element count, treated as size_t, can be zero
1619   //
1620   address generate_conjoint_long_copy(bool aligned,
1621                                       address nooverlap_target, address *entry,
1622                                       const char *name, bool dest_uninitialized = false) {
1623     const bool not_oop = false;
1624     return generate_conjoint_copy(sizeof (jlong), aligned, not_oop, nooverlap_target, entry, name);
1625   }
1626 
1627   // Arguments:
1628   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1629   //             ignored
1630   //   name    - stub name string
1631   //
1632   // Inputs:
1633   //   c_rarg0   - source array address
1634   //   c_rarg1   - destination array address
1635   //   c_rarg2   - element count, treated as size_t, can be zero
1636   //
1637   // Side Effects:
1638   //   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the
1639   //   no-overlap entry point used by generate_conjoint_long_oop_copy().
1640   //
1641   address generate_disjoint_oop_copy(bool aligned, address *entry,
1642                                      const char *name, bool dest_uninitialized) {
1643     const bool is_oop = true;
<a name="10" id="anc10"></a><span class="line-modified">1644     const int size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);</span>
1645     return generate_disjoint_copy(size, aligned, is_oop, entry, name, dest_uninitialized);
1646   }
1647 
1648   // Arguments:
1649   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1650   //             ignored
1651   //   name    - stub name string
1652   //
1653   // Inputs:
1654   //   c_rarg0   - source array address
1655   //   c_rarg1   - destination array address
1656   //   c_rarg2   - element count, treated as size_t, can be zero
1657   //
1658   address generate_conjoint_oop_copy(bool aligned,
1659                                      address nooverlap_target, address *entry,
1660                                      const char *name, bool dest_uninitialized) {
1661     const bool is_oop = true;
<a name="11" id="anc11"></a><span class="line-modified">1662     const int size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);</span>
1663     return generate_conjoint_copy(size, aligned, is_oop, nooverlap_target, entry,
1664                                   name, dest_uninitialized);
1665   }
1666 
1667 
1668   // Helper for generating a dynamic type check.
1669   // Smashes rscratch1, rscratch2.
1670   void generate_type_check(Register sub_klass,
1671                            Register super_check_offset,
1672                            Register super_klass,
1673                            Label&amp; L_success) {
1674     assert_different_registers(sub_klass, super_check_offset, super_klass);
1675 
1676     BLOCK_COMMENT(&quot;type_check:&quot;);
1677 
1678     Label L_miss;
1679 
1680     __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &amp;L_success, &amp;L_miss, NULL,
1681                                      super_check_offset);
1682     __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &amp;L_success, NULL);
1683 
1684     // Fall through on failure!
1685     __ BIND(L_miss);
1686   }
1687 
1688   //
1689   //  Generate checkcasting array copy stub
1690   //
1691   //  Input:
1692   //    c_rarg0   - source array address
1693   //    c_rarg1   - destination array address
1694   //    c_rarg2   - element count, treated as ssize_t, can be zero
1695   //    c_rarg3   - size_t ckoff (super_check_offset)
1696   //    c_rarg4   - oop ckval (super_klass)
1697   //
1698   //  Output:
1699   //    r0 ==  0  -  success
1700   //    r0 == -1^K - failure, where K is partial transfer count
1701   //
1702   address generate_checkcast_copy(const char *name, address *entry,
1703                                   bool dest_uninitialized = false) {
1704 
1705     Label L_load_element, L_store_element, L_do_card_marks, L_done, L_done_pop;
1706 
1707     // Input registers (after setup_arg_regs)
1708     const Register from        = c_rarg0;   // source array address
1709     const Register to          = c_rarg1;   // destination array address
1710     const Register count       = c_rarg2;   // elementscount
1711     const Register ckoff       = c_rarg3;   // super_check_offset
1712     const Register ckval       = c_rarg4;   // super_klass
1713 
1714     RegSet wb_pre_saved_regs = RegSet::range(c_rarg0, c_rarg4);
1715     RegSet wb_post_saved_regs = RegSet::of(count);
1716 
<a name="12" id="anc12"></a><span class="line-modified">1717     // Registers used as temps (r19, r20, r21, r22 are save-on-entry)</span>
<span class="line-added">1718     const Register copied_oop  = r22;       // actual oop copied</span>
1719     const Register count_save  = r21;       // orig elementscount
1720     const Register start_to    = r20;       // destination array start address
<a name="13" id="anc13"></a>
1721     const Register r19_klass   = r19;       // oop._klass
1722 
1723     //---------------------------------------------------------------
1724     // Assembler stub will be used for this call to arraycopy
1725     // if the two arrays are subtypes of Object[] but the
1726     // destination array type is not equal to or a supertype
1727     // of the source type.  Each element must be separately
1728     // checked.
1729 
1730     assert_different_registers(from, to, count, ckoff, ckval, start_to,
1731                                copied_oop, r19_klass, count_save);
1732 
1733     __ align(CodeEntryAlignment);
1734     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1735     address start = __ pc();
1736 
1737     __ enter(); // required for proper stackwalking of RuntimeStub frame
1738 
1739 #ifdef ASSERT
1740     // caller guarantees that the arrays really are different
1741     // otherwise, we would have to make conjoint checks
1742     { Label L;
1743       array_overlap_test(L, TIMES_OOP);
1744       __ stop(&quot;checkcast_copy within a single array&quot;);
1745       __ bind(L);
1746     }
1747 #endif //ASSERT
1748 
1749     // Caller of this entry point must set up the argument registers.
1750     if (entry != NULL) {
1751       *entry = __ pc();
1752       BLOCK_COMMENT(&quot;Entry:&quot;);
1753     }
1754 
1755      // Empty array:  Nothing to do.
1756     __ cbz(count, L_done);
<a name="14" id="anc14"></a><span class="line-modified">1757     __ push(RegSet::of(r19, r20, r21, r22), sp);</span>

1758 
1759 #ifdef ASSERT
1760     BLOCK_COMMENT(&quot;assert consistent ckoff/ckval&quot;);
1761     // The ckoff and ckval must be mutually consistent,
1762     // even though caller generates both.
1763     { Label L;
1764       int sco_offset = in_bytes(Klass::super_check_offset_offset());
1765       __ ldrw(start_to, Address(ckval, sco_offset));
1766       __ cmpw(ckoff, start_to);
1767       __ br(Assembler::EQ, L);
1768       __ stop(&quot;super_check_offset inconsistent&quot;);
1769       __ bind(L);
1770     }
1771 #endif //ASSERT
1772 
1773     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;
1774     bool is_oop = true;
1775     if (dest_uninitialized) {
1776       decorators |= IS_DEST_UNINITIALIZED;
1777     }
1778 
1779     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1780     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, from, to, count, wb_pre_saved_regs);
1781 
1782     // save the original count
1783     __ mov(count_save, count);
1784 
1785     // Copy from low to high addresses
1786     __ mov(start_to, to);              // Save destination array start address
1787     __ b(L_load_element);
1788 
1789     // ======== begin loop ========
1790     // (Loop is rotated; its entry is L_load_element.)
1791     // Loop control:
1792     //   for (; count != 0; count--) {
1793     //     copied_oop = load_heap_oop(from++);
1794     //     ... generate_type_check ...;
1795     //     store_heap_oop(to++, copied_oop);
1796     //   }
1797     __ align(OptoLoopAlignment);
1798 
1799     __ BIND(L_store_element);
1800     __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, AS_RAW);  // store the oop
1801     __ sub(count, count, 1);
1802     __ cbz(count, L_do_card_marks);
1803 
1804     // ======== loop entry is here ========
1805     __ BIND(L_load_element);
1806     __ load_heap_oop(copied_oop, __ post(from, UseCompressedOops ? 4 : 8), noreg, noreg, AS_RAW); // load the oop
1807     __ cbz(copied_oop, L_store_element);
1808 
1809     __ load_klass(r19_klass, copied_oop);// query the object klass
1810     generate_type_check(r19_klass, ckoff, ckval, L_store_element);
1811     // ======== end loop ========
1812 
1813     // It was a real error; we must depend on the caller to finish the job.
1814     // Register count = remaining oops, count_orig = total oops.
1815     // Emit GC store barriers for the oops we have copied and report
1816     // their number to the caller.
1817 
1818     __ subs(count, count_save, count);     // K = partially copied oop count
1819     __ eon(count, count, zr);                   // report (-1^K) to caller
1820     __ br(Assembler::EQ, L_done_pop);
1821 
1822     __ BIND(L_do_card_marks);
1823     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, start_to, count_save, rscratch1, wb_post_saved_regs);
1824 
1825     __ bind(L_done_pop);
<a name="15" id="anc15"></a><span class="line-modified">1826     __ pop(RegSet::of(r19, r20, r21, r22), sp);</span>
1827     inc_counter_np(SharedRuntime::_checkcast_array_copy_ctr);
1828 
1829     __ bind(L_done);
1830     __ mov(r0, count);
1831     __ leave();
1832     __ ret(lr);
1833 
1834     return start;
1835   }
1836 
1837   // Perform range checks on the proposed arraycopy.
1838   // Kills temp, but nothing else.
1839   // Also, clean the sign bits of src_pos and dst_pos.
1840   void arraycopy_range_checks(Register src,     // source array oop (c_rarg0)
1841                               Register src_pos, // source position (c_rarg1)
1842                               Register dst,     // destination array oo (c_rarg2)
1843                               Register dst_pos, // destination position (c_rarg3)
1844                               Register length,
1845                               Register temp,
1846                               Label&amp; L_failed) {
1847     BLOCK_COMMENT(&quot;arraycopy_range_checks:&quot;);
1848 
1849     assert_different_registers(rscratch1, temp);
1850 
1851     //  if (src_pos + length &gt; arrayOop(src)-&gt;length())  FAIL;
1852     __ ldrw(rscratch1, Address(src, arrayOopDesc::length_offset_in_bytes()));
1853     __ addw(temp, length, src_pos);
1854     __ cmpw(temp, rscratch1);
1855     __ br(Assembler::HI, L_failed);
1856 
1857     //  if (dst_pos + length &gt; arrayOop(dst)-&gt;length())  FAIL;
1858     __ ldrw(rscratch1, Address(dst, arrayOopDesc::length_offset_in_bytes()));
1859     __ addw(temp, length, dst_pos);
1860     __ cmpw(temp, rscratch1);
1861     __ br(Assembler::HI, L_failed);
1862 
1863     // Have to clean up high 32 bits of &#39;src_pos&#39; and &#39;dst_pos&#39;.
1864     __ movw(src_pos, src_pos);
1865     __ movw(dst_pos, dst_pos);
1866 
1867     BLOCK_COMMENT(&quot;arraycopy_range_checks done&quot;);
1868   }
1869 
1870   // These stubs get called from some dumb test routine.
1871   // I&#39;ll write them properly when they&#39;re called from
1872   // something that&#39;s actually doing something.
1873   static void fake_arraycopy_stub(address src, address dst, int count) {
1874     assert(count == 0, &quot;huh?&quot;);
1875   }
1876 
1877 
1878   //
1879   //  Generate &#39;unsafe&#39; array copy stub
1880   //  Though just as safe as the other stubs, it takes an unscaled
1881   //  size_t argument instead of an element count.
1882   //
1883   //  Input:
1884   //    c_rarg0   - source array address
1885   //    c_rarg1   - destination array address
1886   //    c_rarg2   - byte count, treated as ssize_t, can be zero
1887   //
1888   // Examines the alignment of the operands and dispatches
1889   // to a long, int, short, or byte copy loop.
1890   //
1891   address generate_unsafe_copy(const char *name,
1892                                address byte_copy_entry,
1893                                address short_copy_entry,
1894                                address int_copy_entry,
1895                                address long_copy_entry) {
1896     Label L_long_aligned, L_int_aligned, L_short_aligned;
1897     Register s = c_rarg0, d = c_rarg1, count = c_rarg2;
1898 
1899     __ align(CodeEntryAlignment);
1900     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1901     address start = __ pc();
1902     __ enter(); // required for proper stackwalking of RuntimeStub frame
1903 
1904     // bump this on entry, not on exit:
1905     inc_counter_np(SharedRuntime::_unsafe_array_copy_ctr);
1906 
1907     __ orr(rscratch1, s, d);
1908     __ orr(rscratch1, rscratch1, count);
1909 
1910     __ andr(rscratch1, rscratch1, BytesPerLong-1);
1911     __ cbz(rscratch1, L_long_aligned);
1912     __ andr(rscratch1, rscratch1, BytesPerInt-1);
1913     __ cbz(rscratch1, L_int_aligned);
1914     __ tbz(rscratch1, 0, L_short_aligned);
1915     __ b(RuntimeAddress(byte_copy_entry));
1916 
1917     __ BIND(L_short_aligned);
1918     __ lsr(count, count, LogBytesPerShort);  // size =&gt; short_count
1919     __ b(RuntimeAddress(short_copy_entry));
1920     __ BIND(L_int_aligned);
1921     __ lsr(count, count, LogBytesPerInt);    // size =&gt; int_count
1922     __ b(RuntimeAddress(int_copy_entry));
1923     __ BIND(L_long_aligned);
1924     __ lsr(count, count, LogBytesPerLong);   // size =&gt; long_count
1925     __ b(RuntimeAddress(long_copy_entry));
1926 
1927     return start;
1928   }
1929 
1930   //
1931   //  Generate generic array copy stubs
1932   //
1933   //  Input:
1934   //    c_rarg0    -  src oop
1935   //    c_rarg1    -  src_pos (32-bits)
1936   //    c_rarg2    -  dst oop
1937   //    c_rarg3    -  dst_pos (32-bits)
1938   //    c_rarg4    -  element count (32-bits)
1939   //
1940   //  Output:
1941   //    r0 ==  0  -  success
1942   //    r0 == -1^K - failure, where K is partial transfer count
1943   //
1944   address generate_generic_copy(const char *name,
1945                                 address byte_copy_entry, address short_copy_entry,
1946                                 address int_copy_entry, address oop_copy_entry,
1947                                 address long_copy_entry, address checkcast_copy_entry) {
1948 
1949     Label L_failed, L_objArray;
1950     Label L_copy_bytes, L_copy_shorts, L_copy_ints, L_copy_longs;
1951 
1952     // Input registers
1953     const Register src        = c_rarg0;  // source array oop
1954     const Register src_pos    = c_rarg1;  // source position
1955     const Register dst        = c_rarg2;  // destination array oop
1956     const Register dst_pos    = c_rarg3;  // destination position
1957     const Register length     = c_rarg4;
1958 
1959 
1960     // Registers used as temps
1961     const Register dst_klass  = c_rarg5;
1962 
1963     __ align(CodeEntryAlignment);
1964 
1965     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1966 
1967     address start = __ pc();
1968 
1969     __ enter(); // required for proper stackwalking of RuntimeStub frame
1970 
1971     // bump this on entry, not on exit:
1972     inc_counter_np(SharedRuntime::_generic_array_copy_ctr);
1973 
1974     //-----------------------------------------------------------------------
1975     // Assembler stub will be used for this call to arraycopy
1976     // if the following conditions are met:
1977     //
1978     // (1) src and dst must not be null.
1979     // (2) src_pos must not be negative.
1980     // (3) dst_pos must not be negative.
1981     // (4) length  must not be negative.
1982     // (5) src klass and dst klass should be the same and not NULL.
1983     // (6) src and dst should be arrays.
1984     // (7) src_pos + length must not exceed length of src.
1985     // (8) dst_pos + length must not exceed length of dst.
1986     //
1987 
1988     //  if (src == NULL) return -1;
1989     __ cbz(src, L_failed);
1990 
1991     //  if (src_pos &lt; 0) return -1;
1992     __ tbnz(src_pos, 31, L_failed);  // i.e. sign bit set
1993 
1994     //  if (dst == NULL) return -1;
1995     __ cbz(dst, L_failed);
1996 
1997     //  if (dst_pos &lt; 0) return -1;
1998     __ tbnz(dst_pos, 31, L_failed);  // i.e. sign bit set
1999 
2000     // registers used as temp
2001     const Register scratch_length    = r16; // elements count to copy
2002     const Register scratch_src_klass = r17; // array klass
<a name="16" id="anc16"></a><span class="line-modified">2003     const Register lh                = r15; // layout helper</span>
2004 
2005     //  if (length &lt; 0) return -1;
2006     __ movw(scratch_length, length);        // length (elements count, 32-bits value)
2007     __ tbnz(scratch_length, 31, L_failed);  // i.e. sign bit set
2008 
2009     __ load_klass(scratch_src_klass, src);
2010 #ifdef ASSERT
2011     //  assert(src-&gt;klass() != NULL);
2012     {
2013       BLOCK_COMMENT(&quot;assert klasses not null {&quot;);
2014       Label L1, L2;
2015       __ cbnz(scratch_src_klass, L2);   // it is broken if klass is NULL
2016       __ bind(L1);
2017       __ stop(&quot;broken null klass&quot;);
2018       __ bind(L2);
2019       __ load_klass(rscratch1, dst);
2020       __ cbz(rscratch1, L1);     // this would be broken also
2021       BLOCK_COMMENT(&quot;} assert klasses not null done&quot;);
2022     }
2023 #endif
2024 
2025     // Load layout helper (32-bits)
2026     //
2027     //  |array_tag|     | header_size | element_type |     |log2_element_size|
2028     // 32        30    24            16              8     2                 0
2029     //
2030     //   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0
2031     //
2032 
2033     const int lh_offset = in_bytes(Klass::layout_helper_offset());
2034 
2035     // Handle objArrays completely differently...
2036     const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2037     __ ldrw(lh, Address(scratch_src_klass, lh_offset));
2038     __ movw(rscratch1, objArray_lh);
2039     __ eorw(rscratch2, lh, rscratch1);
2040     __ cbzw(rscratch2, L_objArray);
2041 
2042     //  if (src-&gt;klass() != dst-&gt;klass()) return -1;
2043     __ load_klass(rscratch2, dst);
2044     __ eor(rscratch2, rscratch2, scratch_src_klass);
2045     __ cbnz(rscratch2, L_failed);
2046 
2047     //  if (!src-&gt;is_Array()) return -1;
2048     __ tbz(lh, 31, L_failed);  // i.e. (lh &gt;= 0)
2049 
2050     // At this point, it is known to be a typeArray (array_tag 0x3).
2051 #ifdef ASSERT
2052     {
2053       BLOCK_COMMENT(&quot;assert primitive array {&quot;);
2054       Label L;
2055       __ movw(rscratch2, Klass::_lh_array_tag_type_value &lt;&lt; Klass::_lh_array_tag_shift);
2056       __ cmpw(lh, rscratch2);
2057       __ br(Assembler::GE, L);
2058       __ stop(&quot;must be a primitive array&quot;);
2059       __ bind(L);
2060       BLOCK_COMMENT(&quot;} assert primitive array done&quot;);
2061     }
2062 #endif
2063 
2064     arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,
2065                            rscratch2, L_failed);
2066 
2067     // TypeArrayKlass
2068     //
2069     // src_addr = (src + array_header_in_bytes()) + (src_pos &lt;&lt; log2elemsize);
2070     // dst_addr = (dst + array_header_in_bytes()) + (dst_pos &lt;&lt; log2elemsize);
2071     //
2072 
2073     const Register rscratch1_offset = rscratch1;    // array offset
<a name="17" id="anc17"></a><span class="line-modified">2074     const Register r15_elsize = lh; // element size</span>
2075 
2076     __ ubfx(rscratch1_offset, lh, Klass::_lh_header_size_shift,
2077            exact_log2(Klass::_lh_header_size_mask+1));   // array_offset
2078     __ add(src, src, rscratch1_offset);           // src array offset
2079     __ add(dst, dst, rscratch1_offset);           // dst array offset
2080     BLOCK_COMMENT(&quot;choose copy loop based on element size&quot;);
2081 
2082     // next registers should be set before the jump to corresponding stub
2083     const Register from     = c_rarg0;  // source array address
2084     const Register to       = c_rarg1;  // destination array address
2085     const Register count    = c_rarg2;  // elements count
2086 
2087     // &#39;from&#39;, &#39;to&#39;, &#39;count&#39; registers should be set in such order
2088     // since they are the same as &#39;src&#39;, &#39;src_pos&#39;, &#39;dst&#39;.
2089 
2090     assert(Klass::_lh_log2_element_size_shift == 0, &quot;fix this code&quot;);
2091 
2092     // The possible values of elsize are 0-3, i.e. exact_log2(element
2093     // size in bytes).  We do a simple bitwise binary search.
2094   __ BIND(L_copy_bytes);
<a name="18" id="anc18"></a><span class="line-modified">2095     __ tbnz(r15_elsize, 1, L_copy_ints);</span>
<span class="line-modified">2096     __ tbnz(r15_elsize, 0, L_copy_shorts);</span>
2097     __ lea(from, Address(src, src_pos));// src_addr
2098     __ lea(to,   Address(dst, dst_pos));// dst_addr
2099     __ movw(count, scratch_length); // length
2100     __ b(RuntimeAddress(byte_copy_entry));
2101 
2102   __ BIND(L_copy_shorts);
2103     __ lea(from, Address(src, src_pos, Address::lsl(1)));// src_addr
2104     __ lea(to,   Address(dst, dst_pos, Address::lsl(1)));// dst_addr
2105     __ movw(count, scratch_length); // length
2106     __ b(RuntimeAddress(short_copy_entry));
2107 
2108   __ BIND(L_copy_ints);
<a name="19" id="anc19"></a><span class="line-modified">2109     __ tbnz(r15_elsize, 0, L_copy_longs);</span>
2110     __ lea(from, Address(src, src_pos, Address::lsl(2)));// src_addr
2111     __ lea(to,   Address(dst, dst_pos, Address::lsl(2)));// dst_addr
2112     __ movw(count, scratch_length); // length
2113     __ b(RuntimeAddress(int_copy_entry));
2114 
2115   __ BIND(L_copy_longs);
2116 #ifdef ASSERT
2117     {
2118       BLOCK_COMMENT(&quot;assert long copy {&quot;);
2119       Label L;
<a name="20" id="anc20"></a><span class="line-modified">2120       __ andw(lh, lh, Klass::_lh_log2_element_size_mask); // lh -&gt; r15_elsize</span>
<span class="line-modified">2121       __ cmpw(r15_elsize, LogBytesPerLong);</span>
2122       __ br(Assembler::EQ, L);
2123       __ stop(&quot;must be long copy, but elsize is wrong&quot;);
2124       __ bind(L);
2125       BLOCK_COMMENT(&quot;} assert long copy done&quot;);
2126     }
2127 #endif
2128     __ lea(from, Address(src, src_pos, Address::lsl(3)));// src_addr
2129     __ lea(to,   Address(dst, dst_pos, Address::lsl(3)));// dst_addr
2130     __ movw(count, scratch_length); // length
2131     __ b(RuntimeAddress(long_copy_entry));
2132 
2133     // ObjArrayKlass
2134   __ BIND(L_objArray);
2135     // live at this point:  scratch_src_klass, scratch_length, src[_pos], dst[_pos]
2136 
2137     Label L_plain_copy, L_checkcast_copy;
2138     //  test array classes for subtyping
<a name="21" id="anc21"></a><span class="line-modified">2139     __ load_klass(r15, dst);</span>
<span class="line-modified">2140     __ cmp(scratch_src_klass, r15); // usual case is exact equality</span>
2141     __ br(Assembler::NE, L_checkcast_copy);
2142 
2143     // Identically typed arrays can be copied without element-wise checks.
2144     arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,
2145                            rscratch2, L_failed);
2146 
2147     __ lea(from, Address(src, src_pos, Address::lsl(LogBytesPerHeapOop)));
2148     __ add(from, from, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2149     __ lea(to, Address(dst, dst_pos, Address::lsl(LogBytesPerHeapOop)));
2150     __ add(to, to, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2151     __ movw(count, scratch_length); // length
2152   __ BIND(L_plain_copy);
2153     __ b(RuntimeAddress(oop_copy_entry));
2154 
2155   __ BIND(L_checkcast_copy);
<a name="22" id="anc22"></a><span class="line-modified">2156     // live at this point:  scratch_src_klass, scratch_length, r15 (dst_klass)</span>
2157     {
2158       // Before looking at dst.length, make sure dst is also an objArray.
<a name="23" id="anc23"></a><span class="line-modified">2159       __ ldrw(rscratch1, Address(r15, lh_offset));</span>
2160       __ movw(rscratch2, objArray_lh);
2161       __ eorw(rscratch1, rscratch1, rscratch2);
2162       __ cbnzw(rscratch1, L_failed);
2163 
2164       // It is safe to examine both src.length and dst.length.
2165       arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,
<a name="24" id="anc24"></a><span class="line-modified">2166                              r15, L_failed);</span>
2167 
2168       __ load_klass(dst_klass, dst); // reload
2169 
2170       // Marshal the base address arguments now, freeing registers.
2171       __ lea(from, Address(src, src_pos, Address::lsl(LogBytesPerHeapOop)));
2172       __ add(from, from, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2173       __ lea(to, Address(dst, dst_pos, Address::lsl(LogBytesPerHeapOop)));
2174       __ add(to, to, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2175       __ movw(count, length);           // length (reloaded)
2176       Register sco_temp = c_rarg3;      // this register is free now
2177       assert_different_registers(from, to, count, sco_temp,
2178                                  dst_klass, scratch_src_klass);
2179       // assert_clean_int(count, sco_temp);
2180 
2181       // Generate the type check.
2182       const int sco_offset = in_bytes(Klass::super_check_offset_offset());
2183       __ ldrw(sco_temp, Address(dst_klass, sco_offset));
2184 
2185       // Smashes rscratch1, rscratch2
2186       generate_type_check(scratch_src_klass, sco_temp, dst_klass, L_plain_copy);
2187 
2188       // Fetch destination element klass from the ObjArrayKlass header.
2189       int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
2190       __ ldr(dst_klass, Address(dst_klass, ek_offset));
2191       __ ldrw(sco_temp, Address(dst_klass, sco_offset));
2192 
2193       // the checkcast_copy loop needs two extra arguments:
2194       assert(c_rarg3 == sco_temp, &quot;#3 already in place&quot;);
2195       // Set up arguments for checkcast_copy_entry.
2196       __ mov(c_rarg4, dst_klass);  // dst.klass.element_klass
2197       __ b(RuntimeAddress(checkcast_copy_entry));
2198     }
2199 
2200   __ BIND(L_failed);
2201     __ mov(r0, -1);
2202     __ leave();   // required for proper stackwalking of RuntimeStub frame
2203     __ ret(lr);
2204 
2205     return start;
2206   }
2207 
2208   //
2209   // Generate stub for array fill. If &quot;aligned&quot; is true, the
2210   // &quot;to&quot; address is assumed to be heapword aligned.
2211   //
2212   // Arguments for generated stub:
2213   //   to:    c_rarg0
2214   //   value: c_rarg1
2215   //   count: c_rarg2 treated as signed
2216   //
2217   address generate_fill(BasicType t, bool aligned, const char *name) {
2218     __ align(CodeEntryAlignment);
2219     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2220     address start = __ pc();
2221 
2222     BLOCK_COMMENT(&quot;Entry:&quot;);
2223 
2224     const Register to        = c_rarg0;  // source array address
2225     const Register value     = c_rarg1;  // value
2226     const Register count     = c_rarg2;  // elements count
2227 
2228     const Register bz_base = r10;        // base for block_zero routine
2229     const Register cnt_words = r11;      // temp register
2230 
2231     __ enter();
2232 
2233     Label L_fill_elements, L_exit1;
2234 
2235     int shift = -1;
2236     switch (t) {
2237       case T_BYTE:
2238         shift = 0;
2239         __ cmpw(count, 8 &gt;&gt; shift); // Short arrays (&lt; 8 bytes) fill by element
2240         __ bfi(value, value, 8, 8);   // 8 bit -&gt; 16 bit
2241         __ bfi(value, value, 16, 16); // 16 bit -&gt; 32 bit
2242         __ br(Assembler::LO, L_fill_elements);
2243         break;
2244       case T_SHORT:
2245         shift = 1;
2246         __ cmpw(count, 8 &gt;&gt; shift); // Short arrays (&lt; 8 bytes) fill by element
2247         __ bfi(value, value, 16, 16); // 16 bit -&gt; 32 bit
2248         __ br(Assembler::LO, L_fill_elements);
2249         break;
2250       case T_INT:
2251         shift = 2;
2252         __ cmpw(count, 8 &gt;&gt; shift); // Short arrays (&lt; 8 bytes) fill by element
2253         __ br(Assembler::LO, L_fill_elements);
2254         break;
2255       default: ShouldNotReachHere();
2256     }
2257 
2258     // Align source address at 8 bytes address boundary.
2259     Label L_skip_align1, L_skip_align2, L_skip_align4;
2260     if (!aligned) {
2261       switch (t) {
2262         case T_BYTE:
2263           // One byte misalignment happens only for byte arrays.
2264           __ tbz(to, 0, L_skip_align1);
2265           __ strb(value, Address(__ post(to, 1)));
2266           __ subw(count, count, 1);
2267           __ bind(L_skip_align1);
2268           // Fallthrough
2269         case T_SHORT:
2270           // Two bytes misalignment happens only for byte and short (char) arrays.
2271           __ tbz(to, 1, L_skip_align2);
2272           __ strh(value, Address(__ post(to, 2)));
2273           __ subw(count, count, 2 &gt;&gt; shift);
2274           __ bind(L_skip_align2);
2275           // Fallthrough
2276         case T_INT:
2277           // Align to 8 bytes, we know we are 4 byte aligned to start.
2278           __ tbz(to, 2, L_skip_align4);
2279           __ strw(value, Address(__ post(to, 4)));
2280           __ subw(count, count, 4 &gt;&gt; shift);
2281           __ bind(L_skip_align4);
2282           break;
2283         default: ShouldNotReachHere();
2284       }
2285     }
2286 
2287     //
2288     //  Fill large chunks
2289     //
2290     __ lsrw(cnt_words, count, 3 - shift); // number of words
2291     __ bfi(value, value, 32, 32);         // 32 bit -&gt; 64 bit
2292     __ subw(count, count, cnt_words, Assembler::LSL, 3 - shift);
2293     if (UseBlockZeroing) {
2294       Label non_block_zeroing, rest;
2295       // If the fill value is zero we can use the fast zero_words().
2296       __ cbnz(value, non_block_zeroing);
2297       __ mov(bz_base, to);
2298       __ add(to, to, cnt_words, Assembler::LSL, LogBytesPerWord);
2299       __ zero_words(bz_base, cnt_words);
2300       __ b(rest);
2301       __ bind(non_block_zeroing);
2302       __ fill_words(to, cnt_words, value);
2303       __ bind(rest);
2304     } else {
2305       __ fill_words(to, cnt_words, value);
2306     }
2307 
2308     // Remaining count is less than 8 bytes. Fill it by a single store.
2309     // Note that the total length is no less than 8 bytes.
2310     if (t == T_BYTE || t == T_SHORT) {
2311       Label L_exit1;
2312       __ cbzw(count, L_exit1);
2313       __ add(to, to, count, Assembler::LSL, shift); // points to the end
2314       __ str(value, Address(to, -8));    // overwrite some elements
2315       __ bind(L_exit1);
2316       __ leave();
2317       __ ret(lr);
2318     }
2319 
2320     // Handle copies less than 8 bytes.
2321     Label L_fill_2, L_fill_4, L_exit2;
2322     __ bind(L_fill_elements);
2323     switch (t) {
2324       case T_BYTE:
2325         __ tbz(count, 0, L_fill_2);
2326         __ strb(value, Address(__ post(to, 1)));
2327         __ bind(L_fill_2);
2328         __ tbz(count, 1, L_fill_4);
2329         __ strh(value, Address(__ post(to, 2)));
2330         __ bind(L_fill_4);
2331         __ tbz(count, 2, L_exit2);
2332         __ strw(value, Address(to));
2333         break;
2334       case T_SHORT:
2335         __ tbz(count, 0, L_fill_4);
2336         __ strh(value, Address(__ post(to, 2)));
2337         __ bind(L_fill_4);
2338         __ tbz(count, 1, L_exit2);
2339         __ strw(value, Address(to));
2340         break;
2341       case T_INT:
2342         __ cbzw(count, L_exit2);
2343         __ strw(value, Address(to));
2344         break;
2345       default: ShouldNotReachHere();
2346     }
2347     __ bind(L_exit2);
2348     __ leave();
2349     __ ret(lr);
2350     return start;
2351   }
2352 
2353   address generate_data_cache_writeback() {
2354     const Register line        = c_rarg0;  // address of line to write back
2355 
2356     __ align(CodeEntryAlignment);
2357 
2358     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback&quot;);
2359 
2360     address start = __ pc();
2361     __ enter();
2362     __ cache_wb(Address(line, 0));
2363     __ leave();
2364     __ ret(lr);
2365 
2366     return start;
2367   }
2368 
2369   address generate_data_cache_writeback_sync() {
2370     const Register is_pre     = c_rarg0;  // pre or post sync
2371 
2372     __ align(CodeEntryAlignment);
2373 
2374     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback_sync&quot;);
2375 
2376     // pre wbsync is a no-op
2377     // post wbsync translates to an sfence
2378 
2379     Label skip;
2380     address start = __ pc();
2381     __ enter();
2382     __ cbnz(is_pre, skip);
2383     __ cache_wbsync(false);
2384     __ bind(skip);
2385     __ leave();
2386     __ ret(lr);
2387 
2388     return start;
2389   }
2390 
2391   void generate_arraycopy_stubs() {
2392     address entry;
2393     address entry_jbyte_arraycopy;
2394     address entry_jshort_arraycopy;
2395     address entry_jint_arraycopy;
2396     address entry_oop_arraycopy;
2397     address entry_jlong_arraycopy;
2398     address entry_checkcast_arraycopy;
2399 
2400     generate_copy_longs(copy_f, r0, r1, rscratch2, copy_forwards);
2401     generate_copy_longs(copy_b, r0, r1, rscratch2, copy_backwards);
2402 
2403     StubRoutines::aarch64::_zero_blocks = generate_zero_blocks();
2404 
2405     //*** jbyte
2406     // Always need aligned and unaligned versions
2407     StubRoutines::_jbyte_disjoint_arraycopy         = generate_disjoint_byte_copy(false, &amp;entry,
2408                                                                                   &quot;jbyte_disjoint_arraycopy&quot;);
2409     StubRoutines::_jbyte_arraycopy                  = generate_conjoint_byte_copy(false, entry,
2410                                                                                   &amp;entry_jbyte_arraycopy,
2411                                                                                   &quot;jbyte_arraycopy&quot;);
2412     StubRoutines::_arrayof_jbyte_disjoint_arraycopy = generate_disjoint_byte_copy(true, &amp;entry,
2413                                                                                   &quot;arrayof_jbyte_disjoint_arraycopy&quot;);
2414     StubRoutines::_arrayof_jbyte_arraycopy          = generate_conjoint_byte_copy(true, entry, NULL,
2415                                                                                   &quot;arrayof_jbyte_arraycopy&quot;);
2416 
2417     //*** jshort
2418     // Always need aligned and unaligned versions
2419     StubRoutines::_jshort_disjoint_arraycopy         = generate_disjoint_short_copy(false, &amp;entry,
2420                                                                                     &quot;jshort_disjoint_arraycopy&quot;);
2421     StubRoutines::_jshort_arraycopy                  = generate_conjoint_short_copy(false, entry,
2422                                                                                     &amp;entry_jshort_arraycopy,
2423                                                                                     &quot;jshort_arraycopy&quot;);
2424     StubRoutines::_arrayof_jshort_disjoint_arraycopy = generate_disjoint_short_copy(true, &amp;entry,
2425                                                                                     &quot;arrayof_jshort_disjoint_arraycopy&quot;);
2426     StubRoutines::_arrayof_jshort_arraycopy          = generate_conjoint_short_copy(true, entry, NULL,
2427                                                                                     &quot;arrayof_jshort_arraycopy&quot;);
2428 
2429     //*** jint
2430     // Aligned versions
2431     StubRoutines::_arrayof_jint_disjoint_arraycopy = generate_disjoint_int_copy(true, &amp;entry,
2432                                                                                 &quot;arrayof_jint_disjoint_arraycopy&quot;);
2433     StubRoutines::_arrayof_jint_arraycopy          = generate_conjoint_int_copy(true, entry, &amp;entry_jint_arraycopy,
2434                                                                                 &quot;arrayof_jint_arraycopy&quot;);
2435     // In 64 bit we need both aligned and unaligned versions of jint arraycopy.
2436     // entry_jint_arraycopy always points to the unaligned version
2437     StubRoutines::_jint_disjoint_arraycopy         = generate_disjoint_int_copy(false, &amp;entry,
2438                                                                                 &quot;jint_disjoint_arraycopy&quot;);
2439     StubRoutines::_jint_arraycopy                  = generate_conjoint_int_copy(false, entry,
2440                                                                                 &amp;entry_jint_arraycopy,
2441                                                                                 &quot;jint_arraycopy&quot;);
2442 
2443     //*** jlong
2444     // It is always aligned
2445     StubRoutines::_arrayof_jlong_disjoint_arraycopy = generate_disjoint_long_copy(true, &amp;entry,
2446                                                                                   &quot;arrayof_jlong_disjoint_arraycopy&quot;);
2447     StubRoutines::_arrayof_jlong_arraycopy          = generate_conjoint_long_copy(true, entry, &amp;entry_jlong_arraycopy,
2448                                                                                   &quot;arrayof_jlong_arraycopy&quot;);
2449     StubRoutines::_jlong_disjoint_arraycopy         = StubRoutines::_arrayof_jlong_disjoint_arraycopy;
2450     StubRoutines::_jlong_arraycopy                  = StubRoutines::_arrayof_jlong_arraycopy;
2451 
2452     //*** oops
2453     {
2454       // With compressed oops we need unaligned versions; notice that
2455       // we overwrite entry_oop_arraycopy.
2456       bool aligned = !UseCompressedOops;
2457 
2458       StubRoutines::_arrayof_oop_disjoint_arraycopy
2459         = generate_disjoint_oop_copy(aligned, &amp;entry, &quot;arrayof_oop_disjoint_arraycopy&quot;,
2460                                      /*dest_uninitialized*/false);
2461       StubRoutines::_arrayof_oop_arraycopy
2462         = generate_conjoint_oop_copy(aligned, entry, &amp;entry_oop_arraycopy, &quot;arrayof_oop_arraycopy&quot;,
2463                                      /*dest_uninitialized*/false);
2464       // Aligned versions without pre-barriers
2465       StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit
2466         = generate_disjoint_oop_copy(aligned, &amp;entry, &quot;arrayof_oop_disjoint_arraycopy_uninit&quot;,
2467                                      /*dest_uninitialized*/true);
2468       StubRoutines::_arrayof_oop_arraycopy_uninit
2469         = generate_conjoint_oop_copy(aligned, entry, NULL, &quot;arrayof_oop_arraycopy_uninit&quot;,
2470                                      /*dest_uninitialized*/true);
2471     }
2472 
2473     StubRoutines::_oop_disjoint_arraycopy            = StubRoutines::_arrayof_oop_disjoint_arraycopy;
2474     StubRoutines::_oop_arraycopy                     = StubRoutines::_arrayof_oop_arraycopy;
2475     StubRoutines::_oop_disjoint_arraycopy_uninit     = StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit;
2476     StubRoutines::_oop_arraycopy_uninit              = StubRoutines::_arrayof_oop_arraycopy_uninit;
2477 
2478     StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(&quot;checkcast_arraycopy&quot;, &amp;entry_checkcast_arraycopy);
2479     StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(&quot;checkcast_arraycopy_uninit&quot;, NULL,
2480                                                                         /*dest_uninitialized*/true);
2481 
2482     StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(&quot;unsafe_arraycopy&quot;,
2483                                                               entry_jbyte_arraycopy,
2484                                                               entry_jshort_arraycopy,
2485                                                               entry_jint_arraycopy,
2486                                                               entry_jlong_arraycopy);
2487 
2488     StubRoutines::_generic_arraycopy   = generate_generic_copy(&quot;generic_arraycopy&quot;,
2489                                                                entry_jbyte_arraycopy,
2490                                                                entry_jshort_arraycopy,
2491                                                                entry_jint_arraycopy,
2492                                                                entry_oop_arraycopy,
2493                                                                entry_jlong_arraycopy,
2494                                                                entry_checkcast_arraycopy);
2495 
2496     StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, &quot;jbyte_fill&quot;);
2497     StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, &quot;jshort_fill&quot;);
2498     StubRoutines::_jint_fill = generate_fill(T_INT, false, &quot;jint_fill&quot;);
2499     StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, &quot;arrayof_jbyte_fill&quot;);
2500     StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, &quot;arrayof_jshort_fill&quot;);
2501     StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, &quot;arrayof_jint_fill&quot;);
2502   }
2503 
2504   void generate_math_stubs() { Unimplemented(); }
2505 
2506   // Arguments:
2507   //
2508   // Inputs:
2509   //   c_rarg0   - source byte array address
2510   //   c_rarg1   - destination byte array address
2511   //   c_rarg2   - K (key) in little endian int array
2512   //
2513   address generate_aescrypt_encryptBlock() {
2514     __ align(CodeEntryAlignment);
2515     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;aescrypt_encryptBlock&quot;);
2516 
2517     Label L_doLast;
2518 
2519     const Register from        = c_rarg0;  // source array address
2520     const Register to          = c_rarg1;  // destination array address
2521     const Register key         = c_rarg2;  // key array address
2522     const Register keylen      = rscratch1;
2523 
2524     address start = __ pc();
2525     __ enter();
2526 
2527     __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2528 
2529     __ ld1(v0, __ T16B, from); // get 16 bytes of input
2530 
2531     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2532     __ rev32(v1, __ T16B, v1);
2533     __ rev32(v2, __ T16B, v2);
2534     __ rev32(v3, __ T16B, v3);
2535     __ rev32(v4, __ T16B, v4);
2536     __ aese(v0, v1);
2537     __ aesmc(v0, v0);
2538     __ aese(v0, v2);
2539     __ aesmc(v0, v0);
2540     __ aese(v0, v3);
2541     __ aesmc(v0, v0);
2542     __ aese(v0, v4);
2543     __ aesmc(v0, v0);
2544 
2545     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2546     __ rev32(v1, __ T16B, v1);
2547     __ rev32(v2, __ T16B, v2);
2548     __ rev32(v3, __ T16B, v3);
2549     __ rev32(v4, __ T16B, v4);
2550     __ aese(v0, v1);
2551     __ aesmc(v0, v0);
2552     __ aese(v0, v2);
2553     __ aesmc(v0, v0);
2554     __ aese(v0, v3);
2555     __ aesmc(v0, v0);
2556     __ aese(v0, v4);
2557     __ aesmc(v0, v0);
2558 
2559     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2560     __ rev32(v1, __ T16B, v1);
2561     __ rev32(v2, __ T16B, v2);
2562 
2563     __ cmpw(keylen, 44);
2564     __ br(Assembler::EQ, L_doLast);
2565 
2566     __ aese(v0, v1);
2567     __ aesmc(v0, v0);
2568     __ aese(v0, v2);
2569     __ aesmc(v0, v0);
2570 
2571     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2572     __ rev32(v1, __ T16B, v1);
2573     __ rev32(v2, __ T16B, v2);
2574 
2575     __ cmpw(keylen, 52);
2576     __ br(Assembler::EQ, L_doLast);
2577 
2578     __ aese(v0, v1);
2579     __ aesmc(v0, v0);
2580     __ aese(v0, v2);
2581     __ aesmc(v0, v0);
2582 
2583     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2584     __ rev32(v1, __ T16B, v1);
2585     __ rev32(v2, __ T16B, v2);
2586 
2587     __ BIND(L_doLast);
2588 
2589     __ aese(v0, v1);
2590     __ aesmc(v0, v0);
2591     __ aese(v0, v2);
2592 
2593     __ ld1(v1, __ T16B, key);
2594     __ rev32(v1, __ T16B, v1);
2595     __ eor(v0, __ T16B, v0, v1);
2596 
2597     __ st1(v0, __ T16B, to);
2598 
2599     __ mov(r0, 0);
2600 
2601     __ leave();
2602     __ ret(lr);
2603 
2604     return start;
2605   }
2606 
2607   // Arguments:
2608   //
2609   // Inputs:
2610   //   c_rarg0   - source byte array address
2611   //   c_rarg1   - destination byte array address
2612   //   c_rarg2   - K (key) in little endian int array
2613   //
2614   address generate_aescrypt_decryptBlock() {
2615     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
2616     __ align(CodeEntryAlignment);
2617     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;aescrypt_decryptBlock&quot;);
2618     Label L_doLast;
2619 
2620     const Register from        = c_rarg0;  // source array address
2621     const Register to          = c_rarg1;  // destination array address
2622     const Register key         = c_rarg2;  // key array address
2623     const Register keylen      = rscratch1;
2624 
2625     address start = __ pc();
2626     __ enter(); // required for proper stackwalking of RuntimeStub frame
2627 
2628     __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2629 
2630     __ ld1(v0, __ T16B, from); // get 16 bytes of input
2631 
2632     __ ld1(v5, __ T16B, __ post(key, 16));
2633     __ rev32(v5, __ T16B, v5);
2634 
2635     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2636     __ rev32(v1, __ T16B, v1);
2637     __ rev32(v2, __ T16B, v2);
2638     __ rev32(v3, __ T16B, v3);
2639     __ rev32(v4, __ T16B, v4);
2640     __ aesd(v0, v1);
2641     __ aesimc(v0, v0);
2642     __ aesd(v0, v2);
2643     __ aesimc(v0, v0);
2644     __ aesd(v0, v3);
2645     __ aesimc(v0, v0);
2646     __ aesd(v0, v4);
2647     __ aesimc(v0, v0);
2648 
2649     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2650     __ rev32(v1, __ T16B, v1);
2651     __ rev32(v2, __ T16B, v2);
2652     __ rev32(v3, __ T16B, v3);
2653     __ rev32(v4, __ T16B, v4);
2654     __ aesd(v0, v1);
2655     __ aesimc(v0, v0);
2656     __ aesd(v0, v2);
2657     __ aesimc(v0, v0);
2658     __ aesd(v0, v3);
2659     __ aesimc(v0, v0);
2660     __ aesd(v0, v4);
2661     __ aesimc(v0, v0);
2662 
2663     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2664     __ rev32(v1, __ T16B, v1);
2665     __ rev32(v2, __ T16B, v2);
2666 
2667     __ cmpw(keylen, 44);
2668     __ br(Assembler::EQ, L_doLast);
2669 
2670     __ aesd(v0, v1);
2671     __ aesimc(v0, v0);
2672     __ aesd(v0, v2);
2673     __ aesimc(v0, v0);
2674 
2675     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2676     __ rev32(v1, __ T16B, v1);
2677     __ rev32(v2, __ T16B, v2);
2678 
2679     __ cmpw(keylen, 52);
2680     __ br(Assembler::EQ, L_doLast);
2681 
2682     __ aesd(v0, v1);
2683     __ aesimc(v0, v0);
2684     __ aesd(v0, v2);
2685     __ aesimc(v0, v0);
2686 
2687     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2688     __ rev32(v1, __ T16B, v1);
2689     __ rev32(v2, __ T16B, v2);
2690 
2691     __ BIND(L_doLast);
2692 
2693     __ aesd(v0, v1);
2694     __ aesimc(v0, v0);
2695     __ aesd(v0, v2);
2696 
2697     __ eor(v0, __ T16B, v0, v5);
2698 
2699     __ st1(v0, __ T16B, to);
2700 
2701     __ mov(r0, 0);
2702 
2703     __ leave();
2704     __ ret(lr);
2705 
2706     return start;
2707   }
2708 
2709   // Arguments:
2710   //
2711   // Inputs:
2712   //   c_rarg0   - source byte array address
2713   //   c_rarg1   - destination byte array address
2714   //   c_rarg2   - K (key) in little endian int array
2715   //   c_rarg3   - r vector byte array address
2716   //   c_rarg4   - input length
2717   //
2718   // Output:
2719   //   x0        - input length
2720   //
2721   address generate_cipherBlockChaining_encryptAESCrypt() {
2722     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
2723     __ align(CodeEntryAlignment);
2724     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_encryptAESCrypt&quot;);
2725 
2726     Label L_loadkeys_44, L_loadkeys_52, L_aes_loop, L_rounds_44, L_rounds_52;
2727 
2728     const Register from        = c_rarg0;  // source array address
2729     const Register to          = c_rarg1;  // destination array address
2730     const Register key         = c_rarg2;  // key array address
2731     const Register rvec        = c_rarg3;  // r byte array initialized from initvector array address
2732                                            // and left with the results of the last encryption block
2733     const Register len_reg     = c_rarg4;  // src len (must be multiple of blocksize 16)
2734     const Register keylen      = rscratch1;
2735 
2736     address start = __ pc();
2737 
2738       __ enter();
2739 
2740       __ movw(rscratch2, len_reg);
2741 
2742       __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2743 
2744       __ ld1(v0, __ T16B, rvec);
2745 
2746       __ cmpw(keylen, 52);
2747       __ br(Assembler::CC, L_loadkeys_44);
2748       __ br(Assembler::EQ, L_loadkeys_52);
2749 
2750       __ ld1(v17, v18, __ T16B, __ post(key, 32));
2751       __ rev32(v17, __ T16B, v17);
2752       __ rev32(v18, __ T16B, v18);
2753     __ BIND(L_loadkeys_52);
2754       __ ld1(v19, v20, __ T16B, __ post(key, 32));
2755       __ rev32(v19, __ T16B, v19);
2756       __ rev32(v20, __ T16B, v20);
2757     __ BIND(L_loadkeys_44);
2758       __ ld1(v21, v22, v23, v24, __ T16B, __ post(key, 64));
2759       __ rev32(v21, __ T16B, v21);
2760       __ rev32(v22, __ T16B, v22);
2761       __ rev32(v23, __ T16B, v23);
2762       __ rev32(v24, __ T16B, v24);
2763       __ ld1(v25, v26, v27, v28, __ T16B, __ post(key, 64));
2764       __ rev32(v25, __ T16B, v25);
2765       __ rev32(v26, __ T16B, v26);
2766       __ rev32(v27, __ T16B, v27);
2767       __ rev32(v28, __ T16B, v28);
2768       __ ld1(v29, v30, v31, __ T16B, key);
2769       __ rev32(v29, __ T16B, v29);
2770       __ rev32(v30, __ T16B, v30);
2771       __ rev32(v31, __ T16B, v31);
2772 
2773     __ BIND(L_aes_loop);
2774       __ ld1(v1, __ T16B, __ post(from, 16));
2775       __ eor(v0, __ T16B, v0, v1);
2776 
2777       __ br(Assembler::CC, L_rounds_44);
2778       __ br(Assembler::EQ, L_rounds_52);
2779 
2780       __ aese(v0, v17); __ aesmc(v0, v0);
2781       __ aese(v0, v18); __ aesmc(v0, v0);
2782     __ BIND(L_rounds_52);
2783       __ aese(v0, v19); __ aesmc(v0, v0);
2784       __ aese(v0, v20); __ aesmc(v0, v0);
2785     __ BIND(L_rounds_44);
2786       __ aese(v0, v21); __ aesmc(v0, v0);
2787       __ aese(v0, v22); __ aesmc(v0, v0);
2788       __ aese(v0, v23); __ aesmc(v0, v0);
2789       __ aese(v0, v24); __ aesmc(v0, v0);
2790       __ aese(v0, v25); __ aesmc(v0, v0);
2791       __ aese(v0, v26); __ aesmc(v0, v0);
2792       __ aese(v0, v27); __ aesmc(v0, v0);
2793       __ aese(v0, v28); __ aesmc(v0, v0);
2794       __ aese(v0, v29); __ aesmc(v0, v0);
2795       __ aese(v0, v30);
2796       __ eor(v0, __ T16B, v0, v31);
2797 
2798       __ st1(v0, __ T16B, __ post(to, 16));
2799 
2800       __ subw(len_reg, len_reg, 16);
2801       __ cbnzw(len_reg, L_aes_loop);
2802 
2803       __ st1(v0, __ T16B, rvec);
2804 
2805       __ mov(r0, rscratch2);
2806 
2807       __ leave();
2808       __ ret(lr);
2809 
2810       return start;
2811   }
2812 
2813   // Arguments:
2814   //
2815   // Inputs:
2816   //   c_rarg0   - source byte array address
2817   //   c_rarg1   - destination byte array address
2818   //   c_rarg2   - K (key) in little endian int array
2819   //   c_rarg3   - r vector byte array address
2820   //   c_rarg4   - input length
2821   //
2822   // Output:
2823   //   r0        - input length
2824   //
2825   address generate_cipherBlockChaining_decryptAESCrypt() {
2826     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
2827     __ align(CodeEntryAlignment);
2828     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_decryptAESCrypt&quot;);
2829 
2830     Label L_loadkeys_44, L_loadkeys_52, L_aes_loop, L_rounds_44, L_rounds_52;
2831 
2832     const Register from        = c_rarg0;  // source array address
2833     const Register to          = c_rarg1;  // destination array address
2834     const Register key         = c_rarg2;  // key array address
2835     const Register rvec        = c_rarg3;  // r byte array initialized from initvector array address
2836                                            // and left with the results of the last encryption block
2837     const Register len_reg     = c_rarg4;  // src len (must be multiple of blocksize 16)
2838     const Register keylen      = rscratch1;
2839 
2840     address start = __ pc();
2841 
2842       __ enter();
2843 
2844       __ movw(rscratch2, len_reg);
2845 
2846       __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2847 
2848       __ ld1(v2, __ T16B, rvec);
2849 
2850       __ ld1(v31, __ T16B, __ post(key, 16));
2851       __ rev32(v31, __ T16B, v31);
2852 
2853       __ cmpw(keylen, 52);
2854       __ br(Assembler::CC, L_loadkeys_44);
2855       __ br(Assembler::EQ, L_loadkeys_52);
2856 
2857       __ ld1(v17, v18, __ T16B, __ post(key, 32));
2858       __ rev32(v17, __ T16B, v17);
2859       __ rev32(v18, __ T16B, v18);
2860     __ BIND(L_loadkeys_52);
2861       __ ld1(v19, v20, __ T16B, __ post(key, 32));
2862       __ rev32(v19, __ T16B, v19);
2863       __ rev32(v20, __ T16B, v20);
2864     __ BIND(L_loadkeys_44);
2865       __ ld1(v21, v22, v23, v24, __ T16B, __ post(key, 64));
2866       __ rev32(v21, __ T16B, v21);
2867       __ rev32(v22, __ T16B, v22);
2868       __ rev32(v23, __ T16B, v23);
2869       __ rev32(v24, __ T16B, v24);
2870       __ ld1(v25, v26, v27, v28, __ T16B, __ post(key, 64));
2871       __ rev32(v25, __ T16B, v25);
2872       __ rev32(v26, __ T16B, v26);
2873       __ rev32(v27, __ T16B, v27);
2874       __ rev32(v28, __ T16B, v28);
2875       __ ld1(v29, v30, __ T16B, key);
2876       __ rev32(v29, __ T16B, v29);
2877       __ rev32(v30, __ T16B, v30);
2878 
2879     __ BIND(L_aes_loop);
2880       __ ld1(v0, __ T16B, __ post(from, 16));
2881       __ orr(v1, __ T16B, v0, v0);
2882 
2883       __ br(Assembler::CC, L_rounds_44);
2884       __ br(Assembler::EQ, L_rounds_52);
2885 
2886       __ aesd(v0, v17); __ aesimc(v0, v0);
2887       __ aesd(v0, v18); __ aesimc(v0, v0);
2888     __ BIND(L_rounds_52);
2889       __ aesd(v0, v19); __ aesimc(v0, v0);
2890       __ aesd(v0, v20); __ aesimc(v0, v0);
2891     __ BIND(L_rounds_44);
2892       __ aesd(v0, v21); __ aesimc(v0, v0);
2893       __ aesd(v0, v22); __ aesimc(v0, v0);
2894       __ aesd(v0, v23); __ aesimc(v0, v0);
2895       __ aesd(v0, v24); __ aesimc(v0, v0);
2896       __ aesd(v0, v25); __ aesimc(v0, v0);
2897       __ aesd(v0, v26); __ aesimc(v0, v0);
2898       __ aesd(v0, v27); __ aesimc(v0, v0);
2899       __ aesd(v0, v28); __ aesimc(v0, v0);
2900       __ aesd(v0, v29); __ aesimc(v0, v0);
2901       __ aesd(v0, v30);
2902       __ eor(v0, __ T16B, v0, v31);
2903       __ eor(v0, __ T16B, v0, v2);
2904 
2905       __ st1(v0, __ T16B, __ post(to, 16));
2906       __ orr(v2, __ T16B, v1, v1);
2907 
2908       __ subw(len_reg, len_reg, 16);
2909       __ cbnzw(len_reg, L_aes_loop);
2910 
2911       __ st1(v2, __ T16B, rvec);
2912 
2913       __ mov(r0, rscratch2);
2914 
2915       __ leave();
2916       __ ret(lr);
2917 
2918     return start;
2919   }
2920 
2921   // Arguments:
2922   //
2923   // Inputs:
2924   //   c_rarg0   - byte[]  source+offset
2925   //   c_rarg1   - int[]   SHA.state
2926   //   c_rarg2   - int     offset
2927   //   c_rarg3   - int     limit
2928   //
2929   address generate_sha1_implCompress(bool multi_block, const char *name) {
2930     __ align(CodeEntryAlignment);
2931     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2932     address start = __ pc();
2933 
2934     Register buf   = c_rarg0;
2935     Register state = c_rarg1;
2936     Register ofs   = c_rarg2;
2937     Register limit = c_rarg3;
2938 
2939     Label keys;
2940     Label sha1_loop;
2941 
2942     // load the keys into v0..v3
2943     __ adr(rscratch1, keys);
2944     __ ld4r(v0, v1, v2, v3, __ T4S, Address(rscratch1));
2945     // load 5 words state into v6, v7
2946     __ ldrq(v6, Address(state, 0));
2947     __ ldrs(v7, Address(state, 16));
2948 
2949 
2950     __ BIND(sha1_loop);
2951     // load 64 bytes of data into v16..v19
2952     __ ld1(v16, v17, v18, v19, __ T4S, multi_block ? __ post(buf, 64) : buf);
2953     __ rev32(v16, __ T16B, v16);
2954     __ rev32(v17, __ T16B, v17);
2955     __ rev32(v18, __ T16B, v18);
2956     __ rev32(v19, __ T16B, v19);
2957 
2958     // do the sha1
2959     __ addv(v4, __ T4S, v16, v0);
2960     __ orr(v20, __ T16B, v6, v6);
2961 
2962     FloatRegister d0 = v16;
2963     FloatRegister d1 = v17;
2964     FloatRegister d2 = v18;
2965     FloatRegister d3 = v19;
2966 
2967     for (int round = 0; round &lt; 20; round++) {
2968       FloatRegister tmp1 = (round &amp; 1) ? v4 : v5;
2969       FloatRegister tmp2 = (round &amp; 1) ? v21 : v22;
2970       FloatRegister tmp3 = round ? ((round &amp; 1) ? v22 : v21) : v7;
2971       FloatRegister tmp4 = (round &amp; 1) ? v5 : v4;
2972       FloatRegister key = (round &lt; 4) ? v0 : ((round &lt; 9) ? v1 : ((round &lt; 14) ? v2 : v3));
2973 
2974       if (round &lt; 16) __ sha1su0(d0, __ T4S, d1, d2);
2975       if (round &lt; 19) __ addv(tmp1, __ T4S, d1, key);
2976       __ sha1h(tmp2, __ T4S, v20);
2977       if (round &lt; 5)
2978         __ sha1c(v20, __ T4S, tmp3, tmp4);
2979       else if (round &lt; 10 || round &gt;= 15)
2980         __ sha1p(v20, __ T4S, tmp3, tmp4);
2981       else
2982         __ sha1m(v20, __ T4S, tmp3, tmp4);
2983       if (round &lt; 16) __ sha1su1(d0, __ T4S, d3);
2984 
2985       tmp1 = d0; d0 = d1; d1 = d2; d2 = d3; d3 = tmp1;
2986     }
2987 
2988     __ addv(v7, __ T2S, v7, v21);
2989     __ addv(v6, __ T4S, v6, v20);
2990 
2991     if (multi_block) {
2992       __ add(ofs, ofs, 64);
2993       __ cmp(ofs, limit);
2994       __ br(Assembler::LE, sha1_loop);
2995       __ mov(c_rarg0, ofs); // return ofs
2996     }
2997 
2998     __ strq(v6, Address(state, 0));
2999     __ strs(v7, Address(state, 16));
3000 
3001     __ ret(lr);
3002 
3003     __ bind(keys);
3004     __ emit_int32(0x5a827999);
3005     __ emit_int32(0x6ed9eba1);
3006     __ emit_int32(0x8f1bbcdc);
3007     __ emit_int32(0xca62c1d6);
3008 
3009     return start;
3010   }
3011 
3012 
3013   // Arguments:
3014   //
3015   // Inputs:
3016   //   c_rarg0   - byte[]  source+offset
3017   //   c_rarg1   - int[]   SHA.state
3018   //   c_rarg2   - int     offset
3019   //   c_rarg3   - int     limit
3020   //
3021   address generate_sha256_implCompress(bool multi_block, const char *name) {
3022     static const uint32_t round_consts[64] = {
3023       0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5,
3024       0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,
3025       0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,
3026       0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,
3027       0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc,
3028       0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,
3029       0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7,
3030       0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,
3031       0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,
3032       0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,
3033       0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3,
3034       0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,
3035       0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5,
3036       0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,
3037       0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,
3038       0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2,
3039     };
3040     __ align(CodeEntryAlignment);
3041     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3042     address start = __ pc();
3043 
3044     Register buf   = c_rarg0;
3045     Register state = c_rarg1;
3046     Register ofs   = c_rarg2;
3047     Register limit = c_rarg3;
3048 
3049     Label sha1_loop;
3050 
3051     __ stpd(v8, v9, __ pre(sp, -32));
3052     __ stpd(v10, v11, Address(sp, 16));
3053 
3054 // dga == v0
3055 // dgb == v1
3056 // dg0 == v2
3057 // dg1 == v3
3058 // dg2 == v4
3059 // t0 == v6
3060 // t1 == v7
3061 
3062     // load 16 keys to v16..v31
3063     __ lea(rscratch1, ExternalAddress((address)round_consts));
3064     __ ld1(v16, v17, v18, v19, __ T4S, __ post(rscratch1, 64));
3065     __ ld1(v20, v21, v22, v23, __ T4S, __ post(rscratch1, 64));
3066     __ ld1(v24, v25, v26, v27, __ T4S, __ post(rscratch1, 64));
3067     __ ld1(v28, v29, v30, v31, __ T4S, rscratch1);
3068 
3069     // load 8 words (256 bits) state
3070     __ ldpq(v0, v1, state);
3071 
3072     __ BIND(sha1_loop);
3073     // load 64 bytes of data into v8..v11
3074     __ ld1(v8, v9, v10, v11, __ T4S, multi_block ? __ post(buf, 64) : buf);
3075     __ rev32(v8, __ T16B, v8);
3076     __ rev32(v9, __ T16B, v9);
3077     __ rev32(v10, __ T16B, v10);
3078     __ rev32(v11, __ T16B, v11);
3079 
3080     __ addv(v6, __ T4S, v8, v16);
3081     __ orr(v2, __ T16B, v0, v0);
3082     __ orr(v3, __ T16B, v1, v1);
3083 
3084     FloatRegister d0 = v8;
3085     FloatRegister d1 = v9;
3086     FloatRegister d2 = v10;
3087     FloatRegister d3 = v11;
3088 
3089 
3090     for (int round = 0; round &lt; 16; round++) {
3091       FloatRegister tmp1 = (round &amp; 1) ? v6 : v7;
3092       FloatRegister tmp2 = (round &amp; 1) ? v7 : v6;
3093       FloatRegister tmp3 = (round &amp; 1) ? v2 : v4;
3094       FloatRegister tmp4 = (round &amp; 1) ? v4 : v2;
3095 
3096       if (round &lt; 12) __ sha256su0(d0, __ T4S, d1);
3097        __ orr(v4, __ T16B, v2, v2);
3098       if (round &lt; 15)
3099         __ addv(tmp1, __ T4S, d1, as_FloatRegister(round + 17));
3100       __ sha256h(v2, __ T4S, v3, tmp2);
3101       __ sha256h2(v3, __ T4S, v4, tmp2);
3102       if (round &lt; 12) __ sha256su1(d0, __ T4S, d2, d3);
3103 
3104       tmp1 = d0; d0 = d1; d1 = d2; d2 = d3; d3 = tmp1;
3105     }
3106 
3107     __ addv(v0, __ T4S, v0, v2);
3108     __ addv(v1, __ T4S, v1, v3);
3109 
3110     if (multi_block) {
3111       __ add(ofs, ofs, 64);
3112       __ cmp(ofs, limit);
3113       __ br(Assembler::LE, sha1_loop);
3114       __ mov(c_rarg0, ofs); // return ofs
3115     }
3116 
3117     __ ldpd(v10, v11, Address(sp, 16));
3118     __ ldpd(v8, v9, __ post(sp, 32));
3119 
3120     __ stpq(v0, v1, state);
3121 
3122     __ ret(lr);
3123 
3124     return start;
3125   }
3126 
3127   // Safefetch stubs.
3128   void generate_safefetch(const char* name, int size, address* entry,
3129                           address* fault_pc, address* continuation_pc) {
3130     // safefetch signatures:
3131     //   int      SafeFetch32(int*      adr, int      errValue);
3132     //   intptr_t SafeFetchN (intptr_t* adr, intptr_t errValue);
3133     //
3134     // arguments:
3135     //   c_rarg0 = adr
3136     //   c_rarg1 = errValue
3137     //
3138     // result:
3139     //   PPC_RET  = *adr or errValue
3140 
3141     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3142 
3143     // Entry point, pc or function descriptor.
3144     *entry = __ pc();
3145 
3146     // Load *adr into c_rarg1, may fault.
3147     *fault_pc = __ pc();
3148     switch (size) {
3149       case 4:
3150         // int32_t
3151         __ ldrw(c_rarg1, Address(c_rarg0, 0));
3152         break;
3153       case 8:
3154         // int64_t
3155         __ ldr(c_rarg1, Address(c_rarg0, 0));
3156         break;
3157       default:
3158         ShouldNotReachHere();
3159     }
3160 
3161     // return errValue or *adr
3162     *continuation_pc = __ pc();
3163     __ mov(r0, c_rarg1);
3164     __ ret(lr);
3165   }
3166 
3167   /**
3168    *  Arguments:
3169    *
3170    * Inputs:
3171    *   c_rarg0   - int crc
3172    *   c_rarg1   - byte* buf
3173    *   c_rarg2   - int length
3174    *
3175    * Ouput:
3176    *       rax   - int crc result
3177    */
3178   address generate_updateBytesCRC32() {
3179     assert(UseCRC32Intrinsics, &quot;what are we doing here?&quot;);
3180 
3181     __ align(CodeEntryAlignment);
3182     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesCRC32&quot;);
3183 
3184     address start = __ pc();
3185 
3186     const Register crc   = c_rarg0;  // crc
3187     const Register buf   = c_rarg1;  // source java byte array address
3188     const Register len   = c_rarg2;  // length
3189     const Register table0 = c_rarg3; // crc_table address
3190     const Register table1 = c_rarg4;
3191     const Register table2 = c_rarg5;
3192     const Register table3 = c_rarg6;
3193     const Register tmp3 = c_rarg7;
3194 
3195     BLOCK_COMMENT(&quot;Entry:&quot;);
3196     __ enter(); // required for proper stackwalking of RuntimeStub frame
3197 
3198     __ kernel_crc32(crc, buf, len,
3199               table0, table1, table2, table3, rscratch1, rscratch2, tmp3);
3200 
3201     __ leave(); // required for proper stackwalking of RuntimeStub frame
3202     __ ret(lr);
3203 
3204     return start;
3205   }
3206 
3207   /**
3208    *  Arguments:
3209    *
3210    * Inputs:
3211    *   c_rarg0   - int crc
3212    *   c_rarg1   - byte* buf
3213    *   c_rarg2   - int length
3214    *   c_rarg3   - int* table
3215    *
3216    * Ouput:
3217    *       r0   - int crc result
3218    */
3219   address generate_updateBytesCRC32C() {
3220     assert(UseCRC32CIntrinsics, &quot;what are we doing here?&quot;);
3221 
3222     __ align(CodeEntryAlignment);
3223     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesCRC32C&quot;);
3224 
3225     address start = __ pc();
3226 
3227     const Register crc   = c_rarg0;  // crc
3228     const Register buf   = c_rarg1;  // source java byte array address
3229     const Register len   = c_rarg2;  // length
3230     const Register table0 = c_rarg3; // crc_table address
3231     const Register table1 = c_rarg4;
3232     const Register table2 = c_rarg5;
3233     const Register table3 = c_rarg6;
3234     const Register tmp3 = c_rarg7;
3235 
3236     BLOCK_COMMENT(&quot;Entry:&quot;);
3237     __ enter(); // required for proper stackwalking of RuntimeStub frame
3238 
3239     __ kernel_crc32c(crc, buf, len,
3240               table0, table1, table2, table3, rscratch1, rscratch2, tmp3);
3241 
3242     __ leave(); // required for proper stackwalking of RuntimeStub frame
3243     __ ret(lr);
3244 
3245     return start;
3246   }
3247 
3248   /***
3249    *  Arguments:
3250    *
3251    *  Inputs:
3252    *   c_rarg0   - int   adler
3253    *   c_rarg1   - byte* buff
3254    *   c_rarg2   - int   len
3255    *
3256    * Output:
3257    *   c_rarg0   - int adler result
3258    */
3259   address generate_updateBytesAdler32() {
3260     __ align(CodeEntryAlignment);
3261     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesAdler32&quot;);
3262     address start = __ pc();
3263 
3264     Label L_simple_by1_loop, L_nmax, L_nmax_loop, L_by16, L_by16_loop, L_by1_loop, L_do_mod, L_combine, L_by1;
3265 
3266     // Aliases
3267     Register adler  = c_rarg0;
3268     Register s1     = c_rarg0;
3269     Register s2     = c_rarg3;
3270     Register buff   = c_rarg1;
3271     Register len    = c_rarg2;
3272     Register nmax  = r4;
3273     Register base  = r5;
3274     Register count = r6;
3275     Register temp0 = rscratch1;
3276     Register temp1 = rscratch2;
3277     FloatRegister vbytes = v0;
3278     FloatRegister vs1acc = v1;
3279     FloatRegister vs2acc = v2;
3280     FloatRegister vtable = v3;
3281 
3282     // Max number of bytes we can process before having to take the mod
3283     // 0x15B0 is 5552 in decimal, the largest n such that 255n(n+1)/2 + (n+1)(BASE-1) &lt;= 2^32-1
<a name="25" id="anc25"></a><span class="line-modified">3284     uint64_t BASE = 0xfff1;</span>
<span class="line-modified">3285     uint64_t NMAX = 0x15B0;</span>
3286 
3287     __ mov(base, BASE);
3288     __ mov(nmax, NMAX);
3289 
3290     // Load accumulation coefficients for the upper 16 bits
3291     __ lea(temp0, ExternalAddress((address) StubRoutines::aarch64::_adler_table));
3292     __ ld1(vtable, __ T16B, Address(temp0));
3293 
3294     // s1 is initialized to the lower 16 bits of adler
3295     // s2 is initialized to the upper 16 bits of adler
3296     __ ubfx(s2, adler, 16, 16);  // s2 = ((adler &gt;&gt; 16) &amp; 0xffff)
3297     __ uxth(s1, adler);          // s1 = (adler &amp; 0xffff)
3298 
3299     // The pipelined loop needs at least 16 elements for 1 iteration
3300     // It does check this, but it is more effective to skip to the cleanup loop
3301     __ cmp(len, (u1)16);
3302     __ br(Assembler::HS, L_nmax);
3303     __ cbz(len, L_combine);
3304 
3305     __ bind(L_simple_by1_loop);
3306     __ ldrb(temp0, Address(__ post(buff, 1)));
3307     __ add(s1, s1, temp0);
3308     __ add(s2, s2, s1);
3309     __ subs(len, len, 1);
3310     __ br(Assembler::HI, L_simple_by1_loop);
3311 
3312     // s1 = s1 % BASE
3313     __ subs(temp0, s1, base);
3314     __ csel(s1, temp0, s1, Assembler::HS);
3315 
3316     // s2 = s2 % BASE
3317     __ lsr(temp0, s2, 16);
3318     __ lsl(temp1, temp0, 4);
3319     __ sub(temp1, temp1, temp0);
3320     __ add(s2, temp1, s2, ext::uxth);
3321 
3322     __ subs(temp0, s2, base);
3323     __ csel(s2, temp0, s2, Assembler::HS);
3324 
3325     __ b(L_combine);
3326 
3327     __ bind(L_nmax);
3328     __ subs(len, len, nmax);
3329     __ sub(count, nmax, 16);
3330     __ br(Assembler::LO, L_by16);
3331 
3332     __ bind(L_nmax_loop);
3333 
3334     generate_updateBytesAdler32_accum(s1, s2, buff, temp0, temp1,
3335                                       vbytes, vs1acc, vs2acc, vtable);
3336 
3337     __ subs(count, count, 16);
3338     __ br(Assembler::HS, L_nmax_loop);
3339 
3340     // s1 = s1 % BASE
3341     __ lsr(temp0, s1, 16);
3342     __ lsl(temp1, temp0, 4);
3343     __ sub(temp1, temp1, temp0);
3344     __ add(temp1, temp1, s1, ext::uxth);
3345 
3346     __ lsr(temp0, temp1, 16);
3347     __ lsl(s1, temp0, 4);
3348     __ sub(s1, s1, temp0);
3349     __ add(s1, s1, temp1, ext:: uxth);
3350 
3351     __ subs(temp0, s1, base);
3352     __ csel(s1, temp0, s1, Assembler::HS);
3353 
3354     // s2 = s2 % BASE
3355     __ lsr(temp0, s2, 16);
3356     __ lsl(temp1, temp0, 4);
3357     __ sub(temp1, temp1, temp0);
3358     __ add(temp1, temp1, s2, ext::uxth);
3359 
3360     __ lsr(temp0, temp1, 16);
3361     __ lsl(s2, temp0, 4);
3362     __ sub(s2, s2, temp0);
3363     __ add(s2, s2, temp1, ext:: uxth);
3364 
3365     __ subs(temp0, s2, base);
3366     __ csel(s2, temp0, s2, Assembler::HS);
3367 
3368     __ subs(len, len, nmax);
3369     __ sub(count, nmax, 16);
3370     __ br(Assembler::HS, L_nmax_loop);
3371 
3372     __ bind(L_by16);
3373     __ adds(len, len, count);
3374     __ br(Assembler::LO, L_by1);
3375 
3376     __ bind(L_by16_loop);
3377 
3378     generate_updateBytesAdler32_accum(s1, s2, buff, temp0, temp1,
3379                                       vbytes, vs1acc, vs2acc, vtable);
3380 
3381     __ subs(len, len, 16);
3382     __ br(Assembler::HS, L_by16_loop);
3383 
3384     __ bind(L_by1);
3385     __ adds(len, len, 15);
3386     __ br(Assembler::LO, L_do_mod);
3387 
3388     __ bind(L_by1_loop);
3389     __ ldrb(temp0, Address(__ post(buff, 1)));
3390     __ add(s1, temp0, s1);
3391     __ add(s2, s2, s1);
3392     __ subs(len, len, 1);
3393     __ br(Assembler::HS, L_by1_loop);
3394 
3395     __ bind(L_do_mod);
3396     // s1 = s1 % BASE
3397     __ lsr(temp0, s1, 16);
3398     __ lsl(temp1, temp0, 4);
3399     __ sub(temp1, temp1, temp0);
3400     __ add(temp1, temp1, s1, ext::uxth);
3401 
3402     __ lsr(temp0, temp1, 16);
3403     __ lsl(s1, temp0, 4);
3404     __ sub(s1, s1, temp0);
3405     __ add(s1, s1, temp1, ext:: uxth);
3406 
3407     __ subs(temp0, s1, base);
3408     __ csel(s1, temp0, s1, Assembler::HS);
3409 
3410     // s2 = s2 % BASE
3411     __ lsr(temp0, s2, 16);
3412     __ lsl(temp1, temp0, 4);
3413     __ sub(temp1, temp1, temp0);
3414     __ add(temp1, temp1, s2, ext::uxth);
3415 
3416     __ lsr(temp0, temp1, 16);
3417     __ lsl(s2, temp0, 4);
3418     __ sub(s2, s2, temp0);
3419     __ add(s2, s2, temp1, ext:: uxth);
3420 
3421     __ subs(temp0, s2, base);
3422     __ csel(s2, temp0, s2, Assembler::HS);
3423 
3424     // Combine lower bits and higher bits
3425     __ bind(L_combine);
3426     __ orr(s1, s1, s2, Assembler::LSL, 16); // adler = s1 | (s2 &lt;&lt; 16)
3427 
3428     __ ret(lr);
3429 
3430     return start;
3431   }
3432 
3433   void generate_updateBytesAdler32_accum(Register s1, Register s2, Register buff,
3434           Register temp0, Register temp1, FloatRegister vbytes,
3435           FloatRegister vs1acc, FloatRegister vs2acc, FloatRegister vtable) {
3436     // Below is a vectorized implementation of updating s1 and s2 for 16 bytes.
3437     // We use b1, b2, ..., b16 to denote the 16 bytes loaded in each iteration.
3438     // In non-vectorized code, we update s1 and s2 as:
3439     //   s1 &lt;- s1 + b1
3440     //   s2 &lt;- s2 + s1
3441     //   s1 &lt;- s1 + b2
3442     //   s2 &lt;- s2 + b1
3443     //   ...
3444     //   s1 &lt;- s1 + b16
3445     //   s2 &lt;- s2 + s1
3446     // Putting above assignments together, we have:
3447     //   s1_new = s1 + b1 + b2 + ... + b16
3448     //   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b16)
3449     //          = s2 + s1 * 16 + (b1 * 16 + b2 * 15 + ... + b16 * 1)
3450     //          = s2 + s1 * 16 + (b1, b2, ... b16) dot (16, 15, ... 1)
3451     __ ld1(vbytes, __ T16B, Address(__ post(buff, 16)));
3452 
3453     // s2 = s2 + s1 * 16
3454     __ add(s2, s2, s1, Assembler::LSL, 4);
3455 
3456     // vs1acc = b1 + b2 + b3 + ... + b16
3457     // vs2acc = (b1 * 16) + (b2 * 15) + (b3 * 14) + ... + (b16 * 1)
3458     __ umullv(vs2acc, __ T8B, vtable, vbytes);
3459     __ umlalv(vs2acc, __ T16B, vtable, vbytes);
3460     __ uaddlv(vs1acc, __ T16B, vbytes);
3461     __ uaddlv(vs2acc, __ T8H, vs2acc);
3462 
3463     // s1 = s1 + vs1acc, s2 = s2 + vs2acc
3464     __ fmovd(temp0, vs1acc);
3465     __ fmovd(temp1, vs2acc);
3466     __ add(s1, s1, temp0);
3467     __ add(s2, s2, temp1);
3468   }
3469 
3470   /**
3471    *  Arguments:
3472    *
3473    *  Input:
3474    *    c_rarg0   - x address
3475    *    c_rarg1   - x length
3476    *    c_rarg2   - y address
3477    *    c_rarg3   - y lenth
3478    *    c_rarg4   - z address
3479    *    c_rarg5   - z length
3480    */
3481   address generate_multiplyToLen() {
3482     __ align(CodeEntryAlignment);
3483     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;multiplyToLen&quot;);
3484 
3485     address start = __ pc();
3486     const Register x     = r0;
3487     const Register xlen  = r1;
3488     const Register y     = r2;
3489     const Register ylen  = r3;
3490     const Register z     = r4;
3491     const Register zlen  = r5;
3492 
3493     const Register tmp1  = r10;
3494     const Register tmp2  = r11;
3495     const Register tmp3  = r12;
3496     const Register tmp4  = r13;
3497     const Register tmp5  = r14;
3498     const Register tmp6  = r15;
3499     const Register tmp7  = r16;
3500 
3501     BLOCK_COMMENT(&quot;Entry:&quot;);
3502     __ enter(); // required for proper stackwalking of RuntimeStub frame
3503     __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
3504     __ leave(); // required for proper stackwalking of RuntimeStub frame
3505     __ ret(lr);
3506 
3507     return start;
3508   }
3509 
3510   address generate_squareToLen() {
3511     // squareToLen algorithm for sizes 1..127 described in java code works
3512     // faster than multiply_to_len on some CPUs and slower on others, but
3513     // multiply_to_len shows a bit better overall results
3514     __ align(CodeEntryAlignment);
3515     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;squareToLen&quot;);
3516     address start = __ pc();
3517 
3518     const Register x     = r0;
3519     const Register xlen  = r1;
3520     const Register z     = r2;
3521     const Register zlen  = r3;
3522     const Register y     = r4; // == x
3523     const Register ylen  = r5; // == xlen
3524 
3525     const Register tmp1  = r10;
3526     const Register tmp2  = r11;
3527     const Register tmp3  = r12;
3528     const Register tmp4  = r13;
3529     const Register tmp5  = r14;
3530     const Register tmp6  = r15;
3531     const Register tmp7  = r16;
3532 
3533     RegSet spilled_regs = RegSet::of(y, ylen);
3534     BLOCK_COMMENT(&quot;Entry:&quot;);
3535     __ enter();
3536     __ push(spilled_regs, sp);
3537     __ mov(y, x);
3538     __ mov(ylen, xlen);
3539     __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
3540     __ pop(spilled_regs, sp);
3541     __ leave();
3542     __ ret(lr);
3543     return start;
3544   }
3545 
3546   address generate_mulAdd() {
3547     __ align(CodeEntryAlignment);
3548     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;mulAdd&quot;);
3549 
3550     address start = __ pc();
3551 
3552     const Register out     = r0;
3553     const Register in      = r1;
3554     const Register offset  = r2;
3555     const Register len     = r3;
3556     const Register k       = r4;
3557 
3558     BLOCK_COMMENT(&quot;Entry:&quot;);
3559     __ enter();
3560     __ mul_add(out, in, offset, len, k);
3561     __ leave();
3562     __ ret(lr);
3563 
3564     return start;
3565   }
3566 
3567   void ghash_multiply(FloatRegister result_lo, FloatRegister result_hi,
3568                       FloatRegister a, FloatRegister b, FloatRegister a1_xor_a0,
3569                       FloatRegister tmp1, FloatRegister tmp2, FloatRegister tmp3, FloatRegister tmp4) {
3570     // Karatsuba multiplication performs a 128*128 -&gt; 256-bit
3571     // multiplication in three 128-bit multiplications and a few
3572     // additions.
3573     //
3574     // (C1:C0) = A1*B1, (D1:D0) = A0*B0, (E1:E0) = (A0+A1)(B0+B1)
3575     // (A1:A0)(B1:B0) = C1:(C0+C1+D1+E1):(D1+C0+D0+E0):D0
3576     //
3577     // Inputs:
3578     //
3579     // A0 in a.d[0]     (subkey)
3580     // A1 in a.d[1]
3581     // (A1+A0) in a1_xor_a0.d[0]
3582     //
3583     // B0 in b.d[0]     (state)
3584     // B1 in b.d[1]
3585 
3586     __ ext(tmp1, __ T16B, b, b, 0x08);
3587     __ pmull2(result_hi, __ T1Q, b, a, __ T2D);  // A1*B1
3588     __ eor(tmp1, __ T16B, tmp1, b);            // (B1+B0)
3589     __ pmull(result_lo,  __ T1Q, b, a, __ T1D);  // A0*B0
3590     __ pmull(tmp2, __ T1Q, tmp1, a1_xor_a0, __ T1D); // (A1+A0)(B1+B0)
3591 
3592     __ ext(tmp4, __ T16B, result_lo, result_hi, 0x08);
3593     __ eor(tmp3, __ T16B, result_hi, result_lo); // A1*B1+A0*B0
3594     __ eor(tmp2, __ T16B, tmp2, tmp4);
3595     __ eor(tmp2, __ T16B, tmp2, tmp3);
3596 
3597     // Register pair &lt;result_hi:result_lo&gt; holds the result of carry-less multiplication
3598     __ ins(result_hi, __ D, tmp2, 0, 1);
3599     __ ins(result_lo, __ D, tmp2, 1, 0);
3600   }
3601 
3602   void ghash_reduce(FloatRegister result, FloatRegister lo, FloatRegister hi,
3603                     FloatRegister p, FloatRegister z, FloatRegister t1) {
3604     const FloatRegister t0 = result;
3605 
3606     // The GCM field polynomial f is z^128 + p(z), where p =
3607     // z^7+z^2+z+1.
3608     //
3609     //    z^128 === -p(z)  (mod (z^128 + p(z)))
3610     //
3611     // so, given that the product we&#39;re reducing is
3612     //    a == lo + hi * z^128
3613     // substituting,
3614     //      === lo - hi * p(z)  (mod (z^128 + p(z)))
3615     //
3616     // we reduce by multiplying hi by p(z) and subtracting the result
3617     // from (i.e. XORing it with) lo.  Because p has no nonzero high
3618     // bits we can do this with two 64-bit multiplications, lo*p and
3619     // hi*p.
3620 
3621     __ pmull2(t0, __ T1Q, hi, p, __ T2D);
3622     __ ext(t1, __ T16B, t0, z, 8);
3623     __ eor(hi, __ T16B, hi, t1);
3624     __ ext(t1, __ T16B, z, t0, 8);
3625     __ eor(lo, __ T16B, lo, t1);
3626     __ pmull(t0, __ T1Q, hi, p, __ T1D);
3627     __ eor(result, __ T16B, lo, t0);
3628   }
3629 
3630   address generate_has_negatives(address &amp;has_negatives_long) {
3631     const u1 large_loop_size = 64;
3632     const uint64_t UPPER_BIT_MASK=0x8080808080808080;
3633     int dcache_line = VM_Version::dcache_line_size();
3634 
3635     Register ary1 = r1, len = r2, result = r0;
3636 
3637     __ align(CodeEntryAlignment);
3638 
3639     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;has_negatives&quot;);
3640 
3641     address entry = __ pc();
3642 
3643     __ enter();
3644 
3645   Label RET_TRUE, RET_TRUE_NO_POP, RET_FALSE, ALIGNED, LOOP16, CHECK_16, DONE,
3646         LARGE_LOOP, POST_LOOP16, LEN_OVER_15, LEN_OVER_8, POST_LOOP16_LOAD_TAIL;
3647 
3648   __ cmp(len, (u1)15);
3649   __ br(Assembler::GT, LEN_OVER_15);
3650   // The only case when execution falls into this code is when pointer is near
3651   // the end of memory page and we have to avoid reading next page
3652   __ add(ary1, ary1, len);
3653   __ subs(len, len, 8);
3654   __ br(Assembler::GT, LEN_OVER_8);
3655   __ ldr(rscratch2, Address(ary1, -8));
3656   __ sub(rscratch1, zr, len, __ LSL, 3);  // LSL 3 is to get bits from bytes.
3657   __ lsrv(rscratch2, rscratch2, rscratch1);
3658   __ tst(rscratch2, UPPER_BIT_MASK);
3659   __ cset(result, Assembler::NE);
3660   __ leave();
3661   __ ret(lr);
3662   __ bind(LEN_OVER_8);
3663   __ ldp(rscratch1, rscratch2, Address(ary1, -16));
3664   __ sub(len, len, 8); // no data dep., then sub can be executed while loading
3665   __ tst(rscratch2, UPPER_BIT_MASK);
3666   __ br(Assembler::NE, RET_TRUE_NO_POP);
3667   __ sub(rscratch2, zr, len, __ LSL, 3); // LSL 3 is to get bits from bytes
3668   __ lsrv(rscratch1, rscratch1, rscratch2);
3669   __ tst(rscratch1, UPPER_BIT_MASK);
3670   __ cset(result, Assembler::NE);
3671   __ leave();
3672   __ ret(lr);
3673 
3674   Register tmp1 = r3, tmp2 = r4, tmp3 = r5, tmp4 = r6, tmp5 = r7, tmp6 = r10;
3675   const RegSet spilled_regs = RegSet::range(tmp1, tmp5) + tmp6;
3676 
3677   has_negatives_long = __ pc(); // 2nd entry point
3678 
3679   __ enter();
3680 
3681   __ bind(LEN_OVER_15);
3682     __ push(spilled_regs, sp);
3683     __ andr(rscratch2, ary1, 15); // check pointer for 16-byte alignment
3684     __ cbz(rscratch2, ALIGNED);
3685     __ ldp(tmp6, tmp1, Address(ary1));
3686     __ mov(tmp5, 16);
3687     __ sub(rscratch1, tmp5, rscratch2); // amount of bytes until aligned address
3688     __ add(ary1, ary1, rscratch1);
3689     __ sub(len, len, rscratch1);
3690     __ orr(tmp6, tmp6, tmp1);
3691     __ tst(tmp6, UPPER_BIT_MASK);
3692     __ br(Assembler::NE, RET_TRUE);
3693 
3694   __ bind(ALIGNED);
3695     __ cmp(len, large_loop_size);
3696     __ br(Assembler::LT, CHECK_16);
3697     // Perform 16-byte load as early return in pre-loop to handle situation
3698     // when initially aligned large array has negative values at starting bytes,
3699     // so LARGE_LOOP would do 4 reads instead of 1 (in worst case), which is
3700     // slower. Cases with negative bytes further ahead won&#39;t be affected that
3701     // much. In fact, it&#39;ll be faster due to early loads, less instructions and
3702     // less branches in LARGE_LOOP.
3703     __ ldp(tmp6, tmp1, Address(__ post(ary1, 16)));
3704     __ sub(len, len, 16);
3705     __ orr(tmp6, tmp6, tmp1);
3706     __ tst(tmp6, UPPER_BIT_MASK);
3707     __ br(Assembler::NE, RET_TRUE);
3708     __ cmp(len, large_loop_size);
3709     __ br(Assembler::LT, CHECK_16);
3710 
3711     if (SoftwarePrefetchHintDistance &gt;= 0
3712         &amp;&amp; SoftwarePrefetchHintDistance &gt;= dcache_line) {
3713       // initial prefetch
3714       __ prfm(Address(ary1, SoftwarePrefetchHintDistance - dcache_line));
3715     }
3716   __ bind(LARGE_LOOP);
3717     if (SoftwarePrefetchHintDistance &gt;= 0) {
3718       __ prfm(Address(ary1, SoftwarePrefetchHintDistance));
3719     }
3720     // Issue load instructions first, since it can save few CPU/MEM cycles, also
3721     // instead of 4 triples of &quot;orr(...), addr(...);cbnz(...);&quot; (for each ldp)
3722     // better generate 7 * orr(...) + 1 andr(...) + 1 cbnz(...) which saves 3
3723     // instructions per cycle and have less branches, but this approach disables
3724     // early return, thus, all 64 bytes are loaded and checked every time.
3725     __ ldp(tmp2, tmp3, Address(ary1));
3726     __ ldp(tmp4, tmp5, Address(ary1, 16));
3727     __ ldp(rscratch1, rscratch2, Address(ary1, 32));
3728     __ ldp(tmp6, tmp1, Address(ary1, 48));
3729     __ add(ary1, ary1, large_loop_size);
3730     __ sub(len, len, large_loop_size);
3731     __ orr(tmp2, tmp2, tmp3);
3732     __ orr(tmp4, tmp4, tmp5);
3733     __ orr(rscratch1, rscratch1, rscratch2);
3734     __ orr(tmp6, tmp6, tmp1);
3735     __ orr(tmp2, tmp2, tmp4);
3736     __ orr(rscratch1, rscratch1, tmp6);
3737     __ orr(tmp2, tmp2, rscratch1);
3738     __ tst(tmp2, UPPER_BIT_MASK);
3739     __ br(Assembler::NE, RET_TRUE);
3740     __ cmp(len, large_loop_size);
3741     __ br(Assembler::GE, LARGE_LOOP);
3742 
3743   __ bind(CHECK_16); // small 16-byte load pre-loop
3744     __ cmp(len, (u1)16);
3745     __ br(Assembler::LT, POST_LOOP16);
3746 
3747   __ bind(LOOP16); // small 16-byte load loop
3748     __ ldp(tmp2, tmp3, Address(__ post(ary1, 16)));
3749     __ sub(len, len, 16);
3750     __ orr(tmp2, tmp2, tmp3);
3751     __ tst(tmp2, UPPER_BIT_MASK);
3752     __ br(Assembler::NE, RET_TRUE);
3753     __ cmp(len, (u1)16);
3754     __ br(Assembler::GE, LOOP16); // 16-byte load loop end
3755 
3756   __ bind(POST_LOOP16); // 16-byte aligned, so we can read unconditionally
3757     __ cmp(len, (u1)8);
3758     __ br(Assembler::LE, POST_LOOP16_LOAD_TAIL);
3759     __ ldr(tmp3, Address(__ post(ary1, 8)));
3760     __ sub(len, len, 8);
3761     __ tst(tmp3, UPPER_BIT_MASK);
3762     __ br(Assembler::NE, RET_TRUE);
3763 
3764   __ bind(POST_LOOP16_LOAD_TAIL);
3765     __ cbz(len, RET_FALSE); // Can&#39;t shift left by 64 when len==0
3766     __ ldr(tmp1, Address(ary1));
3767     __ mov(tmp2, 64);
3768     __ sub(tmp4, tmp2, len, __ LSL, 3);
3769     __ lslv(tmp1, tmp1, tmp4);
3770     __ tst(tmp1, UPPER_BIT_MASK);
3771     __ br(Assembler::NE, RET_TRUE);
3772     // Fallthrough
3773 
3774   __ bind(RET_FALSE);
3775     __ pop(spilled_regs, sp);
3776     __ leave();
3777     __ mov(result, zr);
3778     __ ret(lr);
3779 
3780   __ bind(RET_TRUE);
3781     __ pop(spilled_regs, sp);
3782   __ bind(RET_TRUE_NO_POP);
3783     __ leave();
3784     __ mov(result, 1);
3785     __ ret(lr);
3786 
3787   __ bind(DONE);
3788     __ pop(spilled_regs, sp);
3789     __ leave();
3790     __ ret(lr);
3791     return entry;
3792   }
3793 
3794   void generate_large_array_equals_loop_nonsimd(int loopThreshold,
3795         bool usePrefetch, Label &amp;NOT_EQUAL) {
3796     Register a1 = r1, a2 = r2, result = r0, cnt1 = r10, tmp1 = rscratch1,
3797         tmp2 = rscratch2, tmp3 = r3, tmp4 = r4, tmp5 = r5, tmp6 = r11,
3798         tmp7 = r12, tmp8 = r13;
3799     Label LOOP;
3800 
3801     __ ldp(tmp1, tmp3, Address(__ post(a1, 2 * wordSize)));
3802     __ ldp(tmp2, tmp4, Address(__ post(a2, 2 * wordSize)));
3803     __ bind(LOOP);
3804     if (usePrefetch) {
3805       __ prfm(Address(a1, SoftwarePrefetchHintDistance));
3806       __ prfm(Address(a2, SoftwarePrefetchHintDistance));
3807     }
3808     __ ldp(tmp5, tmp7, Address(__ post(a1, 2 * wordSize)));
3809     __ eor(tmp1, tmp1, tmp2);
3810     __ eor(tmp3, tmp3, tmp4);
3811     __ ldp(tmp6, tmp8, Address(__ post(a2, 2 * wordSize)));
3812     __ orr(tmp1, tmp1, tmp3);
3813     __ cbnz(tmp1, NOT_EQUAL);
3814     __ ldp(tmp1, tmp3, Address(__ post(a1, 2 * wordSize)));
3815     __ eor(tmp5, tmp5, tmp6);
3816     __ eor(tmp7, tmp7, tmp8);
3817     __ ldp(tmp2, tmp4, Address(__ post(a2, 2 * wordSize)));
3818     __ orr(tmp5, tmp5, tmp7);
3819     __ cbnz(tmp5, NOT_EQUAL);
3820     __ ldp(tmp5, tmp7, Address(__ post(a1, 2 * wordSize)));
3821     __ eor(tmp1, tmp1, tmp2);
3822     __ eor(tmp3, tmp3, tmp4);
3823     __ ldp(tmp6, tmp8, Address(__ post(a2, 2 * wordSize)));
3824     __ orr(tmp1, tmp1, tmp3);
3825     __ cbnz(tmp1, NOT_EQUAL);
3826     __ ldp(tmp1, tmp3, Address(__ post(a1, 2 * wordSize)));
3827     __ eor(tmp5, tmp5, tmp6);
3828     __ sub(cnt1, cnt1, 8 * wordSize);
3829     __ eor(tmp7, tmp7, tmp8);
3830     __ ldp(tmp2, tmp4, Address(__ post(a2, 2 * wordSize)));
3831     // tmp6 is not used. MacroAssembler::subs is used here (rather than
3832     // cmp) because subs allows an unlimited range of immediate operand.
3833     __ subs(tmp6, cnt1, loopThreshold);
3834     __ orr(tmp5, tmp5, tmp7);
3835     __ cbnz(tmp5, NOT_EQUAL);
3836     __ br(__ GE, LOOP);
3837     // post-loop
3838     __ eor(tmp1, tmp1, tmp2);
3839     __ eor(tmp3, tmp3, tmp4);
3840     __ orr(tmp1, tmp1, tmp3);
3841     __ sub(cnt1, cnt1, 2 * wordSize);
3842     __ cbnz(tmp1, NOT_EQUAL);
3843   }
3844 
3845   void generate_large_array_equals_loop_simd(int loopThreshold,
3846         bool usePrefetch, Label &amp;NOT_EQUAL) {
3847     Register a1 = r1, a2 = r2, result = r0, cnt1 = r10, tmp1 = rscratch1,
3848         tmp2 = rscratch2;
3849     Label LOOP;
3850 
3851     __ bind(LOOP);
3852     if (usePrefetch) {
3853       __ prfm(Address(a1, SoftwarePrefetchHintDistance));
3854       __ prfm(Address(a2, SoftwarePrefetchHintDistance));
3855     }
3856     __ ld1(v0, v1, v2, v3, __ T2D, Address(__ post(a1, 4 * 2 * wordSize)));
3857     __ sub(cnt1, cnt1, 8 * wordSize);
3858     __ ld1(v4, v5, v6, v7, __ T2D, Address(__ post(a2, 4 * 2 * wordSize)));
3859     __ subs(tmp1, cnt1, loopThreshold);
3860     __ eor(v0, __ T16B, v0, v4);
3861     __ eor(v1, __ T16B, v1, v5);
3862     __ eor(v2, __ T16B, v2, v6);
3863     __ eor(v3, __ T16B, v3, v7);
3864     __ orr(v0, __ T16B, v0, v1);
3865     __ orr(v1, __ T16B, v2, v3);
3866     __ orr(v0, __ T16B, v0, v1);
3867     __ umov(tmp1, v0, __ D, 0);
3868     __ umov(tmp2, v0, __ D, 1);
3869     __ orr(tmp1, tmp1, tmp2);
3870     __ cbnz(tmp1, NOT_EQUAL);
3871     __ br(__ GE, LOOP);
3872   }
3873 
3874   // a1 = r1 - array1 address
3875   // a2 = r2 - array2 address
3876   // result = r0 - return value. Already contains &quot;false&quot;
3877   // cnt1 = r10 - amount of elements left to check, reduced by wordSize
3878   // r3-r5 are reserved temporary registers
3879   address generate_large_array_equals() {
3880     Register a1 = r1, a2 = r2, result = r0, cnt1 = r10, tmp1 = rscratch1,
3881         tmp2 = rscratch2, tmp3 = r3, tmp4 = r4, tmp5 = r5, tmp6 = r11,
3882         tmp7 = r12, tmp8 = r13;
3883     Label TAIL, NOT_EQUAL, EQUAL, NOT_EQUAL_NO_POP, NO_PREFETCH_LARGE_LOOP,
3884         SMALL_LOOP, POST_LOOP;
3885     const int PRE_LOOP_SIZE = UseSIMDForArrayEquals ? 0 : 16;
3886     // calculate if at least 32 prefetched bytes are used
3887     int prefetchLoopThreshold = SoftwarePrefetchHintDistance + 32;
3888     int nonPrefetchLoopThreshold = (64 + PRE_LOOP_SIZE);
3889     RegSet spilled_regs = RegSet::range(tmp6, tmp8);
3890     assert_different_registers(a1, a2, result, cnt1, tmp1, tmp2, tmp3, tmp4,
3891         tmp5, tmp6, tmp7, tmp8);
3892 
3893     __ align(CodeEntryAlignment);
3894 
3895     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;large_array_equals&quot;);
3896 
3897     address entry = __ pc();
3898     __ enter();
3899     __ sub(cnt1, cnt1, wordSize);  // first 8 bytes were loaded outside of stub
3900     // also advance pointers to use post-increment instead of pre-increment
3901     __ add(a1, a1, wordSize);
3902     __ add(a2, a2, wordSize);
3903     if (AvoidUnalignedAccesses) {
3904       // both implementations (SIMD/nonSIMD) are using relatively large load
3905       // instructions (ld1/ldp), which has huge penalty (up to x2 exec time)
3906       // on some CPUs in case of address is not at least 16-byte aligned.
3907       // Arrays are 8-byte aligned currently, so, we can make additional 8-byte
3908       // load if needed at least for 1st address and make if 16-byte aligned.
3909       Label ALIGNED16;
3910       __ tbz(a1, 3, ALIGNED16);
3911       __ ldr(tmp1, Address(__ post(a1, wordSize)));
3912       __ ldr(tmp2, Address(__ post(a2, wordSize)));
3913       __ sub(cnt1, cnt1, wordSize);
3914       __ eor(tmp1, tmp1, tmp2);
3915       __ cbnz(tmp1, NOT_EQUAL_NO_POP);
3916       __ bind(ALIGNED16);
3917     }
3918     if (UseSIMDForArrayEquals) {
3919       if (SoftwarePrefetchHintDistance &gt;= 0) {
3920         __ subs(tmp1, cnt1, prefetchLoopThreshold);
3921         __ br(__ LE, NO_PREFETCH_LARGE_LOOP);
3922         generate_large_array_equals_loop_simd(prefetchLoopThreshold,
3923             /* prfm = */ true, NOT_EQUAL);
3924         __ subs(zr, cnt1, nonPrefetchLoopThreshold);
3925         __ br(__ LT, TAIL);
3926       }
3927       __ bind(NO_PREFETCH_LARGE_LOOP);
3928       generate_large_array_equals_loop_simd(nonPrefetchLoopThreshold,
3929           /* prfm = */ false, NOT_EQUAL);
3930     } else {
3931       __ push(spilled_regs, sp);
3932       if (SoftwarePrefetchHintDistance &gt;= 0) {
3933         __ subs(tmp1, cnt1, prefetchLoopThreshold);
3934         __ br(__ LE, NO_PREFETCH_LARGE_LOOP);
3935         generate_large_array_equals_loop_nonsimd(prefetchLoopThreshold,
3936             /* prfm = */ true, NOT_EQUAL);
3937         __ subs(zr, cnt1, nonPrefetchLoopThreshold);
3938         __ br(__ LT, TAIL);
3939       }
3940       __ bind(NO_PREFETCH_LARGE_LOOP);
3941       generate_large_array_equals_loop_nonsimd(nonPrefetchLoopThreshold,
3942           /* prfm = */ false, NOT_EQUAL);
3943     }
3944     __ bind(TAIL);
3945       __ cbz(cnt1, EQUAL);
3946       __ subs(cnt1, cnt1, wordSize);
3947       __ br(__ LE, POST_LOOP);
3948     __ bind(SMALL_LOOP);
3949       __ ldr(tmp1, Address(__ post(a1, wordSize)));
3950       __ ldr(tmp2, Address(__ post(a2, wordSize)));
3951       __ subs(cnt1, cnt1, wordSize);
3952       __ eor(tmp1, tmp1, tmp2);
3953       __ cbnz(tmp1, NOT_EQUAL);
3954       __ br(__ GT, SMALL_LOOP);
3955     __ bind(POST_LOOP);
3956       __ ldr(tmp1, Address(a1, cnt1));
3957       __ ldr(tmp2, Address(a2, cnt1));
3958       __ eor(tmp1, tmp1, tmp2);
3959       __ cbnz(tmp1, NOT_EQUAL);
3960     __ bind(EQUAL);
3961       __ mov(result, true);
3962     __ bind(NOT_EQUAL);
3963       if (!UseSIMDForArrayEquals) {
3964         __ pop(spilled_regs, sp);
3965       }
3966     __ bind(NOT_EQUAL_NO_POP);
3967     __ leave();
3968     __ ret(lr);
3969     return entry;
3970   }
3971 
3972   address generate_dsin_dcos(bool isCos) {
3973     __ align(CodeEntryAlignment);
3974     StubCodeMark mark(this, &quot;StubRoutines&quot;, isCos ? &quot;libmDcos&quot; : &quot;libmDsin&quot;);
3975     address start = __ pc();
3976     __ generate_dsin_dcos(isCos, (address)StubRoutines::aarch64::_npio2_hw,
3977         (address)StubRoutines::aarch64::_two_over_pi,
3978         (address)StubRoutines::aarch64::_pio2,
3979         (address)StubRoutines::aarch64::_dsin_coef,
3980         (address)StubRoutines::aarch64::_dcos_coef);
3981     return start;
3982   }
3983 
3984   address generate_dlog() {
3985     __ align(CodeEntryAlignment);
3986     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;dlog&quot;);
3987     address entry = __ pc();
3988     FloatRegister vtmp0 = v0, vtmp1 = v1, vtmp2 = v2, vtmp3 = v3, vtmp4 = v4,
3989         vtmp5 = v5, tmpC1 = v16, tmpC2 = v17, tmpC3 = v18, tmpC4 = v19;
3990     Register tmp1 = r0, tmp2 = r1, tmp3 = r2, tmp4 = r3, tmp5 = r4;
3991     __ fast_log(vtmp0, vtmp1, vtmp2, vtmp3, vtmp4, vtmp5, tmpC1, tmpC2, tmpC3,
3992         tmpC4, tmp1, tmp2, tmp3, tmp4, tmp5);
3993     return entry;
3994   }
3995 
3996   // code for comparing 16 bytes of strings with same encoding
3997   void compare_string_16_bytes_same(Label &amp;DIFF1, Label &amp;DIFF2) {
3998     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, tmp1 = r10, tmp2 = r11;
3999     __ ldr(rscratch1, Address(__ post(str1, 8)));
4000     __ eor(rscratch2, tmp1, tmp2);
4001     __ ldr(cnt1, Address(__ post(str2, 8)));
4002     __ cbnz(rscratch2, DIFF1);
4003     __ ldr(tmp1, Address(__ post(str1, 8)));
4004     __ eor(rscratch2, rscratch1, cnt1);
4005     __ ldr(tmp2, Address(__ post(str2, 8)));
4006     __ cbnz(rscratch2, DIFF2);
4007   }
4008 
4009   // code for comparing 16 characters of strings with Latin1 and Utf16 encoding
4010   void compare_string_16_x_LU(Register tmpL, Register tmpU, Label &amp;DIFF1,
4011       Label &amp;DIFF2) {
4012     Register cnt1 = r2, tmp2 = r11, tmp3 = r12;
4013     FloatRegister vtmp = v1, vtmpZ = v0, vtmp3 = v2;
4014 
4015     __ ldrq(vtmp, Address(__ post(tmp2, 16)));
4016     __ ldr(tmpU, Address(__ post(cnt1, 8)));
4017     __ zip1(vtmp3, __ T16B, vtmp, vtmpZ);
4018     // now we have 32 bytes of characters (converted to U) in vtmp:vtmp3
4019 
4020     __ fmovd(tmpL, vtmp3);
4021     __ eor(rscratch2, tmp3, tmpL);
4022     __ cbnz(rscratch2, DIFF2);
4023 
4024     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4025     __ umov(tmpL, vtmp3, __ D, 1);
4026     __ eor(rscratch2, tmpU, tmpL);
4027     __ cbnz(rscratch2, DIFF1);
4028 
4029     __ zip2(vtmp, __ T16B, vtmp, vtmpZ);
4030     __ ldr(tmpU, Address(__ post(cnt1, 8)));
4031     __ fmovd(tmpL, vtmp);
4032     __ eor(rscratch2, tmp3, tmpL);
4033     __ cbnz(rscratch2, DIFF2);
4034 
4035     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4036     __ umov(tmpL, vtmp, __ D, 1);
4037     __ eor(rscratch2, tmpU, tmpL);
4038     __ cbnz(rscratch2, DIFF1);
4039   }
4040 
4041   // r0  = result
4042   // r1  = str1
4043   // r2  = cnt1
4044   // r3  = str2
4045   // r4  = cnt2
4046   // r10 = tmp1
4047   // r11 = tmp2
4048   address generate_compare_long_string_different_encoding(bool isLU) {
4049     __ align(CodeEntryAlignment);
4050     StubCodeMark mark(this, &quot;StubRoutines&quot;, isLU
4051         ? &quot;compare_long_string_different_encoding LU&quot;
4052         : &quot;compare_long_string_different_encoding UL&quot;);
4053     address entry = __ pc();
4054     Label SMALL_LOOP, TAIL, TAIL_LOAD_16, LOAD_LAST, DIFF1, DIFF2,
4055         DONE, CALCULATE_DIFFERENCE, LARGE_LOOP_PREFETCH, NO_PREFETCH,
4056         LARGE_LOOP_PREFETCH_REPEAT1, LARGE_LOOP_PREFETCH_REPEAT2;
4057     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, cnt2 = r4,
4058         tmp1 = r10, tmp2 = r11, tmp3 = r12, tmp4 = r14;
4059     FloatRegister vtmpZ = v0, vtmp = v1, vtmp3 = v2;
4060     RegSet spilled_regs = RegSet::of(tmp3, tmp4);
4061 
<a name="26" id="anc26"></a><span class="line-modified">4062     int prefetchLoopExitCondition = MAX2(64, SoftwarePrefetchHintDistance/2);</span>
4063 
4064     __ eor(vtmpZ, __ T16B, vtmpZ, vtmpZ);
4065     // cnt2 == amount of characters left to compare
4066     // Check already loaded first 4 symbols(vtmp and tmp2(LU)/tmp1(UL))
4067     __ zip1(vtmp, __ T8B, vtmp, vtmpZ);
4068     __ add(str1, str1, isLU ? wordSize/2 : wordSize);
4069     __ add(str2, str2, isLU ? wordSize : wordSize/2);
4070     __ fmovd(isLU ? tmp1 : tmp2, vtmp);
4071     __ subw(cnt2, cnt2, 8); // Already loaded 4 symbols. Last 4 is special case.
4072     __ eor(rscratch2, tmp1, tmp2);
4073     __ mov(rscratch1, tmp2);
4074     __ cbnz(rscratch2, CALCULATE_DIFFERENCE);
4075     Register tmpU = isLU ? rscratch1 : tmp1, // where to keep U for comparison
4076              tmpL = isLU ? tmp1 : rscratch1; // where to keep L for comparison
4077     __ push(spilled_regs, sp);
4078     __ mov(tmp2, isLU ? str1 : str2); // init the pointer to L next load
4079     __ mov(cnt1, isLU ? str2 : str1); // init the pointer to U next load
4080 
4081     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4082 
4083     if (SoftwarePrefetchHintDistance &gt;= 0) {
4084       __ subs(rscratch2, cnt2, prefetchLoopExitCondition);
4085       __ br(__ LT, NO_PREFETCH);
4086       __ bind(LARGE_LOOP_PREFETCH);
4087         __ prfm(Address(tmp2, SoftwarePrefetchHintDistance));
4088         __ mov(tmp4, 2);
4089         __ prfm(Address(cnt1, SoftwarePrefetchHintDistance));
4090         __ bind(LARGE_LOOP_PREFETCH_REPEAT1);
4091           compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4092           __ subs(tmp4, tmp4, 1);
4093           __ br(__ GT, LARGE_LOOP_PREFETCH_REPEAT1);
4094           __ prfm(Address(cnt1, SoftwarePrefetchHintDistance));
4095           __ mov(tmp4, 2);
4096         __ bind(LARGE_LOOP_PREFETCH_REPEAT2);
4097           compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4098           __ subs(tmp4, tmp4, 1);
4099           __ br(__ GT, LARGE_LOOP_PREFETCH_REPEAT2);
4100           __ sub(cnt2, cnt2, 64);
4101           __ subs(rscratch2, cnt2, prefetchLoopExitCondition);
4102           __ br(__ GE, LARGE_LOOP_PREFETCH);
4103     }
4104     __ cbz(cnt2, LOAD_LAST); // no characters left except last load
4105     __ bind(NO_PREFETCH);
4106     __ subs(cnt2, cnt2, 16);
4107     __ br(__ LT, TAIL);
4108     __ align(OptoLoopAlignment);
4109     __ bind(SMALL_LOOP); // smaller loop
4110       __ subs(cnt2, cnt2, 16);
4111       compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4112       __ br(__ GE, SMALL_LOOP);
4113       __ cmn(cnt2, (u1)16);
4114       __ br(__ EQ, LOAD_LAST);
4115     __ bind(TAIL); // 1..15 characters left until last load (last 4 characters)
4116       __ add(cnt1, cnt1, cnt2, __ LSL, 1); // Address of 32 bytes before last 4 characters in UTF-16 string
4117       __ add(tmp2, tmp2, cnt2); // Address of 16 bytes before last 4 characters in Latin1 string
4118       __ ldr(tmp3, Address(cnt1, -8));
4119       compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2); // last 16 characters before last load
4120       __ b(LOAD_LAST);
4121     __ bind(DIFF2);
4122       __ mov(tmpU, tmp3);
4123     __ bind(DIFF1);
4124       __ pop(spilled_regs, sp);
4125       __ b(CALCULATE_DIFFERENCE);
4126     __ bind(LOAD_LAST);
4127       // Last 4 UTF-16 characters are already pre-loaded into tmp3 by compare_string_16_x_LU.
4128       // No need to load it again
4129       __ mov(tmpU, tmp3);
4130       __ pop(spilled_regs, sp);
4131 
4132       // tmp2 points to the address of the last 4 Latin1 characters right now
4133       __ ldrs(vtmp, Address(tmp2));
4134       __ zip1(vtmp, __ T8B, vtmp, vtmpZ);
4135       __ fmovd(tmpL, vtmp);
4136 
4137       __ eor(rscratch2, tmpU, tmpL);
4138       __ cbz(rscratch2, DONE);
4139 
4140     // Find the first different characters in the longwords and
4141     // compute their difference.
4142     __ bind(CALCULATE_DIFFERENCE);
4143       __ rev(rscratch2, rscratch2);
4144       __ clz(rscratch2, rscratch2);
4145       __ andr(rscratch2, rscratch2, -16);
4146       __ lsrv(tmp1, tmp1, rscratch2);
4147       __ uxthw(tmp1, tmp1);
4148       __ lsrv(rscratch1, rscratch1, rscratch2);
4149       __ uxthw(rscratch1, rscratch1);
4150       __ subw(result, tmp1, rscratch1);
4151     __ bind(DONE);
4152       __ ret(lr);
4153     return entry;
4154   }
4155 
4156   // r0  = result
4157   // r1  = str1
4158   // r2  = cnt1
4159   // r3  = str2
4160   // r4  = cnt2
4161   // r10 = tmp1
4162   // r11 = tmp2
4163   address generate_compare_long_string_same_encoding(bool isLL) {
4164     __ align(CodeEntryAlignment);
4165     StubCodeMark mark(this, &quot;StubRoutines&quot;, isLL
4166         ? &quot;compare_long_string_same_encoding LL&quot;
4167         : &quot;compare_long_string_same_encoding UU&quot;);
4168     address entry = __ pc();
4169     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, cnt2 = r4,
4170         tmp1 = r10, tmp2 = r11;
4171     Label SMALL_LOOP, LARGE_LOOP_PREFETCH, CHECK_LAST, DIFF2, TAIL,
4172         LENGTH_DIFF, DIFF, LAST_CHECK_AND_LENGTH_DIFF,
4173         DIFF_LAST_POSITION, DIFF_LAST_POSITION2;
4174     // exit from large loop when less than 64 bytes left to read or we&#39;re about
4175     // to prefetch memory behind array border
<a name="27" id="anc27"></a><span class="line-modified">4176     int largeLoopExitCondition = MAX2(64, SoftwarePrefetchHintDistance)/(isLL ? 1 : 2);</span>
4177     // cnt1/cnt2 contains amount of characters to compare. cnt1 can be re-used
4178     // update cnt2 counter with already loaded 8 bytes
4179     __ sub(cnt2, cnt2, wordSize/(isLL ? 1 : 2));
4180     // update pointers, because of previous read
4181     __ add(str1, str1, wordSize);
4182     __ add(str2, str2, wordSize);
4183     if (SoftwarePrefetchHintDistance &gt;= 0) {
4184       __ bind(LARGE_LOOP_PREFETCH);
4185         __ prfm(Address(str1, SoftwarePrefetchHintDistance));
4186         __ prfm(Address(str2, SoftwarePrefetchHintDistance));
4187         compare_string_16_bytes_same(DIFF, DIFF2);
4188         compare_string_16_bytes_same(DIFF, DIFF2);
4189         __ sub(cnt2, cnt2, isLL ? 64 : 32);
4190         compare_string_16_bytes_same(DIFF, DIFF2);
4191         __ subs(rscratch2, cnt2, largeLoopExitCondition);
4192         compare_string_16_bytes_same(DIFF, DIFF2);
4193         __ br(__ GT, LARGE_LOOP_PREFETCH);
4194         __ cbz(cnt2, LAST_CHECK_AND_LENGTH_DIFF); // no more chars left?
4195     }
4196     // less than 16 bytes left?
4197     __ subs(cnt2, cnt2, isLL ? 16 : 8);
4198     __ br(__ LT, TAIL);
4199     __ align(OptoLoopAlignment);
4200     __ bind(SMALL_LOOP);
4201       compare_string_16_bytes_same(DIFF, DIFF2);
4202       __ subs(cnt2, cnt2, isLL ? 16 : 8);
4203       __ br(__ GE, SMALL_LOOP);
4204     __ bind(TAIL);
4205       __ adds(cnt2, cnt2, isLL ? 16 : 8);
4206       __ br(__ EQ, LAST_CHECK_AND_LENGTH_DIFF);
4207       __ subs(cnt2, cnt2, isLL ? 8 : 4);
4208       __ br(__ LE, CHECK_LAST);
4209       __ eor(rscratch2, tmp1, tmp2);
4210       __ cbnz(rscratch2, DIFF);
4211       __ ldr(tmp1, Address(__ post(str1, 8)));
4212       __ ldr(tmp2, Address(__ post(str2, 8)));
4213       __ sub(cnt2, cnt2, isLL ? 8 : 4);
4214     __ bind(CHECK_LAST);
4215       if (!isLL) {
4216         __ add(cnt2, cnt2, cnt2); // now in bytes
4217       }
4218       __ eor(rscratch2, tmp1, tmp2);
4219       __ cbnz(rscratch2, DIFF);
4220       __ ldr(rscratch1, Address(str1, cnt2));
4221       __ ldr(cnt1, Address(str2, cnt2));
4222       __ eor(rscratch2, rscratch1, cnt1);
4223       __ cbz(rscratch2, LENGTH_DIFF);
4224       // Find the first different characters in the longwords and
4225       // compute their difference.
4226     __ bind(DIFF2);
4227       __ rev(rscratch2, rscratch2);
4228       __ clz(rscratch2, rscratch2);
4229       __ andr(rscratch2, rscratch2, isLL ? -8 : -16);
4230       __ lsrv(rscratch1, rscratch1, rscratch2);
4231       if (isLL) {
4232         __ lsrv(cnt1, cnt1, rscratch2);
4233         __ uxtbw(rscratch1, rscratch1);
4234         __ uxtbw(cnt1, cnt1);
4235       } else {
4236         __ lsrv(cnt1, cnt1, rscratch2);
4237         __ uxthw(rscratch1, rscratch1);
4238         __ uxthw(cnt1, cnt1);
4239       }
4240       __ subw(result, rscratch1, cnt1);
4241       __ b(LENGTH_DIFF);
4242     __ bind(DIFF);
4243       __ rev(rscratch2, rscratch2);
4244       __ clz(rscratch2, rscratch2);
4245       __ andr(rscratch2, rscratch2, isLL ? -8 : -16);
4246       __ lsrv(tmp1, tmp1, rscratch2);
4247       if (isLL) {
4248         __ lsrv(tmp2, tmp2, rscratch2);
4249         __ uxtbw(tmp1, tmp1);
4250         __ uxtbw(tmp2, tmp2);
4251       } else {
4252         __ lsrv(tmp2, tmp2, rscratch2);
4253         __ uxthw(tmp1, tmp1);
4254         __ uxthw(tmp2, tmp2);
4255       }
4256       __ subw(result, tmp1, tmp2);
4257       __ b(LENGTH_DIFF);
4258     __ bind(LAST_CHECK_AND_LENGTH_DIFF);
4259       __ eor(rscratch2, tmp1, tmp2);
4260       __ cbnz(rscratch2, DIFF);
4261     __ bind(LENGTH_DIFF);
4262       __ ret(lr);
4263     return entry;
4264   }
4265 
4266   void generate_compare_long_strings() {
4267       StubRoutines::aarch64::_compare_long_string_LL
4268           = generate_compare_long_string_same_encoding(true);
4269       StubRoutines::aarch64::_compare_long_string_UU
4270           = generate_compare_long_string_same_encoding(false);
4271       StubRoutines::aarch64::_compare_long_string_LU
4272           = generate_compare_long_string_different_encoding(true);
4273       StubRoutines::aarch64::_compare_long_string_UL
4274           = generate_compare_long_string_different_encoding(false);
4275   }
4276 
4277   // R0 = result
4278   // R1 = str2
4279   // R2 = cnt1
4280   // R3 = str1
4281   // R4 = cnt2
4282   // This generic linear code use few additional ideas, which makes it faster:
4283   // 1) we can safely keep at least 1st register of pattern(since length &gt;= 8)
4284   // in order to skip initial loading(help in systems with 1 ld pipeline)
4285   // 2) we can use &quot;fast&quot; algorithm of finding single character to search for
4286   // first symbol with less branches(1 branch per each loaded register instead
4287   // of branch for each symbol), so, this is where constants like
4288   // 0x0101...01, 0x00010001...0001, 0x7f7f...7f, 0x7fff7fff...7fff comes from
4289   // 3) after loading and analyzing 1st register of source string, it can be
4290   // used to search for every 1st character entry, saving few loads in
4291   // comparison with &quot;simplier-but-slower&quot; implementation
4292   // 4) in order to avoid lots of push/pop operations, code below is heavily
4293   // re-using/re-initializing/compressing register values, which makes code
4294   // larger and a bit less readable, however, most of extra operations are
4295   // issued during loads or branches, so, penalty is minimal
4296   address generate_string_indexof_linear(bool str1_isL, bool str2_isL) {
4297     const char* stubName = str1_isL
4298         ? (str2_isL ? &quot;indexof_linear_ll&quot; : &quot;indexof_linear_ul&quot;)
4299         : &quot;indexof_linear_uu&quot;;
4300     __ align(CodeEntryAlignment);
4301     StubCodeMark mark(this, &quot;StubRoutines&quot;, stubName);
4302     address entry = __ pc();
4303 
4304     int str1_chr_size = str1_isL ? 1 : 2;
4305     int str2_chr_size = str2_isL ? 1 : 2;
4306     int str1_chr_shift = str1_isL ? 0 : 1;
4307     int str2_chr_shift = str2_isL ? 0 : 1;
4308     bool isL = str1_isL &amp;&amp; str2_isL;
4309    // parameters
4310     Register result = r0, str2 = r1, cnt1 = r2, str1 = r3, cnt2 = r4;
4311     // temporary registers
4312     Register tmp1 = r20, tmp2 = r21, tmp3 = r22, tmp4 = r23;
4313     RegSet spilled_regs = RegSet::range(tmp1, tmp4);
4314     // redefinitions
4315     Register ch1 = rscratch1, ch2 = rscratch2, first = tmp3;
4316 
4317     __ push(spilled_regs, sp);
4318     Label L_LOOP, L_LOOP_PROCEED, L_SMALL, L_HAS_ZERO,
4319         L_HAS_ZERO_LOOP, L_CMP_LOOP, L_CMP_LOOP_NOMATCH, L_SMALL_PROCEED,
4320         L_SMALL_HAS_ZERO_LOOP, L_SMALL_CMP_LOOP_NOMATCH, L_SMALL_CMP_LOOP,
4321         L_POST_LOOP, L_CMP_LOOP_LAST_CMP, L_HAS_ZERO_LOOP_NOMATCH,
4322         L_SMALL_CMP_LOOP_LAST_CMP, L_SMALL_CMP_LOOP_LAST_CMP2,
4323         L_CMP_LOOP_LAST_CMP2, DONE, NOMATCH;
4324     // Read whole register from str1. It is safe, because length &gt;=8 here
4325     __ ldr(ch1, Address(str1));
4326     // Read whole register from str2. It is safe, because length &gt;=8 here
4327     __ ldr(ch2, Address(str2));
4328     __ sub(cnt2, cnt2, cnt1);
4329     __ andr(first, ch1, str1_isL ? 0xFF : 0xFFFF);
4330     if (str1_isL != str2_isL) {
4331       __ eor(v0, __ T16B, v0, v0);
4332     }
<a name="28" id="anc28"></a><span class="line-modified">4333     __ mov(tmp1, (uint64_t)(str2_isL ? 0x0101010101010101 : 0x0001000100010001));</span>
4334     __ mul(first, first, tmp1);
4335     // check if we have less than 1 register to check
4336     __ subs(cnt2, cnt2, wordSize/str2_chr_size - 1);
4337     if (str1_isL != str2_isL) {
4338       __ fmovd(v1, ch1);
4339     }
4340     __ br(__ LE, L_SMALL);
4341     __ eor(ch2, first, ch2);
4342     if (str1_isL != str2_isL) {
4343       __ zip1(v1, __ T16B, v1, v0);
4344     }
4345     __ sub(tmp2, ch2, tmp1);
4346     __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4347     __ bics(tmp2, tmp2, ch2);
4348     if (str1_isL != str2_isL) {
4349       __ fmovd(ch1, v1);
4350     }
4351     __ br(__ NE, L_HAS_ZERO);
4352     __ subs(cnt2, cnt2, wordSize/str2_chr_size);
4353     __ add(result, result, wordSize/str2_chr_size);
4354     __ add(str2, str2, wordSize);
4355     __ br(__ LT, L_POST_LOOP);
4356     __ BIND(L_LOOP);
4357       __ ldr(ch2, Address(str2));
4358       __ eor(ch2, first, ch2);
4359       __ sub(tmp2, ch2, tmp1);
4360       __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4361       __ bics(tmp2, tmp2, ch2);
4362       __ br(__ NE, L_HAS_ZERO);
4363     __ BIND(L_LOOP_PROCEED);
4364       __ subs(cnt2, cnt2, wordSize/str2_chr_size);
4365       __ add(str2, str2, wordSize);
4366       __ add(result, result, wordSize/str2_chr_size);
4367       __ br(__ GE, L_LOOP);
4368     __ BIND(L_POST_LOOP);
4369       __ subs(zr, cnt2, -wordSize/str2_chr_size); // no extra characters to check
4370       __ br(__ LE, NOMATCH);
4371       __ ldr(ch2, Address(str2));
4372       __ sub(cnt2, zr, cnt2, __ LSL, LogBitsPerByte + str2_chr_shift);
4373       __ eor(ch2, first, ch2);
4374       __ sub(tmp2, ch2, tmp1);
4375       __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4376       __ mov(tmp4, -1); // all bits set
4377       __ b(L_SMALL_PROCEED);
4378     __ align(OptoLoopAlignment);
4379     __ BIND(L_SMALL);
4380       __ sub(cnt2, zr, cnt2, __ LSL, LogBitsPerByte + str2_chr_shift);
4381       __ eor(ch2, first, ch2);
4382       if (str1_isL != str2_isL) {
4383         __ zip1(v1, __ T16B, v1, v0);
4384       }
4385       __ sub(tmp2, ch2, tmp1);
4386       __ mov(tmp4, -1); // all bits set
4387       __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4388       if (str1_isL != str2_isL) {
4389         __ fmovd(ch1, v1); // move converted 4 symbols
4390       }
4391     __ BIND(L_SMALL_PROCEED);
4392       __ lsrv(tmp4, tmp4, cnt2); // mask. zeroes on useless bits.
4393       __ bic(tmp2, tmp2, ch2);
4394       __ ands(tmp2, tmp2, tmp4); // clear useless bits and check
4395       __ rbit(tmp2, tmp2);
4396       __ br(__ EQ, NOMATCH);
4397     __ BIND(L_SMALL_HAS_ZERO_LOOP);
4398       __ clz(tmp4, tmp2); // potentially long. Up to 4 cycles on some cpu&#39;s
4399       __ cmp(cnt1, u1(wordSize/str2_chr_size));
4400       __ br(__ LE, L_SMALL_CMP_LOOP_LAST_CMP2);
4401       if (str2_isL) { // LL
4402         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte); // address of &quot;index&quot;
4403         __ ldr(ch2, Address(str2)); // read whole register of str2. Safe.
4404         __ lslv(tmp2, tmp2, tmp4); // shift off leading zeroes from match info
4405         __ add(result, result, tmp4, __ LSR, LogBitsPerByte);
4406         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4407       } else {
4408         __ mov(ch2, 0xE); // all bits in byte set except last one
4409         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4410         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4411         __ lslv(tmp2, tmp2, tmp4);
4412         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4413         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4414         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4415         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4416       }
4417       __ cmp(ch1, ch2);
4418       __ mov(tmp4, wordSize/str2_chr_size);
4419       __ br(__ NE, L_SMALL_CMP_LOOP_NOMATCH);
4420     __ BIND(L_SMALL_CMP_LOOP);
4421       str1_isL ? __ ldrb(first, Address(str1, tmp4, Address::lsl(str1_chr_shift)))
4422                : __ ldrh(first, Address(str1, tmp4, Address::lsl(str1_chr_shift)));
4423       str2_isL ? __ ldrb(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)))
4424                : __ ldrh(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)));
4425       __ add(tmp4, tmp4, 1);
4426       __ cmp(tmp4, cnt1);
4427       __ br(__ GE, L_SMALL_CMP_LOOP_LAST_CMP);
4428       __ cmp(first, ch2);
4429       __ br(__ EQ, L_SMALL_CMP_LOOP);
4430     __ BIND(L_SMALL_CMP_LOOP_NOMATCH);
4431       __ cbz(tmp2, NOMATCH); // no more matches. exit
4432       __ clz(tmp4, tmp2);
4433       __ add(result, result, 1); // advance index
4434       __ add(str2, str2, str2_chr_size); // advance pointer
4435       __ b(L_SMALL_HAS_ZERO_LOOP);
4436     __ align(OptoLoopAlignment);
4437     __ BIND(L_SMALL_CMP_LOOP_LAST_CMP);
4438       __ cmp(first, ch2);
4439       __ br(__ NE, L_SMALL_CMP_LOOP_NOMATCH);
4440       __ b(DONE);
4441     __ align(OptoLoopAlignment);
4442     __ BIND(L_SMALL_CMP_LOOP_LAST_CMP2);
4443       if (str2_isL) { // LL
4444         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte); // address of &quot;index&quot;
4445         __ ldr(ch2, Address(str2)); // read whole register of str2. Safe.
4446         __ lslv(tmp2, tmp2, tmp4); // shift off leading zeroes from match info
4447         __ add(result, result, tmp4, __ LSR, LogBitsPerByte);
4448         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4449       } else {
4450         __ mov(ch2, 0xE); // all bits in byte set except last one
4451         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4452         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4453         __ lslv(tmp2, tmp2, tmp4);
4454         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4455         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4456         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4457         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4458       }
4459       __ cmp(ch1, ch2);
4460       __ br(__ NE, L_SMALL_CMP_LOOP_NOMATCH);
4461       __ b(DONE);
4462     __ align(OptoLoopAlignment);
4463     __ BIND(L_HAS_ZERO);
4464       __ rbit(tmp2, tmp2);
4465       __ clz(tmp4, tmp2); // potentially long. Up to 4 cycles on some CPU&#39;s
4466       // Now, perform compression of counters(cnt2 and cnt1) into one register.
4467       // It&#39;s fine because both counters are 32bit and are not changed in this
4468       // loop. Just restore it on exit. So, cnt1 can be re-used in this loop.
4469       __ orr(cnt2, cnt2, cnt1, __ LSL, BitsPerByte * wordSize / 2);
4470       __ sub(result, result, 1);
4471     __ BIND(L_HAS_ZERO_LOOP);
4472       __ mov(cnt1, wordSize/str2_chr_size);
4473       __ cmp(cnt1, cnt2, __ LSR, BitsPerByte * wordSize / 2);
4474       __ br(__ GE, L_CMP_LOOP_LAST_CMP2); // case of 8 bytes only to compare
4475       if (str2_isL) {
4476         __ lsr(ch2, tmp4, LogBitsPerByte + str2_chr_shift); // char index
4477         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4478         __ lslv(tmp2, tmp2, tmp4);
4479         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4480         __ add(tmp4, tmp4, 1);
4481         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4482         __ lsl(tmp2, tmp2, 1);
4483         __ mov(tmp4, wordSize/str2_chr_size);
4484       } else {
4485         __ mov(ch2, 0xE);
4486         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4487         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4488         __ lslv(tmp2, tmp2, tmp4);
4489         __ add(tmp4, tmp4, 1);
4490         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4491         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte);
4492         __ lsl(tmp2, tmp2, 1);
4493         __ mov(tmp4, wordSize/str2_chr_size);
4494         __ sub(str2, str2, str2_chr_size);
4495       }
4496       __ cmp(ch1, ch2);
4497       __ mov(tmp4, wordSize/str2_chr_size);
4498       __ br(__ NE, L_CMP_LOOP_NOMATCH);
4499     __ BIND(L_CMP_LOOP);
4500       str1_isL ? __ ldrb(cnt1, Address(str1, tmp4, Address::lsl(str1_chr_shift)))
4501                : __ ldrh(cnt1, Address(str1, tmp4, Address::lsl(str1_chr_shift)));
4502       str2_isL ? __ ldrb(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)))
4503                : __ ldrh(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)));
4504       __ add(tmp4, tmp4, 1);
4505       __ cmp(tmp4, cnt2, __ LSR, BitsPerByte * wordSize / 2);
4506       __ br(__ GE, L_CMP_LOOP_LAST_CMP);
4507       __ cmp(cnt1, ch2);
4508       __ br(__ EQ, L_CMP_LOOP);
4509     __ BIND(L_CMP_LOOP_NOMATCH);
4510       // here we&#39;re not matched
4511       __ cbz(tmp2, L_HAS_ZERO_LOOP_NOMATCH); // no more matches. Proceed to main loop
4512       __ clz(tmp4, tmp2);
4513       __ add(str2, str2, str2_chr_size); // advance pointer
4514       __ b(L_HAS_ZERO_LOOP);
4515     __ align(OptoLoopAlignment);
4516     __ BIND(L_CMP_LOOP_LAST_CMP);
4517       __ cmp(cnt1, ch2);
4518       __ br(__ NE, L_CMP_LOOP_NOMATCH);
4519       __ b(DONE);
4520     __ align(OptoLoopAlignment);
4521     __ BIND(L_CMP_LOOP_LAST_CMP2);
4522       if (str2_isL) {
4523         __ lsr(ch2, tmp4, LogBitsPerByte + str2_chr_shift); // char index
4524         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4525         __ lslv(tmp2, tmp2, tmp4);
4526         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4527         __ add(tmp4, tmp4, 1);
4528         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4529         __ lsl(tmp2, tmp2, 1);
4530       } else {
4531         __ mov(ch2, 0xE);
4532         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4533         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4534         __ lslv(tmp2, tmp2, tmp4);
4535         __ add(tmp4, tmp4, 1);
4536         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4537         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte);
4538         __ lsl(tmp2, tmp2, 1);
4539         __ sub(str2, str2, str2_chr_size);
4540       }
4541       __ cmp(ch1, ch2);
4542       __ br(__ NE, L_CMP_LOOP_NOMATCH);
4543       __ b(DONE);
4544     __ align(OptoLoopAlignment);
4545     __ BIND(L_HAS_ZERO_LOOP_NOMATCH);
4546       // 1) Restore &quot;result&quot; index. Index was wordSize/str2_chr_size * N until
4547       // L_HAS_ZERO block. Byte octet was analyzed in L_HAS_ZERO_LOOP,
4548       // so, result was increased at max by wordSize/str2_chr_size - 1, so,
4549       // respective high bit wasn&#39;t changed. L_LOOP_PROCEED will increase
4550       // result by analyzed characters value, so, we can just reset lower bits
4551       // in result here. Clear 2 lower bits for UU/UL and 3 bits for LL
4552       // 2) restore cnt1 and cnt2 values from &quot;compressed&quot; cnt2
4553       // 3) advance str2 value to represent next str2 octet. result &amp; 7/3 is
4554       // index of last analyzed substring inside current octet. So, str2 in at
4555       // respective start address. We need to advance it to next octet
4556       __ andr(tmp2, result, wordSize/str2_chr_size - 1); // symbols analyzed
4557       __ lsr(cnt1, cnt2, BitsPerByte * wordSize / 2);
4558       __ bfm(result, zr, 0, 2 - str2_chr_shift);
4559       __ sub(str2, str2, tmp2, __ LSL, str2_chr_shift); // restore str2
4560       __ movw(cnt2, cnt2);
4561       __ b(L_LOOP_PROCEED);
4562     __ align(OptoLoopAlignment);
4563     __ BIND(NOMATCH);
4564       __ mov(result, -1);
4565     __ BIND(DONE);
4566       __ pop(spilled_regs, sp);
4567       __ ret(lr);
4568     return entry;
4569   }
4570 
4571   void generate_string_indexof_stubs() {
4572     StubRoutines::aarch64::_string_indexof_linear_ll = generate_string_indexof_linear(true, true);
4573     StubRoutines::aarch64::_string_indexof_linear_uu = generate_string_indexof_linear(false, false);
4574     StubRoutines::aarch64::_string_indexof_linear_ul = generate_string_indexof_linear(true, false);
4575   }
4576 
4577   void inflate_and_store_2_fp_registers(bool generatePrfm,
4578       FloatRegister src1, FloatRegister src2) {
4579     Register dst = r1;
4580     __ zip1(v1, __ T16B, src1, v0);
4581     __ zip2(v2, __ T16B, src1, v0);
4582     if (generatePrfm) {
4583       __ prfm(Address(dst, SoftwarePrefetchHintDistance), PSTL1STRM);
4584     }
4585     __ zip1(v3, __ T16B, src2, v0);
4586     __ zip2(v4, __ T16B, src2, v0);
4587     __ st1(v1, v2, v3, v4, __ T16B, Address(__ post(dst, 64)));
4588   }
4589 
4590   // R0 = src
4591   // R1 = dst
4592   // R2 = len
4593   // R3 = len &gt;&gt; 3
4594   // V0 = 0
4595   // v1 = loaded 8 bytes
4596   address generate_large_byte_array_inflate() {
4597     __ align(CodeEntryAlignment);
4598     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;large_byte_array_inflate&quot;);
4599     address entry = __ pc();
4600     Label LOOP, LOOP_START, LOOP_PRFM, LOOP_PRFM_START, DONE;
4601     Register src = r0, dst = r1, len = r2, octetCounter = r3;
<a name="29" id="anc29"></a><span class="line-modified">4602     const int large_loop_threshold = MAX2(64, SoftwarePrefetchHintDistance)/8 + 4;</span>
4603 
4604     // do one more 8-byte read to have address 16-byte aligned in most cases
4605     // also use single store instruction
4606     __ ldrd(v2, __ post(src, 8));
4607     __ sub(octetCounter, octetCounter, 2);
4608     __ zip1(v1, __ T16B, v1, v0);
4609     __ zip1(v2, __ T16B, v2, v0);
4610     __ st1(v1, v2, __ T16B, __ post(dst, 32));
4611     __ ld1(v3, v4, v5, v6, __ T16B, Address(__ post(src, 64)));
4612     __ subs(rscratch1, octetCounter, large_loop_threshold);
4613     __ br(__ LE, LOOP_START);
4614     __ b(LOOP_PRFM_START);
4615     __ bind(LOOP_PRFM);
4616       __ ld1(v3, v4, v5, v6, __ T16B, Address(__ post(src, 64)));
4617     __ bind(LOOP_PRFM_START);
4618       __ prfm(Address(src, SoftwarePrefetchHintDistance));
4619       __ sub(octetCounter, octetCounter, 8);
4620       __ subs(rscratch1, octetCounter, large_loop_threshold);
4621       inflate_and_store_2_fp_registers(true, v3, v4);
4622       inflate_and_store_2_fp_registers(true, v5, v6);
4623       __ br(__ GT, LOOP_PRFM);
4624       __ cmp(octetCounter, (u1)8);
4625       __ br(__ LT, DONE);
4626     __ bind(LOOP);
4627       __ ld1(v3, v4, v5, v6, __ T16B, Address(__ post(src, 64)));
4628       __ bind(LOOP_START);
4629       __ sub(octetCounter, octetCounter, 8);
4630       __ cmp(octetCounter, (u1)8);
4631       inflate_and_store_2_fp_registers(false, v3, v4);
4632       inflate_and_store_2_fp_registers(false, v5, v6);
4633       __ br(__ GE, LOOP);
4634     __ bind(DONE);
4635       __ ret(lr);
4636     return entry;
4637   }
4638 
4639   /**
4640    *  Arguments:
4641    *
4642    *  Input:
4643    *  c_rarg0   - current state address
4644    *  c_rarg1   - H key address
4645    *  c_rarg2   - data address
4646    *  c_rarg3   - number of blocks
4647    *
4648    *  Output:
4649    *  Updated state at c_rarg0
4650    */
4651   address generate_ghash_processBlocks() {
4652     // Bafflingly, GCM uses little-endian for the byte order, but
4653     // big-endian for the bit order.  For example, the polynomial 1 is
4654     // represented as the 16-byte string 80 00 00 00 | 12 bytes of 00.
4655     //
4656     // So, we must either reverse the bytes in each word and do
4657     // everything big-endian or reverse the bits in each byte and do
4658     // it little-endian.  On AArch64 it&#39;s more idiomatic to reverse
4659     // the bits in each byte (we have an instruction, RBIT, to do
4660     // that) and keep the data in little-endian bit order throught the
4661     // calculation, bit-reversing the inputs and outputs.
4662 
4663     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;ghash_processBlocks&quot;);
4664     __ align(wordSize * 2);
4665     address p = __ pc();
4666     __ emit_int64(0x87);  // The low-order bits of the field
4667                           // polynomial (i.e. p = z^7+z^2+z+1)
4668                           // repeated in the low and high parts of a
4669                           // 128-bit vector
4670     __ emit_int64(0x87);
4671 
4672     __ align(CodeEntryAlignment);
4673     address start = __ pc();
4674 
4675     Register state   = c_rarg0;
4676     Register subkeyH = c_rarg1;
4677     Register data    = c_rarg2;
4678     Register blocks  = c_rarg3;
4679 
4680     FloatRegister vzr = v30;
4681     __ eor(vzr, __ T16B, vzr, vzr); // zero register
4682 
4683     __ ldrq(v0, Address(state));
4684     __ ldrq(v1, Address(subkeyH));
4685 
4686     __ rev64(v0, __ T16B, v0);          // Bit-reverse words in state and subkeyH
4687     __ rbit(v0, __ T16B, v0);
4688     __ rev64(v1, __ T16B, v1);
4689     __ rbit(v1, __ T16B, v1);
4690 
4691     __ ldrq(v26, p);
4692 
4693     __ ext(v16, __ T16B, v1, v1, 0x08); // long-swap subkeyH into v1
4694     __ eor(v16, __ T16B, v16, v1);      // xor subkeyH into subkeyL (Karatsuba: (A1+A0))
4695 
4696     {
4697       Label L_ghash_loop;
4698       __ bind(L_ghash_loop);
4699 
4700       __ ldrq(v2, Address(__ post(data, 0x10))); // Load the data, bit
4701                                                  // reversing each byte
4702       __ rbit(v2, __ T16B, v2);
4703       __ eor(v2, __ T16B, v0, v2);   // bit-swapped data ^ bit-swapped state
4704 
4705       // Multiply state in v2 by subkey in v1
4706       ghash_multiply(/*result_lo*/v5, /*result_hi*/v7,
4707                      /*a*/v1, /*b*/v2, /*a1_xor_a0*/v16,
4708                      /*temps*/v6, v20, v18, v21);
4709       // Reduce v7:v5 by the field polynomial
4710       ghash_reduce(v0, v5, v7, v26, vzr, v20);
4711 
4712       __ sub(blocks, blocks, 1);
4713       __ cbnz(blocks, L_ghash_loop);
4714     }
4715 
4716     // The bit-reversed result is at this point in v0
4717     __ rev64(v1, __ T16B, v0);
4718     __ rbit(v1, __ T16B, v1);
4719 
4720     __ st1(v1, __ T16B, state);
4721     __ ret(lr);
4722 
4723     return start;
4724   }
4725 
4726   // Continuation point for throwing of implicit exceptions that are
4727   // not handled in the current activation. Fabricates an exception
4728   // oop and initiates normal exception dispatching in this
4729   // frame. Since we need to preserve callee-saved values (currently
4730   // only for C2, but done for C1 as well) we need a callee-saved oop
4731   // map and therefore have to make these stubs into RuntimeStubs
4732   // rather than BufferBlobs.  If the compiler needs all registers to
4733   // be preserved between the fault point and the exception handler
4734   // then it must assume responsibility for that in
4735   // AbstractCompiler::continuation_for_implicit_null_exception or
4736   // continuation_for_implicit_division_by_zero_exception. All other
4737   // implicit exceptions (e.g., NullPointerException or
4738   // AbstractMethodError on entry) are either at call sites or
4739   // otherwise assume that stack unwinding will be initiated, so
4740   // caller saved registers were assumed volatile in the compiler.
4741 
4742 #undef __
4743 #define __ masm-&gt;
4744 
4745   address generate_throw_exception(const char* name,
4746                                    address runtime_entry,
4747                                    Register arg1 = noreg,
4748                                    Register arg2 = noreg) {
4749     // Information about frame layout at time of blocking runtime call.
4750     // Note that we only have to preserve callee-saved registers since
4751     // the compilers are responsible for supplying a continuation point
4752     // if they expect all registers to be preserved.
4753     // n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0
4754     enum layout {
4755       rfp_off = 0,
4756       rfp_off2,
4757       return_off,
4758       return_off2,
4759       framesize // inclusive of return address
4760     };
4761 
4762     int insts_size = 512;
4763     int locs_size  = 64;
4764 
4765     CodeBuffer code(name, insts_size, locs_size);
4766     OopMapSet* oop_maps  = new OopMapSet();
4767     MacroAssembler* masm = new MacroAssembler(&amp;code);
4768 
4769     address start = __ pc();
4770 
4771     // This is an inlined and slightly modified version of call_VM
4772     // which has the ability to fetch the return PC out of
4773     // thread-local storage and also sets up last_Java_sp slightly
4774     // differently than the real call_VM
4775 
4776     __ enter(); // Save FP and LR before call
4777 
4778     assert(is_even(framesize/2), &quot;sp not 16-byte aligned&quot;);
4779 
4780     // lr and fp are already in place
4781     __ sub(sp, rfp, ((unsigned)framesize-4) &lt;&lt; LogBytesPerInt); // prolog
4782 
4783     int frame_complete = __ pc() - start;
4784 
4785     // Set up last_Java_sp and last_Java_fp
4786     address the_pc = __ pc();
4787     __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);
4788 
4789     // Call runtime
4790     if (arg1 != noreg) {
4791       assert(arg2 != c_rarg1, &quot;clobbered&quot;);
4792       __ mov(c_rarg1, arg1);
4793     }
4794     if (arg2 != noreg) {
4795       __ mov(c_rarg2, arg2);
4796     }
4797     __ mov(c_rarg0, rthread);
4798     BLOCK_COMMENT(&quot;call runtime_entry&quot;);
4799     __ mov(rscratch1, runtime_entry);
4800     __ blr(rscratch1);
4801 
4802     // Generate oop map
4803     OopMap* map = new OopMap(framesize, 0);
4804 
4805     oop_maps-&gt;add_gc_map(the_pc - start, map);
4806 
4807     __ reset_last_Java_frame(true);
4808     __ maybe_isb();
4809 
4810     __ leave();
4811 
4812     // check for pending exceptions
4813 #ifdef ASSERT
4814     Label L;
4815     __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
4816     __ cbnz(rscratch1, L);
4817     __ should_not_reach_here();
4818     __ bind(L);
4819 #endif // ASSERT
4820     __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
4821 
4822 
4823     // codeBlob framesize is in words (not VMRegImpl::slot_size)
4824     RuntimeStub* stub =
4825       RuntimeStub::new_runtime_stub(name,
4826                                     &amp;code,
4827                                     frame_complete,
4828                                     (framesize &gt;&gt; (LogBytesPerWord - LogBytesPerInt)),
4829                                     oop_maps, false);
4830     return stub-&gt;entry_point();
4831   }
4832 
4833   class MontgomeryMultiplyGenerator : public MacroAssembler {
4834 
4835     Register Pa_base, Pb_base, Pn_base, Pm_base, inv, Rlen, Ra, Rb, Rm, Rn,
4836       Pa, Pb, Pn, Pm, Rhi_ab, Rlo_ab, Rhi_mn, Rlo_mn, t0, t1, t2, Ri, Rj;
4837 
4838     RegSet _toSave;
4839     bool _squaring;
4840 
4841   public:
4842     MontgomeryMultiplyGenerator (Assembler *as, bool squaring)
4843       : MacroAssembler(as-&gt;code()), _squaring(squaring) {
4844 
4845       // Register allocation
4846 
4847       Register reg = c_rarg0;
4848       Pa_base = reg;       // Argument registers
4849       if (squaring)
4850         Pb_base = Pa_base;
4851       else
<a name="30" id="anc30"></a><span class="line-modified">4852         Pb_base = next_reg(reg);</span>
<span class="line-modified">4853       Pn_base = next_reg(reg);</span>
<span class="line-modified">4854       Rlen= next_reg(reg);</span>
<span class="line-modified">4855       inv = next_reg(reg);</span>
<span class="line-modified">4856       Pm_base = next_reg(reg);</span>
4857 
4858                           // Working registers:
<a name="31" id="anc31"></a><span class="line-modified">4859       Ra =  next_reg(reg); // The current digit of a, b, n, and m.</span>
<span class="line-modified">4860       Rb =  next_reg(reg);</span>
<span class="line-modified">4861       Rm =  next_reg(reg);</span>
<span class="line-modified">4862       Rn =  next_reg(reg);</span>
4863 
<a name="32" id="anc32"></a><span class="line-modified">4864       Pa =  next_reg(reg); // Pointers to the current/next digit of a, b, n, and m.</span>
<span class="line-modified">4865       Pb =  next_reg(reg);</span>
<span class="line-modified">4866       Pm =  next_reg(reg);</span>
<span class="line-modified">4867       Pn =  next_reg(reg);</span>
4868 
<a name="33" id="anc33"></a><span class="line-modified">4869       t0 =  next_reg(reg); // Three registers which form a</span>
<span class="line-modified">4870       t1 =  next_reg(reg); // triple-precision accumuator.</span>
<span class="line-modified">4871       t2 =  next_reg(reg);</span>
4872 
<a name="34" id="anc34"></a><span class="line-modified">4873       Ri =  next_reg(reg); // Inner and outer loop indexes.</span>
<span class="line-modified">4874       Rj =  next_reg(reg);</span>
4875 
<a name="35" id="anc35"></a><span class="line-modified">4876       Rhi_ab = next_reg(reg); // Product registers: low and high parts</span>
<span class="line-modified">4877       Rlo_ab = next_reg(reg); // of a*b and m*n.</span>
<span class="line-modified">4878       Rhi_mn = next_reg(reg);</span>
<span class="line-modified">4879       Rlo_mn = next_reg(reg);</span>
4880 
4881       // r19 and up are callee-saved.
4882       _toSave = RegSet::range(r19, reg) + Pm_base;
4883     }
4884 
4885   private:
<a name="36" id="anc36"></a><span class="line-added">4886     Register next_reg(Register &amp;reg) {</span>
<span class="line-added">4887 #ifdef _WIN64</span>
<span class="line-added">4888       // skip r18 on Windows, it&#39;s used by native TLS</span>
<span class="line-added">4889       return ++reg == r18 ? ++reg : reg;</span>
<span class="line-added">4890 #else</span>
<span class="line-added">4891       return ++reg;</span>
<span class="line-added">4892 #endif</span>
<span class="line-added">4893     }</span>
<span class="line-added">4894 </span>
4895     void save_regs() {
4896       push(_toSave, sp);
4897     }
4898 
4899     void restore_regs() {
4900       pop(_toSave, sp);
4901     }
4902 
4903     template &lt;typename T&gt;
4904     void unroll_2(Register count, T block) {
4905       Label loop, end, odd;
4906       tbnz(count, 0, odd);
4907       cbz(count, end);
4908       align(16);
4909       bind(loop);
4910       (this-&gt;*block)();
4911       bind(odd);
4912       (this-&gt;*block)();
4913       subs(count, count, 2);
4914       br(Assembler::GT, loop);
4915       bind(end);
4916     }
4917 
4918     template &lt;typename T&gt;
4919     void unroll_2(Register count, T block, Register d, Register s, Register tmp) {
4920       Label loop, end, odd;
4921       tbnz(count, 0, odd);
4922       cbz(count, end);
4923       align(16);
4924       bind(loop);
4925       (this-&gt;*block)(d, s, tmp);
4926       bind(odd);
4927       (this-&gt;*block)(d, s, tmp);
4928       subs(count, count, 2);
4929       br(Assembler::GT, loop);
4930       bind(end);
4931     }
4932 
4933     void pre1(RegisterOrConstant i) {
4934       block_comment(&quot;pre1&quot;);
4935       // Pa = Pa_base;
4936       // Pb = Pb_base + i;
4937       // Pm = Pm_base;
4938       // Pn = Pn_base + i;
4939       // Ra = *Pa;
4940       // Rb = *Pb;
4941       // Rm = *Pm;
4942       // Rn = *Pn;
4943       ldr(Ra, Address(Pa_base));
4944       ldr(Rb, Address(Pb_base, i, Address::uxtw(LogBytesPerWord)));
4945       ldr(Rm, Address(Pm_base));
4946       ldr(Rn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
4947       lea(Pa, Address(Pa_base));
4948       lea(Pb, Address(Pb_base, i, Address::uxtw(LogBytesPerWord)));
4949       lea(Pm, Address(Pm_base));
4950       lea(Pn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
4951 
4952       // Zero the m*n result.
4953       mov(Rhi_mn, zr);
4954       mov(Rlo_mn, zr);
4955     }
4956 
4957     // The core multiply-accumulate step of a Montgomery
4958     // multiplication.  The idea is to schedule operations as a
4959     // pipeline so that instructions with long latencies (loads and
4960     // multiplies) have time to complete before their results are
4961     // used.  This most benefits in-order implementations of the
4962     // architecture but out-of-order ones also benefit.
4963     void step() {
4964       block_comment(&quot;step&quot;);
4965       // MACC(Ra, Rb, t0, t1, t2);
4966       // Ra = *++Pa;
4967       // Rb = *--Pb;
4968       umulh(Rhi_ab, Ra, Rb);
4969       mul(Rlo_ab, Ra, Rb);
4970       ldr(Ra, pre(Pa, wordSize));
4971       ldr(Rb, pre(Pb, -wordSize));
4972       acc(Rhi_mn, Rlo_mn, t0, t1, t2); // The pending m*n from the
4973                                        // previous iteration.
4974       // MACC(Rm, Rn, t0, t1, t2);
4975       // Rm = *++Pm;
4976       // Rn = *--Pn;
4977       umulh(Rhi_mn, Rm, Rn);
4978       mul(Rlo_mn, Rm, Rn);
4979       ldr(Rm, pre(Pm, wordSize));
4980       ldr(Rn, pre(Pn, -wordSize));
4981       acc(Rhi_ab, Rlo_ab, t0, t1, t2);
4982     }
4983 
4984     void post1() {
4985       block_comment(&quot;post1&quot;);
4986 
4987       // MACC(Ra, Rb, t0, t1, t2);
4988       // Ra = *++Pa;
4989       // Rb = *--Pb;
4990       umulh(Rhi_ab, Ra, Rb);
4991       mul(Rlo_ab, Ra, Rb);
4992       acc(Rhi_mn, Rlo_mn, t0, t1, t2);  // The pending m*n
4993       acc(Rhi_ab, Rlo_ab, t0, t1, t2);
4994 
4995       // *Pm = Rm = t0 * inv;
4996       mul(Rm, t0, inv);
4997       str(Rm, Address(Pm));
4998 
4999       // MACC(Rm, Rn, t0, t1, t2);
5000       // t0 = t1; t1 = t2; t2 = 0;
5001       umulh(Rhi_mn, Rm, Rn);
5002 
5003 #ifndef PRODUCT
5004       // assert(m[i] * n[0] + t0 == 0, &quot;broken Montgomery multiply&quot;);
5005       {
5006         mul(Rlo_mn, Rm, Rn);
5007         add(Rlo_mn, t0, Rlo_mn);
5008         Label ok;
5009         cbz(Rlo_mn, ok); {
5010           stop(&quot;broken Montgomery multiply&quot;);
5011         } bind(ok);
5012       }
5013 #endif
5014       // We have very carefully set things up so that
5015       // m[i]*n[0] + t0 == 0 (mod b), so we don&#39;t have to calculate
5016       // the lower half of Rm * Rn because we know the result already:
5017       // it must be -t0.  t0 + (-t0) must generate a carry iff
5018       // t0 != 0.  So, rather than do a mul and an adds we just set
5019       // the carry flag iff t0 is nonzero.
5020       //
5021       // mul(Rlo_mn, Rm, Rn);
5022       // adds(zr, t0, Rlo_mn);
5023       subs(zr, t0, 1); // Set carry iff t0 is nonzero
5024       adcs(t0, t1, Rhi_mn);
5025       adc(t1, t2, zr);
5026       mov(t2, zr);
5027     }
5028 
5029     void pre2(RegisterOrConstant i, RegisterOrConstant len) {
5030       block_comment(&quot;pre2&quot;);
5031       // Pa = Pa_base + i-len;
5032       // Pb = Pb_base + len;
5033       // Pm = Pm_base + i-len;
5034       // Pn = Pn_base + len;
5035 
5036       if (i.is_register()) {
5037         sub(Rj, i.as_register(), len);
5038       } else {
5039         mov(Rj, i.as_constant());
5040         sub(Rj, Rj, len);
5041       }
5042       // Rj == i-len
5043 
5044       lea(Pa, Address(Pa_base, Rj, Address::uxtw(LogBytesPerWord)));
5045       lea(Pb, Address(Pb_base, len, Address::uxtw(LogBytesPerWord)));
5046       lea(Pm, Address(Pm_base, Rj, Address::uxtw(LogBytesPerWord)));
5047       lea(Pn, Address(Pn_base, len, Address::uxtw(LogBytesPerWord)));
5048 
5049       // Ra = *++Pa;
5050       // Rb = *--Pb;
5051       // Rm = *++Pm;
5052       // Rn = *--Pn;
5053       ldr(Ra, pre(Pa, wordSize));
5054       ldr(Rb, pre(Pb, -wordSize));
5055       ldr(Rm, pre(Pm, wordSize));
5056       ldr(Rn, pre(Pn, -wordSize));
5057 
5058       mov(Rhi_mn, zr);
5059       mov(Rlo_mn, zr);
5060     }
5061 
5062     void post2(RegisterOrConstant i, RegisterOrConstant len) {
5063       block_comment(&quot;post2&quot;);
5064       if (i.is_constant()) {
5065         mov(Rj, i.as_constant()-len.as_constant());
5066       } else {
5067         sub(Rj, i.as_register(), len);
5068       }
5069 
5070       adds(t0, t0, Rlo_mn); // The pending m*n, low part
5071 
5072       // As soon as we know the least significant digit of our result,
5073       // store it.
5074       // Pm_base[i-len] = t0;
5075       str(t0, Address(Pm_base, Rj, Address::uxtw(LogBytesPerWord)));
5076 
5077       // t0 = t1; t1 = t2; t2 = 0;
5078       adcs(t0, t1, Rhi_mn); // The pending m*n, high part
5079       adc(t1, t2, zr);
5080       mov(t2, zr);
5081     }
5082 
5083     // A carry in t0 after Montgomery multiplication means that we
5084     // should subtract multiples of n from our result in m.  We&#39;ll
5085     // keep doing that until there is no carry.
5086     void normalize(RegisterOrConstant len) {
5087       block_comment(&quot;normalize&quot;);
5088       // while (t0)
5089       //   t0 = sub(Pm_base, Pn_base, t0, len);
5090       Label loop, post, again;
5091       Register cnt = t1, i = t2; // Re-use registers; we&#39;re done with them now
5092       cbz(t0, post); {
5093         bind(again); {
5094           mov(i, zr);
5095           mov(cnt, len);
5096           ldr(Rm, Address(Pm_base, i, Address::uxtw(LogBytesPerWord)));
5097           ldr(Rn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
5098           subs(zr, zr, zr); // set carry flag, i.e. no borrow
5099           align(16);
5100           bind(loop); {
5101             sbcs(Rm, Rm, Rn);
5102             str(Rm, Address(Pm_base, i, Address::uxtw(LogBytesPerWord)));
5103             add(i, i, 1);
5104             ldr(Rm, Address(Pm_base, i, Address::uxtw(LogBytesPerWord)));
5105             ldr(Rn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
5106             sub(cnt, cnt, 1);
5107           } cbnz(cnt, loop);
5108           sbc(t0, t0, zr);
5109         } cbnz(t0, again);
5110       } bind(post);
5111     }
5112 
5113     // Move memory at s to d, reversing words.
5114     //    Increments d to end of copied memory
5115     //    Destroys tmp1, tmp2
5116     //    Preserves len
5117     //    Leaves s pointing to the address which was in d at start
5118     void reverse(Register d, Register s, Register len, Register tmp1, Register tmp2) {
5119       assert(tmp1 &lt; r19 &amp;&amp; tmp2 &lt; r19, &quot;register corruption&quot;);
5120 
5121       lea(s, Address(s, len, Address::uxtw(LogBytesPerWord)));
5122       mov(tmp1, len);
5123       unroll_2(tmp1, &amp;MontgomeryMultiplyGenerator::reverse1, d, s, tmp2);
5124       sub(s, d, len, ext::uxtw, LogBytesPerWord);
5125     }
5126     // where
5127     void reverse1(Register d, Register s, Register tmp) {
5128       ldr(tmp, pre(s, -wordSize));
5129       ror(tmp, tmp, 32);
5130       str(tmp, post(d, wordSize));
5131     }
5132 
5133     void step_squaring() {
5134       // An extra ACC
5135       step();
5136       acc(Rhi_ab, Rlo_ab, t0, t1, t2);
5137     }
5138 
5139     void last_squaring(RegisterOrConstant i) {
5140       Label dont;
5141       // if ((i &amp; 1) == 0) {
5142       tbnz(i.as_register(), 0, dont); {
5143         // MACC(Ra, Rb, t0, t1, t2);
5144         // Ra = *++Pa;
5145         // Rb = *--Pb;
5146         umulh(Rhi_ab, Ra, Rb);
5147         mul(Rlo_ab, Ra, Rb);
5148         acc(Rhi_ab, Rlo_ab, t0, t1, t2);
5149       } bind(dont);
5150     }
5151 
5152     void extra_step_squaring() {
5153       acc(Rhi_mn, Rlo_mn, t0, t1, t2);  // The pending m*n
5154 
5155       // MACC(Rm, Rn, t0, t1, t2);
5156       // Rm = *++Pm;
5157       // Rn = *--Pn;
5158       umulh(Rhi_mn, Rm, Rn);
5159       mul(Rlo_mn, Rm, Rn);
5160       ldr(Rm, pre(Pm, wordSize));
5161       ldr(Rn, pre(Pn, -wordSize));
5162     }
5163 
5164     void post1_squaring() {
5165       acc(Rhi_mn, Rlo_mn, t0, t1, t2);  // The pending m*n
5166 
5167       // *Pm = Rm = t0 * inv;
5168       mul(Rm, t0, inv);
5169       str(Rm, Address(Pm));
5170 
5171       // MACC(Rm, Rn, t0, t1, t2);
5172       // t0 = t1; t1 = t2; t2 = 0;
5173       umulh(Rhi_mn, Rm, Rn);
5174 
5175 #ifndef PRODUCT
5176       // assert(m[i] * n[0] + t0 == 0, &quot;broken Montgomery multiply&quot;);
5177       {
5178         mul(Rlo_mn, Rm, Rn);
5179         add(Rlo_mn, t0, Rlo_mn);
5180         Label ok;
5181         cbz(Rlo_mn, ok); {
5182           stop(&quot;broken Montgomery multiply&quot;);
5183         } bind(ok);
5184       }
5185 #endif
5186       // We have very carefully set things up so that
5187       // m[i]*n[0] + t0 == 0 (mod b), so we don&#39;t have to calculate
5188       // the lower half of Rm * Rn because we know the result already:
5189       // it must be -t0.  t0 + (-t0) must generate a carry iff
5190       // t0 != 0.  So, rather than do a mul and an adds we just set
5191       // the carry flag iff t0 is nonzero.
5192       //
5193       // mul(Rlo_mn, Rm, Rn);
5194       // adds(zr, t0, Rlo_mn);
5195       subs(zr, t0, 1); // Set carry iff t0 is nonzero
5196       adcs(t0, t1, Rhi_mn);
5197       adc(t1, t2, zr);
5198       mov(t2, zr);
5199     }
5200 
5201     void acc(Register Rhi, Register Rlo,
5202              Register t0, Register t1, Register t2) {
5203       adds(t0, t0, Rlo);
5204       adcs(t1, t1, Rhi);
5205       adc(t2, t2, zr);
5206     }
5207 
5208   public:
5209     /**
5210      * Fast Montgomery multiplication.  The derivation of the
5211      * algorithm is in A Cryptographic Library for the Motorola
5212      * DSP56000, Dusse and Kaliski, Proc. EUROCRYPT 90, pp. 230-237.
5213      *
5214      * Arguments:
5215      *
5216      * Inputs for multiplication:
5217      *   c_rarg0   - int array elements a
5218      *   c_rarg1   - int array elements b
5219      *   c_rarg2   - int array elements n (the modulus)
5220      *   c_rarg3   - int length
5221      *   c_rarg4   - int inv
5222      *   c_rarg5   - int array elements m (the result)
5223      *
5224      * Inputs for squaring:
5225      *   c_rarg0   - int array elements a
5226      *   c_rarg1   - int array elements n (the modulus)
5227      *   c_rarg2   - int length
5228      *   c_rarg3   - int inv
5229      *   c_rarg4   - int array elements m (the result)
5230      *
5231      */
5232     address generate_multiply() {
5233       Label argh, nothing;
5234       bind(argh);
5235       stop(&quot;MontgomeryMultiply total_allocation must be &lt;= 8192&quot;);
5236 
5237       align(CodeEntryAlignment);
5238       address entry = pc();
5239 
5240       cbzw(Rlen, nothing);
5241 
5242       enter();
5243 
5244       // Make room.
5245       cmpw(Rlen, 512);
5246       br(Assembler::HI, argh);
5247       sub(Ra, sp, Rlen, ext::uxtw, exact_log2(4 * sizeof (jint)));
5248       andr(sp, Ra, -2 * wordSize);
5249 
5250       lsrw(Rlen, Rlen, 1);  // length in longwords = len/2
5251 
5252       {
5253         // Copy input args, reversing as we go.  We use Ra as a
5254         // temporary variable.
5255         reverse(Ra, Pa_base, Rlen, t0, t1);
5256         if (!_squaring)
5257           reverse(Ra, Pb_base, Rlen, t0, t1);
5258         reverse(Ra, Pn_base, Rlen, t0, t1);
5259       }
5260 
5261       // Push all call-saved registers and also Pm_base which we&#39;ll need
5262       // at the end.
5263       save_regs();
5264 
5265 #ifndef PRODUCT
5266       // assert(inv * n[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
5267       {
5268         ldr(Rn, Address(Pn_base, 0));
5269         mul(Rlo_mn, Rn, inv);
5270         subs(zr, Rlo_mn, -1);
5271         Label ok;
5272         br(EQ, ok); {
5273           stop(&quot;broken inverse in Montgomery multiply&quot;);
5274         } bind(ok);
5275       }
5276 #endif
5277 
5278       mov(Pm_base, Ra);
5279 
5280       mov(t0, zr);
5281       mov(t1, zr);
5282       mov(t2, zr);
5283 
5284       block_comment(&quot;for (int i = 0; i &lt; len; i++) {&quot;);
5285       mov(Ri, zr); {
5286         Label loop, end;
5287         cmpw(Ri, Rlen);
5288         br(Assembler::GE, end);
5289 
5290         bind(loop);
5291         pre1(Ri);
5292 
5293         block_comment(&quot;  for (j = i; j; j--) {&quot;); {
5294           movw(Rj, Ri);
5295           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step);
5296         } block_comment(&quot;  } // j&quot;);
5297 
5298         post1();
5299         addw(Ri, Ri, 1);
5300         cmpw(Ri, Rlen);
5301         br(Assembler::LT, loop);
5302         bind(end);
5303         block_comment(&quot;} // i&quot;);
5304       }
5305 
5306       block_comment(&quot;for (int i = len; i &lt; 2*len; i++) {&quot;);
5307       mov(Ri, Rlen); {
5308         Label loop, end;
5309         cmpw(Ri, Rlen, Assembler::LSL, 1);
5310         br(Assembler::GE, end);
5311 
5312         bind(loop);
5313         pre2(Ri, Rlen);
5314 
5315         block_comment(&quot;  for (j = len*2-i-1; j; j--) {&quot;); {
5316           lslw(Rj, Rlen, 1);
5317           subw(Rj, Rj, Ri);
5318           subw(Rj, Rj, 1);
5319           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step);
5320         } block_comment(&quot;  } // j&quot;);
5321 
5322         post2(Ri, Rlen);
5323         addw(Ri, Ri, 1);
5324         cmpw(Ri, Rlen, Assembler::LSL, 1);
5325         br(Assembler::LT, loop);
5326         bind(end);
5327       }
5328       block_comment(&quot;} // i&quot;);
5329 
5330       normalize(Rlen);
5331 
5332       mov(Ra, Pm_base);  // Save Pm_base in Ra
5333       restore_regs();  // Restore caller&#39;s Pm_base
5334 
5335       // Copy our result into caller&#39;s Pm_base
5336       reverse(Pm_base, Ra, Rlen, t0, t1);
5337 
5338       leave();
5339       bind(nothing);
5340       ret(lr);
5341 
5342       return entry;
5343     }
5344     // In C, approximately:
5345 
5346     // void
5347     // montgomery_multiply(unsigned long Pa_base[], unsigned long Pb_base[],
5348     //                     unsigned long Pn_base[], unsigned long Pm_base[],
5349     //                     unsigned long inv, int len) {
5350     //   unsigned long t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
5351     //   unsigned long *Pa, *Pb, *Pn, *Pm;
5352     //   unsigned long Ra, Rb, Rn, Rm;
5353 
5354     //   int i;
5355 
5356     //   assert(inv * Pn_base[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
5357 
5358     //   for (i = 0; i &lt; len; i++) {
5359     //     int j;
5360 
5361     //     Pa = Pa_base;
5362     //     Pb = Pb_base + i;
5363     //     Pm = Pm_base;
5364     //     Pn = Pn_base + i;
5365 
5366     //     Ra = *Pa;
5367     //     Rb = *Pb;
5368     //     Rm = *Pm;
5369     //     Rn = *Pn;
5370 
5371     //     int iters = i;
5372     //     for (j = 0; iters--; j++) {
5373     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pb_base[i-j], &quot;must be&quot;);
5374     //       MACC(Ra, Rb, t0, t1, t2);
5375     //       Ra = *++Pa;
5376     //       Rb = *--Pb;
5377     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5378     //       MACC(Rm, Rn, t0, t1, t2);
5379     //       Rm = *++Pm;
5380     //       Rn = *--Pn;
5381     //     }
5382 
5383     //     assert(Ra == Pa_base[i] &amp;&amp; Rb == Pb_base[0], &quot;must be&quot;);
5384     //     MACC(Ra, Rb, t0, t1, t2);
5385     //     *Pm = Rm = t0 * inv;
5386     //     assert(Rm == Pm_base[i] &amp;&amp; Rn == Pn_base[0], &quot;must be&quot;);
5387     //     MACC(Rm, Rn, t0, t1, t2);
5388 
5389     //     assert(t0 == 0, &quot;broken Montgomery multiply&quot;);
5390 
5391     //     t0 = t1; t1 = t2; t2 = 0;
5392     //   }
5393 
5394     //   for (i = len; i &lt; 2*len; i++) {
5395     //     int j;
5396 
5397     //     Pa = Pa_base + i-len;
5398     //     Pb = Pb_base + len;
5399     //     Pm = Pm_base + i-len;
5400     //     Pn = Pn_base + len;
5401 
5402     //     Ra = *++Pa;
5403     //     Rb = *--Pb;
5404     //     Rm = *++Pm;
5405     //     Rn = *--Pn;
5406 
5407     //     int iters = len*2-i-1;
5408     //     for (j = i-len+1; iters--; j++) {
5409     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pb_base[i-j], &quot;must be&quot;);
5410     //       MACC(Ra, Rb, t0, t1, t2);
5411     //       Ra = *++Pa;
5412     //       Rb = *--Pb;
5413     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5414     //       MACC(Rm, Rn, t0, t1, t2);
5415     //       Rm = *++Pm;
5416     //       Rn = *--Pn;
5417     //     }
5418 
5419     //     Pm_base[i-len] = t0;
5420     //     t0 = t1; t1 = t2; t2 = 0;
5421     //   }
5422 
5423     //   while (t0)
5424     //     t0 = sub(Pm_base, Pn_base, t0, len);
5425     // }
5426 
5427     /**
5428      * Fast Montgomery squaring.  This uses asymptotically 25% fewer
5429      * multiplies than Montgomery multiplication so it should be up to
5430      * 25% faster.  However, its loop control is more complex and it
5431      * may actually run slower on some machines.
5432      *
5433      * Arguments:
5434      *
5435      * Inputs:
5436      *   c_rarg0   - int array elements a
5437      *   c_rarg1   - int array elements n (the modulus)
5438      *   c_rarg2   - int length
5439      *   c_rarg3   - int inv
5440      *   c_rarg4   - int array elements m (the result)
5441      *
5442      */
5443     address generate_square() {
5444       Label argh;
5445       bind(argh);
5446       stop(&quot;MontgomeryMultiply total_allocation must be &lt;= 8192&quot;);
5447 
5448       align(CodeEntryAlignment);
5449       address entry = pc();
5450 
5451       enter();
5452 
5453       // Make room.
5454       cmpw(Rlen, 512);
5455       br(Assembler::HI, argh);
5456       sub(Ra, sp, Rlen, ext::uxtw, exact_log2(4 * sizeof (jint)));
5457       andr(sp, Ra, -2 * wordSize);
5458 
5459       lsrw(Rlen, Rlen, 1);  // length in longwords = len/2
5460 
5461       {
5462         // Copy input args, reversing as we go.  We use Ra as a
5463         // temporary variable.
5464         reverse(Ra, Pa_base, Rlen, t0, t1);
5465         reverse(Ra, Pn_base, Rlen, t0, t1);
5466       }
5467 
5468       // Push all call-saved registers and also Pm_base which we&#39;ll need
5469       // at the end.
5470       save_regs();
5471 
5472       mov(Pm_base, Ra);
5473 
5474       mov(t0, zr);
5475       mov(t1, zr);
5476       mov(t2, zr);
5477 
5478       block_comment(&quot;for (int i = 0; i &lt; len; i++) {&quot;);
5479       mov(Ri, zr); {
5480         Label loop, end;
5481         bind(loop);
5482         cmp(Ri, Rlen);
5483         br(Assembler::GE, end);
5484 
5485         pre1(Ri);
5486 
5487         block_comment(&quot;for (j = (i+1)/2; j; j--) {&quot;); {
5488           add(Rj, Ri, 1);
5489           lsr(Rj, Rj, 1);
5490           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step_squaring);
5491         } block_comment(&quot;  } // j&quot;);
5492 
5493         last_squaring(Ri);
5494 
5495         block_comment(&quot;  for (j = i/2; j; j--) {&quot;); {
5496           lsr(Rj, Ri, 1);
5497           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::extra_step_squaring);
5498         } block_comment(&quot;  } // j&quot;);
5499 
5500         post1_squaring();
5501         add(Ri, Ri, 1);
5502         cmp(Ri, Rlen);
5503         br(Assembler::LT, loop);
5504 
5505         bind(end);
5506         block_comment(&quot;} // i&quot;);
5507       }
5508 
5509       block_comment(&quot;for (int i = len; i &lt; 2*len; i++) {&quot;);
5510       mov(Ri, Rlen); {
5511         Label loop, end;
5512         bind(loop);
5513         cmp(Ri, Rlen, Assembler::LSL, 1);
5514         br(Assembler::GE, end);
5515 
5516         pre2(Ri, Rlen);
5517 
5518         block_comment(&quot;  for (j = (2*len-i-1)/2; j; j--) {&quot;); {
5519           lsl(Rj, Rlen, 1);
5520           sub(Rj, Rj, Ri);
5521           sub(Rj, Rj, 1);
5522           lsr(Rj, Rj, 1);
5523           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step_squaring);
5524         } block_comment(&quot;  } // j&quot;);
5525 
5526         last_squaring(Ri);
5527 
5528         block_comment(&quot;  for (j = (2*len-i)/2; j; j--) {&quot;); {
5529           lsl(Rj, Rlen, 1);
5530           sub(Rj, Rj, Ri);
5531           lsr(Rj, Rj, 1);
5532           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::extra_step_squaring);
5533         } block_comment(&quot;  } // j&quot;);
5534 
5535         post2(Ri, Rlen);
5536         add(Ri, Ri, 1);
5537         cmp(Ri, Rlen, Assembler::LSL, 1);
5538 
5539         br(Assembler::LT, loop);
5540         bind(end);
5541         block_comment(&quot;} // i&quot;);
5542       }
5543 
5544       normalize(Rlen);
5545 
5546       mov(Ra, Pm_base);  // Save Pm_base in Ra
5547       restore_regs();  // Restore caller&#39;s Pm_base
5548 
5549       // Copy our result into caller&#39;s Pm_base
5550       reverse(Pm_base, Ra, Rlen, t0, t1);
5551 
5552       leave();
5553       ret(lr);
5554 
5555       return entry;
5556     }
5557     // In C, approximately:
5558 
5559     // void
5560     // montgomery_square(unsigned long Pa_base[], unsigned long Pn_base[],
5561     //                   unsigned long Pm_base[], unsigned long inv, int len) {
5562     //   unsigned long t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
5563     //   unsigned long *Pa, *Pb, *Pn, *Pm;
5564     //   unsigned long Ra, Rb, Rn, Rm;
5565 
5566     //   int i;
5567 
5568     //   assert(inv * Pn_base[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
5569 
5570     //   for (i = 0; i &lt; len; i++) {
5571     //     int j;
5572 
5573     //     Pa = Pa_base;
5574     //     Pb = Pa_base + i;
5575     //     Pm = Pm_base;
5576     //     Pn = Pn_base + i;
5577 
5578     //     Ra = *Pa;
5579     //     Rb = *Pb;
5580     //     Rm = *Pm;
5581     //     Rn = *Pn;
5582 
5583     //     int iters = (i+1)/2;
5584     //     for (j = 0; iters--; j++) {
5585     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pa_base[i-j], &quot;must be&quot;);
5586     //       MACC2(Ra, Rb, t0, t1, t2);
5587     //       Ra = *++Pa;
5588     //       Rb = *--Pb;
5589     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5590     //       MACC(Rm, Rn, t0, t1, t2);
5591     //       Rm = *++Pm;
5592     //       Rn = *--Pn;
5593     //     }
5594     //     if ((i &amp; 1) == 0) {
5595     //       assert(Ra == Pa_base[j], &quot;must be&quot;);
5596     //       MACC(Ra, Ra, t0, t1, t2);
5597     //     }
5598     //     iters = i/2;
5599     //     assert(iters == i-j, &quot;must be&quot;);
5600     //     for (; iters--; j++) {
5601     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5602     //       MACC(Rm, Rn, t0, t1, t2);
5603     //       Rm = *++Pm;
5604     //       Rn = *--Pn;
5605     //     }
5606 
5607     //     *Pm = Rm = t0 * inv;
5608     //     assert(Rm == Pm_base[i] &amp;&amp; Rn == Pn_base[0], &quot;must be&quot;);
5609     //     MACC(Rm, Rn, t0, t1, t2);
5610 
5611     //     assert(t0 == 0, &quot;broken Montgomery multiply&quot;);
5612 
5613     //     t0 = t1; t1 = t2; t2 = 0;
5614     //   }
5615 
5616     //   for (i = len; i &lt; 2*len; i++) {
5617     //     int start = i-len+1;
5618     //     int end = start + (len - start)/2;
5619     //     int j;
5620 
5621     //     Pa = Pa_base + i-len;
5622     //     Pb = Pa_base + len;
5623     //     Pm = Pm_base + i-len;
5624     //     Pn = Pn_base + len;
5625 
5626     //     Ra = *++Pa;
5627     //     Rb = *--Pb;
5628     //     Rm = *++Pm;
5629     //     Rn = *--Pn;
5630 
5631     //     int iters = (2*len-i-1)/2;
5632     //     assert(iters == end-start, &quot;must be&quot;);
5633     //     for (j = start; iters--; j++) {
5634     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pa_base[i-j], &quot;must be&quot;);
5635     //       MACC2(Ra, Rb, t0, t1, t2);
5636     //       Ra = *++Pa;
5637     //       Rb = *--Pb;
5638     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5639     //       MACC(Rm, Rn, t0, t1, t2);
5640     //       Rm = *++Pm;
5641     //       Rn = *--Pn;
5642     //     }
5643     //     if ((i &amp; 1) == 0) {
5644     //       assert(Ra == Pa_base[j], &quot;must be&quot;);
5645     //       MACC(Ra, Ra, t0, t1, t2);
5646     //     }
5647     //     iters =  (2*len-i)/2;
5648     //     assert(iters == len-j, &quot;must be&quot;);
5649     //     for (; iters--; j++) {
5650     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5651     //       MACC(Rm, Rn, t0, t1, t2);
5652     //       Rm = *++Pm;
5653     //       Rn = *--Pn;
5654     //     }
5655     //     Pm_base[i-len] = t0;
5656     //     t0 = t1; t1 = t2; t2 = 0;
5657     //   }
5658 
5659     //   while (t0)
5660     //     t0 = sub(Pm_base, Pn_base, t0, len);
5661     // }
5662   };
5663 
5664 
5665   // Initialization
5666   void generate_initial() {
5667     // Generate initial stubs and initializes the entry points
5668 
5669     // entry points that exist in all platforms Note: This is code
5670     // that could be shared among different platforms - however the
5671     // benefit seems to be smaller than the disadvantage of having a
5672     // much more complicated generator structure. See also comment in
5673     // stubRoutines.hpp.
5674 
5675     StubRoutines::_forward_exception_entry = generate_forward_exception();
5676 
5677     StubRoutines::_call_stub_entry =
5678       generate_call_stub(StubRoutines::_call_stub_return_address);
5679 
5680     // is referenced by megamorphic call
5681     StubRoutines::_catch_exception_entry = generate_catch_exception();
5682 
5683     // Build this early so it&#39;s available for the interpreter.
5684     StubRoutines::_throw_StackOverflowError_entry =
5685       generate_throw_exception(&quot;StackOverflowError throw_exception&quot;,
5686                                CAST_FROM_FN_PTR(address,
5687                                                 SharedRuntime::throw_StackOverflowError));
5688     StubRoutines::_throw_delayed_StackOverflowError_entry =
5689       generate_throw_exception(&quot;delayed StackOverflowError throw_exception&quot;,
5690                                CAST_FROM_FN_PTR(address,
5691                                                 SharedRuntime::throw_delayed_StackOverflowError));
5692     if (UseCRC32Intrinsics) {
5693       // set table address before stub generation which use it
5694       StubRoutines::_crc_table_adr = (address)StubRoutines::aarch64::_crc_table;
5695       StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();
5696     }
5697 
5698     if (UseCRC32CIntrinsics) {
5699       StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C();
5700     }
5701 
5702     // Disabled until JDK-8210858 is fixed
5703     // if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {
5704     //   StubRoutines::_dlog = generate_dlog();
5705     // }
5706 
5707     if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {
5708       StubRoutines::_dsin = generate_dsin_dcos(/* isCos = */ false);
5709     }
5710 
5711     if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {
5712       StubRoutines::_dcos = generate_dsin_dcos(/* isCos = */ true);
5713     }
5714   }
5715 
5716   void generate_all() {
5717     // support for verify_oop (must happen after universe_init)
5718     StubRoutines::_verify_oop_subroutine_entry     = generate_verify_oop();
5719     StubRoutines::_throw_AbstractMethodError_entry =
5720       generate_throw_exception(&quot;AbstractMethodError throw_exception&quot;,
5721                                CAST_FROM_FN_PTR(address,
5722                                                 SharedRuntime::
5723                                                 throw_AbstractMethodError));
5724 
5725     StubRoutines::_throw_IncompatibleClassChangeError_entry =
5726       generate_throw_exception(&quot;IncompatibleClassChangeError throw_exception&quot;,
5727                                CAST_FROM_FN_PTR(address,
5728                                                 SharedRuntime::
5729                                                 throw_IncompatibleClassChangeError));
5730 
5731     StubRoutines::_throw_NullPointerException_at_call_entry =
5732       generate_throw_exception(&quot;NullPointerException at call throw_exception&quot;,
5733                                CAST_FROM_FN_PTR(address,
5734                                                 SharedRuntime::
5735                                                 throw_NullPointerException_at_call));
5736 
5737     // arraycopy stubs used by compilers
5738     generate_arraycopy_stubs();
5739 
5740     // has negatives stub for large arrays.
5741     StubRoutines::aarch64::_has_negatives = generate_has_negatives(StubRoutines::aarch64::_has_negatives_long);
5742 
5743     // array equals stub for large arrays.
5744     if (!UseSimpleArrayEquals) {
5745       StubRoutines::aarch64::_large_array_equals = generate_large_array_equals();
5746     }
5747 
5748     generate_compare_long_strings();
5749 
5750     generate_string_indexof_stubs();
5751 
5752     // byte_array_inflate stub for large arrays.
5753     StubRoutines::aarch64::_large_byte_array_inflate = generate_large_byte_array_inflate();
5754 
5755 #ifdef COMPILER2
5756     if (UseMultiplyToLenIntrinsic) {
5757       StubRoutines::_multiplyToLen = generate_multiplyToLen();
5758     }
5759 
5760     if (UseSquareToLenIntrinsic) {
5761       StubRoutines::_squareToLen = generate_squareToLen();
5762     }
5763 
5764     if (UseMulAddIntrinsic) {
5765       StubRoutines::_mulAdd = generate_mulAdd();
5766     }
5767 
5768     if (UseMontgomeryMultiplyIntrinsic) {
5769       StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;montgomeryMultiply&quot;);
5770       MontgomeryMultiplyGenerator g(_masm, /*squaring*/false);
5771       StubRoutines::_montgomeryMultiply = g.generate_multiply();
5772     }
5773 
5774     if (UseMontgomerySquareIntrinsic) {
5775       StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;montgomerySquare&quot;);
5776       MontgomeryMultiplyGenerator g(_masm, /*squaring*/true);
5777       // We use generate_multiply() rather than generate_square()
5778       // because it&#39;s faster for the sizes of modulus we care about.
5779       StubRoutines::_montgomerySquare = g.generate_multiply();
5780     }
5781 #endif // COMPILER2
5782 
5783     // generate GHASH intrinsics code
5784     if (UseGHASHIntrinsics) {
5785       StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();
5786     }
5787 
5788     // data cache line writeback
5789     StubRoutines::_data_cache_writeback = generate_data_cache_writeback();
5790     StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();
5791 
5792     if (UseAESIntrinsics) {
5793       StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();
5794       StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();
5795       StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();
5796       StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt();
5797     }
5798 
5799     if (UseSHA1Intrinsics) {
5800       StubRoutines::_sha1_implCompress     = generate_sha1_implCompress(false,   &quot;sha1_implCompress&quot;);
5801       StubRoutines::_sha1_implCompressMB   = generate_sha1_implCompress(true,    &quot;sha1_implCompressMB&quot;);
5802     }
5803     if (UseSHA256Intrinsics) {
5804       StubRoutines::_sha256_implCompress   = generate_sha256_implCompress(false, &quot;sha256_implCompress&quot;);
5805       StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true,  &quot;sha256_implCompressMB&quot;);
5806     }
5807 
5808     // generate Adler32 intrinsics code
5809     if (UseAdler32Intrinsics) {
5810       StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();
5811     }
5812 
5813     // Safefetch stubs.
5814     generate_safefetch(&quot;SafeFetch32&quot;, sizeof(int),     &amp;StubRoutines::_safefetch32_entry,
5815                                                        &amp;StubRoutines::_safefetch32_fault_pc,
5816                                                        &amp;StubRoutines::_safefetch32_continuation_pc);
5817     generate_safefetch(&quot;SafeFetchN&quot;, sizeof(intptr_t), &amp;StubRoutines::_safefetchN_entry,
5818                                                        &amp;StubRoutines::_safefetchN_fault_pc,
5819                                                        &amp;StubRoutines::_safefetchN_continuation_pc);
5820     StubRoutines::aarch64::set_completed();
5821   }
5822 
5823  public:
5824   StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {
5825     if (all) {
5826       generate_all();
5827     } else {
5828       generate_initial();
5829     }
5830   }
5831 }; // end class declaration
5832 
5833 #define UCM_TABLE_MAX_ENTRIES 8
5834 void StubGenerator_generate(CodeBuffer* code, bool all) {
5835   if (UnsafeCopyMemory::_table == NULL) {
5836     UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);
5837   }
5838   StubGenerator g(code, all);
5839 }
<a name="37" id="anc37"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="37" type="hidden" />
</body>
</html>