<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src\hotspot\os_cpu\windows_x86\atomic_windows_x86.hpp</title>
    <link rel="stylesheet" href="..\..\..\..\style.css" />
  </head>
<body>
<center><a href="..\windows_aarch64\vm_version_windows_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="..\..\..\..\index.html" target="_top">index</a> <a href="bytes_windows_x86.inline.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src\hotspot\os_cpu\windows_x86\atomic_windows_x86.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
 26 #define OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
 27 

 28 #include &quot;runtime/os.hpp&quot;
 29 
 30 // Note that in MSVC, volatile memory accesses are explicitly
 31 // guaranteed to have acquire release semantics (w.r.t. compiler
 32 // reordering) and therefore does not even need a compiler barrier
 33 // for normal acquire release accesses. And all generalized
 34 // bound calls like release_store go through Atomic::load
 35 // and Atomic::store which do volatile memory accesses.
 36 template&lt;&gt; inline void ScopedFence&lt;X_ACQUIRE&gt;::postfix()       { }
 37 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X&gt;::prefix()        { }
 38 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::prefix()  { }
 39 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::postfix() { OrderAccess::fence(); }
 40 
 41 // The following alternative implementations are needed because
 42 // Windows 95 doesn&#39;t support (some of) the corresponding Windows NT
 43 // calls. Furthermore, these versions allow inlining in the caller.
 44 // (More precisely: The documentation for InterlockedExchange says
 45 // it is supported for Windows 95. However, when single-stepping
 46 // through the assembly code we cannot step into the routine and
 47 // when looking at the routine address we see only garbage code.
 48 // Better safe then sorry!). Was bug 7/31/98 (gri).
 49 //
 50 // Performance note: On uniprocessors, the &#39;lock&#39; prefixes are not
 51 // necessary (and expensive). We should generate separate cases if
 52 // this becomes a performance problem.
 53 
<span class="line-removed"> 54 #pragma warning(disable: 4035) // Disables warnings reporting missing return statement</span>
 55 
 56 template&lt;size_t byte_size&gt;
 57 struct Atomic::PlatformAdd
 58   : Atomic::AddAndFetch&lt;Atomic::PlatformAdd&lt;byte_size&gt; &gt;
 59 {
 60   template&lt;typename D, typename I&gt;
 61   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;
 62 };
 63 
<span class="line-removed"> 64 #ifdef AMD64</span>
<span class="line-removed"> 65 template&lt;&gt;</span>
<span class="line-removed"> 66 template&lt;typename D, typename I&gt;</span>
<span class="line-removed"> 67 inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="line-removed"> 68                                                atomic_memory_order order) const {</span>
<span class="line-removed"> 69   return add_using_helper&lt;int32_t&gt;(os::atomic_add_func, dest, add_value);</span>
<span class="line-removed"> 70 }</span>
 71 
<span class="line-modified"> 72 template&lt;&gt;</span>
<span class="line-modified"> 73 template&lt;typename D, typename I&gt;</span>
<span class="line-modified"> 74 inline D Atomic::PlatformAdd&lt;8&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="line-modified"> 75                                                atomic_memory_order order) const {</span>
<span class="line-modified"> 76   return add_using_helper&lt;int64_t&gt;(os::atomic_add_long_func, dest, add_value);</span>
<span class="line-modified"> 77 }</span>










 78 
 79 #define DEFINE_STUB_XCHG(ByteSize, StubType, StubName)                  \
 80   template&lt;&gt;                                                            \
 81   template&lt;typename T&gt;                                                  \
 82   inline T Atomic::PlatformXchg&lt;ByteSize&gt;::operator()(T volatile* dest, \
 83                                                       T exchange_value, \
 84                                                       atomic_memory_order order) const { \
 85     STATIC_ASSERT(ByteSize == sizeof(T));                               \
<span class="line-modified"> 86     return xchg_using_helper&lt;StubType&gt;(StubName, dest, exchange_value); \</span>


 87   }
 88 
<span class="line-modified"> 89 DEFINE_STUB_XCHG(4, int32_t, os::atomic_xchg_func)</span>
<span class="line-modified"> 90 DEFINE_STUB_XCHG(8, int64_t, os::atomic_xchg_long_func)</span>
 91 
 92 #undef DEFINE_STUB_XCHG
 93 
 94 #define DEFINE_STUB_CMPXCHG(ByteSize, StubType, StubName)                  \
 95   template&lt;&gt;                                                               \
 96   template&lt;typename T&gt;                                                     \
 97   inline T Atomic::PlatformCmpxchg&lt;ByteSize&gt;::operator()(T volatile* dest, \
 98                                                          T compare_value,  \
 99                                                          T exchange_value, \
100                                                          atomic_memory_order order) const { \
101     STATIC_ASSERT(ByteSize == sizeof(T));                                  \
<span class="line-modified">102     return cmpxchg_using_helper&lt;StubType&gt;(StubName, dest, compare_value, exchange_value); \</span>



103   }
104 
<span class="line-modified">105 DEFINE_STUB_CMPXCHG(1, int8_t,  os::atomic_cmpxchg_byte_func)</span>
<span class="line-modified">106 DEFINE_STUB_CMPXCHG(4, int32_t, os::atomic_cmpxchg_func)</span>
<span class="line-modified">107 DEFINE_STUB_CMPXCHG(8, int64_t, os::atomic_cmpxchg_long_func)</span>
108 
109 #undef DEFINE_STUB_CMPXCHG
110 
<span class="line-removed">111 #else // !AMD64</span>
112 
<span class="line-modified">113 template&lt;&gt;</span>
<span class="line-removed">114 template&lt;typename D, typename I&gt;</span>
<span class="line-removed">115 inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="line-removed">116                                                atomic_memory_order order) const {</span>
<span class="line-removed">117   STATIC_ASSERT(4 == sizeof(I));</span>
<span class="line-removed">118   STATIC_ASSERT(4 == sizeof(D));</span>
<span class="line-removed">119   __asm {</span>
<span class="line-removed">120     mov edx, dest;</span>
<span class="line-removed">121     mov eax, add_value;</span>
<span class="line-removed">122     mov ecx, eax;</span>
<span class="line-removed">123     lock xadd dword ptr [edx], eax;</span>
<span class="line-removed">124     add eax, ecx;</span>
<span class="line-removed">125   }</span>
<span class="line-removed">126 }</span>
<span class="line-removed">127 </span>
<span class="line-removed">128 template&lt;&gt;</span>
<span class="line-removed">129 template&lt;typename T&gt;</span>
<span class="line-removed">130 inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">131                                              T exchange_value,</span>
<span class="line-removed">132                                              atomic_memory_order order) const {</span>
<span class="line-removed">133   STATIC_ASSERT(4 == sizeof(T));</span>
<span class="line-removed">134   // alternative for InterlockedExchange</span>
<span class="line-removed">135   __asm {</span>
<span class="line-removed">136     mov eax, exchange_value;</span>
<span class="line-removed">137     mov ecx, dest;</span>
<span class="line-removed">138     xchg eax, dword ptr [ecx];</span>
<span class="line-removed">139   }</span>
<span class="line-removed">140 }</span>
<span class="line-removed">141 </span>
<span class="line-removed">142 template&lt;&gt;</span>
<span class="line-removed">143 template&lt;typename T&gt;</span>
<span class="line-removed">144 inline T Atomic::PlatformCmpxchg&lt;1&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">145                                                 T compare_value,</span>
<span class="line-removed">146                                                 T exchange_value,</span>
<span class="line-removed">147                                                 atomic_memory_order order) const {</span>
<span class="line-removed">148   STATIC_ASSERT(1 == sizeof(T));</span>
<span class="line-removed">149   // alternative for InterlockedCompareExchange</span>
<span class="line-removed">150   __asm {</span>
<span class="line-removed">151     mov edx, dest</span>
<span class="line-removed">152     mov cl, exchange_value</span>
<span class="line-removed">153     mov al, compare_value</span>
<span class="line-removed">154     lock cmpxchg byte ptr [edx], cl</span>
<span class="line-removed">155   }</span>
<span class="line-removed">156 }</span>
<span class="line-removed">157 </span>
<span class="line-removed">158 template&lt;&gt;</span>
<span class="line-removed">159 template&lt;typename T&gt;</span>
<span class="line-removed">160 inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">161                                                 T compare_value,</span>
<span class="line-removed">162                                                 T exchange_value,</span>
<span class="line-removed">163                                                 atomic_memory_order order) const {</span>
<span class="line-removed">164   STATIC_ASSERT(4 == sizeof(T));</span>
<span class="line-removed">165   // alternative for InterlockedCompareExchange</span>
<span class="line-removed">166   __asm {</span>
<span class="line-removed">167     mov edx, dest</span>
<span class="line-removed">168     mov ecx, exchange_value</span>
<span class="line-removed">169     mov eax, compare_value</span>
<span class="line-removed">170     lock cmpxchg dword ptr [edx], ecx</span>
<span class="line-removed">171   }</span>
<span class="line-removed">172 }</span>
173 
<span class="line-modified">174 template&lt;&gt;</span>
<span class="line-removed">175 template&lt;typename T&gt;</span>
<span class="line-removed">176 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">177                                                 T compare_value,</span>
<span class="line-removed">178                                                 T exchange_value,</span>
<span class="line-removed">179                                                 atomic_memory_order order) const {</span>
<span class="line-removed">180   STATIC_ASSERT(8 == sizeof(T));</span>
<span class="line-removed">181   int32_t ex_lo  = (int32_t)exchange_value;</span>
<span class="line-removed">182   int32_t ex_hi  = *( ((int32_t*)&amp;exchange_value) + 1 );</span>
<span class="line-removed">183   int32_t cmp_lo = (int32_t)compare_value;</span>
<span class="line-removed">184   int32_t cmp_hi = *( ((int32_t*)&amp;compare_value) + 1 );</span>
<span class="line-removed">185   __asm {</span>
<span class="line-removed">186     push ebx</span>
<span class="line-removed">187     push edi</span>
<span class="line-removed">188     mov eax, cmp_lo</span>
<span class="line-removed">189     mov edx, cmp_hi</span>
<span class="line-removed">190     mov edi, dest</span>
<span class="line-removed">191     mov ebx, ex_lo</span>
<span class="line-removed">192     mov ecx, ex_hi</span>
<span class="line-removed">193     lock cmpxchg8b qword ptr [edi]</span>
<span class="line-removed">194     pop edi</span>
<span class="line-removed">195     pop ebx</span>
<span class="line-removed">196   }</span>
<span class="line-removed">197 }</span>
198 
199 template&lt;&gt;
200 template&lt;typename T&gt;
201 inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
202   STATIC_ASSERT(8 == sizeof(T));
203   volatile T dest;
204   volatile T* pdest = &amp;dest;
205   __asm {
206     mov eax, src
207     fild     qword ptr [eax]
208     mov eax, pdest
209     fistp    qword ptr [eax]
210   }
211   return dest;
212 }
213 
214 template&lt;&gt;
215 template&lt;typename T&gt;
216 inline void Atomic::PlatformStore&lt;8&gt;::operator()(T volatile* dest,
217                                                  T store_value) const {
218   STATIC_ASSERT(8 == sizeof(T));
219   volatile T* src = &amp;store_value;
220   __asm {
221     mov eax, src
222     fild     qword ptr [eax]
223     mov eax, dest
224     fistp    qword ptr [eax]
225   }
226 }
227 
<span class="line-removed">228 #endif // AMD64</span>
<span class="line-removed">229 </span>
230 #pragma warning(default: 4035) // Enables warnings reporting missing return statement
231 
<span class="line-removed">232 #ifndef AMD64</span>
233 template&lt;&gt;
234 struct Atomic::PlatformOrderedStore&lt;1, RELEASE_X_FENCE&gt;
235 {
236   template &lt;typename T&gt;
237   void operator()(volatile T* p, T v) const {
238     __asm {
239       mov edx, p;
240       mov al, v;
241       xchg al, byte ptr [edx];
242     }
243   }
244 };
245 
246 template&lt;&gt;
247 struct Atomic::PlatformOrderedStore&lt;2, RELEASE_X_FENCE&gt;
248 {
249   template &lt;typename T&gt;
250   void operator()(volatile T* p, T v) const {
251     __asm {
252       mov edx, p;
</pre>
</td>
<td>
<hr />
<pre>
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
 26 #define OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
 27 
<span class="line-added"> 28 #include &lt;intrin.h&gt;</span>
 29 #include &quot;runtime/os.hpp&quot;
 30 
 31 // Note that in MSVC, volatile memory accesses are explicitly
 32 // guaranteed to have acquire release semantics (w.r.t. compiler
 33 // reordering) and therefore does not even need a compiler barrier
 34 // for normal acquire release accesses. And all generalized
 35 // bound calls like release_store go through Atomic::load
 36 // and Atomic::store which do volatile memory accesses.
 37 template&lt;&gt; inline void ScopedFence&lt;X_ACQUIRE&gt;::postfix()       { }
 38 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X&gt;::prefix()        { }
 39 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::prefix()  { }
 40 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::postfix() { OrderAccess::fence(); }
 41 
 42 // The following alternative implementations are needed because
 43 // Windows 95 doesn&#39;t support (some of) the corresponding Windows NT
 44 // calls. Furthermore, these versions allow inlining in the caller.
 45 // (More precisely: The documentation for InterlockedExchange says
 46 // it is supported for Windows 95. However, when single-stepping
 47 // through the assembly code we cannot step into the routine and
 48 // when looking at the routine address we see only garbage code.
 49 // Better safe then sorry!). Was bug 7/31/98 (gri).
 50 //
 51 // Performance note: On uniprocessors, the &#39;lock&#39; prefixes are not
 52 // necessary (and expensive). We should generate separate cases if
 53 // this becomes a performance problem.
 54 

 55 
 56 template&lt;size_t byte_size&gt;
 57 struct Atomic::PlatformAdd
 58   : Atomic::AddAndFetch&lt;Atomic::PlatformAdd&lt;byte_size&gt; &gt;
 59 {
 60   template&lt;typename D, typename I&gt;
 61   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;
 62 };
 63 







 64 
<span class="line-modified"> 65 #define DEFINE_STUB_ADD(ByteSize, StubType, StubName)                     \</span>
<span class="line-modified"> 66   template&lt;&gt;                                                              \</span>
<span class="line-modified"> 67   template&lt;typename D, typename I&gt;                                        \</span>
<span class="line-modified"> 68   inline D Atomic::PlatformAdd&lt;ByteSize&gt;::add_and_fetch(D volatile* dest, \</span>
<span class="line-modified"> 69                                                         I add_value,      \</span>
<span class="line-modified"> 70                                                         atomic_memory_order order) const { \</span>
<span class="line-added"> 71     STATIC_ASSERT(ByteSize == sizeof(D));                                 \</span>
<span class="line-added"> 72     return PrimitiveConversions::cast&lt;D&gt;(                                 \</span>
<span class="line-added"> 73       StubName(reinterpret_cast&lt;StubType volatile *&gt;(dest),               \</span>
<span class="line-added"> 74                PrimitiveConversions::cast&lt;StubType&gt;(add_value)));         \</span>
<span class="line-added"> 75   }</span>
<span class="line-added"> 76 </span>
<span class="line-added"> 77 DEFINE_STUB_ADD(4, long,    InterlockedAdd)</span>
<span class="line-added"> 78 DEFINE_STUB_ADD(8, __int64, InterlockedAdd64)</span>
<span class="line-added"> 79 </span>
<span class="line-added"> 80 #undef DEFINE_STUB_ADD</span>
 81 
 82 #define DEFINE_STUB_XCHG(ByteSize, StubType, StubName)                  \
 83   template&lt;&gt;                                                            \
 84   template&lt;typename T&gt;                                                  \
 85   inline T Atomic::PlatformXchg&lt;ByteSize&gt;::operator()(T volatile* dest, \
 86                                                       T exchange_value, \
 87                                                       atomic_memory_order order) const { \
 88     STATIC_ASSERT(ByteSize == sizeof(T));                               \
<span class="line-modified"> 89     return PrimitiveConversions::cast&lt;T&gt;(                               \</span>
<span class="line-added"> 90       StubName(reinterpret_cast&lt;StubType volatile *&gt;(dest),             \</span>
<span class="line-added"> 91                PrimitiveConversions::cast&lt;StubType&gt;(exchange_value)));  \</span>
 92   }
 93 
<span class="line-modified"> 94 DEFINE_STUB_XCHG(4, long,    InterlockedExchange)</span>
<span class="line-modified"> 95 DEFINE_STUB_XCHG(8, __int64, InterlockedExchange64)</span>
 96 
 97 #undef DEFINE_STUB_XCHG
 98 
 99 #define DEFINE_STUB_CMPXCHG(ByteSize, StubType, StubName)                  \
100   template&lt;&gt;                                                               \
101   template&lt;typename T&gt;                                                     \
102   inline T Atomic::PlatformCmpxchg&lt;ByteSize&gt;::operator()(T volatile* dest, \
103                                                          T compare_value,  \
104                                                          T exchange_value, \
105                                                          atomic_memory_order order) const { \
106     STATIC_ASSERT(ByteSize == sizeof(T));                                  \
<span class="line-modified">107     return PrimitiveConversions::cast&lt;T&gt;(                                  \</span>
<span class="line-added">108       StubName(reinterpret_cast&lt;StubType volatile *&gt;(dest),                \</span>
<span class="line-added">109                PrimitiveConversions::cast&lt;StubType&gt;(exchange_value),       \</span>
<span class="line-added">110                PrimitiveConversions::cast&lt;StubType&gt;(compare_value)));      \</span>
111   }
112 
<span class="line-modified">113 DEFINE_STUB_CMPXCHG(1, char,    _InterlockedCompareExchange8) // Use the intrinsic as InterlockedCompareExchange8 does not exist</span>
<span class="line-modified">114 DEFINE_STUB_CMPXCHG(4, long,    InterlockedCompareExchange)</span>
<span class="line-modified">115 DEFINE_STUB_CMPXCHG(8, __int64, InterlockedCompareExchange64)</span>
116 
117 #undef DEFINE_STUB_CMPXCHG
118 

119 
<span class="line-modified">120 #ifndef AMD64</span>



























































121 
<span class="line-modified">122 #pragma warning(disable: 4035) // Disables warnings reporting missing return statement</span>























123 
124 template&lt;&gt;
125 template&lt;typename T&gt;
126 inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
127   STATIC_ASSERT(8 == sizeof(T));
128   volatile T dest;
129   volatile T* pdest = &amp;dest;
130   __asm {
131     mov eax, src
132     fild     qword ptr [eax]
133     mov eax, pdest
134     fistp    qword ptr [eax]
135   }
136   return dest;
137 }
138 
139 template&lt;&gt;
140 template&lt;typename T&gt;
141 inline void Atomic::PlatformStore&lt;8&gt;::operator()(T volatile* dest,
142                                                  T store_value) const {
143   STATIC_ASSERT(8 == sizeof(T));
144   volatile T* src = &amp;store_value;
145   __asm {
146     mov eax, src
147     fild     qword ptr [eax]
148     mov eax, dest
149     fistp    qword ptr [eax]
150   }
151 }
152 


153 #pragma warning(default: 4035) // Enables warnings reporting missing return statement
154 

155 template&lt;&gt;
156 struct Atomic::PlatformOrderedStore&lt;1, RELEASE_X_FENCE&gt;
157 {
158   template &lt;typename T&gt;
159   void operator()(volatile T* p, T v) const {
160     __asm {
161       mov edx, p;
162       mov al, v;
163       xchg al, byte ptr [edx];
164     }
165   }
166 };
167 
168 template&lt;&gt;
169 struct Atomic::PlatformOrderedStore&lt;2, RELEASE_X_FENCE&gt;
170 {
171   template &lt;typename T&gt;
172   void operator()(volatile T* p, T v) const {
173     __asm {
174       mov edx, p;
</pre>
</td>
</tr>
</table>
<center><a href="..\windows_aarch64\vm_version_windows_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="..\..\..\..\index.html" target="_top">index</a> <a href="bytes_windows_x86.inline.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>