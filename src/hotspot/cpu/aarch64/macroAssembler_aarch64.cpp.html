<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src\hotspot\cpu\aarch64\macroAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="..\..\..\..\style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &lt;sys/types.h&gt;
  27 
  28 #include &quot;precompiled.hpp&quot;
  29 #include &quot;jvm.h&quot;
  30 #include &quot;asm/assembler.hpp&quot;
  31 #include &quot;asm/assembler.inline.hpp&quot;
  32 #include &quot;gc/shared/barrierSet.hpp&quot;
  33 #include &quot;gc/shared/cardTable.hpp&quot;
  34 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  35 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  36 #include &quot;interpreter/interpreter.hpp&quot;
  37 #include &quot;compiler/disassembler.hpp&quot;
  38 #include &quot;memory/resourceArea.hpp&quot;
  39 #include &quot;memory/universe.hpp&quot;
  40 #include &quot;nativeInst_aarch64.hpp&quot;
  41 #include &quot;oops/accessDecorators.hpp&quot;
  42 #include &quot;oops/compressedOops.inline.hpp&quot;
  43 #include &quot;oops/klass.inline.hpp&quot;
  44 #include &quot;runtime/biasedLocking.hpp&quot;
  45 #include &quot;runtime/icache.hpp&quot;
  46 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  47 #include &quot;runtime/jniHandles.inline.hpp&quot;
  48 #include &quot;runtime/sharedRuntime.hpp&quot;
  49 #include &quot;runtime/thread.hpp&quot;
  50 #ifdef COMPILER1
  51 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  52 #endif
  53 #ifdef COMPILER2
  54 #include &quot;oops/oop.hpp&quot;
  55 #include &quot;opto/compile.hpp&quot;
  56 #include &quot;opto/intrinsicnode.hpp&quot;
  57 #include &quot;opto/node.hpp&quot;
  58 #endif
  59 
  60 #ifdef PRODUCT
  61 #define BLOCK_COMMENT(str) /* nothing */
  62 #define STOP(error) stop(error)
  63 #else
  64 #define BLOCK_COMMENT(str) block_comment(str)
  65 #define STOP(error) block_comment(error); stop(error)
  66 #endif
  67 
  68 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  69 
  70 // Patch any kind of instruction; there may be several instructions.
  71 // Return the total length (in bytes) of the instructions.
  72 int MacroAssembler::pd_patch_instruction_size(address branch, address target) {
  73   int instructions = 1;
  74   assert((uint64_t)target &lt; ((uint64_t)1 &lt;&lt; 48), &quot;48-bit overflow in address constant&quot;);
  75   int64_t offset = (target - branch) &gt;&gt; 2;
  76   unsigned insn = *(unsigned*)branch;
  77   if ((Instruction_aarch64::extract(insn, 29, 24) &amp; 0b111011) == 0b011000) {
  78     // Load register (literal)
  79     Instruction_aarch64::spatch(branch, 23, 5, offset);
  80   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
  81     // Unconditional branch (immediate)
  82     Instruction_aarch64::spatch(branch, 25, 0, offset);
  83   } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {
  84     // Conditional branch (immediate)
  85     Instruction_aarch64::spatch(branch, 23, 5, offset);
  86   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {
  87     // Compare &amp; branch (immediate)
  88     Instruction_aarch64::spatch(branch, 23, 5, offset);
  89   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {
  90     // Test &amp; branch (immediate)
  91     Instruction_aarch64::spatch(branch, 18, 5, offset);
  92   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
  93     // PC-rel. addressing
  94     offset = target-branch;
  95     int shift = Instruction_aarch64::extract(insn, 31, 31);
  96     if (shift) {
  97       uint64_t dest = (uint64_t)target;
  98       uint64_t pc_page = (uint64_t)branch &gt;&gt; 12;
  99       uint64_t adr_page = (uint64_t)target &gt;&gt; 12;
 100       unsigned offset_lo = dest &amp; 0xfff;
 101       offset = adr_page - pc_page;
 102 
 103       // We handle 4 types of PC relative addressing
 104       //   1 - adrp    Rx, target_page
 105       //       ldr/str Ry, [Rx, #offset_in_page]
 106       //   2 - adrp    Rx, target_page
 107       //       add     Ry, Rx, #offset_in_page
 108       //   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 109       //       movk    Rx, #imm16&lt;&lt;32
 110       //   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 111       // In the first 3 cases we must check that Rx is the same in the adrp and the
 112       // subsequent ldr/str, add or movk instruction. Otherwise we could accidentally end
 113       // up treating a type 4 relocation as a type 1, 2 or 3 just because it happened
 114       // to be followed by a random unrelated ldr/str, add or movk instruction.
 115       //
 116       unsigned insn2 = ((unsigned*)branch)[1];
 117       if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &amp;&amp;
 118                 Instruction_aarch64::extract(insn, 4, 0) ==
 119                         Instruction_aarch64::extract(insn2, 9, 5)) {
 120         // Load/store register (unsigned immediate)
 121         unsigned size = Instruction_aarch64::extract(insn2, 31, 30);
 122         Instruction_aarch64::patch(branch + sizeof (unsigned),
 123                                     21, 10, offset_lo &gt;&gt; size);
 124         guarantee(((dest &gt;&gt; size) &lt;&lt; size) == dest, &quot;misaligned target&quot;);
 125         instructions = 2;
 126       } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &amp;&amp;
 127                 Instruction_aarch64::extract(insn, 4, 0) ==
 128                         Instruction_aarch64::extract(insn2, 4, 0)) {
 129         // add (immediate)
 130         Instruction_aarch64::patch(branch + sizeof (unsigned),
 131                                    21, 10, offset_lo);
 132         instructions = 2;
 133       } else if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110 &amp;&amp;
 134                    Instruction_aarch64::extract(insn, 4, 0) ==
 135                      Instruction_aarch64::extract(insn2, 4, 0)) {
 136         // movk #imm16&lt;&lt;32
 137         Instruction_aarch64::patch(branch + 4, 20, 5, (uint64_t)target &gt;&gt; 32);
 138         int64_t dest = ((int64_t)target &amp; 0xffffffffL) | ((int64_t)branch &amp; 0xffff00000000L);
 139         int64_t pc_page = (int64_t)branch &gt;&gt; 12;
 140         int64_t adr_page = (int64_t)dest &gt;&gt; 12;
 141         offset = adr_page - pc_page;
 142         instructions = 2;
 143       }
 144     }
 145     int offset_lo = offset &amp; 3;
 146     offset &gt;&gt;= 2;
 147     Instruction_aarch64::spatch(branch, 23, 5, offset);
 148     Instruction_aarch64::patch(branch, 30, 29, offset_lo);
 149   } else if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010100) {
 150     uint64_t dest = (uint64_t)target;
 151     // Move wide constant
 152     assert(nativeInstruction_at(branch+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 153     assert(nativeInstruction_at(branch+8)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 154     Instruction_aarch64::patch(branch, 20, 5, dest &amp; 0xffff);
 155     Instruction_aarch64::patch(branch+4, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 156     Instruction_aarch64::patch(branch+8, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 157     assert(target_addr_for_insn(branch) == target, &quot;should be&quot;);
 158     instructions = 3;
 159   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 160              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 161     // nothing to do
 162     assert(target == 0, &quot;did not expect to relocate target for polling page load&quot;);
 163   } else {
 164     ShouldNotReachHere();
 165   }
 166   return instructions * NativeInstruction::instruction_size;
 167 }
 168 
 169 int MacroAssembler::patch_oop(address insn_addr, address o) {
 170   int instructions;
 171   unsigned insn = *(unsigned*)insn_addr;
 172   assert(nativeInstruction_at(insn_addr+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 173 
 174   // OOPs are either narrow (32 bits) or wide (48 bits).  We encode
 175   // narrow OOPs by setting the upper 16 bits in the first
 176   // instruction.
 177   if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010101) {
 178     // Move narrow OOP
 179     narrowOop n = CompressedOops::encode((oop)o);
 180     Instruction_aarch64::patch(insn_addr, 20, 5, n &gt;&gt; 16);
 181     Instruction_aarch64::patch(insn_addr+4, 20, 5, n &amp; 0xffff);
 182     instructions = 2;
 183   } else {
 184     // Move wide OOP
 185     assert(nativeInstruction_at(insn_addr+8)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 186     uintptr_t dest = (uintptr_t)o;
 187     Instruction_aarch64::patch(insn_addr, 20, 5, dest &amp; 0xffff);
 188     Instruction_aarch64::patch(insn_addr+4, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 189     Instruction_aarch64::patch(insn_addr+8, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 190     instructions = 3;
 191   }
 192   return instructions * NativeInstruction::instruction_size;
 193 }
 194 
 195 int MacroAssembler::patch_narrow_klass(address insn_addr, narrowKlass n) {
 196   // Metatdata pointers are either narrow (32 bits) or wide (48 bits).
 197   // We encode narrow ones by setting the upper 16 bits in the first
 198   // instruction.
 199   NativeInstruction *insn = nativeInstruction_at(insn_addr);
 200   assert(Instruction_aarch64::extract(insn-&gt;encoding(), 31, 21) == 0b11010010101 &amp;&amp;
 201          nativeInstruction_at(insn_addr+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 202 
 203   Instruction_aarch64::patch(insn_addr, 20, 5, n &gt;&gt; 16);
 204   Instruction_aarch64::patch(insn_addr+4, 20, 5, n &amp; 0xffff);
 205   return 2 * NativeInstruction::instruction_size;
 206 }
 207 
 208 address MacroAssembler::target_addr_for_insn(address insn_addr, unsigned insn) {
 209   int64_t offset = 0;
 210   if ((Instruction_aarch64::extract(insn, 29, 24) &amp; 0b011011) == 0b00011000) {
 211     // Load register (literal)
 212     offset = Instruction_aarch64::sextract(insn, 23, 5);
 213     return address(((uint64_t)insn_addr + (offset &lt;&lt; 2)));
 214   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
 215     // Unconditional branch (immediate)
 216     offset = Instruction_aarch64::sextract(insn, 25, 0);
 217   } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {
 218     // Conditional branch (immediate)
 219     offset = Instruction_aarch64::sextract(insn, 23, 5);
 220   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {
 221     // Compare &amp; branch (immediate)
 222     offset = Instruction_aarch64::sextract(insn, 23, 5);
 223    } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {
 224     // Test &amp; branch (immediate)
 225     offset = Instruction_aarch64::sextract(insn, 18, 5);
 226   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
 227     // PC-rel. addressing
 228     offset = Instruction_aarch64::extract(insn, 30, 29);
 229     offset |= Instruction_aarch64::sextract(insn, 23, 5) &lt;&lt; 2;
 230     int shift = Instruction_aarch64::extract(insn, 31, 31) ? 12 : 0;
 231     if (shift) {
 232       offset &lt;&lt;= shift;
 233       uint64_t target_page = ((uint64_t)insn_addr) + offset;
 234       target_page &amp;= ((uint64_t)-1) &lt;&lt; shift;
 235       // Return the target address for the following sequences
 236       //   1 - adrp    Rx, target_page
 237       //       ldr/str Ry, [Rx, #offset_in_page]
 238       //   2 - adrp    Rx, target_page
 239       //       add     Ry, Rx, #offset_in_page
 240       //   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 241       //       movk    Rx, #imm12&lt;&lt;32
 242       //   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 243       //
 244       // In the first two cases  we check that the register is the same and
 245       // return the target_page + the offset within the page.
 246       // Otherwise we assume it is a page aligned relocation and return
 247       // the target page only.
 248       //
 249       unsigned insn2 = ((unsigned*)insn_addr)[1];
 250       if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &amp;&amp;
 251                 Instruction_aarch64::extract(insn, 4, 0) ==
 252                         Instruction_aarch64::extract(insn2, 9, 5)) {
 253         // Load/store register (unsigned immediate)
 254         unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);
 255         unsigned int size = Instruction_aarch64::extract(insn2, 31, 30);
 256         return address(target_page + (byte_offset &lt;&lt; size));
 257       } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &amp;&amp;
 258                 Instruction_aarch64::extract(insn, 4, 0) ==
 259                         Instruction_aarch64::extract(insn2, 4, 0)) {
 260         // add (immediate)
 261         unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);
 262         return address(target_page + byte_offset);
 263       } else {
 264         if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110  &amp;&amp;
 265                Instruction_aarch64::extract(insn, 4, 0) ==
 266                  Instruction_aarch64::extract(insn2, 4, 0)) {
 267           target_page = (target_page &amp; 0xffffffff) |
 268                          ((uint64_t)Instruction_aarch64::extract(insn2, 20, 5) &lt;&lt; 32);
 269         }
 270         return (address)target_page;
 271       }
 272     } else {
 273       ShouldNotReachHere();
 274     }
 275   } else if (Instruction_aarch64::extract(insn, 31, 23) == 0b110100101) {
 276     uint32_t *insns = (uint32_t *)insn_addr;
 277     // Move wide constant: movz, movk, movk.  See movptr().
 278     assert(nativeInstruction_at(insns+1)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 279     assert(nativeInstruction_at(insns+2)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 280     return address(uint64_t(Instruction_aarch64::extract(insns[0], 20, 5))
 281                    + (uint64_t(Instruction_aarch64::extract(insns[1], 20, 5)) &lt;&lt; 16)
 282                    + (uint64_t(Instruction_aarch64::extract(insns[2], 20, 5)) &lt;&lt; 32));
 283   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 284              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 285     return 0;
 286   } else {
 287     ShouldNotReachHere();
 288   }
 289   return address(((uint64_t)insn_addr + (offset &lt;&lt; 2)));
 290 }
 291 
 292 void MacroAssembler::safepoint_poll(Label&amp; slow_path) {
 293   if (SafepointMechanism::uses_thread_local_poll()) {
 294     ldr(rscratch1, Address(rthread, Thread::polling_page_offset()));
 295     tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);
 296   } else {
 297     uint64_t offset;
 298     adrp(rscratch1, ExternalAddress(SafepointSynchronize::address_of_state()), offset);
 299     ldrw(rscratch1, Address(rscratch1, offset));
 300     assert(SafepointSynchronize::_not_synchronized == 0, &quot;rewrite this code&quot;);
 301     cbnz(rscratch1, slow_path);
 302   }
 303 }
 304 
 305 // Just like safepoint_poll, but use an acquiring load for thread-
 306 // local polling.
 307 //
 308 // We need an acquire here to ensure that any subsequent load of the
 309 // global SafepointSynchronize::_state flag is ordered after this load
 310 // of the local Thread::_polling page.  We don&#39;t want this poll to
 311 // return false (i.e. not safepointing) and a later poll of the global
 312 // SafepointSynchronize::_state spuriously to return true.
 313 //
 314 // This is to avoid a race when we&#39;re in a native-&gt;Java transition
 315 // racing the code which wakes up from a safepoint.
 316 //
 317 void MacroAssembler::safepoint_poll_acquire(Label&amp; slow_path) {
 318   if (SafepointMechanism::uses_thread_local_poll()) {
 319     lea(rscratch1, Address(rthread, Thread::polling_page_offset()));
 320     ldar(rscratch1, rscratch1);
 321     tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);
 322   } else {
 323     safepoint_poll(slow_path);
 324   }
 325 }
 326 
 327 void MacroAssembler::reset_last_Java_frame(bool clear_fp) {
 328   // we must set sp to zero to clear frame
 329   str(zr, Address(rthread, JavaThread::last_Java_sp_offset()));
 330 
 331   // must clear fp, so that compiled frames are not confused; it is
 332   // possible that we need it only for debugging
 333   if (clear_fp) {
 334     str(zr, Address(rthread, JavaThread::last_Java_fp_offset()));
 335   }
 336 
 337   // Always clear the pc because it could have been set by make_walkable()
 338   str(zr, Address(rthread, JavaThread::last_Java_pc_offset()));
 339 }
 340 
 341 // Calls to C land
 342 //
 343 // When entering C land, the rfp, &amp; resp of the last Java frame have to be recorded
 344 // in the (thread-local) JavaThread object. When leaving C land, the last Java fp
 345 // has to be reset to 0. This is required to allow proper stack traversal.
 346 void MacroAssembler::set_last_Java_frame(Register last_java_sp,
 347                                          Register last_java_fp,
 348                                          Register last_java_pc,
 349                                          Register scratch) {
 350 
 351   if (last_java_pc-&gt;is_valid()) {
 352       str(last_java_pc, Address(rthread,
 353                                 JavaThread::frame_anchor_offset()
 354                                 + JavaFrameAnchor::last_Java_pc_offset()));
 355     }
 356 
 357   // determine last_java_sp register
 358   if (last_java_sp == sp) {
 359     mov(scratch, sp);
 360     last_java_sp = scratch;
 361   } else if (!last_java_sp-&gt;is_valid()) {
 362     last_java_sp = esp;
 363   }
 364 
 365   str(last_java_sp, Address(rthread, JavaThread::last_Java_sp_offset()));
 366 
 367   // last_java_fp is optional
 368   if (last_java_fp-&gt;is_valid()) {
 369     str(last_java_fp, Address(rthread, JavaThread::last_Java_fp_offset()));
 370   }
 371 }
 372 
 373 void MacroAssembler::set_last_Java_frame(Register last_java_sp,
 374                                          Register last_java_fp,
 375                                          address  last_java_pc,
 376                                          Register scratch) {
 377   assert(last_java_pc != NULL, &quot;must provide a valid PC&quot;);
 378 
 379   adr(scratch, last_java_pc);
 380   str(scratch, Address(rthread,
 381                        JavaThread::frame_anchor_offset()
 382                        + JavaFrameAnchor::last_Java_pc_offset()));
 383 
 384   set_last_Java_frame(last_java_sp, last_java_fp, noreg, scratch);
 385 }
 386 
 387 void MacroAssembler::set_last_Java_frame(Register last_java_sp,
 388                                          Register last_java_fp,
 389                                          Label &amp;L,
 390                                          Register scratch) {
 391   if (L.is_bound()) {
 392     set_last_Java_frame(last_java_sp, last_java_fp, target(L), scratch);
 393   } else {
 394     InstructionMark im(this);
 395     L.add_patch_at(code(), locator());
 396     set_last_Java_frame(last_java_sp, last_java_fp, pc() /* Patched later */, scratch);
 397   }
 398 }
 399 
 400 void MacroAssembler::far_call(Address entry, CodeBuffer *cbuf, Register tmp) {
 401   assert(ReservedCodeCacheSize &lt; 4*G, &quot;branch out of range&quot;);
 402   assert(CodeCache::find_blob(entry.target()) != NULL,
 403          &quot;destination of far call not found in code cache&quot;);
 404   if (far_branches()) {
 405     uint64_t offset;
 406     // We can use ADRP here because we know that the total size of
 407     // the code cache cannot exceed 2Gb.
 408     adrp(tmp, entry, offset);
 409     add(tmp, tmp, offset);
 410     if (cbuf) cbuf-&gt;set_insts_mark();
 411     blr(tmp);
 412   } else {
 413     if (cbuf) cbuf-&gt;set_insts_mark();
 414     bl(entry);
 415   }
 416 }
 417 
 418 void MacroAssembler::far_jump(Address entry, CodeBuffer *cbuf, Register tmp) {
 419   assert(ReservedCodeCacheSize &lt; 4*G, &quot;branch out of range&quot;);
 420   assert(CodeCache::find_blob(entry.target()) != NULL,
 421          &quot;destination of far call not found in code cache&quot;);
 422   if (far_branches()) {
 423     uint64_t offset;
 424     // We can use ADRP here because we know that the total size of
 425     // the code cache cannot exceed 2Gb.
 426     adrp(tmp, entry, offset);
 427     add(tmp, tmp, offset);
 428     if (cbuf) cbuf-&gt;set_insts_mark();
 429     br(tmp);
 430   } else {
 431     if (cbuf) cbuf-&gt;set_insts_mark();
 432     b(entry);
 433   }
 434 }
 435 
 436 void MacroAssembler::reserved_stack_check() {
 437     // testing if reserved zone needs to be enabled
 438     Label no_reserved_zone_enabling;
 439 
 440     ldr(rscratch1, Address(rthread, JavaThread::reserved_stack_activation_offset()));
 441     cmp(sp, rscratch1);
 442     br(Assembler::LO, no_reserved_zone_enabling);
 443 
 444     enter();   // LR and FP are live.
 445     lea(rscratch1, CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone));
 446     mov(c_rarg0, rthread);
 447     blr(rscratch1);
 448     leave();
 449 
 450     // We have already removed our own frame.
 451     // throw_delayed_StackOverflowError will think that it&#39;s been
 452     // called by our caller.
 453     lea(rscratch1, RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));
 454     br(rscratch1);
 455     should_not_reach_here();
 456 
 457     bind(no_reserved_zone_enabling);
 458 }
 459 
 460 int MacroAssembler::biased_locking_enter(Register lock_reg,
 461                                          Register obj_reg,
 462                                          Register swap_reg,
 463                                          Register tmp_reg,
 464                                          bool swap_reg_contains_mark,
 465                                          Label&amp; done,
 466                                          Label* slow_case,
 467                                          BiasedLockingCounters* counters) {
 468   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 469   assert_different_registers(lock_reg, obj_reg, swap_reg);
 470 
 471   if (PrintBiasedLockingStatistics &amp;&amp; counters == NULL)
 472     counters = BiasedLocking::counters();
 473 
 474   assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg, rscratch1, rscratch2, noreg);
 475   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);
 476   Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());
 477   Address klass_addr     (obj_reg, oopDesc::klass_offset_in_bytes());
 478   Address saved_mark_addr(lock_reg, 0);
 479 
 480   // Biased locking
 481   // See whether the lock is currently biased toward our thread and
 482   // whether the epoch is still valid
 483   // Note that the runtime guarantees sufficient alignment of JavaThread
 484   // pointers to allow age to be placed into low bits
 485   // First check to see whether biasing is even enabled for this object
 486   Label cas_label;
 487   int null_check_offset = -1;
 488   if (!swap_reg_contains_mark) {
 489     null_check_offset = offset();
 490     ldr(swap_reg, mark_addr);
 491   }
 492   andr(tmp_reg, swap_reg, markWord::biased_lock_mask_in_place);
 493   cmp(tmp_reg, (u1)markWord::biased_lock_pattern);
 494   br(Assembler::NE, cas_label);
 495   // The bias pattern is present in the object&#39;s header. Need to check
 496   // whether the bias owner and the epoch are both still current.
 497   load_prototype_header(tmp_reg, obj_reg);
 498   orr(tmp_reg, tmp_reg, rthread);
 499   eor(tmp_reg, swap_reg, tmp_reg);
 500   andr(tmp_reg, tmp_reg, ~((int) markWord::age_mask_in_place));
 501   if (counters != NULL) {
 502     Label around;
 503     cbnz(tmp_reg, around);
 504     atomic_incw(Address((address)counters-&gt;biased_lock_entry_count_addr()), tmp_reg, rscratch1, rscratch2);
 505     b(done);
 506     bind(around);
 507   } else {
 508     cbz(tmp_reg, done);
 509   }
 510 
 511   Label try_revoke_bias;
 512   Label try_rebias;
 513 
 514   // At this point we know that the header has the bias pattern and
 515   // that we are not the bias owner in the current epoch. We need to
 516   // figure out more details about the state of the header in order to
 517   // know what operations can be legally performed on the object&#39;s
 518   // header.
 519 
 520   // If the low three bits in the xor result aren&#39;t clear, that means
 521   // the prototype header is no longer biased and we have to revoke
 522   // the bias on this object.
 523   andr(rscratch1, tmp_reg, markWord::biased_lock_mask_in_place);
 524   cbnz(rscratch1, try_revoke_bias);
 525 
 526   // Biasing is still enabled for this data type. See whether the
 527   // epoch of the current bias is still valid, meaning that the epoch
 528   // bits of the mark word are equal to the epoch bits of the
 529   // prototype header. (Note that the prototype header&#39;s epoch bits
 530   // only change at a safepoint.) If not, attempt to rebias the object
 531   // toward the current thread. Note that we must be absolutely sure
 532   // that the current epoch is invalid in order to do this because
 533   // otherwise the manipulations it performs on the mark word are
 534   // illegal.
 535   andr(rscratch1, tmp_reg, markWord::epoch_mask_in_place);
 536   cbnz(rscratch1, try_rebias);
 537 
 538   // The epoch of the current bias is still valid but we know nothing
 539   // about the owner; it might be set or it might be clear. Try to
 540   // acquire the bias of the object using an atomic operation. If this
 541   // fails we will go in to the runtime to revoke the object&#39;s bias.
 542   // Note that we first construct the presumed unbiased header so we
 543   // don&#39;t accidentally blow away another thread&#39;s valid bias.
 544   {
 545     Label here;
 546     mov(rscratch1, markWord::biased_lock_mask_in_place | markWord::age_mask_in_place | markWord::epoch_mask_in_place);
 547     andr(swap_reg, swap_reg, rscratch1);
 548     orr(tmp_reg, swap_reg, rthread);
 549     cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, rscratch1, here, slow_case);
 550     // If the biasing toward our thread failed, this means that
 551     // another thread succeeded in biasing it toward itself and we
 552     // need to revoke that bias. The revocation will occur in the
 553     // interpreter runtime in the slow case.
 554     bind(here);
 555     if (counters != NULL) {
 556       atomic_incw(Address((address)counters-&gt;anonymously_biased_lock_entry_count_addr()),
 557                   tmp_reg, rscratch1, rscratch2);
 558     }
 559   }
 560   b(done);
 561 
 562   bind(try_rebias);
 563   // At this point we know the epoch has expired, meaning that the
 564   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
 565   // circumstances _only_, we are allowed to use the current header&#39;s
 566   // value as the comparison value when doing the cas to acquire the
 567   // bias in the current epoch. In other words, we allow transfer of
 568   // the bias from one thread to another directly in this situation.
 569   //
 570   // FIXME: due to a lack of registers we currently blow away the age
 571   // bits in this situation. Should attempt to preserve them.
 572   {
 573     Label here;
 574     load_prototype_header(tmp_reg, obj_reg);
 575     orr(tmp_reg, rthread, tmp_reg);
 576     cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, rscratch1, here, slow_case);
 577     // If the biasing toward our thread failed, then another thread
 578     // succeeded in biasing it toward itself and we need to revoke that
 579     // bias. The revocation will occur in the runtime in the slow case.
 580     bind(here);
 581     if (counters != NULL) {
 582       atomic_incw(Address((address)counters-&gt;rebiased_lock_entry_count_addr()),
 583                   tmp_reg, rscratch1, rscratch2);
 584     }
 585   }
 586   b(done);
 587 
 588   bind(try_revoke_bias);
 589   // The prototype mark in the klass doesn&#39;t have the bias bit set any
 590   // more, indicating that objects of this data type are not supposed
 591   // to be biased any more. We are going to try to reset the mark of
 592   // this object to the prototype value and fall through to the
 593   // CAS-based locking scheme. Note that if our CAS fails, it means
 594   // that another thread raced us for the privilege of revoking the
 595   // bias of this particular object, so it&#39;s okay to continue in the
 596   // normal locking code.
 597   //
 598   // FIXME: due to a lack of registers we currently blow away the age
 599   // bits in this situation. Should attempt to preserve them.
 600   {
 601     Label here, nope;
 602     load_prototype_header(tmp_reg, obj_reg);
 603     cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, rscratch1, here, &amp;nope);
 604     bind(here);
 605 
 606     // Fall through to the normal CAS-based lock, because no matter what
 607     // the result of the above CAS, some thread must have succeeded in
 608     // removing the bias bit from the object&#39;s header.
 609     if (counters != NULL) {
 610       atomic_incw(Address((address)counters-&gt;revoked_lock_entry_count_addr()), tmp_reg,
 611                   rscratch1, rscratch2);
 612     }
 613     bind(nope);
 614   }
 615 
 616   bind(cas_label);
 617 
 618   return null_check_offset;
 619 }
 620 
 621 void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label&amp; done) {
 622   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 623 
 624   // Check for biased locking unlock case, which is a no-op
 625   // Note: we do not have to check the thread ID for two reasons.
 626   // First, the interpreter checks for IllegalMonitorStateException at
 627   // a higher level. Second, if the bias was revoked while we held the
 628   // lock, the object could not be rebiased toward another thread, so
 629   // the bias bit would be clear.
 630   ldr(temp_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
 631   andr(temp_reg, temp_reg, markWord::biased_lock_mask_in_place);
 632   cmp(temp_reg, (u1)markWord::biased_lock_pattern);
 633   br(Assembler::EQ, done);
 634 }
 635 
 636 static void pass_arg0(MacroAssembler* masm, Register arg) {
 637   if (c_rarg0 != arg ) {
 638     masm-&gt;mov(c_rarg0, arg);
 639   }
 640 }
 641 
 642 static void pass_arg1(MacroAssembler* masm, Register arg) {
 643   if (c_rarg1 != arg ) {
 644     masm-&gt;mov(c_rarg1, arg);
 645   }
 646 }
 647 
 648 static void pass_arg2(MacroAssembler* masm, Register arg) {
 649   if (c_rarg2 != arg ) {
 650     masm-&gt;mov(c_rarg2, arg);
 651   }
 652 }
 653 
 654 static void pass_arg3(MacroAssembler* masm, Register arg) {
 655   if (c_rarg3 != arg ) {
 656     masm-&gt;mov(c_rarg3, arg);
 657   }
 658 }
 659 
 660 void MacroAssembler::call_VM_base(Register oop_result,
 661                                   Register java_thread,
 662                                   Register last_java_sp,
 663                                   address  entry_point,
 664                                   int      number_of_arguments,
 665                                   bool     check_exceptions) {
 666    // determine java_thread register
 667   if (!java_thread-&gt;is_valid()) {
 668     java_thread = rthread;
 669   }
 670 
 671   // determine last_java_sp register
 672   if (!last_java_sp-&gt;is_valid()) {
 673     last_java_sp = esp;
 674   }
 675 
 676   // debugging support
 677   assert(number_of_arguments &gt;= 0   , &quot;cannot have negative number of arguments&quot;);
 678   assert(java_thread == rthread, &quot;unexpected register&quot;);
 679 #ifdef ASSERT
 680   // TraceBytecodes does not use r12 but saves it over the call, so don&#39;t verify
 681   // if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp; !TraceBytecodes) verify_heapbase(&quot;call_VM_base: heap base corrupted?&quot;);
 682 #endif // ASSERT
 683 
 684   assert(java_thread != oop_result  , &quot;cannot use the same register for java_thread &amp; oop_result&quot;);
 685   assert(java_thread != last_java_sp, &quot;cannot use the same register for java_thread &amp; last_java_sp&quot;);
 686 
 687   // push java thread (becomes first argument of C function)
 688 
 689   mov(c_rarg0, java_thread);
 690 
 691   // set last Java frame before call
 692   assert(last_java_sp != rfp, &quot;can&#39;t use rfp&quot;);
 693 
 694   Label l;
 695   set_last_Java_frame(last_java_sp, rfp, l, rscratch1);
 696 
 697   // do the call, remove parameters
 698   MacroAssembler::call_VM_leaf_base(entry_point, number_of_arguments, &amp;l);
 699 
 700   // reset last Java frame
 701   // Only interpreter should have to clear fp
 702   reset_last_Java_frame(true);
 703 
 704    // C++ interp handles this in the interpreter
 705   check_and_handle_popframe(java_thread);
 706   check_and_handle_earlyret(java_thread);
 707 
 708   if (check_exceptions) {
 709     // check for pending exceptions (java_thread is set upon return)
 710     ldr(rscratch1, Address(java_thread, in_bytes(Thread::pending_exception_offset())));
 711     Label ok;
 712     cbz(rscratch1, ok);
 713     lea(rscratch1, RuntimeAddress(StubRoutines::forward_exception_entry()));
 714     br(rscratch1);
 715     bind(ok);
 716   }
 717 
 718   // get oop result if there is one and reset the value in the thread
 719   if (oop_result-&gt;is_valid()) {
 720     get_vm_result(oop_result, java_thread);
 721   }
 722 }
 723 
 724 void MacroAssembler::call_VM_helper(Register oop_result, address entry_point, int number_of_arguments, bool check_exceptions) {
 725   call_VM_base(oop_result, noreg, noreg, entry_point, number_of_arguments, check_exceptions);
 726 }
 727 
 728 // Maybe emit a call via a trampoline.  If the code cache is small
 729 // trampolines won&#39;t be emitted.
 730 
 731 address MacroAssembler::trampoline_call(Address entry, CodeBuffer *cbuf) {
 732   assert(JavaThread::current()-&gt;is_Compiler_thread(), &quot;just checking&quot;);
 733   assert(entry.rspec().type() == relocInfo::runtime_call_type
 734          || entry.rspec().type() == relocInfo::opt_virtual_call_type
 735          || entry.rspec().type() == relocInfo::static_call_type
 736          || entry.rspec().type() == relocInfo::virtual_call_type, &quot;wrong reloc type&quot;);
 737 
 738   // We need a trampoline if branches are far.
 739   if (far_branches()) {
 740     bool in_scratch_emit_size = false;
 741 #ifdef COMPILER2
 742     // We don&#39;t want to emit a trampoline if C2 is generating dummy
 743     // code during its branch shortening phase.
 744     CompileTask* task = ciEnv::current()-&gt;task();
 745     in_scratch_emit_size =
 746       (task != NULL &amp;&amp; is_c2_compile(task-&gt;comp_level()) &amp;&amp;
 747        Compile::current()-&gt;in_scratch_emit_size());
 748 #endif
 749     if (!in_scratch_emit_size) {
 750       address stub = emit_trampoline_stub(offset(), entry.target());
 751       if (stub == NULL) {
 752         return NULL; // CodeCache is full
 753       }
 754     }
 755   }
 756 
 757   if (cbuf) cbuf-&gt;set_insts_mark();
 758   relocate(entry.rspec());
 759   if (!far_branches()) {
 760     bl(entry.target());
 761   } else {
 762     bl(pc());
 763   }
 764   // just need to return a non-null address
 765   return pc();
 766 }
 767 
 768 
 769 // Emit a trampoline stub for a call to a target which is too far away.
 770 //
 771 // code sequences:
 772 //
 773 // call-site:
 774 //   branch-and-link to &lt;destination&gt; or &lt;trampoline stub&gt;
 775 //
 776 // Related trampoline stub for this call site in the stub section:
 777 //   load the call target from the constant pool
 778 //   branch (LR still points to the call site above)
 779 
 780 address MacroAssembler::emit_trampoline_stub(int insts_call_instruction_offset,
 781                                              address dest) {
 782   // Max stub size: alignment nop, TrampolineStub.
 783   address stub = start_a_stub(NativeInstruction::instruction_size
 784                    + NativeCallTrampolineStub::instruction_size);
 785   if (stub == NULL) {
 786     return NULL;  // CodeBuffer::expand failed
 787   }
 788 
 789   // Create a trampoline stub relocation which relates this trampoline stub
 790   // with the call instruction at insts_call_instruction_offset in the
 791   // instructions code-section.
 792   align(wordSize);
 793   relocate(trampoline_stub_Relocation::spec(code()-&gt;insts()-&gt;start()
 794                                             + insts_call_instruction_offset));
 795   const int stub_start_offset = offset();
 796 
 797   // Now, create the trampoline stub&#39;s code:
 798   // - load the call
 799   // - call
 800   Label target;
 801   ldr(rscratch1, target);
 802   br(rscratch1);
 803   bind(target);
 804   assert(offset() - stub_start_offset == NativeCallTrampolineStub::data_offset,
 805          &quot;should be&quot;);
 806   emit_int64((int64_t)dest);
 807 
 808   const address stub_start_addr = addr_at(stub_start_offset);
 809 
 810   assert(is_NativeCallTrampolineStub_at(stub_start_addr), &quot;doesn&#39;t look like a trampoline&quot;);
 811 
 812   end_a_stub();
 813   return stub_start_addr;
 814 }
 815 
 816 void MacroAssembler::emit_static_call_stub() {
 817   // CompiledDirectStaticCall::set_to_interpreted knows the
 818   // exact layout of this stub.
 819 
 820   isb();
 821   mov_metadata(rmethod, (Metadata*)NULL);
 822 
 823   // Jump to the entry point of the i2c stub.
 824   movptr(rscratch1, 0);
 825   br(rscratch1);
 826 }
 827 
 828 void MacroAssembler::c2bool(Register x) {
 829   // implements x == 0 ? 0 : 1
 830   // note: must only look at least-significant byte of x
 831   //       since C-style booleans are stored in one byte
 832   //       only! (was bug)
 833   tst(x, 0xff);
 834   cset(x, Assembler::NE);
 835 }
 836 
 837 address MacroAssembler::ic_call(address entry, jint method_index) {
 838   RelocationHolder rh = virtual_call_Relocation::spec(pc(), method_index);
 839   // address const_ptr = long_constant((jlong)Universe::non_oop_word());
 840   // unsigned long offset;
 841   // ldr_constant(rscratch2, const_ptr);
 842   movptr(rscratch2, (uintptr_t)Universe::non_oop_word());
 843   return trampoline_call(Address(entry, rh));
 844 }
 845 
 846 // Implementation of call_VM versions
 847 
 848 void MacroAssembler::call_VM(Register oop_result,
 849                              address entry_point,
 850                              bool check_exceptions) {
 851   call_VM_helper(oop_result, entry_point, 0, check_exceptions);
 852 }
 853 
 854 void MacroAssembler::call_VM(Register oop_result,
 855                              address entry_point,
 856                              Register arg_1,
 857                              bool check_exceptions) {
 858   pass_arg1(this, arg_1);
 859   call_VM_helper(oop_result, entry_point, 1, check_exceptions);
 860 }
 861 
 862 void MacroAssembler::call_VM(Register oop_result,
 863                              address entry_point,
 864                              Register arg_1,
 865                              Register arg_2,
 866                              bool check_exceptions) {
 867   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
 868   pass_arg2(this, arg_2);
 869   pass_arg1(this, arg_1);
 870   call_VM_helper(oop_result, entry_point, 2, check_exceptions);
 871 }
 872 
 873 void MacroAssembler::call_VM(Register oop_result,
 874                              address entry_point,
 875                              Register arg_1,
 876                              Register arg_2,
 877                              Register arg_3,
 878                              bool check_exceptions) {
 879   assert(arg_1 != c_rarg3, &quot;smashed arg&quot;);
 880   assert(arg_2 != c_rarg3, &quot;smashed arg&quot;);
 881   pass_arg3(this, arg_3);
 882 
 883   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
 884   pass_arg2(this, arg_2);
 885 
 886   pass_arg1(this, arg_1);
 887   call_VM_helper(oop_result, entry_point, 3, check_exceptions);
 888 }
 889 
 890 void MacroAssembler::call_VM(Register oop_result,
 891                              Register last_java_sp,
 892                              address entry_point,
 893                              int number_of_arguments,
 894                              bool check_exceptions) {
 895   call_VM_base(oop_result, rthread, last_java_sp, entry_point, number_of_arguments, check_exceptions);
 896 }
 897 
 898 void MacroAssembler::call_VM(Register oop_result,
 899                              Register last_java_sp,
 900                              address entry_point,
 901                              Register arg_1,
 902                              bool check_exceptions) {
 903   pass_arg1(this, arg_1);
 904   call_VM(oop_result, last_java_sp, entry_point, 1, check_exceptions);
 905 }
 906 
 907 void MacroAssembler::call_VM(Register oop_result,
 908                              Register last_java_sp,
 909                              address entry_point,
 910                              Register arg_1,
 911                              Register arg_2,
 912                              bool check_exceptions) {
 913 
 914   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
 915   pass_arg2(this, arg_2);
 916   pass_arg1(this, arg_1);
 917   call_VM(oop_result, last_java_sp, entry_point, 2, check_exceptions);
 918 }
 919 
 920 void MacroAssembler::call_VM(Register oop_result,
 921                              Register last_java_sp,
 922                              address entry_point,
 923                              Register arg_1,
 924                              Register arg_2,
 925                              Register arg_3,
 926                              bool check_exceptions) {
 927   assert(arg_1 != c_rarg3, &quot;smashed arg&quot;);
 928   assert(arg_2 != c_rarg3, &quot;smashed arg&quot;);
 929   pass_arg3(this, arg_3);
 930   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
 931   pass_arg2(this, arg_2);
 932   pass_arg1(this, arg_1);
 933   call_VM(oop_result, last_java_sp, entry_point, 3, check_exceptions);
 934 }
 935 
 936 
 937 void MacroAssembler::get_vm_result(Register oop_result, Register java_thread) {
 938   ldr(oop_result, Address(java_thread, JavaThread::vm_result_offset()));
 939   str(zr, Address(java_thread, JavaThread::vm_result_offset()));
 940   verify_oop(oop_result, &quot;broken oop in call_VM_base&quot;);
 941 }
 942 
 943 void MacroAssembler::get_vm_result_2(Register metadata_result, Register java_thread) {
 944   ldr(metadata_result, Address(java_thread, JavaThread::vm_result_2_offset()));
 945   str(zr, Address(java_thread, JavaThread::vm_result_2_offset()));
 946 }
 947 
 948 void MacroAssembler::align(int modulus) {
 949   while (offset() % modulus != 0) nop();
 950 }
 951 
 952 // these are no-ops overridden by InterpreterMacroAssembler
 953 
 954 void MacroAssembler::check_and_handle_earlyret(Register java_thread) { }
 955 
 956 void MacroAssembler::check_and_handle_popframe(Register java_thread) { }
 957 
 958 
 959 RegisterOrConstant MacroAssembler::delayed_value_impl(intptr_t* delayed_value_addr,
 960                                                       Register tmp,
 961                                                       int offset) {
 962   intptr_t value = *delayed_value_addr;
 963   if (value != 0)
 964     return RegisterOrConstant(value + offset);
 965 
 966   // load indirectly to solve generation ordering problem
 967   ldr(tmp, ExternalAddress((address) delayed_value_addr));
 968 
 969   if (offset != 0)
 970     add(tmp, tmp, offset);
 971 
 972   return RegisterOrConstant(tmp);
 973 }
 974 
 975 // Look up the method for a megamorphic invokeinterface call.
 976 // The target method is determined by &lt;intf_klass, itable_index&gt;.
 977 // The receiver klass is in recv_klass.
 978 // On success, the result will be in method_result, and execution falls through.
 979 // On failure, execution transfers to the given label.
 980 void MacroAssembler::lookup_interface_method(Register recv_klass,
 981                                              Register intf_klass,
 982                                              RegisterOrConstant itable_index,
 983                                              Register method_result,
 984                                              Register scan_temp,
 985                                              Label&amp; L_no_such_interface,
 986                          bool return_method) {
 987   assert_different_registers(recv_klass, intf_klass, scan_temp);
 988   assert_different_registers(method_result, intf_klass, scan_temp);
 989   assert(recv_klass != method_result || !return_method,
 990      &quot;recv_klass can be destroyed when method isn&#39;t needed&quot;);
 991   assert(itable_index.is_constant() || itable_index.as_register() == method_result,
 992          &quot;caller must use same register for non-constant itable index as for method&quot;);
 993 
 994   // Compute start of first itableOffsetEntry (which is at the end of the vtable)
 995   int vtable_base = in_bytes(Klass::vtable_start_offset());
 996   int itentry_off = itableMethodEntry::method_offset_in_bytes();
 997   int scan_step   = itableOffsetEntry::size() * wordSize;
 998   int vte_size    = vtableEntry::size_in_bytes();
 999   assert(vte_size == wordSize, &quot;else adjust times_vte_scale&quot;);
1000 
1001   ldrw(scan_temp, Address(recv_klass, Klass::vtable_length_offset()));
1002 
1003   // %%% Could store the aligned, prescaled offset in the klassoop.
1004   // lea(scan_temp, Address(recv_klass, scan_temp, times_vte_scale, vtable_base));
1005   lea(scan_temp, Address(recv_klass, scan_temp, Address::lsl(3)));
1006   add(scan_temp, scan_temp, vtable_base);
1007 
1008   if (return_method) {
1009     // Adjust recv_klass by scaled itable_index, so we can free itable_index.
1010     assert(itableMethodEntry::size() * wordSize == wordSize, &quot;adjust the scaling in the code below&quot;);
1011     // lea(recv_klass, Address(recv_klass, itable_index, Address::times_ptr, itentry_off));
1012     lea(recv_klass, Address(recv_klass, itable_index, Address::lsl(3)));
1013     if (itentry_off)
1014       add(recv_klass, recv_klass, itentry_off);
1015   }
1016 
1017   // for (scan = klass-&gt;itable(); scan-&gt;interface() != NULL; scan += scan_step) {
1018   //   if (scan-&gt;interface() == intf) {
1019   //     result = (klass + scan-&gt;offset() + itable_index);
1020   //   }
1021   // }
1022   Label search, found_method;
1023 
1024   for (int peel = 1; peel &gt;= 0; peel--) {
1025     ldr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset_in_bytes()));
1026     cmp(intf_klass, method_result);
1027 
1028     if (peel) {
1029       br(Assembler::EQ, found_method);
1030     } else {
1031       br(Assembler::NE, search);
1032       // (invert the test to fall through to found_method...)
1033     }
1034 
1035     if (!peel)  break;
1036 
1037     bind(search);
1038 
1039     // Check that the previous entry is non-null.  A null entry means that
1040     // the receiver class doesn&#39;t implement the interface, and wasn&#39;t the
1041     // same as when the caller was compiled.
1042     cbz(method_result, L_no_such_interface);
1043     add(scan_temp, scan_temp, scan_step);
1044   }
1045 
1046   bind(found_method);
1047 
1048   // Got a hit.
1049   if (return_method) {
1050     ldrw(scan_temp, Address(scan_temp, itableOffsetEntry::offset_offset_in_bytes()));
1051     ldr(method_result, Address(recv_klass, scan_temp, Address::uxtw(0)));
1052   }
1053 }
1054 
1055 // virtual method calling
1056 void MacroAssembler::lookup_virtual_method(Register recv_klass,
1057                                            RegisterOrConstant vtable_index,
1058                                            Register method_result) {
1059   const int base = in_bytes(Klass::vtable_start_offset());
1060   assert(vtableEntry::size() * wordSize == 8,
1061          &quot;adjust the scaling in the code below&quot;);
1062   int vtable_offset_in_bytes = base + vtableEntry::method_offset_in_bytes();
1063 
1064   if (vtable_index.is_register()) {
1065     lea(method_result, Address(recv_klass,
1066                                vtable_index.as_register(),
1067                                Address::lsl(LogBytesPerWord)));
1068     ldr(method_result, Address(method_result, vtable_offset_in_bytes));
1069   } else {
1070     vtable_offset_in_bytes += vtable_index.as_constant() * wordSize;
1071     ldr(method_result,
1072         form_address(rscratch1, recv_klass, vtable_offset_in_bytes, 0));
1073   }
1074 }
1075 
1076 void MacroAssembler::check_klass_subtype(Register sub_klass,
1077                            Register super_klass,
1078                            Register temp_reg,
1079                            Label&amp; L_success) {
1080   Label L_failure;
1081   check_klass_subtype_fast_path(sub_klass, super_klass, temp_reg,        &amp;L_success, &amp;L_failure, NULL);
1082   check_klass_subtype_slow_path(sub_klass, super_klass, temp_reg, noreg, &amp;L_success, NULL);
1083   bind(L_failure);
1084 }
1085 
1086 
1087 void MacroAssembler::check_klass_subtype_fast_path(Register sub_klass,
1088                                                    Register super_klass,
1089                                                    Register temp_reg,
1090                                                    Label* L_success,
1091                                                    Label* L_failure,
1092                                                    Label* L_slow_path,
1093                                         RegisterOrConstant super_check_offset) {
1094   assert_different_registers(sub_klass, super_klass, temp_reg);
1095   bool must_load_sco = (super_check_offset.constant_or_zero() == -1);
1096   if (super_check_offset.is_register()) {
1097     assert_different_registers(sub_klass, super_klass,
1098                                super_check_offset.as_register());
1099   } else if (must_load_sco) {
1100     assert(temp_reg != noreg, &quot;supply either a temp or a register offset&quot;);
1101   }
1102 
1103   Label L_fallthrough;
1104   int label_nulls = 0;
1105   if (L_success == NULL)   { L_success   = &amp;L_fallthrough; label_nulls++; }
1106   if (L_failure == NULL)   { L_failure   = &amp;L_fallthrough; label_nulls++; }
1107   if (L_slow_path == NULL) { L_slow_path = &amp;L_fallthrough; label_nulls++; }
1108   assert(label_nulls &lt;= 1, &quot;at most one NULL in the batch&quot;);
1109 
1110   int sc_offset = in_bytes(Klass::secondary_super_cache_offset());
1111   int sco_offset = in_bytes(Klass::super_check_offset_offset());
1112   Address super_check_offset_addr(super_klass, sco_offset);
1113 
1114   // Hacked jmp, which may only be used just before L_fallthrough.
1115 #define final_jmp(label)                                                \
1116   if (&amp;(label) == &amp;L_fallthrough) { /*do nothing*/ }                    \
1117   else                            b(label)                /*omit semi*/
1118 
1119   // If the pointers are equal, we are done (e.g., String[] elements).
1120   // This self-check enables sharing of secondary supertype arrays among
1121   // non-primary types such as array-of-interface.  Otherwise, each such
1122   // type would need its own customized SSA.
1123   // We move this check to the front of the fast path because many
1124   // type checks are in fact trivially successful in this manner,
1125   // so we get a nicely predicted branch right at the start of the check.
1126   cmp(sub_klass, super_klass);
1127   br(Assembler::EQ, *L_success);
1128 
1129   // Check the supertype display:
1130   if (must_load_sco) {
1131     ldrw(temp_reg, super_check_offset_addr);
1132     super_check_offset = RegisterOrConstant(temp_reg);
1133   }
1134   Address super_check_addr(sub_klass, super_check_offset);
1135   ldr(rscratch1, super_check_addr);
1136   cmp(super_klass, rscratch1); // load displayed supertype
1137 
1138   // This check has worked decisively for primary supers.
1139   // Secondary supers are sought in the super_cache (&#39;super_cache_addr&#39;).
1140   // (Secondary supers are interfaces and very deeply nested subtypes.)
1141   // This works in the same check above because of a tricky aliasing
1142   // between the super_cache and the primary super display elements.
1143   // (The &#39;super_check_addr&#39; can address either, as the case requires.)
1144   // Note that the cache is updated below if it does not help us find
1145   // what we need immediately.
1146   // So if it was a primary super, we can just fail immediately.
1147   // Otherwise, it&#39;s the slow path for us (no success at this point).
1148 
1149   if (super_check_offset.is_register()) {
1150     br(Assembler::EQ, *L_success);
1151     subs(zr, super_check_offset.as_register(), sc_offset);
1152     if (L_failure == &amp;L_fallthrough) {
1153       br(Assembler::EQ, *L_slow_path);
1154     } else {
1155       br(Assembler::NE, *L_failure);
1156       final_jmp(*L_slow_path);
1157     }
1158   } else if (super_check_offset.as_constant() == sc_offset) {
1159     // Need a slow path; fast failure is impossible.
1160     if (L_slow_path == &amp;L_fallthrough) {
1161       br(Assembler::EQ, *L_success);
1162     } else {
1163       br(Assembler::NE, *L_slow_path);
1164       final_jmp(*L_success);
1165     }
1166   } else {
1167     // No slow path; it&#39;s a fast decision.
1168     if (L_failure == &amp;L_fallthrough) {
1169       br(Assembler::EQ, *L_success);
1170     } else {
1171       br(Assembler::NE, *L_failure);
1172       final_jmp(*L_success);
1173     }
1174   }
1175 
1176   bind(L_fallthrough);
1177 
1178 #undef final_jmp
1179 }
1180 
1181 // These two are taken from x86, but they look generally useful
1182 
1183 // scans count pointer sized words at [addr] for occurence of value,
1184 // generic
1185 void MacroAssembler::repne_scan(Register addr, Register value, Register count,
1186                                 Register scratch) {
1187   Label Lloop, Lexit;
1188   cbz(count, Lexit);
1189   bind(Lloop);
1190   ldr(scratch, post(addr, wordSize));
1191   cmp(value, scratch);
1192   br(EQ, Lexit);
1193   sub(count, count, 1);
1194   cbnz(count, Lloop);
1195   bind(Lexit);
1196 }
1197 
1198 // scans count 4 byte words at [addr] for occurence of value,
1199 // generic
1200 void MacroAssembler::repne_scanw(Register addr, Register value, Register count,
1201                                 Register scratch) {
1202   Label Lloop, Lexit;
1203   cbz(count, Lexit);
1204   bind(Lloop);
1205   ldrw(scratch, post(addr, wordSize));
1206   cmpw(value, scratch);
1207   br(EQ, Lexit);
1208   sub(count, count, 1);
1209   cbnz(count, Lloop);
1210   bind(Lexit);
1211 }
1212 
1213 void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,
1214                                                    Register super_klass,
1215                                                    Register temp_reg,
1216                                                    Register temp2_reg,
1217                                                    Label* L_success,
1218                                                    Label* L_failure,
1219                                                    bool set_cond_codes) {
1220   assert_different_registers(sub_klass, super_klass, temp_reg);
1221   if (temp2_reg != noreg)
1222     assert_different_registers(sub_klass, super_klass, temp_reg, temp2_reg, rscratch1);
1223 #define IS_A_TEMP(reg) ((reg) == temp_reg || (reg) == temp2_reg)
1224 
1225   Label L_fallthrough;
1226   int label_nulls = 0;
1227   if (L_success == NULL)   { L_success   = &amp;L_fallthrough; label_nulls++; }
1228   if (L_failure == NULL)   { L_failure   = &amp;L_fallthrough; label_nulls++; }
1229   assert(label_nulls &lt;= 1, &quot;at most one NULL in the batch&quot;);
1230 
1231   // a couple of useful fields in sub_klass:
1232   int ss_offset = in_bytes(Klass::secondary_supers_offset());
1233   int sc_offset = in_bytes(Klass::secondary_super_cache_offset());
1234   Address secondary_supers_addr(sub_klass, ss_offset);
1235   Address super_cache_addr(     sub_klass, sc_offset);
1236 
1237   BLOCK_COMMENT(&quot;check_klass_subtype_slow_path&quot;);
1238 
1239   // Do a linear scan of the secondary super-klass chain.
1240   // This code is rarely used, so simplicity is a virtue here.
1241   // The repne_scan instruction uses fixed registers, which we must spill.
1242   // Don&#39;t worry too much about pre-existing connections with the input regs.
1243 
1244   assert(sub_klass != r0, &quot;killed reg&quot;); // killed by mov(r0, super)
1245   assert(sub_klass != r2, &quot;killed reg&quot;); // killed by lea(r2, &amp;pst_counter)
1246 
1247   RegSet pushed_registers;
1248   if (!IS_A_TEMP(r2))    pushed_registers += r2;
1249   if (!IS_A_TEMP(r5))    pushed_registers += r5;
1250 
1251   if (super_klass != r0 || UseCompressedOops) {
1252     if (!IS_A_TEMP(r0))   pushed_registers += r0;
1253   }
1254 
1255   push(pushed_registers, sp);
1256 
1257   // Get super_klass value into r0 (even if it was in r5 or r2).
1258   if (super_klass != r0) {
1259     mov(r0, super_klass);
1260   }
1261 
1262 #ifndef PRODUCT
1263   mov(rscratch2, (address)&amp;SharedRuntime::_partial_subtype_ctr);
1264   Address pst_counter_addr(rscratch2);
1265   ldr(rscratch1, pst_counter_addr);
1266   add(rscratch1, rscratch1, 1);
1267   str(rscratch1, pst_counter_addr);
1268 #endif //PRODUCT
1269 
1270   // We will consult the secondary-super array.
1271   ldr(r5, secondary_supers_addr);
1272   // Load the array length.
1273   ldrw(r2, Address(r5, Array&lt;Klass*&gt;::length_offset_in_bytes()));
1274   // Skip to start of data.
1275   add(r5, r5, Array&lt;Klass*&gt;::base_offset_in_bytes());
1276 
1277   cmp(sp, zr); // Clear Z flag; SP is never zero
1278   // Scan R2 words at [R5] for an occurrence of R0.
1279   // Set NZ/Z based on last compare.
1280   repne_scan(r5, r0, r2, rscratch1);
1281 
1282   // Unspill the temp. registers:
1283   pop(pushed_registers, sp);
1284 
1285   br(Assembler::NE, *L_failure);
1286 
1287   // Success.  Cache the super we found and proceed in triumph.
1288   str(super_klass, super_cache_addr);
1289 
1290   if (L_success != &amp;L_fallthrough) {
1291     b(*L_success);
1292   }
1293 
1294 #undef IS_A_TEMP
1295 
1296   bind(L_fallthrough);
1297 }
1298 
1299 void MacroAssembler::clinit_barrier(Register klass, Register scratch, Label* L_fast_path, Label* L_slow_path) {
1300   assert(L_fast_path != NULL || L_slow_path != NULL, &quot;at least one is required&quot;);
1301   assert_different_registers(klass, rthread, scratch);
1302 
1303   Label L_fallthrough, L_tmp;
1304   if (L_fast_path == NULL) {
1305     L_fast_path = &amp;L_fallthrough;
1306   } else if (L_slow_path == NULL) {
1307     L_slow_path = &amp;L_fallthrough;
1308   }
1309   // Fast path check: class is fully initialized
1310   ldrb(scratch, Address(klass, InstanceKlass::init_state_offset()));
1311   subs(zr, scratch, InstanceKlass::fully_initialized);
1312   br(Assembler::EQ, *L_fast_path);
1313 
1314   // Fast path check: current thread is initializer thread
1315   ldr(scratch, Address(klass, InstanceKlass::init_thread_offset()));
1316   cmp(rthread, scratch);
1317 
1318   if (L_slow_path == &amp;L_fallthrough) {
1319     br(Assembler::EQ, *L_fast_path);
1320     bind(*L_slow_path);
1321   } else if (L_fast_path == &amp;L_fallthrough) {
1322     br(Assembler::NE, *L_slow_path);
1323     bind(*L_fast_path);
1324   } else {
1325     Unimplemented();
1326   }
1327 }
1328 
1329 void MacroAssembler::verify_oop(Register reg, const char* s) {
1330   if (!VerifyOops) return;
1331 
1332   // Pass register number to verify_oop_subroutine
1333   const char* b = NULL;
1334   {
1335     ResourceMark rm;
1336     stringStream ss;
1337     ss.print(&quot;verify_oop: %s: %s&quot;, reg-&gt;name(), s);
1338     b = code_string(ss.as_string());
1339   }
1340   BLOCK_COMMENT(&quot;verify_oop {&quot;);
1341 
1342   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1343   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1344 
1345   mov(r0, reg);
1346   mov(rscratch1, (address)b);
1347 
1348   // call indirectly to solve generation ordering problem
1349   lea(rscratch2, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
1350   ldr(rscratch2, Address(rscratch2));
1351   blr(rscratch2);
1352 
1353   ldp(rscratch2, lr, Address(post(sp, 2 * wordSize)));
1354   ldp(r0, rscratch1, Address(post(sp, 2 * wordSize)));
1355 
1356   BLOCK_COMMENT(&quot;} verify_oop&quot;);
1357 }
1358 
1359 void MacroAssembler::verify_oop_addr(Address addr, const char* s) {
1360   if (!VerifyOops) return;
1361 
1362   const char* b = NULL;
1363   {
1364     ResourceMark rm;
1365     stringStream ss;
1366     ss.print(&quot;verify_oop_addr: %s&quot;, s);
1367     b = code_string(ss.as_string());
1368   }
1369   BLOCK_COMMENT(&quot;verify_oop_addr {&quot;);
1370 
1371   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1372   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1373 
1374   // addr may contain sp so we will have to adjust it based on the
1375   // pushes that we just did.
1376   if (addr.uses(sp)) {
1377     lea(r0, addr);
1378     ldr(r0, Address(r0, 4 * wordSize));
1379   } else {
1380     ldr(r0, addr);
1381   }
1382   mov(rscratch1, (address)b);
1383 
1384   // call indirectly to solve generation ordering problem
1385   lea(rscratch2, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
1386   ldr(rscratch2, Address(rscratch2));
1387   blr(rscratch2);
1388 
1389   ldp(rscratch2, lr, Address(post(sp, 2 * wordSize)));
1390   ldp(r0, rscratch1, Address(post(sp, 2 * wordSize)));
1391 
1392   BLOCK_COMMENT(&quot;} verify_oop_addr&quot;);
1393 }
1394 
1395 Address MacroAssembler::argument_address(RegisterOrConstant arg_slot,
1396                                          int extra_slot_offset) {
1397   // cf. TemplateTable::prepare_invoke(), if (load_receiver).
1398   int stackElementSize = Interpreter::stackElementSize;
1399   int offset = Interpreter::expr_offset_in_bytes(extra_slot_offset+0);
1400 #ifdef ASSERT
1401   int offset1 = Interpreter::expr_offset_in_bytes(extra_slot_offset+1);
1402   assert(offset1 - offset == stackElementSize, &quot;correct arithmetic&quot;);
1403 #endif
1404   if (arg_slot.is_constant()) {
1405     int calc_offset = arg_slot.as_constant() * stackElementSize + offset;
1406     return Address(esp, calc_offset);
1407   } else {
1408     add(rscratch1, esp, arg_slot.as_register(),
1409         ext::uxtx, exact_log2(stackElementSize));
1410     return Address(rscratch1, offset);
1411   }
1412 }
1413 
1414 void MacroAssembler::call_VM_leaf_base(address entry_point,
1415                                        int number_of_arguments,
1416                                        Label *retaddr) {
1417   Label E, L;
1418 
1419   stp(rscratch1, rmethod, Address(pre(sp, -2 * wordSize)));
1420 
1421   mov(rscratch1, entry_point);
1422   blr(rscratch1);
1423   if (retaddr)
1424     bind(*retaddr);
1425 
1426   ldp(rscratch1, rmethod, Address(post(sp, 2 * wordSize)));
1427   maybe_isb();
1428 }
1429 
1430 void MacroAssembler::call_VM_leaf(address entry_point, int number_of_arguments) {
1431   call_VM_leaf_base(entry_point, number_of_arguments);
1432 }
1433 
1434 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0) {
1435   pass_arg0(this, arg_0);
1436   call_VM_leaf_base(entry_point, 1);
1437 }
1438 
1439 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1440   pass_arg0(this, arg_0);
1441   pass_arg1(this, arg_1);
1442   call_VM_leaf_base(entry_point, 2);
1443 }
1444 
1445 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0,
1446                                   Register arg_1, Register arg_2) {
1447   pass_arg0(this, arg_0);
1448   pass_arg1(this, arg_1);
1449   pass_arg2(this, arg_2);
1450   call_VM_leaf_base(entry_point, 3);
1451 }
1452 
1453 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
1454   pass_arg0(this, arg_0);
1455   MacroAssembler::call_VM_leaf_base(entry_point, 1);
1456 }
1457 
1458 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1459 
1460   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1461   pass_arg1(this, arg_1);
1462   pass_arg0(this, arg_0);
1463   MacroAssembler::call_VM_leaf_base(entry_point, 2);
1464 }
1465 
1466 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2) {
1467   assert(arg_0 != c_rarg2, &quot;smashed arg&quot;);
1468   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1469   pass_arg2(this, arg_2);
1470   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1471   pass_arg1(this, arg_1);
1472   pass_arg0(this, arg_0);
1473   MacroAssembler::call_VM_leaf_base(entry_point, 3);
1474 }
1475 
1476 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2, Register arg_3) {
1477   assert(arg_0 != c_rarg3, &quot;smashed arg&quot;);
1478   assert(arg_1 != c_rarg3, &quot;smashed arg&quot;);
1479   assert(arg_2 != c_rarg3, &quot;smashed arg&quot;);
1480   pass_arg3(this, arg_3);
1481   assert(arg_0 != c_rarg2, &quot;smashed arg&quot;);
1482   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1483   pass_arg2(this, arg_2);
1484   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1485   pass_arg1(this, arg_1);
1486   pass_arg0(this, arg_0);
1487   MacroAssembler::call_VM_leaf_base(entry_point, 4);
1488 }
1489 
1490 void MacroAssembler::null_check(Register reg, int offset) {
1491   if (needs_explicit_null_check(offset)) {
1492     // provoke OS NULL exception if reg = NULL by
1493     // accessing M[reg] w/o changing any registers
1494     // NOTE: this is plenty to provoke a segv
1495     ldr(zr, Address(reg));
1496   } else {
1497     // nothing to do, (later) access of M[reg + offset]
1498     // will provoke OS NULL exception if reg = NULL
1499   }
1500 }
1501 
1502 // MacroAssembler protected routines needed to implement
1503 // public methods
1504 
1505 void MacroAssembler::mov(Register r, Address dest) {
1506   code_section()-&gt;relocate(pc(), dest.rspec());
1507   uint64_t imm64 = (uint64_t)dest.target();
1508   movptr(r, imm64);
1509 }
1510 
1511 // Move a constant pointer into r.  In AArch64 mode the virtual
1512 // address space is 48 bits in size, so we only need three
1513 // instructions to create a patchable instruction sequence that can
1514 // reach anywhere.
1515 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1516 #ifndef PRODUCT
1517   {
1518     char buffer[64];
1519     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1520     block_comment(buffer);
1521   }
1522 #endif
1523   assert(imm64 &lt; (1ull &lt;&lt; 48), &quot;48-bit overflow in address constant&quot;);
1524   movz(r, imm64 &amp; 0xffff);
1525   imm64 &gt;&gt;= 16;
1526   movk(r, imm64 &amp; 0xffff, 16);
1527   imm64 &gt;&gt;= 16;
1528   movk(r, imm64 &amp; 0xffff, 32);
1529 }
1530 
1531 // Macro to mov replicated immediate to vector register.
1532 //  Vd will get the following values for different arrangements in T
1533 //   imm32 == hex 000000gh  T8B:  Vd = ghghghghghghghgh
1534 //   imm32 == hex 000000gh  T16B: Vd = ghghghghghghghghghghghghghghghgh
1535 //   imm32 == hex 0000efgh  T4H:  Vd = efghefghefghefgh
1536 //   imm32 == hex 0000efgh  T8H:  Vd = efghefghefghefghefghefghefghefgh
1537 //   imm32 == hex abcdefgh  T2S:  Vd = abcdefghabcdefgh
1538 //   imm32 == hex abcdefgh  T4S:  Vd = abcdefghabcdefghabcdefghabcdefgh
1539 //   T1D/T2D: invalid
1540 void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, uint32_t imm32) {
1541   assert(T != T1D &amp;&amp; T != T2D, &quot;invalid arrangement&quot;);
1542   if (T == T8B || T == T16B) {
1543     assert((imm32 &amp; ~0xff) == 0, &quot;extraneous bits in unsigned imm32 (T8B/T16B)&quot;);
1544     movi(Vd, T, imm32 &amp; 0xff, 0);
1545     return;
1546   }
1547   uint32_t nimm32 = ~imm32;
1548   if (T == T4H || T == T8H) {
1549     assert((imm32  &amp; ~0xffff) == 0, &quot;extraneous bits in unsigned imm32 (T4H/T8H)&quot;);
1550     imm32 &amp;= 0xffff;
1551     nimm32 &amp;= 0xffff;
1552   }
1553   uint32_t x = imm32;
1554   int movi_cnt = 0;
1555   int movn_cnt = 0;
1556   while (x) { if (x &amp; 0xff) movi_cnt++; x &gt;&gt;= 8; }
1557   x = nimm32;
1558   while (x) { if (x &amp; 0xff) movn_cnt++; x &gt;&gt;= 8; }
1559   if (movn_cnt &lt; movi_cnt) imm32 = nimm32;
1560   unsigned lsl = 0;
1561   while (imm32 &amp;&amp; (imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1562   if (movn_cnt &lt; movi_cnt)
1563     mvni(Vd, T, imm32 &amp; 0xff, lsl);
1564   else
1565     movi(Vd, T, imm32 &amp; 0xff, lsl);
1566   imm32 &gt;&gt;= 8; lsl += 8;
1567   while (imm32) {
1568     while ((imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1569     if (movn_cnt &lt; movi_cnt)
1570       bici(Vd, T, imm32 &amp; 0xff, lsl);
1571     else
1572       orri(Vd, T, imm32 &amp; 0xff, lsl);
1573     lsl += 8; imm32 &gt;&gt;= 8;
1574   }
1575 }
1576 
1577 void MacroAssembler::mov_immediate64(Register dst, uint64_t imm64)
1578 {
1579 #ifndef PRODUCT
1580   {
1581     char buffer[64];
1582     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1583     block_comment(buffer);
1584   }
1585 #endif
1586   if (operand_valid_for_logical_immediate(false, imm64)) {
1587     orr(dst, zr, imm64);
1588   } else {
1589     // we can use a combination of MOVZ or MOVN with
1590     // MOVK to build up the constant
1591     uint64_t imm_h[4];
1592     int zero_count = 0;
1593     int neg_count = 0;
1594     int i;
1595     for (i = 0; i &lt; 4; i++) {
1596       imm_h[i] = ((imm64 &gt;&gt; (i * 16)) &amp; 0xffffL);
1597       if (imm_h[i] == 0) {
1598         zero_count++;
1599       } else if (imm_h[i] == 0xffffL) {
1600         neg_count++;
1601       }
1602     }
1603     if (zero_count == 4) {
1604       // one MOVZ will do
1605       movz(dst, 0);
1606     } else if (neg_count == 4) {
1607       // one MOVN will do
1608       movn(dst, 0);
1609     } else if (zero_count == 3) {
1610       for (i = 0; i &lt; 4; i++) {
1611         if (imm_h[i] != 0L) {
1612           movz(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));
1613           break;
1614         }
1615       }
1616     } else if (neg_count == 3) {
1617       // one MOVN will do
1618       for (int i = 0; i &lt; 4; i++) {
1619         if (imm_h[i] != 0xffffL) {
1620           movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));
1621           break;
1622         }
1623       }
1624     } else if (zero_count == 2) {
1625       // one MOVZ and one MOVK will do
1626       for (i = 0; i &lt; 3; i++) {
1627         if (imm_h[i] != 0L) {
1628           movz(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));
1629           i++;
1630           break;
1631         }
1632       }
1633       for (;i &lt; 4; i++) {
1634         if (imm_h[i] != 0L) {
1635           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));
1636         }
1637       }
1638     } else if (neg_count == 2) {
1639       // one MOVN and one MOVK will do
1640       for (i = 0; i &lt; 4; i++) {
1641         if (imm_h[i] != 0xffffL) {
1642           movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));
1643           i++;
1644           break;
1645         }
1646       }
1647       for (;i &lt; 4; i++) {
1648         if (imm_h[i] != 0xffffL) {
1649           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));
1650         }
1651       }
1652     } else if (zero_count == 1) {
1653       // one MOVZ and two MOVKs will do
1654       for (i = 0; i &lt; 4; i++) {
1655         if (imm_h[i] != 0L) {
1656           movz(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));
1657           i++;
1658           break;
1659         }
1660       }
1661       for (;i &lt; 4; i++) {
1662         if (imm_h[i] != 0x0L) {
1663           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));
1664         }
1665       }
1666     } else if (neg_count == 1) {
1667       // one MOVN and two MOVKs will do
1668       for (i = 0; i &lt; 4; i++) {
1669         if (imm_h[i] != 0xffffL) {
1670           movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));
1671           i++;
1672           break;
1673         }
1674       }
1675       for (;i &lt; 4; i++) {
1676         if (imm_h[i] != 0xffffL) {
1677           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));
1678         }
1679       }
1680     } else {
1681       // use a MOVZ and 3 MOVKs (makes it easier to debug)
1682       movz(dst, (uint32_t)imm_h[0], 0);
1683       for (i = 1; i &lt; 4; i++) {
1684         movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));
1685       }
1686     }
1687   }
1688 }
1689 
1690 void MacroAssembler::mov_immediate32(Register dst, uint32_t imm32)
1691 {
1692 #ifndef PRODUCT
1693     {
1694       char buffer[64];
1695       snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX32, imm32);
1696       block_comment(buffer);
1697     }
1698 #endif
1699   if (operand_valid_for_logical_immediate(true, imm32)) {
1700     orrw(dst, zr, imm32);
1701   } else {
1702     // we can use MOVZ, MOVN or two calls to MOVK to build up the
1703     // constant
1704     uint32_t imm_h[2];
1705     imm_h[0] = imm32 &amp; 0xffff;
1706     imm_h[1] = ((imm32 &gt;&gt; 16) &amp; 0xffff);
1707     if (imm_h[0] == 0) {
1708       movzw(dst, imm_h[1], 16);
1709     } else if (imm_h[0] == 0xffff) {
1710       movnw(dst, imm_h[1] ^ 0xffff, 16);
1711     } else if (imm_h[1] == 0) {
1712       movzw(dst, imm_h[0], 0);
1713     } else if (imm_h[1] == 0xffff) {
1714       movnw(dst, imm_h[0] ^ 0xffff, 0);
1715     } else {
1716       // use a MOVZ and MOVK (makes it easier to debug)
1717       movzw(dst, imm_h[0], 0);
1718       movkw(dst, imm_h[1], 16);
1719     }
1720   }
1721 }
1722 
1723 // Form an address from base + offset in Rd.  Rd may or may
1724 // not actually be used: you must use the Address that is returned.
1725 // It is up to you to ensure that the shift provided matches the size
1726 // of your data.
1727 Address MacroAssembler::form_address(Register Rd, Register base, int64_t byte_offset, int shift) {
1728   if (Address::offset_ok_for_immed(byte_offset, shift))
1729     // It fits; no need for any heroics
1730     return Address(base, byte_offset);
1731 
1732   // Don&#39;t do anything clever with negative or misaligned offsets
1733   unsigned mask = (1 &lt;&lt; shift) - 1;
1734   if (byte_offset &lt; 0 || byte_offset &amp; mask) {
1735     mov(Rd, byte_offset);
1736     add(Rd, base, Rd);
1737     return Address(Rd);
1738   }
1739 
1740   // See if we can do this with two 12-bit offsets
1741   {
1742     uint64_t word_offset = byte_offset &gt;&gt; shift;
1743     uint64_t masked_offset = word_offset &amp; 0xfff000;
1744     if (Address::offset_ok_for_immed(word_offset - masked_offset, 0)
1745         &amp;&amp; Assembler::operand_valid_for_add_sub_immediate(masked_offset &lt;&lt; shift)) {
1746       add(Rd, base, masked_offset &lt;&lt; shift);
1747       word_offset -= masked_offset;
1748       return Address(Rd, word_offset &lt;&lt; shift);
1749     }
1750   }
1751 
1752   // Do it the hard way
1753   mov(Rd, byte_offset);
1754   add(Rd, base, Rd);
1755   return Address(Rd);
1756 }
1757 
1758 void MacroAssembler::atomic_incw(Register counter_addr, Register tmp, Register tmp2) {
1759   if (UseLSE) {
1760     mov(tmp, 1);
1761     ldadd(Assembler::word, tmp, zr, counter_addr);
1762     return;
1763   }
1764   Label retry_load;
1765   if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))
1766     prfm(Address(counter_addr), PSTL1STRM);
1767   bind(retry_load);
1768   // flush and load exclusive from the memory location
1769   ldxrw(tmp, counter_addr);
1770   addw(tmp, tmp, 1);
1771   // if we store+flush with no intervening write tmp wil be zero
1772   stxrw(tmp2, tmp, counter_addr);
1773   cbnzw(tmp2, retry_load);
1774 }
1775 
1776 
1777 int MacroAssembler::corrected_idivl(Register result, Register ra, Register rb,
1778                                     bool want_remainder, Register scratch)
1779 {
1780   // Full implementation of Java idiv and irem.  The function
1781   // returns the (pc) offset of the div instruction - may be needed
1782   // for implicit exceptions.
1783   //
1784   // constraint : ra/rb =/= scratch
1785   //         normal case
1786   //
1787   // input : ra: dividend
1788   //         rb: divisor
1789   //
1790   // result: either
1791   //         quotient  (= ra idiv rb)
1792   //         remainder (= ra irem rb)
1793 
1794   assert(ra != scratch &amp;&amp; rb != scratch, &quot;reg cannot be scratch&quot;);
1795 
1796   int idivl_offset = offset();
1797   if (! want_remainder) {
1798     sdivw(result, ra, rb);
1799   } else {
1800     sdivw(scratch, ra, rb);
1801     Assembler::msubw(result, scratch, rb, ra);
1802   }
1803 
1804   return idivl_offset;
1805 }
1806 
1807 int MacroAssembler::corrected_idivq(Register result, Register ra, Register rb,
1808                                     bool want_remainder, Register scratch)
1809 {
1810   // Full implementation of Java ldiv and lrem.  The function
1811   // returns the (pc) offset of the div instruction - may be needed
1812   // for implicit exceptions.
1813   //
1814   // constraint : ra/rb =/= scratch
1815   //         normal case
1816   //
1817   // input : ra: dividend
1818   //         rb: divisor
1819   //
1820   // result: either
1821   //         quotient  (= ra idiv rb)
1822   //         remainder (= ra irem rb)
1823 
1824   assert(ra != scratch &amp;&amp; rb != scratch, &quot;reg cannot be scratch&quot;);
1825 
1826   int idivq_offset = offset();
1827   if (! want_remainder) {
1828     sdiv(result, ra, rb);
1829   } else {
1830     sdiv(scratch, ra, rb);
1831     Assembler::msub(result, scratch, rb, ra);
1832   }
1833 
1834   return idivq_offset;
1835 }
1836 
1837 void MacroAssembler::membar(Membar_mask_bits order_constraint) {
1838   address prev = pc() - NativeMembar::instruction_size;
1839   address last = code()-&gt;last_insn();
1840   if (last != NULL &amp;&amp; nativeInstruction_at(last)-&gt;is_Membar() &amp;&amp; prev == last) {
1841     NativeMembar *bar = NativeMembar_at(prev);
1842     // We are merging two memory barrier instructions.  On AArch64 we
1843     // can do this simply by ORing them together.
1844     bar-&gt;set_kind(bar-&gt;get_kind() | order_constraint);
1845     BLOCK_COMMENT(&quot;merged membar&quot;);
1846   } else {
1847     code()-&gt;set_last_insn(pc());
1848     dmb(Assembler::barrier(order_constraint));
1849   }
1850 }
1851 
1852 bool MacroAssembler::try_merge_ldst(Register rt, const Address &amp;adr, size_t size_in_bytes, bool is_store) {
1853   if (ldst_can_merge(rt, adr, size_in_bytes, is_store)) {
1854     merge_ldst(rt, adr, size_in_bytes, is_store);
1855     code()-&gt;clear_last_insn();
1856     return true;
1857   } else {
1858     assert(size_in_bytes == 8 || size_in_bytes == 4, &quot;only 8 bytes or 4 bytes load/store is supported.&quot;);
1859     const unsigned mask = size_in_bytes - 1;
1860     if (adr.getMode() == Address::base_plus_offset &amp;&amp;
1861         (adr.offset() &amp; mask) == 0) { // only supports base_plus_offset.
1862       code()-&gt;set_last_insn(pc());
1863     }
1864     return false;
1865   }
1866 }
1867 
1868 void MacroAssembler::ldr(Register Rx, const Address &amp;adr) {
1869   // We always try to merge two adjacent loads into one ldp.
1870   if (!try_merge_ldst(Rx, adr, 8, false)) {
1871     Assembler::ldr(Rx, adr);
1872   }
1873 }
1874 
1875 void MacroAssembler::ldrw(Register Rw, const Address &amp;adr) {
1876   // We always try to merge two adjacent loads into one ldp.
1877   if (!try_merge_ldst(Rw, adr, 4, false)) {
1878     Assembler::ldrw(Rw, adr);
1879   }
1880 }
1881 
1882 void MacroAssembler::str(Register Rx, const Address &amp;adr) {
1883   // We always try to merge two adjacent stores into one stp.
1884   if (!try_merge_ldst(Rx, adr, 8, true)) {
1885     Assembler::str(Rx, adr);
1886   }
1887 }
1888 
1889 void MacroAssembler::strw(Register Rw, const Address &amp;adr) {
1890   // We always try to merge two adjacent stores into one stp.
1891   if (!try_merge_ldst(Rw, adr, 4, true)) {
1892     Assembler::strw(Rw, adr);
1893   }
1894 }
1895 
1896 // MacroAssembler routines found actually to be needed
1897 
1898 void MacroAssembler::push(Register src)
1899 {
1900   str(src, Address(pre(esp, -1 * wordSize)));
1901 }
1902 
1903 void MacroAssembler::pop(Register dst)
1904 {
1905   ldr(dst, Address(post(esp, 1 * wordSize)));
1906 }
1907 
1908 // Note: load_unsigned_short used to be called load_unsigned_word.
1909 int MacroAssembler::load_unsigned_short(Register dst, Address src) {
1910   int off = offset();
1911   ldrh(dst, src);
1912   return off;
1913 }
1914 
1915 int MacroAssembler::load_unsigned_byte(Register dst, Address src) {
1916   int off = offset();
1917   ldrb(dst, src);
1918   return off;
1919 }
1920 
1921 int MacroAssembler::load_signed_short(Register dst, Address src) {
1922   int off = offset();
1923   ldrsh(dst, src);
1924   return off;
1925 }
1926 
1927 int MacroAssembler::load_signed_byte(Register dst, Address src) {
1928   int off = offset();
1929   ldrsb(dst, src);
1930   return off;
1931 }
1932 
1933 int MacroAssembler::load_signed_short32(Register dst, Address src) {
1934   int off = offset();
1935   ldrshw(dst, src);
1936   return off;
1937 }
1938 
1939 int MacroAssembler::load_signed_byte32(Register dst, Address src) {
1940   int off = offset();
1941   ldrsbw(dst, src);
1942   return off;
1943 }
1944 
1945 void MacroAssembler::load_sized_value(Register dst, Address src, size_t size_in_bytes, bool is_signed, Register dst2) {
1946   switch (size_in_bytes) {
1947   case  8:  ldr(dst, src); break;
1948   case  4:  ldrw(dst, src); break;
1949   case  2:  is_signed ? load_signed_short(dst, src) : load_unsigned_short(dst, src); break;
1950   case  1:  is_signed ? load_signed_byte( dst, src) : load_unsigned_byte( dst, src); break;
1951   default:  ShouldNotReachHere();
1952   }
1953 }
1954 
1955 void MacroAssembler::store_sized_value(Address dst, Register src, size_t size_in_bytes, Register src2) {
1956   switch (size_in_bytes) {
1957   case  8:  str(src, dst); break;
1958   case  4:  strw(src, dst); break;
1959   case  2:  strh(src, dst); break;
1960   case  1:  strb(src, dst); break;
1961   default:  ShouldNotReachHere();
1962   }
1963 }
1964 
1965 void MacroAssembler::decrementw(Register reg, int value)
1966 {
1967   if (value &lt; 0)  { incrementw(reg, -value);      return; }
1968   if (value == 0) {                               return; }
1969   if (value &lt; (1 &lt;&lt; 12)) { subw(reg, reg, value); return; }
1970   /* else */ {
1971     guarantee(reg != rscratch2, &quot;invalid dst for register decrement&quot;);
1972     movw(rscratch2, (unsigned)value);
1973     subw(reg, reg, rscratch2);
1974   }
1975 }
1976 
1977 void MacroAssembler::decrement(Register reg, int value)
1978 {
1979   if (value &lt; 0)  { increment(reg, -value);      return; }
1980   if (value == 0) {                              return; }
1981   if (value &lt; (1 &lt;&lt; 12)) { sub(reg, reg, value); return; }
1982   /* else */ {
1983     assert(reg != rscratch2, &quot;invalid dst for register decrement&quot;);
1984     mov(rscratch2, (uint64_t) value);
1985     sub(reg, reg, rscratch2);
1986   }
1987 }
1988 
1989 void MacroAssembler::decrementw(Address dst, int value)
1990 {
1991   assert(!dst.uses(rscratch1), &quot;invalid dst for address decrement&quot;);
1992   if (dst.getMode() == Address::literal) {
1993     assert(abs(value) &lt; (1 &lt;&lt; 12), &quot;invalid value and address mode combination&quot;);
1994     lea(rscratch2, dst);
1995     dst = Address(rscratch2);
1996   }
1997   ldrw(rscratch1, dst);
1998   decrementw(rscratch1, value);
1999   strw(rscratch1, dst);
2000 }
2001 
2002 void MacroAssembler::decrement(Address dst, int value)
2003 {
2004   assert(!dst.uses(rscratch1), &quot;invalid address for decrement&quot;);
2005   if (dst.getMode() == Address::literal) {
2006     assert(abs(value) &lt; (1 &lt;&lt; 12), &quot;invalid value and address mode combination&quot;);
2007     lea(rscratch2, dst);
2008     dst = Address(rscratch2);
2009   }
2010   ldr(rscratch1, dst);
2011   decrement(rscratch1, value);
2012   str(rscratch1, dst);
2013 }
2014 
2015 void MacroAssembler::incrementw(Register reg, int value)
2016 {
2017   if (value &lt; 0)  { decrementw(reg, -value);      return; }
2018   if (value == 0) {                               return; }
2019   if (value &lt; (1 &lt;&lt; 12)) { addw(reg, reg, value); return; }
2020   /* else */ {
2021     assert(reg != rscratch2, &quot;invalid dst for register increment&quot;);
2022     movw(rscratch2, (unsigned)value);
2023     addw(reg, reg, rscratch2);
2024   }
2025 }
2026 
2027 void MacroAssembler::increment(Register reg, int value)
2028 {
2029   if (value &lt; 0)  { decrement(reg, -value);      return; }
2030   if (value == 0) {                              return; }
2031   if (value &lt; (1 &lt;&lt; 12)) { add(reg, reg, value); return; }
2032   /* else */ {
2033     assert(reg != rscratch2, &quot;invalid dst for register increment&quot;);
2034     movw(rscratch2, (unsigned)value);
2035     add(reg, reg, rscratch2);
2036   }
2037 }
2038 
2039 void MacroAssembler::incrementw(Address dst, int value)
2040 {
2041   assert(!dst.uses(rscratch1), &quot;invalid dst for address increment&quot;);
2042   if (dst.getMode() == Address::literal) {
2043     assert(abs(value) &lt; (1 &lt;&lt; 12), &quot;invalid value and address mode combination&quot;);
2044     lea(rscratch2, dst);
2045     dst = Address(rscratch2);
2046   }
2047   ldrw(rscratch1, dst);
2048   incrementw(rscratch1, value);
2049   strw(rscratch1, dst);
2050 }
2051 
2052 void MacroAssembler::increment(Address dst, int value)
2053 {
2054   assert(!dst.uses(rscratch1), &quot;invalid dst for address increment&quot;);
2055   if (dst.getMode() == Address::literal) {
2056     assert(abs(value) &lt; (1 &lt;&lt; 12), &quot;invalid value and address mode combination&quot;);
2057     lea(rscratch2, dst);
2058     dst = Address(rscratch2);
2059   }
2060   ldr(rscratch1, dst);
2061   increment(rscratch1, value);
2062   str(rscratch1, dst);
2063 }
2064 
2065 
2066 void MacroAssembler::pusha() {
2067   push(0x7fffffff, sp);
2068 }
2069 
2070 void MacroAssembler::popa() {
2071   pop(0x7fffffff, sp);
2072 }
2073 
2074 // Push lots of registers in the bit set supplied.  Don&#39;t push sp.
2075 // Return the number of words pushed
2076 int MacroAssembler::push(unsigned int bitset, Register stack) {
2077   int words_pushed = 0;
2078 
2079   // Scan bitset to accumulate register pairs
2080   unsigned char regs[32];
2081   int count = 0;
2082   for (int reg = 0; reg &lt;= 30; reg++) {
2083     if (1 &amp; bitset)
2084       regs[count++] = reg;
2085     bitset &gt;&gt;= 1;
2086   }
2087   regs[count++] = zr-&gt;encoding_nocheck();
2088   count &amp;= ~1;  // Only push an even nuber of regs
2089 
2090   if (count) {
2091     stp(as_Register(regs[0]), as_Register(regs[1]),
2092        Address(pre(stack, -count * wordSize)));
2093     words_pushed += 2;
2094   }
2095   for (int i = 2; i &lt; count; i += 2) {
2096     stp(as_Register(regs[i]), as_Register(regs[i+1]),
2097        Address(stack, i * wordSize));
2098     words_pushed += 2;
2099   }
2100 
2101   assert(words_pushed == count, &quot;oops, pushed != count&quot;);
2102 
2103   return count;
2104 }
2105 
2106 int MacroAssembler::pop(unsigned int bitset, Register stack) {
2107   int words_pushed = 0;
2108 
2109   // Scan bitset to accumulate register pairs
2110   unsigned char regs[32];
2111   int count = 0;
2112   for (int reg = 0; reg &lt;= 30; reg++) {
2113     if (1 &amp; bitset)
2114       regs[count++] = reg;
2115     bitset &gt;&gt;= 1;
2116   }
2117   regs[count++] = zr-&gt;encoding_nocheck();
2118   count &amp;= ~1;
2119 
2120   for (int i = 2; i &lt; count; i += 2) {
2121     ldp(as_Register(regs[i]), as_Register(regs[i+1]),
2122        Address(stack, i * wordSize));
2123     words_pushed += 2;
2124   }
2125   if (count) {
2126     ldp(as_Register(regs[0]), as_Register(regs[1]),
2127        Address(post(stack, count * wordSize)));
2128     words_pushed += 2;
2129   }
2130 
2131   assert(words_pushed == count, &quot;oops, pushed != count&quot;);
2132 
2133   return count;
2134 }
2135 
2136 // Push lots of registers in the bit set supplied.  Don&#39;t push sp.
2137 // Return the number of words pushed
2138 int MacroAssembler::push_fp(unsigned int bitset, Register stack) {
2139   int words_pushed = 0;
2140 
2141   // Scan bitset to accumulate register pairs
2142   unsigned char regs[32];
2143   int count = 0;
2144   for (int reg = 0; reg &lt;= 31; reg++) {
2145     if (1 &amp; bitset)
2146       regs[count++] = reg;
2147     bitset &gt;&gt;= 1;
2148   }
2149   regs[count++] = zr-&gt;encoding_nocheck();
2150   count &amp;= ~1;  // Only push an even number of regs
2151 
2152   // Always pushing full 128 bit registers.
2153   if (count) {
2154     stpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(pre(stack, -count * wordSize * 2)));
2155     words_pushed += 2;
2156   }
2157   for (int i = 2; i &lt; count; i += 2) {
2158     stpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));
2159     words_pushed += 2;
2160   }
2161 
2162   assert(words_pushed == count, &quot;oops, pushed != count&quot;);
2163   return count;
2164 }
2165 
2166 int MacroAssembler::pop_fp(unsigned int bitset, Register stack) {
2167   int words_pushed = 0;
2168 
2169   // Scan bitset to accumulate register pairs
2170   unsigned char regs[32];
2171   int count = 0;
2172   for (int reg = 0; reg &lt;= 31; reg++) {
2173     if (1 &amp; bitset)
2174       regs[count++] = reg;
2175     bitset &gt;&gt;= 1;
2176   }
2177   regs[count++] = zr-&gt;encoding_nocheck();
2178   count &amp;= ~1;
2179 
2180   for (int i = 2; i &lt; count; i += 2) {
2181     ldpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));
2182     words_pushed += 2;
2183   }
2184   if (count) {
2185     ldpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(post(stack, count * wordSize * 2)));
2186     words_pushed += 2;
2187   }
2188 
2189   assert(words_pushed == count, &quot;oops, pushed != count&quot;);
2190 
2191   return count;
2192 }
2193 
2194 #ifdef ASSERT
2195 void MacroAssembler::verify_heapbase(const char* msg) {
2196 #if 0
2197   assert (UseCompressedOops || UseCompressedClassPointers, &quot;should be compressed&quot;);
2198   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
2199   if (CheckCompressedOops) {
2200     Label ok;
2201     push(1 &lt;&lt; rscratch1-&gt;encoding(), sp); // cmpptr trashes rscratch1
2202     cmpptr(rheapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));
2203     br(Assembler::EQ, ok);
2204     stop(msg);
2205     bind(ok);
2206     pop(1 &lt;&lt; rscratch1-&gt;encoding(), sp);
2207   }
2208 #endif
2209 }
2210 #endif
2211 
2212 void MacroAssembler::resolve_jobject(Register value, Register thread, Register tmp) {
2213   Label done, not_weak;
2214   cbz(value, done);           // Use NULL as-is.
2215 
2216   STATIC_ASSERT(JNIHandles::weak_tag_mask == 1u);
2217   tbz(r0, 0, not_weak);    // Test for jweak tag.
2218 
2219   // Resolve jweak.
2220   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF, value,
2221                  Address(value, -JNIHandles::weak_tag_value), tmp, thread);
2222   verify_oop(value);
2223   b(done);
2224 
2225   bind(not_weak);
2226   // Resolve (untagged) jobject.
2227   access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, 0), tmp, thread);
2228   verify_oop(value);
2229   bind(done);
2230 }
2231 
2232 void MacroAssembler::stop(const char* msg) {
2233   address ip = pc();
2234   pusha();
2235   mov(c_rarg0, (address)msg);
2236   mov(c_rarg1, (address)ip);
2237   mov(c_rarg2, sp);
2238   mov(c_rarg3, CAST_FROM_FN_PTR(address, MacroAssembler::debug64));
2239   blr(c_rarg3);
2240   hlt(0);
2241 }
2242 
2243 void MacroAssembler::warn(const char* msg) {
2244   pusha();
2245   mov(c_rarg0, (address)msg);
2246   mov(lr, CAST_FROM_FN_PTR(address, warning));
2247   blr(lr);
2248   popa();
2249 }
2250 
2251 void MacroAssembler::unimplemented(const char* what) {
2252   const char* buf = NULL;
2253   {
2254     ResourceMark rm;
2255     stringStream ss;
2256     ss.print(&quot;unimplemented: %s&quot;, what);
2257     buf = code_string(ss.as_string());
2258   }
2259   stop(buf);
2260 }
2261 
2262 // If a constant does not fit in an immediate field, generate some
2263 // number of MOV instructions and then perform the operation.
2264 void MacroAssembler::wrap_add_sub_imm_insn(Register Rd, Register Rn, unsigned imm,
2265                                            add_sub_imm_insn insn1,
2266                                            add_sub_reg_insn insn2) {
2267   assert(Rd != zr, &quot;Rd = zr and not setting flags?&quot;);
2268   if (operand_valid_for_add_sub_immediate((int)imm)) {
2269     (this-&gt;*insn1)(Rd, Rn, imm);
2270   } else {
2271     if (uabs(imm) &lt; (1 &lt;&lt; 24)) {
2272        (this-&gt;*insn1)(Rd, Rn, imm &amp; -(1 &lt;&lt; 12));
2273        (this-&gt;*insn1)(Rd, Rd, imm &amp; ((1 &lt;&lt; 12)-1));
2274     } else {
2275        assert_different_registers(Rd, Rn);
2276        mov(Rd, (uint64_t)imm);
2277        (this-&gt;*insn2)(Rd, Rn, Rd, LSL, 0);
2278     }
2279   }
2280 }
2281 
2282 // Seperate vsn which sets the flags. Optimisations are more restricted
2283 // because we must set the flags correctly.
2284 void MacroAssembler::wrap_adds_subs_imm_insn(Register Rd, Register Rn, unsigned imm,
2285                                            add_sub_imm_insn insn1,
2286                                            add_sub_reg_insn insn2) {
2287   if (operand_valid_for_add_sub_immediate((int)imm)) {
2288     (this-&gt;*insn1)(Rd, Rn, imm);
2289   } else {
2290     assert_different_registers(Rd, Rn);
2291     assert(Rd != zr, &quot;overflow in immediate operand&quot;);
2292     mov(Rd, (uint64_t)imm);
2293     (this-&gt;*insn2)(Rd, Rn, Rd, LSL, 0);
2294   }
2295 }
2296 
2297 
2298 void MacroAssembler::add(Register Rd, Register Rn, RegisterOrConstant increment) {
2299   if (increment.is_register()) {
2300     add(Rd, Rn, increment.as_register());
2301   } else {
2302     add(Rd, Rn, increment.as_constant());
2303   }
2304 }
2305 
2306 void MacroAssembler::addw(Register Rd, Register Rn, RegisterOrConstant increment) {
2307   if (increment.is_register()) {
2308     addw(Rd, Rn, increment.as_register());
2309   } else {
2310     addw(Rd, Rn, increment.as_constant());
2311   }
2312 }
2313 
2314 void MacroAssembler::sub(Register Rd, Register Rn, RegisterOrConstant decrement) {
2315   if (decrement.is_register()) {
2316     sub(Rd, Rn, decrement.as_register());
2317   } else {
2318     sub(Rd, Rn, decrement.as_constant());
2319   }
2320 }
2321 
2322 void MacroAssembler::subw(Register Rd, Register Rn, RegisterOrConstant decrement) {
2323   if (decrement.is_register()) {
2324     subw(Rd, Rn, decrement.as_register());
2325   } else {
2326     subw(Rd, Rn, decrement.as_constant());
2327   }
2328 }
2329 
2330 void MacroAssembler::reinit_heapbase()
2331 {
2332   if (UseCompressedOops) {
2333     if (Universe::is_fully_initialized()) {
2334       mov(rheapbase, CompressedOops::ptrs_base());
2335     } else {
2336       lea(rheapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));
2337       ldr(rheapbase, Address(rheapbase));
2338     }
2339   }
2340 }
2341 
2342 // this simulates the behaviour of the x86 cmpxchg instruction using a
2343 // load linked/store conditional pair. we use the acquire/release
2344 // versions of these instructions so that we flush pending writes as
2345 // per Java semantics.
2346 
2347 // n.b the x86 version assumes the old value to be compared against is
2348 // in rax and updates rax with the value located in memory if the
2349 // cmpxchg fails. we supply a register for the old value explicitly
2350 
2351 // the aarch64 load linked/store conditional instructions do not
2352 // accept an offset. so, unlike x86, we must provide a plain register
2353 // to identify the memory word to be compared/exchanged rather than a
2354 // register+offset Address.
2355 
2356 void MacroAssembler::cmpxchgptr(Register oldv, Register newv, Register addr, Register tmp,
2357                                 Label &amp;succeed, Label *fail) {
2358   // oldv holds comparison value
2359   // newv holds value to write in exchange
2360   // addr identifies memory word to compare against/update
2361   if (UseLSE) {
2362     mov(tmp, oldv);
2363     casal(Assembler::xword, oldv, newv, addr);
2364     cmp(tmp, oldv);
2365     br(Assembler::EQ, succeed);
2366     membar(AnyAny);
2367   } else {
2368     Label retry_load, nope;
2369     if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))
2370       prfm(Address(addr), PSTL1STRM);
2371     bind(retry_load);
2372     // flush and load exclusive from the memory location
2373     // and fail if it is not what we expect
2374     ldaxr(tmp, addr);
2375     cmp(tmp, oldv);
2376     br(Assembler::NE, nope);
2377     // if we store+flush with no intervening write tmp wil be zero
2378     stlxr(tmp, newv, addr);
2379     cbzw(tmp, succeed);
2380     // retry so we only ever return after a load fails to compare
2381     // ensures we don&#39;t return a stale value after a failed write.
2382     b(retry_load);
2383     // if the memory word differs we return it in oldv and signal a fail
2384     bind(nope);
2385     membar(AnyAny);
2386     mov(oldv, tmp);
2387   }
2388   if (fail)
2389     b(*fail);
2390 }
2391 
2392 void MacroAssembler::cmpxchg_obj_header(Register oldv, Register newv, Register obj, Register tmp,
2393                                         Label &amp;succeed, Label *fail) {
2394   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;assumption&quot;);
2395   cmpxchgptr(oldv, newv, obj, tmp, succeed, fail);
2396 }
2397 
2398 void MacroAssembler::cmpxchgw(Register oldv, Register newv, Register addr, Register tmp,
2399                                 Label &amp;succeed, Label *fail) {
2400   // oldv holds comparison value
2401   // newv holds value to write in exchange
2402   // addr identifies memory word to compare against/update
2403   // tmp returns 0/1 for success/failure
2404   if (UseLSE) {
2405     mov(tmp, oldv);
2406     casal(Assembler::word, oldv, newv, addr);
2407     cmp(tmp, oldv);
2408     br(Assembler::EQ, succeed);
2409     membar(AnyAny);
2410   } else {
2411     Label retry_load, nope;
2412     if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))
2413       prfm(Address(addr), PSTL1STRM);
2414     bind(retry_load);
2415     // flush and load exclusive from the memory location
2416     // and fail if it is not what we expect
2417     ldaxrw(tmp, addr);
2418     cmp(tmp, oldv);
2419     br(Assembler::NE, nope);
2420     // if we store+flush with no intervening write tmp wil be zero
2421     stlxrw(tmp, newv, addr);
2422     cbzw(tmp, succeed);
2423     // retry so we only ever return after a load fails to compare
2424     // ensures we don&#39;t return a stale value after a failed write.
2425     b(retry_load);
2426     // if the memory word differs we return it in oldv and signal a fail
2427     bind(nope);
2428     membar(AnyAny);
2429     mov(oldv, tmp);
2430   }
2431   if (fail)
2432     b(*fail);
2433 }
2434 
2435 // A generic CAS; success or failure is in the EQ flag.  A weak CAS
2436 // doesn&#39;t retry and may fail spuriously.  If the oldval is wanted,
2437 // Pass a register for the result, otherwise pass noreg.
2438 
2439 // Clobbers rscratch1
2440 void MacroAssembler::cmpxchg(Register addr, Register expected,
2441                              Register new_val,
2442                              enum operand_size size,
2443                              bool acquire, bool release,
2444                              bool weak,
2445                              Register result) {
2446   if (result == noreg)  result = rscratch1;
2447   BLOCK_COMMENT(&quot;cmpxchg {&quot;);
2448   if (UseLSE) {
2449     mov(result, expected);
2450     lse_cas(result, new_val, addr, size, acquire, release, /*not_pair*/ true);
2451     compare_eq(result, expected, size);
2452   } else {
2453     Label retry_load, done;
2454     if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))
2455       prfm(Address(addr), PSTL1STRM);
2456     bind(retry_load);
2457     load_exclusive(result, addr, size, acquire);
2458     compare_eq(result, expected, size);
2459     br(Assembler::NE, done);
2460     store_exclusive(rscratch1, new_val, addr, size, release);
2461     if (weak) {
2462       cmpw(rscratch1, 0u);  // If the store fails, return NE to our caller.
2463     } else {
2464       cbnzw(rscratch1, retry_load);
2465     }
2466     bind(done);
2467   }
2468   BLOCK_COMMENT(&quot;} cmpxchg&quot;);
2469 }
2470 
2471 // A generic comparison. Only compares for equality, clobbers rscratch1.
2472 void MacroAssembler::compare_eq(Register rm, Register rn, enum operand_size size) {
2473   if (size == xword) {
2474     cmp(rm, rn);
2475   } else if (size == word) {
2476     cmpw(rm, rn);
2477   } else if (size == halfword) {
2478     eorw(rscratch1, rm, rn);
2479     ands(zr, rscratch1, 0xffff);
2480   } else if (size == byte) {
2481     eorw(rscratch1, rm, rn);
2482     ands(zr, rscratch1, 0xff);
2483   } else {
2484     ShouldNotReachHere();
2485   }
2486 }
2487 
2488 
2489 static bool different(Register a, RegisterOrConstant b, Register c) {
2490   if (b.is_constant())
2491     return a != c;
2492   else
2493     return a != b.as_register() &amp;&amp; a != c &amp;&amp; b.as_register() != c;
2494 }
2495 
2496 #define ATOMIC_OP(NAME, LDXR, OP, IOP, AOP, STXR, sz)                   \
2497 void MacroAssembler::atomic_##NAME(Register prev, RegisterOrConstant incr, Register addr) { \
2498   if (UseLSE) {                                                         \
2499     prev = prev-&gt;is_valid() ? prev : zr;                                \
2500     if (incr.is_register()) {                                           \
2501       AOP(sz, incr.as_register(), prev, addr);                          \
2502     } else {                                                            \
2503       mov(rscratch2, (address)incr.as_constant());                      \
2504       AOP(sz, rscratch2, prev, addr);                                   \
2505     }                                                                   \
2506     return;                                                             \
2507   }                                                                     \
2508   Register result = rscratch2;                                          \
2509   if (prev-&gt;is_valid())                                                 \
2510     result = different(prev, incr, addr) ? prev : rscratch2;            \
2511                                                                         \
2512   Label retry_load;                                                     \
2513   if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))         \
2514     prfm(Address(addr), PSTL1STRM);                                     \
2515   bind(retry_load);                                                     \
2516   LDXR(result, addr);                                                   \
2517   OP(rscratch1, result, incr);                                          \
2518   STXR(rscratch2, rscratch1, addr);                                     \
2519   cbnzw(rscratch2, retry_load);                                         \
2520   if (prev-&gt;is_valid() &amp;&amp; prev != result) {                             \
2521     IOP(prev, rscratch1, incr);                                         \
2522   }                                                                     \
2523 }
2524 
2525 ATOMIC_OP(add, ldxr, add, sub, ldadd, stxr, Assembler::xword)
2526 ATOMIC_OP(addw, ldxrw, addw, subw, ldadd, stxrw, Assembler::word)
2527 ATOMIC_OP(addal, ldaxr, add, sub, ldaddal, stlxr, Assembler::xword)
2528 ATOMIC_OP(addalw, ldaxrw, addw, subw, ldaddal, stlxrw, Assembler::word)
2529 
2530 #undef ATOMIC_OP
2531 
2532 #define ATOMIC_XCHG(OP, AOP, LDXR, STXR, sz)                            \
2533 void MacroAssembler::atomic_##OP(Register prev, Register newv, Register addr) { \
2534   if (UseLSE) {                                                         \
2535     prev = prev-&gt;is_valid() ? prev : zr;                                \
2536     AOP(sz, newv, prev, addr);                                          \
2537     return;                                                             \
2538   }                                                                     \
2539   Register result = rscratch2;                                          \
2540   if (prev-&gt;is_valid())                                                 \
2541     result = different(prev, newv, addr) ? prev : rscratch2;            \
2542                                                                         \
2543   Label retry_load;                                                     \
2544   if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))         \
2545     prfm(Address(addr), PSTL1STRM);                                     \
2546   bind(retry_load);                                                     \
2547   LDXR(result, addr);                                                   \
2548   STXR(rscratch1, newv, addr);                                          \
2549   cbnzw(rscratch1, retry_load);                                         \
2550   if (prev-&gt;is_valid() &amp;&amp; prev != result)                               \
2551     mov(prev, result);                                                  \
2552 }
2553 
2554 ATOMIC_XCHG(xchg, swp, ldxr, stxr, Assembler::xword)
2555 ATOMIC_XCHG(xchgw, swp, ldxrw, stxrw, Assembler::word)
2556 ATOMIC_XCHG(xchgal, swpal, ldaxr, stlxr, Assembler::xword)
2557 ATOMIC_XCHG(xchgalw, swpal, ldaxrw, stlxrw, Assembler::word)
2558 
2559 #undef ATOMIC_XCHG
2560 
2561 #ifndef PRODUCT
2562 extern &quot;C&quot; void findpc(intptr_t x);
2563 #endif
2564 
2565 void MacroAssembler::debug64(char* msg, int64_t pc, int64_t regs[])
2566 {
2567   // In order to get locks to work, we need to fake a in_VM state
2568   if (ShowMessageBoxOnError ) {
2569     JavaThread* thread = JavaThread::current();
2570     JavaThreadState saved_state = thread-&gt;thread_state();
2571     thread-&gt;set_thread_state(_thread_in_vm);
2572 #ifndef PRODUCT
2573     if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {
2574       ttyLocker ttyl;
2575       BytecodeCounter::print();
2576     }
2577 #endif
2578     if (os::message_box(msg, &quot;Execution stopped, print registers?&quot;)) {
2579       ttyLocker ttyl;
2580       tty-&gt;print_cr(&quot; pc = 0x%016lx&quot;, pc);
2581 #ifndef PRODUCT
2582       tty-&gt;cr();
2583       findpc(pc);
2584       tty-&gt;cr();
2585 #endif
2586       tty-&gt;print_cr(&quot; r0 = 0x%016lx&quot;, regs[0]);
2587       tty-&gt;print_cr(&quot; r1 = 0x%016lx&quot;, regs[1]);
2588       tty-&gt;print_cr(&quot; r2 = 0x%016lx&quot;, regs[2]);
2589       tty-&gt;print_cr(&quot; r3 = 0x%016lx&quot;, regs[3]);
2590       tty-&gt;print_cr(&quot; r4 = 0x%016lx&quot;, regs[4]);
2591       tty-&gt;print_cr(&quot; r5 = 0x%016lx&quot;, regs[5]);
2592       tty-&gt;print_cr(&quot; r6 = 0x%016lx&quot;, regs[6]);
2593       tty-&gt;print_cr(&quot; r7 = 0x%016lx&quot;, regs[7]);
2594       tty-&gt;print_cr(&quot; r8 = 0x%016lx&quot;, regs[8]);
2595       tty-&gt;print_cr(&quot; r9 = 0x%016lx&quot;, regs[9]);
2596       tty-&gt;print_cr(&quot;r10 = 0x%016lx&quot;, regs[10]);
2597       tty-&gt;print_cr(&quot;r11 = 0x%016lx&quot;, regs[11]);
2598       tty-&gt;print_cr(&quot;r12 = 0x%016lx&quot;, regs[12]);
2599       tty-&gt;print_cr(&quot;r13 = 0x%016lx&quot;, regs[13]);
2600       tty-&gt;print_cr(&quot;r14 = 0x%016lx&quot;, regs[14]);
2601       tty-&gt;print_cr(&quot;r15 = 0x%016lx&quot;, regs[15]);
2602       tty-&gt;print_cr(&quot;r16 = 0x%016lx&quot;, regs[16]);
2603       tty-&gt;print_cr(&quot;r17 = 0x%016lx&quot;, regs[17]);
2604       tty-&gt;print_cr(&quot;r18 = 0x%016lx&quot;, regs[18]);
2605       tty-&gt;print_cr(&quot;r19 = 0x%016lx&quot;, regs[19]);
2606       tty-&gt;print_cr(&quot;r20 = 0x%016lx&quot;, regs[20]);
2607       tty-&gt;print_cr(&quot;r21 = 0x%016lx&quot;, regs[21]);
2608       tty-&gt;print_cr(&quot;r22 = 0x%016lx&quot;, regs[22]);
2609       tty-&gt;print_cr(&quot;r23 = 0x%016lx&quot;, regs[23]);
2610       tty-&gt;print_cr(&quot;r24 = 0x%016lx&quot;, regs[24]);
2611       tty-&gt;print_cr(&quot;r25 = 0x%016lx&quot;, regs[25]);
2612       tty-&gt;print_cr(&quot;r26 = 0x%016lx&quot;, regs[26]);
2613       tty-&gt;print_cr(&quot;r27 = 0x%016lx&quot;, regs[27]);
2614       tty-&gt;print_cr(&quot;r28 = 0x%016lx&quot;, regs[28]);
2615       tty-&gt;print_cr(&quot;r30 = 0x%016lx&quot;, regs[30]);
2616       tty-&gt;print_cr(&quot;r31 = 0x%016lx&quot;, regs[31]);
2617       BREAKPOINT;
2618     }
2619   }
2620   fatal(&quot;DEBUG MESSAGE: %s&quot;, msg);
2621 }
2622 
2623 void MacroAssembler::push_call_clobbered_registers() {
2624   int step = 4 * wordSize;
2625   push(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2) WIN64_ONLY(- r18), sp);
2626   sub(sp, sp, step);
2627   mov(rscratch1, -step);
2628   // Push v0-v7, v16-v31.
2629   for (int i = 31; i&gt;= 4; i -= 4) {
2630     if (i &lt;= v7-&gt;encoding() || i &gt;= v16-&gt;encoding())
2631       st1(as_FloatRegister(i-3), as_FloatRegister(i-2), as_FloatRegister(i-1),
2632           as_FloatRegister(i), T1D, Address(post(sp, rscratch1)));
2633   }
2634   st1(as_FloatRegister(0), as_FloatRegister(1), as_FloatRegister(2),
2635       as_FloatRegister(3), T1D, Address(sp));
2636 }
2637 
2638 void MacroAssembler::pop_call_clobbered_registers() {
2639   for (int i = 0; i &lt; 32; i += 4) {
2640     if (i &lt;= v7-&gt;encoding() || i &gt;= v16-&gt;encoding())
2641       ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2642           as_FloatRegister(i+3), T1D, Address(post(sp, 4 * wordSize)));
2643   }
2644   pop(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2) WIN64_ONLY(- r18), sp);
2645 }
2646 
2647 void MacroAssembler::push_CPU_state(bool save_vectors) {
2648   int step = (save_vectors ? 8 : 4) * wordSize;
2649   push(0x3fffffff, sp);         // integer registers except lr &amp; sp
2650   mov(rscratch1, -step);
2651   sub(sp, sp, step);
2652   for (int i = 28; i &gt;= 4; i -= 4) {
2653     st1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2654         as_FloatRegister(i+3), save_vectors ? T2D : T1D, Address(post(sp, rscratch1)));
2655   }
2656   st1(v0, v1, v2, v3, save_vectors ? T2D : T1D, sp);
2657 }
2658 
2659 void MacroAssembler::pop_CPU_state(bool restore_vectors) {
2660   int step = (restore_vectors ? 8 : 4) * wordSize;
2661   for (int i = 0; i &lt;= 28; i += 4)
2662     ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2663         as_FloatRegister(i+3), restore_vectors ? T2D : T1D, Address(post(sp, step)));
2664   pop(0x3fffffff, sp);         // integer registers except lr &amp; sp
2665 }
2666 
2667 /**
2668  * Helpers for multiply_to_len().
2669  */
2670 void MacroAssembler::add2_with_carry(Register final_dest_hi, Register dest_hi, Register dest_lo,
2671                                      Register src1, Register src2) {
2672   adds(dest_lo, dest_lo, src1);
2673   adc(dest_hi, dest_hi, zr);
2674   adds(dest_lo, dest_lo, src2);
2675   adc(final_dest_hi, dest_hi, zr);
2676 }
2677 
2678 // Generate an address from (r + r1 extend offset).  &quot;size&quot; is the
2679 // size of the operand.  The result may be in rscratch2.
2680 Address MacroAssembler::offsetted_address(Register r, Register r1,
2681                                           Address::extend ext, int offset, int size) {
2682   if (offset || (ext.shift() % size != 0)) {
2683     lea(rscratch2, Address(r, r1, ext));
2684     return Address(rscratch2, offset);
2685   } else {
2686     return Address(r, r1, ext);
2687   }
2688 }
2689 
2690 Address MacroAssembler::spill_address(int size, int offset, Register tmp)
2691 {
2692   assert(offset &gt;= 0, &quot;spill to negative address?&quot;);
2693   // Offset reachable ?
2694   //   Not aligned - 9 bits signed offset
2695   //   Aligned - 12 bits unsigned offset shifted
2696   Register base = sp;
2697   if ((offset &amp; (size-1)) &amp;&amp; offset &gt;= (1&lt;&lt;8)) {
2698     add(tmp, base, offset &amp; ((1&lt;&lt;12)-1));
2699     base = tmp;
2700     offset &amp;= -1u&lt;&lt;12;
2701   }
2702 
2703   if (offset &gt;= (1&lt;&lt;12) * size) {
2704     add(tmp, base, offset &amp; (((1&lt;&lt;12)-1)&lt;&lt;12));
2705     base = tmp;
2706     offset &amp;= ~(((1&lt;&lt;12)-1)&lt;&lt;12);
2707   }
2708 
2709   return Address(base, offset);
2710 }
2711 
2712 // Checks whether offset is aligned.
2713 // Returns true if it is, else false.
2714 bool MacroAssembler::merge_alignment_check(Register base,
2715                                            size_t size,
2716                                            int64_t cur_offset,
2717                                            int64_t prev_offset) const {
2718   if (AvoidUnalignedAccesses) {
2719     if (base == sp) {
2720       // Checks whether low offset if aligned to pair of registers.
2721       int64_t pair_mask = size * 2 - 1;
2722       int64_t offset = prev_offset &gt; cur_offset ? cur_offset : prev_offset;
2723       return (offset &amp; pair_mask) == 0;
2724     } else { // If base is not sp, we can&#39;t guarantee the access is aligned.
2725       return false;
2726     }
2727   } else {
2728     int64_t mask = size - 1;
2729     // Load/store pair instruction only supports element size aligned offset.
2730     return (cur_offset &amp; mask) == 0 &amp;&amp; (prev_offset &amp; mask) == 0;
2731   }
2732 }
2733 
2734 // Checks whether current and previous loads/stores can be merged.
2735 // Returns true if it can be merged, else false.
2736 bool MacroAssembler::ldst_can_merge(Register rt,
2737                                     const Address &amp;adr,
2738                                     size_t cur_size_in_bytes,
2739                                     bool is_store) const {
2740   address prev = pc() - NativeInstruction::instruction_size;
2741   address last = code()-&gt;last_insn();
2742 
2743   if (last == NULL || !nativeInstruction_at(last)-&gt;is_Imm_LdSt()) {
2744     return false;
2745   }
2746 
2747   if (adr.getMode() != Address::base_plus_offset || prev != last) {
2748     return false;
2749   }
2750 
2751   NativeLdSt* prev_ldst = NativeLdSt_at(prev);
2752   size_t prev_size_in_bytes = prev_ldst-&gt;size_in_bytes();
2753 
2754   assert(prev_size_in_bytes == 4 || prev_size_in_bytes == 8, &quot;only supports 64/32bit merging.&quot;);
2755   assert(cur_size_in_bytes == 4 || cur_size_in_bytes == 8, &quot;only supports 64/32bit merging.&quot;);
2756 
2757   if (cur_size_in_bytes != prev_size_in_bytes || is_store != prev_ldst-&gt;is_store()) {
2758     return false;
2759   }
2760 
2761   int64_t max_offset = 63 * prev_size_in_bytes;
2762   int64_t min_offset = -64 * prev_size_in_bytes;
2763 
2764   assert(prev_ldst-&gt;is_not_pre_post_index(), &quot;pre-index or post-index is not supported to be merged.&quot;);
2765 
2766   // Only same base can be merged.
2767   if (adr.base() != prev_ldst-&gt;base()) {
2768     return false;
2769   }
2770 
2771   int64_t cur_offset = adr.offset();
2772   int64_t prev_offset = prev_ldst-&gt;offset();
2773   size_t diff = abs(cur_offset - prev_offset);
2774   if (diff != prev_size_in_bytes) {
2775     return false;
2776   }
2777 
2778   // Following cases can not be merged:
2779   // ldr x2, [x2, #8]
2780   // ldr x3, [x2, #16]
2781   // or:
2782   // ldr x2, [x3, #8]
2783   // ldr x2, [x3, #16]
2784   // If t1 and t2 is the same in &quot;ldp t1, t2, [xn, #imm]&quot;, we&#39;ll get SIGILL.
2785   if (!is_store &amp;&amp; (adr.base() == prev_ldst-&gt;target() || rt == prev_ldst-&gt;target())) {
2786     return false;
2787   }
2788 
2789   int64_t low_offset = prev_offset &gt; cur_offset ? cur_offset : prev_offset;
2790   // Offset range must be in ldp/stp instruction&#39;s range.
2791   if (low_offset &gt; max_offset || low_offset &lt; min_offset) {
2792     return false;
2793   }
2794 
2795   if (merge_alignment_check(adr.base(), prev_size_in_bytes, cur_offset, prev_offset)) {
2796     return true;
2797   }
2798 
2799   return false;
2800 }
2801 
2802 // Merge current load/store with previous load/store into ldp/stp.
2803 void MacroAssembler::merge_ldst(Register rt,
2804                                 const Address &amp;adr,
2805                                 size_t cur_size_in_bytes,
2806                                 bool is_store) {
2807 
2808   assert(ldst_can_merge(rt, adr, cur_size_in_bytes, is_store) == true, &quot;cur and prev must be able to be merged.&quot;);
2809 
2810   Register rt_low, rt_high;
2811   address prev = pc() - NativeInstruction::instruction_size;
2812   NativeLdSt* prev_ldst = NativeLdSt_at(prev);
2813 
2814   int64_t offset;
2815 
2816   if (adr.offset() &lt; prev_ldst-&gt;offset()) {
2817     offset = adr.offset();
2818     rt_low = rt;
2819     rt_high = prev_ldst-&gt;target();
2820   } else {
2821     offset = prev_ldst-&gt;offset();
2822     rt_low = prev_ldst-&gt;target();
2823     rt_high = rt;
2824   }
2825 
2826   Address adr_p = Address(prev_ldst-&gt;base(), offset);
2827   // Overwrite previous generated binary.
2828   code_section()-&gt;set_end(prev);
2829 
2830   const int sz = prev_ldst-&gt;size_in_bytes();
2831   assert(sz == 8 || sz == 4, &quot;only supports 64/32bit merging.&quot;);
2832   if (!is_store) {
2833     BLOCK_COMMENT(&quot;merged ldr pair&quot;);
2834     if (sz == 8) {
2835       ldp(rt_low, rt_high, adr_p);
2836     } else {
2837       ldpw(rt_low, rt_high, adr_p);
2838     }
2839   } else {
2840     BLOCK_COMMENT(&quot;merged str pair&quot;);
2841     if (sz == 8) {
2842       stp(rt_low, rt_high, adr_p);
2843     } else {
2844       stpw(rt_low, rt_high, adr_p);
2845     }
2846   }
2847 }
2848 
2849 /**
2850  * Multiply 64 bit by 64 bit first loop.
2851  */
2852 void MacroAssembler::multiply_64_x_64_loop(Register x, Register xstart, Register x_xstart,
2853                                            Register y, Register y_idx, Register z,
2854                                            Register carry, Register product,
2855                                            Register idx, Register kdx) {
2856   //
2857   //  jlong carry, x[], y[], z[];
2858   //  for (int idx=ystart, kdx=ystart+1+xstart; idx &gt;= 0; idx-, kdx--) {
2859   //    huge_128 product = y[idx] * x[xstart] + carry;
2860   //    z[kdx] = (jlong)product;
2861   //    carry  = (jlong)(product &gt;&gt;&gt; 64);
2862   //  }
2863   //  z[xstart] = carry;
2864   //
2865 
2866   Label L_first_loop, L_first_loop_exit;
2867   Label L_one_x, L_one_y, L_multiply;
2868 
2869   subsw(xstart, xstart, 1);
2870   br(Assembler::MI, L_one_x);
2871 
2872   lea(rscratch1, Address(x, xstart, Address::lsl(LogBytesPerInt)));
2873   ldr(x_xstart, Address(rscratch1));
2874   ror(x_xstart, x_xstart, 32); // convert big-endian to little-endian
2875 
2876   bind(L_first_loop);
2877   subsw(idx, idx, 1);
2878   br(Assembler::MI, L_first_loop_exit);
2879   subsw(idx, idx, 1);
2880   br(Assembler::MI, L_one_y);
2881   lea(rscratch1, Address(y, idx, Address::uxtw(LogBytesPerInt)));
2882   ldr(y_idx, Address(rscratch1));
2883   ror(y_idx, y_idx, 32); // convert big-endian to little-endian
2884   bind(L_multiply);
2885 
2886   // AArch64 has a multiply-accumulate instruction that we can&#39;t use
2887   // here because it has no way to process carries, so we have to use
2888   // separate add and adc instructions.  Bah.
2889   umulh(rscratch1, x_xstart, y_idx); // x_xstart * y_idx -&gt; rscratch1:product
2890   mul(product, x_xstart, y_idx);
2891   adds(product, product, carry);
2892   adc(carry, rscratch1, zr);   // x_xstart * y_idx + carry -&gt; carry:product
2893 
2894   subw(kdx, kdx, 2);
2895   ror(product, product, 32); // back to big-endian
2896   str(product, offsetted_address(z, kdx, Address::uxtw(LogBytesPerInt), 0, BytesPerLong));
2897 
2898   b(L_first_loop);
2899 
2900   bind(L_one_y);
2901   ldrw(y_idx, Address(y,  0));
2902   b(L_multiply);
2903 
2904   bind(L_one_x);
2905   ldrw(x_xstart, Address(x,  0));
2906   b(L_first_loop);
2907 
2908   bind(L_first_loop_exit);
2909 }
2910 
2911 /**
2912  * Multiply 128 bit by 128. Unrolled inner loop.
2913  *
2914  */
2915 void MacroAssembler::multiply_128_x_128_loop(Register y, Register z,
2916                                              Register carry, Register carry2,
2917                                              Register idx, Register jdx,
2918                                              Register yz_idx1, Register yz_idx2,
2919                                              Register tmp, Register tmp3, Register tmp4,
2920                                              Register tmp6, Register product_hi) {
2921 
2922   //   jlong carry, x[], y[], z[];
2923   //   int kdx = ystart+1;
2924   //   for (int idx=ystart-2; idx &gt;= 0; idx -= 2) { // Third loop
2925   //     huge_128 tmp3 = (y[idx+1] * product_hi) + z[kdx+idx+1] + carry;
2926   //     jlong carry2  = (jlong)(tmp3 &gt;&gt;&gt; 64);
2927   //     huge_128 tmp4 = (y[idx]   * product_hi) + z[kdx+idx] + carry2;
2928   //     carry  = (jlong)(tmp4 &gt;&gt;&gt; 64);
2929   //     z[kdx+idx+1] = (jlong)tmp3;
2930   //     z[kdx+idx] = (jlong)tmp4;
2931   //   }
2932   //   idx += 2;
2933   //   if (idx &gt; 0) {
2934   //     yz_idx1 = (y[idx] * product_hi) + z[kdx+idx] + carry;
2935   //     z[kdx+idx] = (jlong)yz_idx1;
2936   //     carry  = (jlong)(yz_idx1 &gt;&gt;&gt; 64);
2937   //   }
2938   //
2939 
2940   Label L_third_loop, L_third_loop_exit, L_post_third_loop_done;
2941 
2942   lsrw(jdx, idx, 2);
2943 
2944   bind(L_third_loop);
2945 
2946   subsw(jdx, jdx, 1);
2947   br(Assembler::MI, L_third_loop_exit);
2948   subw(idx, idx, 4);
2949 
2950   lea(rscratch1, Address(y, idx, Address::uxtw(LogBytesPerInt)));
2951 
2952   ldp(yz_idx2, yz_idx1, Address(rscratch1, 0));
2953 
2954   lea(tmp6, Address(z, idx, Address::uxtw(LogBytesPerInt)));
2955 
2956   ror(yz_idx1, yz_idx1, 32); // convert big-endian to little-endian
2957   ror(yz_idx2, yz_idx2, 32);
2958 
2959   ldp(rscratch2, rscratch1, Address(tmp6, 0));
2960 
2961   mul(tmp3, product_hi, yz_idx1);  //  yz_idx1 * product_hi -&gt; tmp4:tmp3
2962   umulh(tmp4, product_hi, yz_idx1);
2963 
2964   ror(rscratch1, rscratch1, 32); // convert big-endian to little-endian
2965   ror(rscratch2, rscratch2, 32);
2966 
2967   mul(tmp, product_hi, yz_idx2);   //  yz_idx2 * product_hi -&gt; carry2:tmp
2968   umulh(carry2, product_hi, yz_idx2);
2969 
2970   // propagate sum of both multiplications into carry:tmp4:tmp3
2971   adds(tmp3, tmp3, carry);
2972   adc(tmp4, tmp4, zr);
2973   adds(tmp3, tmp3, rscratch1);
2974   adcs(tmp4, tmp4, tmp);
2975   adc(carry, carry2, zr);
2976   adds(tmp4, tmp4, rscratch2);
2977   adc(carry, carry, zr);
2978 
2979   ror(tmp3, tmp3, 32); // convert little-endian to big-endian
2980   ror(tmp4, tmp4, 32);
2981   stp(tmp4, tmp3, Address(tmp6, 0));
2982 
2983   b(L_third_loop);
2984   bind (L_third_loop_exit);
2985 
2986   andw (idx, idx, 0x3);
2987   cbz(idx, L_post_third_loop_done);
2988 
2989   Label L_check_1;
2990   subsw(idx, idx, 2);
2991   br(Assembler::MI, L_check_1);
2992 
2993   lea(rscratch1, Address(y, idx, Address::uxtw(LogBytesPerInt)));
2994   ldr(yz_idx1, Address(rscratch1, 0));
2995   ror(yz_idx1, yz_idx1, 32);
2996   mul(tmp3, product_hi, yz_idx1);  //  yz_idx1 * product_hi -&gt; tmp4:tmp3
2997   umulh(tmp4, product_hi, yz_idx1);
2998   lea(rscratch1, Address(z, idx, Address::uxtw(LogBytesPerInt)));
2999   ldr(yz_idx2, Address(rscratch1, 0));
3000   ror(yz_idx2, yz_idx2, 32);
3001 
3002   add2_with_carry(carry, tmp4, tmp3, carry, yz_idx2);
3003 
3004   ror(tmp3, tmp3, 32);
3005   str(tmp3, Address(rscratch1, 0));
3006 
3007   bind (L_check_1);
3008 
3009   andw (idx, idx, 0x1);
3010   subsw(idx, idx, 1);
3011   br(Assembler::MI, L_post_third_loop_done);
3012   ldrw(tmp4, Address(y, idx, Address::uxtw(LogBytesPerInt)));
3013   mul(tmp3, tmp4, product_hi);  //  tmp4 * product_hi -&gt; carry2:tmp3
3014   umulh(carry2, tmp4, product_hi);
3015   ldrw(tmp4, Address(z, idx, Address::uxtw(LogBytesPerInt)));
3016 
3017   add2_with_carry(carry2, tmp3, tmp4, carry);
3018 
3019   strw(tmp3, Address(z, idx, Address::uxtw(LogBytesPerInt)));
3020   extr(carry, carry2, tmp3, 32);
3021 
3022   bind(L_post_third_loop_done);
3023 }
3024 
3025 /**
3026  * Code for BigInteger::multiplyToLen() instrinsic.
3027  *
3028  * r0: x
3029  * r1: xlen
3030  * r2: y
3031  * r3: ylen
3032  * r4:  z
3033  * r5: zlen
3034  * r10: tmp1
3035  * r11: tmp2
3036  * r12: tmp3
3037  * r13: tmp4
3038  * r14: tmp5
3039  * r15: tmp6
3040  * r16: tmp7
3041  *
3042  */
3043 void MacroAssembler::multiply_to_len(Register x, Register xlen, Register y, Register ylen,
3044                                      Register z, Register zlen,
3045                                      Register tmp1, Register tmp2, Register tmp3, Register tmp4,
3046                                      Register tmp5, Register tmp6, Register product_hi) {
3047 
3048   assert_different_registers(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6);
3049 
3050   const Register idx = tmp1;
3051   const Register kdx = tmp2;
3052   const Register xstart = tmp3;
3053 
3054   const Register y_idx = tmp4;
3055   const Register carry = tmp5;
3056   const Register product  = xlen;
3057   const Register x_xstart = zlen;  // reuse register
3058 
3059   // First Loop.
3060   //
3061   //  final static long LONG_MASK = 0xffffffffL;
3062   //  int xstart = xlen - 1;
3063   //  int ystart = ylen - 1;
3064   //  long carry = 0;
3065   //  for (int idx=ystart, kdx=ystart+1+xstart; idx &gt;= 0; idx-, kdx--) {
3066   //    long product = (y[idx] &amp; LONG_MASK) * (x[xstart] &amp; LONG_MASK) + carry;
3067   //    z[kdx] = (int)product;
3068   //    carry = product &gt;&gt;&gt; 32;
3069   //  }
3070   //  z[xstart] = (int)carry;
3071   //
3072 
3073   movw(idx, ylen);      // idx = ylen;
3074   movw(kdx, zlen);      // kdx = xlen+ylen;
3075   mov(carry, zr);       // carry = 0;
3076 
3077   Label L_done;
3078 
3079   movw(xstart, xlen);
3080   subsw(xstart, xstart, 1);
3081   br(Assembler::MI, L_done);
3082 
3083   multiply_64_x_64_loop(x, xstart, x_xstart, y, y_idx, z, carry, product, idx, kdx);
3084 
3085   Label L_second_loop;
3086   cbzw(kdx, L_second_loop);
3087 
3088   Label L_carry;
3089   subw(kdx, kdx, 1);
3090   cbzw(kdx, L_carry);
3091 
3092   strw(carry, Address(z, kdx, Address::uxtw(LogBytesPerInt)));
3093   lsr(carry, carry, 32);
3094   subw(kdx, kdx, 1);
3095 
3096   bind(L_carry);
3097   strw(carry, Address(z, kdx, Address::uxtw(LogBytesPerInt)));
3098 
3099   // Second and third (nested) loops.
3100   //
3101   // for (int i = xstart-1; i &gt;= 0; i--) { // Second loop
3102   //   carry = 0;
3103   //   for (int jdx=ystart, k=ystart+1+i; jdx &gt;= 0; jdx--, k--) { // Third loop
3104   //     long product = (y[jdx] &amp; LONG_MASK) * (x[i] &amp; LONG_MASK) +
3105   //                    (z[k] &amp; LONG_MASK) + carry;
3106   //     z[k] = (int)product;
3107   //     carry = product &gt;&gt;&gt; 32;
3108   //   }
3109   //   z[i] = (int)carry;
3110   // }
3111   //
3112   // i = xlen, j = tmp1, k = tmp2, carry = tmp5, x[i] = product_hi
3113 
3114   const Register jdx = tmp1;
3115 
3116   bind(L_second_loop);
3117   mov(carry, zr);                // carry = 0;
3118   movw(jdx, ylen);               // j = ystart+1
3119 
3120   subsw(xstart, xstart, 1);      // i = xstart-1;
3121   br(Assembler::MI, L_done);
3122 
3123   str(z, Address(pre(sp, -4 * wordSize)));
3124 
3125   Label L_last_x;
3126   lea(z, offsetted_address(z, xstart, Address::uxtw(LogBytesPerInt), 4, BytesPerInt)); // z = z + k - j
3127   subsw(xstart, xstart, 1);       // i = xstart-1;
3128   br(Assembler::MI, L_last_x);
3129 
3130   lea(rscratch1, Address(x, xstart, Address::uxtw(LogBytesPerInt)));
3131   ldr(product_hi, Address(rscratch1));
3132   ror(product_hi, product_hi, 32);  // convert big-endian to little-endian
3133 
3134   Label L_third_loop_prologue;
3135   bind(L_third_loop_prologue);
3136 
3137   str(ylen, Address(sp, wordSize));
3138   stp(x, xstart, Address(sp, 2 * wordSize));
3139   multiply_128_x_128_loop(y, z, carry, x, jdx, ylen, product,
3140                           tmp2, x_xstart, tmp3, tmp4, tmp6, product_hi);
3141   ldp(z, ylen, Address(post(sp, 2 * wordSize)));
3142   ldp(x, xlen, Address(post(sp, 2 * wordSize)));   // copy old xstart -&gt; xlen
3143 
3144   addw(tmp3, xlen, 1);
3145   strw(carry, Address(z, tmp3, Address::uxtw(LogBytesPerInt)));
3146   subsw(tmp3, tmp3, 1);
3147   br(Assembler::MI, L_done);
3148 
3149   lsr(carry, carry, 32);
3150   strw(carry, Address(z, tmp3, Address::uxtw(LogBytesPerInt)));
3151   b(L_second_loop);
3152 
3153   // Next infrequent code is moved outside loops.
3154   bind(L_last_x);
3155   ldrw(product_hi, Address(x,  0));
3156   b(L_third_loop_prologue);
3157 
3158   bind(L_done);
3159 }
3160 
3161 // Code for BigInteger::mulAdd instrinsic
3162 // out     = r0
3163 // in      = r1
3164 // offset  = r2  (already out.length-offset)
3165 // len     = r3
3166 // k       = r4
3167 //
3168 // pseudo code from java implementation:
3169 // carry = 0;
3170 // offset = out.length-offset - 1;
3171 // for (int j=len-1; j &gt;= 0; j--) {
3172 //     product = (in[j] &amp; LONG_MASK) * kLong + (out[offset] &amp; LONG_MASK) + carry;
3173 //     out[offset--] = (int)product;
3174 //     carry = product &gt;&gt;&gt; 32;
3175 // }
3176 // return (int)carry;
3177 void MacroAssembler::mul_add(Register out, Register in, Register offset,
3178       Register len, Register k) {
3179     Label LOOP, END;
3180     // pre-loop
3181     cmp(len, zr); // cmp, not cbz/cbnz: to use condition twice =&gt; less branches
3182     csel(out, zr, out, Assembler::EQ);
3183     br(Assembler::EQ, END);
3184     add(in, in, len, LSL, 2); // in[j+1] address
3185     add(offset, out, offset, LSL, 2); // out[offset + 1] address
3186     mov(out, zr); // used to keep carry now
3187     BIND(LOOP);
3188     ldrw(rscratch1, Address(pre(in, -4)));
3189     madd(rscratch1, rscratch1, k, out);
3190     ldrw(rscratch2, Address(pre(offset, -4)));
3191     add(rscratch1, rscratch1, rscratch2);
3192     strw(rscratch1, Address(offset));
3193     lsr(out, rscratch1, 32);
3194     subs(len, len, 1);
3195     br(Assembler::NE, LOOP);
3196     BIND(END);
3197 }
3198 
3199 /**
3200  * Emits code to update CRC-32 with a byte value according to constants in table
3201  *
3202  * @param [in,out]crc   Register containing the crc.
3203  * @param [in]val       Register containing the byte to fold into the CRC.
3204  * @param [in]table     Register containing the table of crc constants.
3205  *
3206  * uint32_t crc;
3207  * val = crc_table[(val ^ crc) &amp; 0xFF];
3208  * crc = val ^ (crc &gt;&gt; 8);
3209  *
3210  */
3211 void MacroAssembler::update_byte_crc32(Register crc, Register val, Register table) {
3212   eor(val, val, crc);
3213   andr(val, val, 0xff);
3214   ldrw(val, Address(table, val, Address::lsl(2)));
3215   eor(crc, val, crc, Assembler::LSR, 8);
3216 }
3217 
3218 /**
3219  * Emits code to update CRC-32 with a 32-bit value according to tables 0 to 3
3220  *
3221  * @param [in,out]crc   Register containing the crc.
3222  * @param [in]v         Register containing the 32-bit to fold into the CRC.
3223  * @param [in]table0    Register containing table 0 of crc constants.
3224  * @param [in]table1    Register containing table 1 of crc constants.
3225  * @param [in]table2    Register containing table 2 of crc constants.
3226  * @param [in]table3    Register containing table 3 of crc constants.
3227  *
3228  * uint32_t crc;
3229  *   v = crc ^ v
3230  *   crc = table3[v&amp;0xff]^table2[(v&gt;&gt;8)&amp;0xff]^table1[(v&gt;&gt;16)&amp;0xff]^table0[v&gt;&gt;24]
3231  *
3232  */
3233 void MacroAssembler::update_word_crc32(Register crc, Register v, Register tmp,
3234         Register table0, Register table1, Register table2, Register table3,
3235         bool upper) {
3236   eor(v, crc, v, upper ? LSR:LSL, upper ? 32:0);
3237   uxtb(tmp, v);
3238   ldrw(crc, Address(table3, tmp, Address::lsl(2)));
3239   ubfx(tmp, v, 8, 8);
3240   ldrw(tmp, Address(table2, tmp, Address::lsl(2)));
3241   eor(crc, crc, tmp);
3242   ubfx(tmp, v, 16, 8);
3243   ldrw(tmp, Address(table1, tmp, Address::lsl(2)));
3244   eor(crc, crc, tmp);
3245   ubfx(tmp, v, 24, 8);
3246   ldrw(tmp, Address(table0, tmp, Address::lsl(2)));
3247   eor(crc, crc, tmp);
3248 }
3249 
3250 void MacroAssembler::kernel_crc32_using_crc32(Register crc, Register buf,
3251         Register len, Register tmp0, Register tmp1, Register tmp2,
3252         Register tmp3) {
3253     Label CRC_by64_loop, CRC_by4_loop, CRC_by1_loop, CRC_less64, CRC_by64_pre, CRC_by32_loop, CRC_less32, L_exit;
3254     assert_different_registers(crc, buf, len, tmp0, tmp1, tmp2, tmp3);
3255 
3256     mvnw(crc, crc);
3257 
3258     subs(len, len, 128);
3259     br(Assembler::GE, CRC_by64_pre);
3260   BIND(CRC_less64);
3261     adds(len, len, 128-32);
3262     br(Assembler::GE, CRC_by32_loop);
3263   BIND(CRC_less32);
3264     adds(len, len, 32-4);
3265     br(Assembler::GE, CRC_by4_loop);
3266     adds(len, len, 4);
3267     br(Assembler::GT, CRC_by1_loop);
3268     b(L_exit);
3269 
3270   BIND(CRC_by32_loop);
3271     ldp(tmp0, tmp1, Address(post(buf, 16)));
3272     subs(len, len, 32);
3273     crc32x(crc, crc, tmp0);
3274     ldr(tmp2, Address(post(buf, 8)));
3275     crc32x(crc, crc, tmp1);
3276     ldr(tmp3, Address(post(buf, 8)));
3277     crc32x(crc, crc, tmp2);
3278     crc32x(crc, crc, tmp3);
3279     br(Assembler::GE, CRC_by32_loop);
3280     cmn(len, 32);
3281     br(Assembler::NE, CRC_less32);
3282     b(L_exit);
3283 
3284   BIND(CRC_by4_loop);
3285     ldrw(tmp0, Address(post(buf, 4)));
3286     subs(len, len, 4);
3287     crc32w(crc, crc, tmp0);
3288     br(Assembler::GE, CRC_by4_loop);
3289     adds(len, len, 4);
3290     br(Assembler::LE, L_exit);
3291   BIND(CRC_by1_loop);
3292     ldrb(tmp0, Address(post(buf, 1)));
3293     subs(len, len, 1);
3294     crc32b(crc, crc, tmp0);
3295     br(Assembler::GT, CRC_by1_loop);
3296     b(L_exit);
3297 
3298   BIND(CRC_by64_pre);
3299     sub(buf, buf, 8);
3300     ldp(tmp0, tmp1, Address(buf, 8));
3301     crc32x(crc, crc, tmp0);
3302     ldr(tmp2, Address(buf, 24));
3303     crc32x(crc, crc, tmp1);
3304     ldr(tmp3, Address(buf, 32));
3305     crc32x(crc, crc, tmp2);
3306     ldr(tmp0, Address(buf, 40));
3307     crc32x(crc, crc, tmp3);
3308     ldr(tmp1, Address(buf, 48));
3309     crc32x(crc, crc, tmp0);
3310     ldr(tmp2, Address(buf, 56));
3311     crc32x(crc, crc, tmp1);
3312     ldr(tmp3, Address(pre(buf, 64)));
3313 
3314     b(CRC_by64_loop);
3315 
3316     align(CodeEntryAlignment);
3317   BIND(CRC_by64_loop);
3318     subs(len, len, 64);
3319     crc32x(crc, crc, tmp2);
3320     ldr(tmp0, Address(buf, 8));
3321     crc32x(crc, crc, tmp3);
3322     ldr(tmp1, Address(buf, 16));
3323     crc32x(crc, crc, tmp0);
3324     ldr(tmp2, Address(buf, 24));
3325     crc32x(crc, crc, tmp1);
3326     ldr(tmp3, Address(buf, 32));
3327     crc32x(crc, crc, tmp2);
3328     ldr(tmp0, Address(buf, 40));
3329     crc32x(crc, crc, tmp3);
3330     ldr(tmp1, Address(buf, 48));
3331     crc32x(crc, crc, tmp0);
3332     ldr(tmp2, Address(buf, 56));
3333     crc32x(crc, crc, tmp1);
3334     ldr(tmp3, Address(pre(buf, 64)));
3335     br(Assembler::GE, CRC_by64_loop);
3336 
3337     // post-loop
3338     crc32x(crc, crc, tmp2);
3339     crc32x(crc, crc, tmp3);
3340 
3341     sub(len, len, 64);
3342     add(buf, buf, 8);
3343     cmn(len, 128);
3344     br(Assembler::NE, CRC_less64);
3345   BIND(L_exit);
3346     mvnw(crc, crc);
3347 }
3348 
3349 /**
3350  * @param crc   register containing existing CRC (32-bit)
3351  * @param buf   register pointing to input byte buffer (byte*)
3352  * @param len   register containing number of bytes
3353  * @param table register that will contain address of CRC table
3354  * @param tmp   scratch register
3355  */
3356 void MacroAssembler::kernel_crc32(Register crc, Register buf, Register len,
3357         Register table0, Register table1, Register table2, Register table3,
3358         Register tmp, Register tmp2, Register tmp3) {
3359   Label L_by16, L_by16_loop, L_by4, L_by4_loop, L_by1, L_by1_loop, L_exit;
3360   uint64_t offset;
3361 
3362   if (UseCRC32) {
3363       kernel_crc32_using_crc32(crc, buf, len, table0, table1, table2, table3);
3364       return;
3365   }
3366 
3367     mvnw(crc, crc);
3368 
3369     adrp(table0, ExternalAddress(StubRoutines::crc_table_addr()), offset);
3370     if (offset) add(table0, table0, offset);
3371     add(table1, table0, 1*256*sizeof(juint));
3372     add(table2, table0, 2*256*sizeof(juint));
3373     add(table3, table0, 3*256*sizeof(juint));
3374 
3375   if (UseNeon) {
3376       cmp(len, (u1)64);
3377       br(Assembler::LT, L_by16);
3378       eor(v16, T16B, v16, v16);
3379 
3380     Label L_fold;
3381 
3382       add(tmp, table0, 4*256*sizeof(juint)); // Point at the Neon constants
3383 
3384       ld1(v0, v1, T2D, post(buf, 32));
3385       ld1r(v4, T2D, post(tmp, 8));
3386       ld1r(v5, T2D, post(tmp, 8));
3387       ld1r(v6, T2D, post(tmp, 8));
3388       ld1r(v7, T2D, post(tmp, 8));
3389       mov(v16, T4S, 0, crc);
3390 
3391       eor(v0, T16B, v0, v16);
3392       sub(len, len, 64);
3393 
3394     BIND(L_fold);
3395       pmull(v22, T8H, v0, v5, T8B);
3396       pmull(v20, T8H, v0, v7, T8B);
3397       pmull(v23, T8H, v0, v4, T8B);
3398       pmull(v21, T8H, v0, v6, T8B);
3399 
3400       pmull2(v18, T8H, v0, v5, T16B);
3401       pmull2(v16, T8H, v0, v7, T16B);
3402       pmull2(v19, T8H, v0, v4, T16B);
3403       pmull2(v17, T8H, v0, v6, T16B);
3404 
3405       uzp1(v24, T8H, v20, v22);
3406       uzp2(v25, T8H, v20, v22);
3407       eor(v20, T16B, v24, v25);
3408 
3409       uzp1(v26, T8H, v16, v18);
3410       uzp2(v27, T8H, v16, v18);
3411       eor(v16, T16B, v26, v27);
3412 
3413       ushll2(v22, T4S, v20, T8H, 8);
3414       ushll(v20, T4S, v20, T4H, 8);
3415 
3416       ushll2(v18, T4S, v16, T8H, 8);
3417       ushll(v16, T4S, v16, T4H, 8);
3418 
3419       eor(v22, T16B, v23, v22);
3420       eor(v18, T16B, v19, v18);
3421       eor(v20, T16B, v21, v20);
3422       eor(v16, T16B, v17, v16);
3423 
3424       uzp1(v17, T2D, v16, v20);
3425       uzp2(v21, T2D, v16, v20);
3426       eor(v17, T16B, v17, v21);
3427 
3428       ushll2(v20, T2D, v17, T4S, 16);
3429       ushll(v16, T2D, v17, T2S, 16);
3430 
3431       eor(v20, T16B, v20, v22);
3432       eor(v16, T16B, v16, v18);
3433 
3434       uzp1(v17, T2D, v20, v16);
3435       uzp2(v21, T2D, v20, v16);
3436       eor(v28, T16B, v17, v21);
3437 
3438       pmull(v22, T8H, v1, v5, T8B);
3439       pmull(v20, T8H, v1, v7, T8B);
3440       pmull(v23, T8H, v1, v4, T8B);
3441       pmull(v21, T8H, v1, v6, T8B);
3442 
3443       pmull2(v18, T8H, v1, v5, T16B);
3444       pmull2(v16, T8H, v1, v7, T16B);
3445       pmull2(v19, T8H, v1, v4, T16B);
3446       pmull2(v17, T8H, v1, v6, T16B);
3447 
3448       ld1(v0, v1, T2D, post(buf, 32));
3449 
3450       uzp1(v24, T8H, v20, v22);
3451       uzp2(v25, T8H, v20, v22);
3452       eor(v20, T16B, v24, v25);
3453 
3454       uzp1(v26, T8H, v16, v18);
3455       uzp2(v27, T8H, v16, v18);
3456       eor(v16, T16B, v26, v27);
3457 
3458       ushll2(v22, T4S, v20, T8H, 8);
3459       ushll(v20, T4S, v20, T4H, 8);
3460 
3461       ushll2(v18, T4S, v16, T8H, 8);
3462       ushll(v16, T4S, v16, T4H, 8);
3463 
3464       eor(v22, T16B, v23, v22);
3465       eor(v18, T16B, v19, v18);
3466       eor(v20, T16B, v21, v20);
3467       eor(v16, T16B, v17, v16);
3468 
3469       uzp1(v17, T2D, v16, v20);
3470       uzp2(v21, T2D, v16, v20);
3471       eor(v16, T16B, v17, v21);
3472 
3473       ushll2(v20, T2D, v16, T4S, 16);
3474       ushll(v16, T2D, v16, T2S, 16);
3475 
3476       eor(v20, T16B, v22, v20);
3477       eor(v16, T16B, v16, v18);
3478 
3479       uzp1(v17, T2D, v20, v16);
3480       uzp2(v21, T2D, v20, v16);
3481       eor(v20, T16B, v17, v21);
3482 
3483       shl(v16, T2D, v28, 1);
3484       shl(v17, T2D, v20, 1);
3485 
3486       eor(v0, T16B, v0, v16);
3487       eor(v1, T16B, v1, v17);
3488 
3489       subs(len, len, 32);
3490       br(Assembler::GE, L_fold);
3491 
3492       mov(crc, 0);
3493       mov(tmp, v0, T1D, 0);
3494       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3495       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3496       mov(tmp, v0, T1D, 1);
3497       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3498       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3499       mov(tmp, v1, T1D, 0);
3500       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3501       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3502       mov(tmp, v1, T1D, 1);
3503       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3504       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3505 
3506       add(len, len, 32);
3507   }
3508 
3509   BIND(L_by16);
3510     subs(len, len, 16);
3511     br(Assembler::GE, L_by16_loop);
3512     adds(len, len, 16-4);
3513     br(Assembler::GE, L_by4_loop);
3514     adds(len, len, 4);
3515     br(Assembler::GT, L_by1_loop);
3516     b(L_exit);
3517 
3518   BIND(L_by4_loop);
3519     ldrw(tmp, Address(post(buf, 4)));
3520     update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3);
3521     subs(len, len, 4);
3522     br(Assembler::GE, L_by4_loop);
3523     adds(len, len, 4);
3524     br(Assembler::LE, L_exit);
3525   BIND(L_by1_loop);
3526     subs(len, len, 1);
3527     ldrb(tmp, Address(post(buf, 1)));
3528     update_byte_crc32(crc, tmp, table0);
3529     br(Assembler::GT, L_by1_loop);
3530     b(L_exit);
3531 
3532     align(CodeEntryAlignment);
3533   BIND(L_by16_loop);
3534     subs(len, len, 16);
3535     ldp(tmp, tmp3, Address(post(buf, 16)));
3536     update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3537     update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3538     update_word_crc32(crc, tmp3, tmp2, table0, table1, table2, table3, false);
3539     update_word_crc32(crc, tmp3, tmp2, table0, table1, table2, table3, true);
3540     br(Assembler::GE, L_by16_loop);
3541     adds(len, len, 16-4);
3542     br(Assembler::GE, L_by4_loop);
3543     adds(len, len, 4);
3544     br(Assembler::GT, L_by1_loop);
3545   BIND(L_exit);
3546     mvnw(crc, crc);
3547 }
3548 
3549 void MacroAssembler::kernel_crc32c_using_crc32c(Register crc, Register buf,
3550         Register len, Register tmp0, Register tmp1, Register tmp2,
3551         Register tmp3) {
3552     Label CRC_by64_loop, CRC_by4_loop, CRC_by1_loop, CRC_less64, CRC_by64_pre, CRC_by32_loop, CRC_less32, L_exit;
3553     assert_different_registers(crc, buf, len, tmp0, tmp1, tmp2, tmp3);
3554 
3555     subs(len, len, 128);
3556     br(Assembler::GE, CRC_by64_pre);
3557   BIND(CRC_less64);
3558     adds(len, len, 128-32);
3559     br(Assembler::GE, CRC_by32_loop);
3560   BIND(CRC_less32);
3561     adds(len, len, 32-4);
3562     br(Assembler::GE, CRC_by4_loop);
3563     adds(len, len, 4);
3564     br(Assembler::GT, CRC_by1_loop);
3565     b(L_exit);
3566 
3567   BIND(CRC_by32_loop);
3568     ldp(tmp0, tmp1, Address(post(buf, 16)));
3569     subs(len, len, 32);
3570     crc32cx(crc, crc, tmp0);
3571     ldr(tmp2, Address(post(buf, 8)));
3572     crc32cx(crc, crc, tmp1);
3573     ldr(tmp3, Address(post(buf, 8)));
3574     crc32cx(crc, crc, tmp2);
3575     crc32cx(crc, crc, tmp3);
3576     br(Assembler::GE, CRC_by32_loop);
3577     cmn(len, 32);
3578     br(Assembler::NE, CRC_less32);
3579     b(L_exit);
3580 
3581   BIND(CRC_by4_loop);
3582     ldrw(tmp0, Address(post(buf, 4)));
3583     subs(len, len, 4);
3584     crc32cw(crc, crc, tmp0);
3585     br(Assembler::GE, CRC_by4_loop);
3586     adds(len, len, 4);
3587     br(Assembler::LE, L_exit);
3588   BIND(CRC_by1_loop);
3589     ldrb(tmp0, Address(post(buf, 1)));
3590     subs(len, len, 1);
3591     crc32cb(crc, crc, tmp0);
3592     br(Assembler::GT, CRC_by1_loop);
3593     b(L_exit);
3594 
3595   BIND(CRC_by64_pre);
3596     sub(buf, buf, 8);
3597     ldp(tmp0, tmp1, Address(buf, 8));
3598     crc32cx(crc, crc, tmp0);
3599     ldr(tmp2, Address(buf, 24));
3600     crc32cx(crc, crc, tmp1);
3601     ldr(tmp3, Address(buf, 32));
3602     crc32cx(crc, crc, tmp2);
3603     ldr(tmp0, Address(buf, 40));
3604     crc32cx(crc, crc, tmp3);
3605     ldr(tmp1, Address(buf, 48));
3606     crc32cx(crc, crc, tmp0);
3607     ldr(tmp2, Address(buf, 56));
3608     crc32cx(crc, crc, tmp1);
3609     ldr(tmp3, Address(pre(buf, 64)));
3610 
3611     b(CRC_by64_loop);
3612 
3613     align(CodeEntryAlignment);
3614   BIND(CRC_by64_loop);
3615     subs(len, len, 64);
3616     crc32cx(crc, crc, tmp2);
3617     ldr(tmp0, Address(buf, 8));
3618     crc32cx(crc, crc, tmp3);
3619     ldr(tmp1, Address(buf, 16));
3620     crc32cx(crc, crc, tmp0);
3621     ldr(tmp2, Address(buf, 24));
3622     crc32cx(crc, crc, tmp1);
3623     ldr(tmp3, Address(buf, 32));
3624     crc32cx(crc, crc, tmp2);
3625     ldr(tmp0, Address(buf, 40));
3626     crc32cx(crc, crc, tmp3);
3627     ldr(tmp1, Address(buf, 48));
3628     crc32cx(crc, crc, tmp0);
3629     ldr(tmp2, Address(buf, 56));
3630     crc32cx(crc, crc, tmp1);
3631     ldr(tmp3, Address(pre(buf, 64)));
3632     br(Assembler::GE, CRC_by64_loop);
3633 
3634     // post-loop
3635     crc32cx(crc, crc, tmp2);
3636     crc32cx(crc, crc, tmp3);
3637 
3638     sub(len, len, 64);
3639     add(buf, buf, 8);
3640     cmn(len, 128);
3641     br(Assembler::NE, CRC_less64);
3642   BIND(L_exit);
3643 }
3644 
3645 /**
3646  * @param crc   register containing existing CRC (32-bit)
3647  * @param buf   register pointing to input byte buffer (byte*)
3648  * @param len   register containing number of bytes
3649  * @param table register that will contain address of CRC table
3650  * @param tmp   scratch register
3651  */
3652 void MacroAssembler::kernel_crc32c(Register crc, Register buf, Register len,
3653         Register table0, Register table1, Register table2, Register table3,
3654         Register tmp, Register tmp2, Register tmp3) {
3655   kernel_crc32c_using_crc32c(crc, buf, len, table0, table1, table2, table3);
3656 }
3657 
3658 
3659 SkipIfEqual::SkipIfEqual(
3660     MacroAssembler* masm, const bool* flag_addr, bool value) {
3661   _masm = masm;
3662   uint64_t offset;
3663   _masm-&gt;adrp(rscratch1, ExternalAddress((address)flag_addr), offset);
3664   _masm-&gt;ldrb(rscratch1, Address(rscratch1, offset));
3665   _masm-&gt;cbzw(rscratch1, _label);
3666 }
3667 
3668 SkipIfEqual::~SkipIfEqual() {
3669   _masm-&gt;bind(_label);
3670 }
3671 
3672 void MacroAssembler::addptr(const Address &amp;dst, int32_t src) {
3673   Address adr;
3674   switch(dst.getMode()) {
3675   case Address::base_plus_offset:
3676     // This is the expected mode, although we allow all the other
3677     // forms below.
3678     adr = form_address(rscratch2, dst.base(), dst.offset(), LogBytesPerWord);
3679     break;
3680   default:
3681     lea(rscratch2, dst);
3682     adr = Address(rscratch2);
3683     break;
3684   }
3685   ldr(rscratch1, adr);
3686   add(rscratch1, rscratch1, src);
3687   str(rscratch1, adr);
3688 }
3689 
3690 void MacroAssembler::cmpptr(Register src1, Address src2) {
3691   uint64_t offset;
3692   adrp(rscratch1, src2, offset);
3693   ldr(rscratch1, Address(rscratch1, offset));
3694   cmp(src1, rscratch1);
3695 }
3696 
3697 void MacroAssembler::cmpoop(Register obj1, Register obj2) {
3698   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3699   bs-&gt;obj_equals(this, obj1, obj2);
3700 }
3701 
3702 void MacroAssembler::load_method_holder(Register holder, Register method) {
3703   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
3704   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
3705   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
3706 }
3707 
3708 void MacroAssembler::load_klass(Register dst, Register src) {
3709   if (UseCompressedClassPointers) {
3710     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3711     decode_klass_not_null(dst);
3712   } else {
3713     ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3714   }
3715 }
3716 
3717 // ((OopHandle)result).resolve();
3718 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
3719   // OopHandle::resolve is an indirection.
3720   access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);
3721 }
3722 
3723 void MacroAssembler::load_mirror(Register dst, Register method, Register tmp) {
3724   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
3725   ldr(dst, Address(rmethod, Method::const_offset()));
3726   ldr(dst, Address(dst, ConstMethod::constants_offset()));
3727   ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));
3728   ldr(dst, Address(dst, mirror_offset));
3729   resolve_oop_handle(dst, tmp);
3730 }
3731 
3732 void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {
3733   if (UseCompressedClassPointers) {
3734     ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3735     if (CompressedKlassPointers::base() == NULL) {
3736       cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());
3737       return;
3738     } else if (((uint64_t)CompressedKlassPointers::base() &amp; 0xffffffff) == 0
3739                &amp;&amp; CompressedKlassPointers::shift() == 0) {
3740       // Only the bottom 32 bits matter
3741       cmpw(trial_klass, tmp);
3742       return;
3743     }
3744     decode_klass_not_null(tmp);
3745   } else {
3746     ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3747   }
3748   cmp(trial_klass, tmp);
3749 }
3750 
3751 void MacroAssembler::load_prototype_header(Register dst, Register src) {
3752   load_klass(dst, src);
3753   ldr(dst, Address(dst, Klass::prototype_header_offset()));
3754 }
3755 
3756 void MacroAssembler::store_klass(Register dst, Register src) {
3757   // FIXME: Should this be a store release?  concurrent gcs assumes
3758   // klass length is valid if klass field is not null.
3759   if (UseCompressedClassPointers) {
3760     encode_klass_not_null(src);
3761     strw(src, Address(dst, oopDesc::klass_offset_in_bytes()));
3762   } else {
3763     str(src, Address(dst, oopDesc::klass_offset_in_bytes()));
3764   }
3765 }
3766 
3767 void MacroAssembler::store_klass_gap(Register dst, Register src) {
3768   if (UseCompressedClassPointers) {
3769     // Store to klass gap in destination
3770     strw(src, Address(dst, oopDesc::klass_gap_offset_in_bytes()));
3771   }
3772 }
3773 
3774 // Algorithm must match CompressedOops::encode.
3775 void MacroAssembler::encode_heap_oop(Register d, Register s) {
3776 #ifdef ASSERT
3777   verify_heapbase(&quot;MacroAssembler::encode_heap_oop: heap base corrupted?&quot;);
3778 #endif
3779   verify_oop(s, &quot;broken oop in encode_heap_oop&quot;);
3780   if (CompressedOops::base() == NULL) {
3781     if (CompressedOops::shift() != 0) {
3782       assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3783       lsr(d, s, LogMinObjAlignmentInBytes);
3784     } else {
3785       mov(d, s);
3786     }
3787   } else {
3788     subs(d, s, rheapbase);
3789     csel(d, d, zr, Assembler::HS);
3790     lsr(d, d, LogMinObjAlignmentInBytes);
3791 
3792     /*  Old algorithm: is this any worse?
3793     Label nonnull;
3794     cbnz(r, nonnull);
3795     sub(r, r, rheapbase);
3796     bind(nonnull);
3797     lsr(r, r, LogMinObjAlignmentInBytes);
3798     */
3799   }
3800 }
3801 
3802 void MacroAssembler::encode_heap_oop_not_null(Register r) {
3803 #ifdef ASSERT
3804   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null: heap base corrupted?&quot;);
3805   if (CheckCompressedOops) {
3806     Label ok;
3807     cbnz(r, ok);
3808     stop(&quot;null oop passed to encode_heap_oop_not_null&quot;);
3809     bind(ok);
3810   }
3811 #endif
3812   verify_oop(r, &quot;broken oop in encode_heap_oop_not_null&quot;);
3813   if (CompressedOops::base() != NULL) {
3814     sub(r, r, rheapbase);
3815   }
3816   if (CompressedOops::shift() != 0) {
3817     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3818     lsr(r, r, LogMinObjAlignmentInBytes);
3819   }
3820 }
3821 
3822 void MacroAssembler::encode_heap_oop_not_null(Register dst, Register src) {
3823 #ifdef ASSERT
3824   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null2: heap base corrupted?&quot;);
3825   if (CheckCompressedOops) {
3826     Label ok;
3827     cbnz(src, ok);
3828     stop(&quot;null oop passed to encode_heap_oop_not_null2&quot;);
3829     bind(ok);
3830   }
3831 #endif
3832   verify_oop(src, &quot;broken oop in encode_heap_oop_not_null2&quot;);
3833 
3834   Register data = src;
3835   if (CompressedOops::base() != NULL) {
3836     sub(dst, src, rheapbase);
3837     data = dst;
3838   }
3839   if (CompressedOops::shift() != 0) {
3840     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3841     lsr(dst, data, LogMinObjAlignmentInBytes);
3842     data = dst;
3843   }
3844   if (data == src)
3845     mov(dst, src);
3846 }
3847 
3848 void  MacroAssembler::decode_heap_oop(Register d, Register s) {
3849 #ifdef ASSERT
3850   verify_heapbase(&quot;MacroAssembler::decode_heap_oop: heap base corrupted?&quot;);
3851 #endif
3852   if (CompressedOops::base() == NULL) {
3853     if (CompressedOops::shift() != 0 || d != s) {
3854       lsl(d, s, CompressedOops::shift());
3855     }
3856   } else {
3857     Label done;
3858     if (d != s)
3859       mov(d, s);
3860     cbz(s, done);
3861     add(d, rheapbase, s, Assembler::LSL, LogMinObjAlignmentInBytes);
3862     bind(done);
3863   }
3864   verify_oop(d, &quot;broken oop in decode_heap_oop&quot;);
3865 }
3866 
3867 void  MacroAssembler::decode_heap_oop_not_null(Register r) {
3868   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
3869   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
3870   // Cannot assert, unverified entry point counts instructions (see .ad file)
3871   // vtableStubs also counts instructions in pd_code_size_limit.
3872   // Also do not verify_oop as this is called by verify_oop.
3873   if (CompressedOops::shift() != 0) {
3874     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3875     if (CompressedOops::base() != NULL) {
3876       add(r, rheapbase, r, Assembler::LSL, LogMinObjAlignmentInBytes);
3877     } else {
3878       add(r, zr, r, Assembler::LSL, LogMinObjAlignmentInBytes);
3879     }
3880   } else {
3881     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);
3882   }
3883 }
3884 
3885 void  MacroAssembler::decode_heap_oop_not_null(Register dst, Register src) {
3886   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
3887   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
3888   // Cannot assert, unverified entry point counts instructions (see .ad file)
3889   // vtableStubs also counts instructions in pd_code_size_limit.
3890   // Also do not verify_oop as this is called by verify_oop.
3891   if (CompressedOops::shift() != 0) {
3892     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3893     if (CompressedOops::base() != NULL) {
3894       add(dst, rheapbase, src, Assembler::LSL, LogMinObjAlignmentInBytes);
3895     } else {
3896       add(dst, zr, src, Assembler::LSL, LogMinObjAlignmentInBytes);
3897     }
3898   } else {
3899     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);
3900     if (dst != src) {
3901       mov(dst, src);
3902     }
3903   }
3904 }
3905 
3906 MacroAssembler::KlassDecodeMode MacroAssembler::_klass_decode_mode(KlassDecodeNone);
3907 
3908 MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode() {
3909   assert(UseCompressedClassPointers, &quot;not using compressed class pointers&quot;);
3910   assert(Metaspace::initialized(), &quot;metaspace not initialized yet&quot;);
3911 
3912   if (_klass_decode_mode != KlassDecodeNone) {
3913     return _klass_decode_mode;
3914   }
3915 
3916   assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()
3917          || 0 == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
3918 
3919   if (CompressedKlassPointers::base() == NULL) {
3920     return (_klass_decode_mode = KlassDecodeZero);
3921   }
3922 
3923   if (operand_valid_for_logical_immediate(
3924         /*is32*/false, (uint64_t)CompressedKlassPointers::base())) {
3925     const uint64_t range_mask =
3926       (1UL &lt;&lt; log2_intptr(CompressedKlassPointers::range())) - 1;
3927     if (((uint64_t)CompressedKlassPointers::base() &amp; range_mask) == 0) {
3928       return (_klass_decode_mode = KlassDecodeXor);
3929     }
3930   }
3931 
3932   const uint64_t shifted_base =
3933     (uint64_t)CompressedKlassPointers::base() &gt;&gt; CompressedKlassPointers::shift();
3934   guarantee((shifted_base &amp; 0xffff0000ffffffff) == 0,
3935             &quot;compressed class base bad alignment&quot;);
3936 
3937   return (_klass_decode_mode = KlassDecodeMovk);
3938 }
3939 
3940 void MacroAssembler::encode_klass_not_null(Register dst, Register src) {
3941   switch (klass_decode_mode()) {
3942   case KlassDecodeZero:
3943     if (CompressedKlassPointers::shift() != 0) {
3944       lsr(dst, src, LogKlassAlignmentInBytes);
3945     } else {
3946       if (dst != src) mov(dst, src);
3947     }
3948     break;
3949 
3950   case KlassDecodeXor:
3951     if (CompressedKlassPointers::shift() != 0) {
3952       eor(dst, src, (uint64_t)CompressedKlassPointers::base());
3953       lsr(dst, dst, LogKlassAlignmentInBytes);
3954     } else {
3955       eor(dst, src, (uint64_t)CompressedKlassPointers::base());
3956     }
3957     break;
3958 
3959   case KlassDecodeMovk:
3960     if (CompressedKlassPointers::shift() != 0) {
3961       ubfx(dst, src, LogKlassAlignmentInBytes, 32);
3962     } else {
3963       movw(dst, src);
3964     }
3965     break;
3966 
3967   case KlassDecodeNone:
3968     ShouldNotReachHere();
3969     break;
3970   }
3971 }
3972 
3973 void MacroAssembler::encode_klass_not_null(Register r) {
3974   encode_klass_not_null(r, r);
3975 }
3976 
3977 void  MacroAssembler::decode_klass_not_null(Register dst, Register src) {
3978   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
3979 
3980   switch (klass_decode_mode()) {
3981   case KlassDecodeZero:
3982     if (CompressedKlassPointers::shift() != 0) {
3983       lsl(dst, src, LogKlassAlignmentInBytes);
3984     } else {
3985       if (dst != src) mov(dst, src);
3986     }
3987     break;
3988 
3989   case KlassDecodeXor:
3990     if (CompressedKlassPointers::shift() != 0) {
3991       lsl(dst, src, LogKlassAlignmentInBytes);
3992       eor(dst, dst, (uint64_t)CompressedKlassPointers::base());
3993     } else {
3994       eor(dst, src, (uint64_t)CompressedKlassPointers::base());
3995     }
3996     break;
3997 
3998   case KlassDecodeMovk: {
3999     const uint64_t shifted_base =
4000       (uint64_t)CompressedKlassPointers::base() &gt;&gt; CompressedKlassPointers::shift();
4001 
4002     if (dst != src) movw(dst, src);
4003     movk(dst, shifted_base &gt;&gt; 32, 32);
4004 
4005     if (CompressedKlassPointers::shift() != 0) {
4006       lsl(dst, dst, LogKlassAlignmentInBytes);
4007     }
4008 
4009     break;
4010   }
4011 
4012   case KlassDecodeNone:
4013     ShouldNotReachHere();
4014     break;
4015   }
4016 }
4017 
4018 void  MacroAssembler::decode_klass_not_null(Register r) {
4019   decode_klass_not_null(r, r);
4020 }
4021 
4022 void  MacroAssembler::set_narrow_oop(Register dst, jobject obj) {
4023 #ifdef ASSERT
4024   {
4025     ThreadInVMfromUnknown tiv;
4026     assert (UseCompressedOops, &quot;should only be used for compressed oops&quot;);
4027     assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
4028     assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4029     assert(Universe::heap()-&gt;is_in(JNIHandles::resolve(obj)), &quot;should be real oop&quot;);
4030   }
4031 #endif
4032   int oop_index = oop_recorder()-&gt;find_index(obj);
4033   InstructionMark im(this);
4034   RelocationHolder rspec = oop_Relocation::spec(oop_index);
4035   code_section()-&gt;relocate(inst_mark(), rspec);
4036   movz(dst, 0xDEAD, 16);
4037   movk(dst, 0xBEEF);
4038 }
4039 
4040 void  MacroAssembler::set_narrow_klass(Register dst, Klass* k) {
4041   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
4042   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4043   int index = oop_recorder()-&gt;find_index(k);
4044   assert(! Universe::heap()-&gt;is_in(k), &quot;should not be an oop&quot;);
4045 
4046   InstructionMark im(this);
4047   RelocationHolder rspec = metadata_Relocation::spec(index);
4048   code_section()-&gt;relocate(inst_mark(), rspec);
4049   narrowKlass nk = CompressedKlassPointers::encode(k);
4050   movz(dst, (nk &gt;&gt; 16), 16);
4051   movk(dst, nk &amp; 0xffff);
4052 }
4053 
4054 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators,
4055                                     Register dst, Address src,
4056                                     Register tmp1, Register thread_tmp) {
4057   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4058   decorators = AccessInternal::decorator_fixup(decorators);
4059   bool as_raw = (decorators &amp; AS_RAW) != 0;
4060   if (as_raw) {
4061     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4062   } else {
4063     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4064   }
4065 }
4066 
4067 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,
4068                                      Address dst, Register src,
4069                                      Register tmp1, Register thread_tmp) {
4070   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4071   decorators = AccessInternal::decorator_fixup(decorators);
4072   bool as_raw = (decorators &amp; AS_RAW) != 0;
4073   if (as_raw) {
4074     bs-&gt;BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4075   } else {
4076     bs-&gt;store_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4077   }
4078 }
4079 
4080 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
4081   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
4082   if ((decorators &amp; (ACCESS_READ | ACCESS_WRITE)) == 0) {
4083     decorators |= ACCESS_READ | ACCESS_WRITE;
4084   }
4085   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4086   return bs-&gt;resolve(this, decorators, obj);
4087 }
4088 
4089 void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1,
4090                                    Register thread_tmp, DecoratorSet decorators) {
4091   access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
4092 }
4093 
4094 void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1,
4095                                             Register thread_tmp, DecoratorSet decorators) {
4096   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
4097 }
4098 
4099 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
4100                                     Register thread_tmp, DecoratorSet decorators) {
4101   access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
4102 }
4103 
4104 // Used for storing NULLs.
4105 void MacroAssembler::store_heap_oop_null(Address dst) {
4106   access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);
4107 }
4108 
4109 Address MacroAssembler::allocate_metadata_address(Metadata* obj) {
4110   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
4111   int index = oop_recorder()-&gt;allocate_metadata_index(obj);
4112   RelocationHolder rspec = metadata_Relocation::spec(index);
4113   return Address((address)obj, rspec);
4114 }
4115 
4116 // Move an oop into a register.  immediate is true if we want
4117 // immediate instrcutions, i.e. we are not going to patch this
4118 // instruction while the code is being executed by another thread.  In
4119 // that case we can use move immediates rather than the constant pool.
4120 void MacroAssembler::movoop(Register dst, jobject obj, bool immediate) {
4121   int oop_index;
4122   if (obj == NULL) {
4123     oop_index = oop_recorder()-&gt;allocate_oop_index(obj);
4124   } else {
4125 #ifdef ASSERT
4126     {
4127       ThreadInVMfromUnknown tiv;
4128       assert(Universe::heap()-&gt;is_in(JNIHandles::resolve(obj)), &quot;should be real oop&quot;);
4129     }
4130 #endif
4131     oop_index = oop_recorder()-&gt;find_index(obj);
4132   }
4133   RelocationHolder rspec = oop_Relocation::spec(oop_index);
4134   if (! immediate) {
4135     address dummy = address(uintptr_t(pc()) &amp; -wordSize); // A nearby aligned address
4136     ldr_constant(dst, Address(dummy, rspec));
4137   } else
4138     mov(dst, Address((address)obj, rspec));
4139 }
4140 
4141 // Move a metadata address into a register.
4142 void MacroAssembler::mov_metadata(Register dst, Metadata* obj) {
4143   int oop_index;
4144   if (obj == NULL) {
4145     oop_index = oop_recorder()-&gt;allocate_metadata_index(obj);
4146   } else {
4147     oop_index = oop_recorder()-&gt;find_index(obj);
4148   }
4149   RelocationHolder rspec = metadata_Relocation::spec(oop_index);
4150   mov(dst, Address((address)obj, rspec));
4151 }
4152 
4153 Address MacroAssembler::constant_oop_address(jobject obj) {
4154 #ifdef ASSERT
4155   {
4156     ThreadInVMfromUnknown tiv;
4157     assert(oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4158     assert(Universe::heap()-&gt;is_in(JNIHandles::resolve(obj)), &quot;not an oop&quot;);
4159   }
4160 #endif
4161   int oop_index = oop_recorder()-&gt;find_index(obj);
4162   return Address((address)obj, oop_Relocation::spec(oop_index));
4163 }
4164 
4165 // Defines obj, preserves var_size_in_bytes, okay for t2 == var_size_in_bytes.
4166 void MacroAssembler::tlab_allocate(Register obj,
4167                                    Register var_size_in_bytes,
4168                                    int con_size_in_bytes,
4169                                    Register t1,
4170                                    Register t2,
4171                                    Label&amp; slow_case) {
4172   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4173   bs-&gt;tlab_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);
4174 }
4175 
4176 // Defines obj, preserves var_size_in_bytes
4177 void MacroAssembler::eden_allocate(Register obj,
4178                                    Register var_size_in_bytes,
4179                                    int con_size_in_bytes,
4180                                    Register t1,
4181                                    Label&amp; slow_case) {
4182   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4183   bs-&gt;eden_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);
4184 }
4185 
4186 // Zero words; len is in bytes
4187 // Destroys all registers except addr
4188 // len must be a nonzero multiple of wordSize
4189 void MacroAssembler::zero_memory(Register addr, Register len, Register t1) {
4190   assert_different_registers(addr, len, t1, rscratch1, rscratch2);
4191 
4192 #ifdef ASSERT
4193   { Label L;
4194     tst(len, BytesPerWord - 1);
4195     br(Assembler::EQ, L);
4196     stop(&quot;len is not a multiple of BytesPerWord&quot;);
4197     bind(L);
4198   }
4199 #endif
4200 
4201 #ifndef PRODUCT
4202   block_comment(&quot;zero memory&quot;);
4203 #endif
4204 
4205   Label loop;
4206   Label entry;
4207 
4208 //  Algorithm:
4209 //
4210 //    scratch1 = cnt &amp; 7;
4211 //    cnt -= scratch1;
4212 //    p += scratch1;
4213 //    switch (scratch1) {
4214 //      do {
4215 //        cnt -= 8;
4216 //          p[-8] = 0;
4217 //        case 7:
4218 //          p[-7] = 0;
4219 //        case 6:
4220 //          p[-6] = 0;
4221 //          // ...
4222 //        case 1:
4223 //          p[-1] = 0;
4224 //        case 0:
4225 //          p += 8;
4226 //      } while (cnt);
4227 //    }
4228 
4229   const int unroll = 8; // Number of str(zr) instructions we&#39;ll unroll
4230 
4231   lsr(len, len, LogBytesPerWord);
4232   andr(rscratch1, len, unroll - 1);  // tmp1 = cnt % unroll
4233   sub(len, len, rscratch1);      // cnt -= unroll
4234   // t1 always points to the end of the region we&#39;re about to zero
4235   add(t1, addr, rscratch1, Assembler::LSL, LogBytesPerWord);
4236   adr(rscratch2, entry);
4237   sub(rscratch2, rscratch2, rscratch1, Assembler::LSL, 2);
4238   br(rscratch2);
4239   bind(loop);
4240   sub(len, len, unroll);
4241   for (int i = -unroll; i &lt; 0; i++)
4242     Assembler::str(zr, Address(t1, i * wordSize));
4243   bind(entry);
4244   add(t1, t1, unroll * wordSize);
4245   cbnz(len, loop);
4246 }
4247 
4248 void MacroAssembler::verify_tlab() {
4249 #ifdef ASSERT
4250   if (UseTLAB &amp;&amp; VerifyOops) {
4251     Label next, ok;
4252 
4253     stp(rscratch2, rscratch1, Address(pre(sp, -16)));
4254 
4255     ldr(rscratch2, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));
4256     ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_start_offset())));
4257     cmp(rscratch2, rscratch1);
4258     br(Assembler::HS, next);
4259     STOP(&quot;assert(top &gt;= start)&quot;);
4260     should_not_reach_here();
4261 
4262     bind(next);
4263     ldr(rscratch2, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));
4264     ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));
4265     cmp(rscratch2, rscratch1);
4266     br(Assembler::HS, ok);
4267     STOP(&quot;assert(top &lt;= end)&quot;);
4268     should_not_reach_here();
4269 
4270     bind(ok);
4271     ldp(rscratch2, rscratch1, Address(post(sp, 16)));
4272   }
4273 #endif
4274 }
4275 
4276 // Writes to stack successive pages until offset reached to check for
4277 // stack overflow + shadow pages.  This clobbers tmp.
4278 void MacroAssembler::bang_stack_size(Register size, Register tmp) {
4279   assert_different_registers(tmp, size, rscratch1);
4280   mov(tmp, sp);
4281   // Bang stack for total size given plus shadow page size.
4282   // Bang one page at a time because large size can bang beyond yellow and
4283   // red zones.
4284   Label loop;
4285   mov(rscratch1, os::vm_page_size());
4286   bind(loop);
4287   lea(tmp, Address(tmp, -os::vm_page_size()));
4288   subsw(size, size, rscratch1);
4289   str(size, Address(tmp));
4290   br(Assembler::GT, loop);
4291 
4292   // Bang down shadow pages too.
4293   // At this point, (tmp-0) is the last address touched, so don&#39;t
4294   // touch it again.  (It was touched as (tmp-pagesize) but then tmp
4295   // was post-decremented.)  Skip this address by starting at i=1, and
4296   // touch a few more pages below.  N.B.  It is important to touch all
4297   // the way down to and including i=StackShadowPages.
4298   for (int i = 0; i &lt; (int)(JavaThread::stack_shadow_zone_size() / os::vm_page_size()) - 1; i++) {
4299     // this could be any sized move but this is can be a debugging crumb
4300     // so the bigger the better.
4301     lea(tmp, Address(tmp, -os::vm_page_size()));
4302     str(size, Address(tmp));
4303   }
4304 }
4305 
4306 
4307 // Move the address of the polling page into dest.
4308 void MacroAssembler::get_polling_page(Register dest, address page, relocInfo::relocType rtype) {
4309   if (SafepointMechanism::uses_thread_local_poll()) {
4310     ldr(dest, Address(rthread, Thread::polling_page_offset()));
4311   } else {
4312     uint64_t off;
4313     adrp(dest, Address(page, rtype), off);
4314     assert(off == 0, &quot;polling page must be page aligned&quot;);
4315   }
4316 }
4317 
4318 // Move the address of the polling page into r, then read the polling
4319 // page.
4320 address MacroAssembler::read_polling_page(Register r, address page, relocInfo::relocType rtype) {
4321   get_polling_page(r, page, rtype);
4322   return read_polling_page(r, rtype);
4323 }
4324 
4325 // Read the polling page.  The address of the polling page must
4326 // already be in r.
4327 address MacroAssembler::read_polling_page(Register r, relocInfo::relocType rtype) {
4328   InstructionMark im(this);
4329   code_section()-&gt;relocate(inst_mark(), rtype);
4330   ldrw(zr, Address(r, 0));
4331   return inst_mark();
4332 }
4333 
4334 void MacroAssembler::adrp(Register reg1, const Address &amp;dest, uint64_t &amp;byte_offset) {
4335   relocInfo::relocType rtype = dest.rspec().reloc()-&gt;type();
4336   uint64_t low_page = (uint64_t)CodeCache::low_bound() &gt;&gt; 12;
4337   uint64_t high_page = (uint64_t)(CodeCache::high_bound() - 1) &gt;&gt; 12;
4338   uint64_t dest_page = (uint64_t)dest.target() &gt;&gt; 12;
4339   int64_t offset_low = dest_page - low_page;
4340   int64_t offset_high = dest_page - high_page;
4341 
4342   assert(is_valid_AArch64_address(dest.target()), &quot;bad address&quot;);
4343   assert(dest.getMode() == Address::literal, &quot;ADRP must be applied to a literal address&quot;);
4344 
4345   InstructionMark im(this);
4346   code_section()-&gt;relocate(inst_mark(), dest.rspec());
4347   // 8143067: Ensure that the adrp can reach the dest from anywhere within
4348   // the code cache so that if it is relocated we know it will still reach
4349   if (offset_high &gt;= -(1&lt;&lt;20) &amp;&amp; offset_low &lt; (1&lt;&lt;20)) {
4350     _adrp(reg1, dest.target());
4351   } else {
4352     uint64_t target = (uint64_t)dest.target();
4353     uint64_t adrp_target
4354       = (target &amp; 0xffffffffUL) | ((uint64_t)pc() &amp; 0xffff00000000UL);
4355 
4356     _adrp(reg1, (address)adrp_target);
4357     movk(reg1, target &gt;&gt; 32, 32);
4358   }
4359   byte_offset = (uint64_t)dest.target() &amp; 0xfff;
4360 }
4361 
4362 void MacroAssembler::load_byte_map_base(Register reg) {
4363   CardTable::CardValue* byte_map_base =
4364     ((CardTableBarrierSet*)(BarrierSet::barrier_set()))-&gt;card_table()-&gt;byte_map_base();
4365 
4366   if (is_valid_AArch64_address((address)byte_map_base)) {
4367     // Strictly speaking the byte_map_base isn&#39;t an address at all,
4368     // and it might even be negative.
4369     uint64_t offset;
4370     adrp(reg, ExternalAddress((address)byte_map_base), offset);
4371     // We expect offset to be zero with most collectors.
4372     if (offset != 0) {
4373       add(reg, reg, offset);
4374     }
4375   } else {
4376     mov(reg, (uint64_t)byte_map_base);
4377   }
4378 }
4379 
4380 void MacroAssembler::build_frame(int framesize) {
4381   assert(framesize &gt; 0, &quot;framesize must be &gt; 0&quot;);
4382   if (framesize &lt; ((1 &lt;&lt; 9) + 2 * wordSize)) {
4383     sub(sp, sp, framesize);
4384     stp(rfp, lr, Address(sp, framesize - 2 * wordSize));
4385     if (PreserveFramePointer) add(rfp, sp, framesize - 2 * wordSize);
4386   } else {
4387     stp(rfp, lr, Address(pre(sp, -2 * wordSize)));
4388     if (PreserveFramePointer) mov(rfp, sp);
4389     if (framesize &lt; ((1 &lt;&lt; 12) + 2 * wordSize))
4390       sub(sp, sp, framesize - 2 * wordSize);
4391     else {
4392       mov(rscratch1, framesize - 2 * wordSize);
4393       sub(sp, sp, rscratch1);
4394     }
4395   }
4396 }
4397 
4398 void MacroAssembler::remove_frame(int framesize) {
4399   assert(framesize &gt; 0, &quot;framesize must be &gt; 0&quot;);
4400   if (framesize &lt; ((1 &lt;&lt; 9) + 2 * wordSize)) {
4401     ldp(rfp, lr, Address(sp, framesize - 2 * wordSize));
4402     add(sp, sp, framesize);
4403   } else {
4404     if (framesize &lt; ((1 &lt;&lt; 12) + 2 * wordSize))
4405       add(sp, sp, framesize - 2 * wordSize);
4406     else {
4407       mov(rscratch1, framesize - 2 * wordSize);
4408       add(sp, sp, rscratch1);
4409     }
4410     ldp(rfp, lr, Address(post(sp, 2 * wordSize)));
4411   }
4412 }
4413 
4414 #ifdef COMPILER2
4415 typedef void (MacroAssembler::* chr_insn)(Register Rt, const Address &amp;adr);
4416 
4417 // Search for str1 in str2 and return index or -1
4418 void MacroAssembler::string_indexof(Register str2, Register str1,
4419                                     Register cnt2, Register cnt1,
4420                                     Register tmp1, Register tmp2,
4421                                     Register tmp3, Register tmp4,
4422                                     Register tmp5, Register tmp6,
4423                                     int icnt1, Register result, int ae) {
4424   // NOTE: tmp5, tmp6 can be zr depending on specific method version
4425   Label LINEARSEARCH, LINEARSTUB, LINEAR_MEDIUM, DONE, NOMATCH, MATCH;
4426 
4427   Register ch1 = rscratch1;
4428   Register ch2 = rscratch2;
4429   Register cnt1tmp = tmp1;
4430   Register cnt2tmp = tmp2;
4431   Register cnt1_neg = cnt1;
4432   Register cnt2_neg = cnt2;
4433   Register result_tmp = tmp4;
4434 
4435   bool isL = ae == StrIntrinsicNode::LL;
4436 
4437   bool str1_isL = ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UL;
4438   bool str2_isL = ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::LU;
4439   int str1_chr_shift = str1_isL ? 0:1;
4440   int str2_chr_shift = str2_isL ? 0:1;
4441   int str1_chr_size = str1_isL ? 1:2;
4442   int str2_chr_size = str2_isL ? 1:2;
4443   chr_insn str1_load_1chr = str1_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
4444                                       (chr_insn)&amp;MacroAssembler::ldrh;
4445   chr_insn str2_load_1chr = str2_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
4446                                       (chr_insn)&amp;MacroAssembler::ldrh;
4447   chr_insn load_2chr = isL ? (chr_insn)&amp;MacroAssembler::ldrh : (chr_insn)&amp;MacroAssembler::ldrw;
4448   chr_insn load_4chr = isL ? (chr_insn)&amp;MacroAssembler::ldrw : (chr_insn)&amp;MacroAssembler::ldr;
4449 
4450   // Note, inline_string_indexOf() generates checks:
4451   // if (substr.count &gt; string.count) return -1;
4452   // if (substr.count == 0) return 0;
4453 
4454   // We have two strings, a source string in str2, cnt2 and a pattern string
4455   // in str1, cnt1. Find the 1st occurence of pattern in source or return -1.
4456 
4457   // For larger pattern and source we use a simplified Boyer Moore algorithm.
4458   // With a small pattern and source we use linear scan.
4459 
4460   if (icnt1 == -1) {
4461     sub(result_tmp, cnt2, cnt1);
4462     cmp(cnt1, (u1)8);             // Use Linear Scan if cnt1 &lt; 8 || cnt1 &gt;= 256
4463     br(LT, LINEARSEARCH);
4464     dup(v0, T16B, cnt1); // done in separate FPU pipeline. Almost no penalty
4465     subs(zr, cnt1, 256);
4466     lsr(tmp1, cnt2, 2);
4467     ccmp(cnt1, tmp1, 0b0000, LT); // Source must be 4 * pattern for BM
4468     br(GE, LINEARSTUB);
4469   }
4470 
4471 // The Boyer Moore alogorithm is based on the description here:-
4472 //
4473 // http://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm
4474 //
4475 // This describes and algorithm with 2 shift rules. The &#39;Bad Character&#39; rule
4476 // and the &#39;Good Suffix&#39; rule.
4477 //
4478 // These rules are essentially heuristics for how far we can shift the
4479 // pattern along the search string.
4480 //
4481 // The implementation here uses the &#39;Bad Character&#39; rule only because of the
4482 // complexity of initialisation for the &#39;Good Suffix&#39; rule.
4483 //
4484 // This is also known as the Boyer-Moore-Horspool algorithm:-
4485 //
4486 // http://en.wikipedia.org/wiki/Boyer-Moore-Horspool_algorithm
4487 //
4488 // This particular implementation has few java-specific optimizations.
4489 //
4490 // #define ASIZE 256
4491 //
4492 //    int bm(unsigned char *x, int m, unsigned char *y, int n) {
4493 //       int i, j;
4494 //       unsigned c;
4495 //       unsigned char bc[ASIZE];
4496 //
4497 //       /* Preprocessing */
4498 //       for (i = 0; i &lt; ASIZE; ++i)
4499 //          bc[i] = m;
4500 //       for (i = 0; i &lt; m - 1; ) {
4501 //          c = x[i];
4502 //          ++i;
4503 //          // c &lt; 256 for Latin1 string, so, no need for branch
4504 //          #ifdef PATTERN_STRING_IS_LATIN1
4505 //          bc[c] = m - i;
4506 //          #else
4507 //          if (c &lt; ASIZE) bc[c] = m - i;
4508 //          #endif
4509 //       }
4510 //
4511 //       /* Searching */
4512 //       j = 0;
4513 //       while (j &lt;= n - m) {
4514 //          c = y[i+j];
4515 //          if (x[m-1] == c)
4516 //            for (i = m - 2; i &gt;= 0 &amp;&amp; x[i] == y[i + j]; --i);
4517 //          if (i &lt; 0) return j;
4518 //          // c &lt; 256 for Latin1 string, so, no need for branch
4519 //          #ifdef SOURCE_STRING_IS_LATIN1
4520 //          // LL case: (c&lt; 256) always true. Remove branch
4521 //          j += bc[y[j+m-1]];
4522 //          #endif
4523 //          #ifndef PATTERN_STRING_IS_UTF
4524 //          // UU case: need if (c&lt;ASIZE) check. Skip 1 character if not.
4525 //          if (c &lt; ASIZE)
4526 //            j += bc[y[j+m-1]];
4527 //          else
4528 //            j += 1
4529 //          #endif
4530 //          #ifdef PATTERN_IS_LATIN1_AND_SOURCE_IS_UTF
4531 //          // UL case: need if (c&lt;ASIZE) check. Skip &lt;pattern length&gt; if not.
4532 //          if (c &lt; ASIZE)
4533 //            j += bc[y[j+m-1]];
4534 //          else
4535 //            j += m
4536 //          #endif
4537 //       }
4538 //    }
4539 
4540   if (icnt1 == -1) {
4541     Label BCLOOP, BCSKIP, BMLOOPSTR2, BMLOOPSTR1, BMSKIP, BMADV, BMMATCH,
4542         BMLOOPSTR1_LASTCMP, BMLOOPSTR1_CMP, BMLOOPSTR1_AFTER_LOAD, BM_INIT_LOOP;
4543     Register cnt1end = tmp2;
4544     Register str2end = cnt2;
4545     Register skipch = tmp2;
4546 
4547     // str1 length is &gt;=8, so, we can read at least 1 register for cases when
4548     // UTF-&gt;Latin1 conversion is not needed(8 LL or 4UU) and half register for
4549     // UL case. We&#39;ll re-read last character in inner pre-loop code to have
4550     // single outer pre-loop load
4551     const int firstStep = isL ? 7 : 3;
4552 
4553     const int ASIZE = 256;
4554     const int STORED_BYTES = 32; // amount of bytes stored per instruction
4555     sub(sp, sp, ASIZE);
4556     mov(tmp5, ASIZE/STORED_BYTES); // loop iterations
4557     mov(ch1, sp);
4558     BIND(BM_INIT_LOOP);
4559       stpq(v0, v0, Address(post(ch1, STORED_BYTES)));
4560       subs(tmp5, tmp5, 1);
4561       br(GT, BM_INIT_LOOP);
4562 
4563       sub(cnt1tmp, cnt1, 1);
4564       mov(tmp5, str2);
4565       add(str2end, str2, result_tmp, LSL, str2_chr_shift);
4566       sub(ch2, cnt1, 1);
4567       mov(tmp3, str1);
4568     BIND(BCLOOP);
4569       (this-&gt;*str1_load_1chr)(ch1, Address(post(tmp3, str1_chr_size)));
4570       if (!str1_isL) {
4571         subs(zr, ch1, ASIZE);
4572         br(HS, BCSKIP);
4573       }
4574       strb(ch2, Address(sp, ch1));
4575     BIND(BCSKIP);
4576       subs(ch2, ch2, 1);
4577       br(GT, BCLOOP);
4578 
4579       add(tmp6, str1, cnt1, LSL, str1_chr_shift); // address after str1
4580       if (str1_isL == str2_isL) {
4581         // load last 8 bytes (8LL/4UU symbols)
4582         ldr(tmp6, Address(tmp6, -wordSize));
4583       } else {
4584         ldrw(tmp6, Address(tmp6, -wordSize/2)); // load last 4 bytes(4 symbols)
4585         // convert Latin1 to UTF. We&#39;ll have to wait until load completed, but
4586         // it&#39;s still faster than per-character loads+checks
4587         lsr(tmp3, tmp6, BitsPerByte * (wordSize/2 - str1_chr_size)); // str1[N-1]
4588         ubfx(ch1, tmp6, 8, 8); // str1[N-2]
4589         ubfx(ch2, tmp6, 16, 8); // str1[N-3]
4590         andr(tmp6, tmp6, 0xFF); // str1[N-4]
4591         orr(ch2, ch1, ch2, LSL, 16);
4592         orr(tmp6, tmp6, tmp3, LSL, 48);
4593         orr(tmp6, tmp6, ch2, LSL, 16);
4594       }
4595     BIND(BMLOOPSTR2);
4596       (this-&gt;*str2_load_1chr)(skipch, Address(str2, cnt1tmp, Address::lsl(str2_chr_shift)));
4597       sub(cnt1tmp, cnt1tmp, firstStep); // cnt1tmp is positive here, because cnt1 &gt;= 8
4598       if (str1_isL == str2_isL) {
4599         // re-init tmp3. It&#39;s for free because it&#39;s executed in parallel with
4600         // load above. Alternative is to initialize it before loop, but it&#39;ll
4601         // affect performance on in-order systems with 2 or more ld/st pipelines
4602         lsr(tmp3, tmp6, BitsPerByte * (wordSize - str1_chr_size));
4603       }
4604       if (!isL) { // UU/UL case
4605         lsl(ch2, cnt1tmp, 1); // offset in bytes
4606       }
4607       cmp(tmp3, skipch);
4608       br(NE, BMSKIP);
4609       ldr(ch2, Address(str2, isL ? cnt1tmp : ch2));
4610       mov(ch1, tmp6);
4611       if (isL) {
4612         b(BMLOOPSTR1_AFTER_LOAD);
4613       } else {
4614         sub(cnt1tmp, cnt1tmp, 1); // no need to branch for UU/UL case. cnt1 &gt;= 8
4615         b(BMLOOPSTR1_CMP);
4616       }
4617     BIND(BMLOOPSTR1);
4618       (this-&gt;*str1_load_1chr)(ch1, Address(str1, cnt1tmp, Address::lsl(str1_chr_shift)));
4619       (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt1tmp, Address::lsl(str2_chr_shift)));
4620     BIND(BMLOOPSTR1_AFTER_LOAD);
4621       subs(cnt1tmp, cnt1tmp, 1);
4622       br(LT, BMLOOPSTR1_LASTCMP);
4623     BIND(BMLOOPSTR1_CMP);
4624       cmp(ch1, ch2);
4625       br(EQ, BMLOOPSTR1);
4626     BIND(BMSKIP);
4627       if (!isL) {
4628         // if we&#39;ve met UTF symbol while searching Latin1 pattern, then we can
4629         // skip cnt1 symbols
4630         if (str1_isL != str2_isL) {
4631           mov(result_tmp, cnt1);
4632         } else {
4633           mov(result_tmp, 1);
4634         }
4635         subs(zr, skipch, ASIZE);
4636         br(HS, BMADV);
4637       }
4638       ldrb(result_tmp, Address(sp, skipch)); // load skip distance
4639     BIND(BMADV);
4640       sub(cnt1tmp, cnt1, 1);
4641       add(str2, str2, result_tmp, LSL, str2_chr_shift);
4642       cmp(str2, str2end);
4643       br(LE, BMLOOPSTR2);
4644       add(sp, sp, ASIZE);
4645       b(NOMATCH);
4646     BIND(BMLOOPSTR1_LASTCMP);
4647       cmp(ch1, ch2);
4648       br(NE, BMSKIP);
4649     BIND(BMMATCH);
4650       sub(result, str2, tmp5);
4651       if (!str2_isL) lsr(result, result, 1);
4652       add(sp, sp, ASIZE);
4653       b(DONE);
4654 
4655     BIND(LINEARSTUB);
4656     cmp(cnt1, (u1)16); // small patterns still should be handled by simple algorithm
4657     br(LT, LINEAR_MEDIUM);
4658     mov(result, zr);
4659     RuntimeAddress stub = NULL;
4660     if (isL) {
4661       stub = RuntimeAddress(StubRoutines::aarch64::string_indexof_linear_ll());
4662       assert(stub.target() != NULL, &quot;string_indexof_linear_ll stub has not been generated&quot;);
4663     } else if (str1_isL) {
4664       stub = RuntimeAddress(StubRoutines::aarch64::string_indexof_linear_ul());
4665        assert(stub.target() != NULL, &quot;string_indexof_linear_ul stub has not been generated&quot;);
4666     } else {
4667       stub = RuntimeAddress(StubRoutines::aarch64::string_indexof_linear_uu());
4668       assert(stub.target() != NULL, &quot;string_indexof_linear_uu stub has not been generated&quot;);
4669     }
4670     trampoline_call(stub);
4671     b(DONE);
4672   }
4673 
4674   BIND(LINEARSEARCH);
4675   {
4676     Label DO1, DO2, DO3;
4677 
4678     Register str2tmp = tmp2;
4679     Register first = tmp3;
4680 
4681     if (icnt1 == -1)
4682     {
4683         Label DOSHORT, FIRST_LOOP, STR2_NEXT, STR1_LOOP, STR1_NEXT;
4684 
4685         cmp(cnt1, u1(str1_isL == str2_isL ? 4 : 2));
4686         br(LT, DOSHORT);
4687       BIND(LINEAR_MEDIUM);
4688         (this-&gt;*str1_load_1chr)(first, Address(str1));
4689         lea(str1, Address(str1, cnt1, Address::lsl(str1_chr_shift)));
4690         sub(cnt1_neg, zr, cnt1, LSL, str1_chr_shift);
4691         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4692         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4693 
4694       BIND(FIRST_LOOP);
4695         (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt2_neg));
4696         cmp(first, ch2);
4697         br(EQ, STR1_LOOP);
4698       BIND(STR2_NEXT);
4699         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4700         br(LE, FIRST_LOOP);
4701         b(NOMATCH);
4702 
4703       BIND(STR1_LOOP);
4704         adds(cnt1tmp, cnt1_neg, str1_chr_size);
4705         add(cnt2tmp, cnt2_neg, str2_chr_size);
4706         br(GE, MATCH);
4707 
4708       BIND(STR1_NEXT);
4709         (this-&gt;*str1_load_1chr)(ch1, Address(str1, cnt1tmp));
4710         (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt2tmp));
4711         cmp(ch1, ch2);
4712         br(NE, STR2_NEXT);
4713         adds(cnt1tmp, cnt1tmp, str1_chr_size);
4714         add(cnt2tmp, cnt2tmp, str2_chr_size);
4715         br(LT, STR1_NEXT);
4716         b(MATCH);
4717 
4718       BIND(DOSHORT);
4719       if (str1_isL == str2_isL) {
4720         cmp(cnt1, (u1)2);
4721         br(LT, DO1);
4722         br(GT, DO3);
4723       }
4724     }
4725 
4726     if (icnt1 == 4) {
4727       Label CH1_LOOP;
4728 
4729         (this-&gt;*load_4chr)(ch1, str1);
4730         sub(result_tmp, cnt2, 4);
4731         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4732         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4733 
4734       BIND(CH1_LOOP);
4735         (this-&gt;*load_4chr)(ch2, Address(str2, cnt2_neg));
4736         cmp(ch1, ch2);
4737         br(EQ, MATCH);
4738         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4739         br(LE, CH1_LOOP);
4740         b(NOMATCH);
4741       }
4742 
4743     if ((icnt1 == -1 &amp;&amp; str1_isL == str2_isL) || icnt1 == 2) {
4744       Label CH1_LOOP;
4745 
4746       BIND(DO2);
4747         (this-&gt;*load_2chr)(ch1, str1);
4748         if (icnt1 == 2) {
4749           sub(result_tmp, cnt2, 2);
4750         }
4751         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4752         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4753       BIND(CH1_LOOP);
4754         (this-&gt;*load_2chr)(ch2, Address(str2, cnt2_neg));
4755         cmp(ch1, ch2);
4756         br(EQ, MATCH);
4757         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4758         br(LE, CH1_LOOP);
4759         b(NOMATCH);
4760     }
4761 
4762     if ((icnt1 == -1 &amp;&amp; str1_isL == str2_isL) || icnt1 == 3) {
4763       Label FIRST_LOOP, STR2_NEXT, STR1_LOOP;
4764 
4765       BIND(DO3);
4766         (this-&gt;*load_2chr)(first, str1);
4767         (this-&gt;*str1_load_1chr)(ch1, Address(str1, 2*str1_chr_size));
4768         if (icnt1 == 3) {
4769           sub(result_tmp, cnt2, 3);
4770         }
4771         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4772         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4773       BIND(FIRST_LOOP);
4774         (this-&gt;*load_2chr)(ch2, Address(str2, cnt2_neg));
4775         cmpw(first, ch2);
4776         br(EQ, STR1_LOOP);
4777       BIND(STR2_NEXT);
4778         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4779         br(LE, FIRST_LOOP);
4780         b(NOMATCH);
4781 
4782       BIND(STR1_LOOP);
4783         add(cnt2tmp, cnt2_neg, 2*str2_chr_size);
4784         (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt2tmp));
4785         cmp(ch1, ch2);
4786         br(NE, STR2_NEXT);
4787         b(MATCH);
4788     }
4789 
4790     if (icnt1 == -1 || icnt1 == 1) {
4791       Label CH1_LOOP, HAS_ZERO, DO1_SHORT, DO1_LOOP;
4792 
4793       BIND(DO1);
4794         (this-&gt;*str1_load_1chr)(ch1, str1);
4795         cmp(cnt2, (u1)8);
4796         br(LT, DO1_SHORT);
4797 
4798         sub(result_tmp, cnt2, 8/str2_chr_size);
4799         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4800         mov(tmp3, (uint64_t)(str2_isL ? 0x0101010101010101 : 0x0001000100010001));
4801         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4802 
4803         if (str2_isL) {
4804           orr(ch1, ch1, ch1, LSL, 8);
4805         }
4806         orr(ch1, ch1, ch1, LSL, 16);
4807         orr(ch1, ch1, ch1, LSL, 32);
4808       BIND(CH1_LOOP);
4809         ldr(ch2, Address(str2, cnt2_neg));
4810         eor(ch2, ch1, ch2);
4811         sub(tmp1, ch2, tmp3);
4812         orr(tmp2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4813         bics(tmp1, tmp1, tmp2);
4814         br(NE, HAS_ZERO);
4815         adds(cnt2_neg, cnt2_neg, 8);
4816         br(LT, CH1_LOOP);
4817 
4818         cmp(cnt2_neg, (u1)8);
4819         mov(cnt2_neg, 0);
4820         br(LT, CH1_LOOP);
4821         b(NOMATCH);
4822 
4823       BIND(HAS_ZERO);
4824         rev(tmp1, tmp1);
4825         clz(tmp1, tmp1);
4826         add(cnt2_neg, cnt2_neg, tmp1, LSR, 3);
4827         b(MATCH);
4828 
4829       BIND(DO1_SHORT);
4830         mov(result_tmp, cnt2);
4831         lea(str2, Address(str2, cnt2, Address::lsl(str2_chr_shift)));
4832         sub(cnt2_neg, zr, cnt2, LSL, str2_chr_shift);
4833       BIND(DO1_LOOP);
4834         (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt2_neg));
4835         cmpw(ch1, ch2);
4836         br(EQ, MATCH);
4837         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4838         br(LT, DO1_LOOP);
4839     }
4840   }
4841   BIND(NOMATCH);
4842     mov(result, -1);
4843     b(DONE);
4844   BIND(MATCH);
4845     add(result, result_tmp, cnt2_neg, ASR, str2_chr_shift);
4846   BIND(DONE);
4847 }
4848 
4849 typedef void (MacroAssembler::* chr_insn)(Register Rt, const Address &amp;adr);
4850 typedef void (MacroAssembler::* uxt_insn)(Register Rd, Register Rn);
4851 
4852 void MacroAssembler::string_indexof_char(Register str1, Register cnt1,
4853                                          Register ch, Register result,
4854                                          Register tmp1, Register tmp2, Register tmp3)
4855 {
4856   Label CH1_LOOP, HAS_ZERO, DO1_SHORT, DO1_LOOP, MATCH, NOMATCH, DONE;
4857   Register cnt1_neg = cnt1;
4858   Register ch1 = rscratch1;
4859   Register result_tmp = rscratch2;
4860 
4861   cmp(cnt1, (u1)4);
4862   br(LT, DO1_SHORT);
4863 
4864   orr(ch, ch, ch, LSL, 16);
4865   orr(ch, ch, ch, LSL, 32);
4866 
4867   sub(cnt1, cnt1, 4);
4868   mov(result_tmp, cnt1);
4869   lea(str1, Address(str1, cnt1, Address::uxtw(1)));
4870   sub(cnt1_neg, zr, cnt1, LSL, 1);
4871 
4872   mov(tmp3, (uint64_t)0x0001000100010001);
4873 
4874   BIND(CH1_LOOP);
4875     ldr(ch1, Address(str1, cnt1_neg));
4876     eor(ch1, ch, ch1);
4877     sub(tmp1, ch1, tmp3);
4878     orr(tmp2, ch1, 0x7fff7fff7fff7fff);
4879     bics(tmp1, tmp1, tmp2);
4880     br(NE, HAS_ZERO);
4881     adds(cnt1_neg, cnt1_neg, 8);
4882     br(LT, CH1_LOOP);
4883 
4884     cmp(cnt1_neg, (u1)8);
4885     mov(cnt1_neg, 0);
4886     br(LT, CH1_LOOP);
4887     b(NOMATCH);
4888 
4889   BIND(HAS_ZERO);
4890     rev(tmp1, tmp1);
4891     clz(tmp1, tmp1);
4892     add(cnt1_neg, cnt1_neg, tmp1, LSR, 3);
4893     b(MATCH);
4894 
4895   BIND(DO1_SHORT);
4896     mov(result_tmp, cnt1);
4897     lea(str1, Address(str1, cnt1, Address::uxtw(1)));
4898     sub(cnt1_neg, zr, cnt1, LSL, 1);
4899   BIND(DO1_LOOP);
4900     ldrh(ch1, Address(str1, cnt1_neg));
4901     cmpw(ch, ch1);
4902     br(EQ, MATCH);
4903     adds(cnt1_neg, cnt1_neg, 2);
4904     br(LT, DO1_LOOP);
4905   BIND(NOMATCH);
4906     mov(result, -1);
4907     b(DONE);
4908   BIND(MATCH);
4909     add(result, result_tmp, cnt1_neg, ASR, 1);
4910   BIND(DONE);
4911 }
4912 
4913 // Compare strings.
4914 void MacroAssembler::string_compare(Register str1, Register str2,
4915     Register cnt1, Register cnt2, Register result, Register tmp1, Register tmp2,
4916     FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3, int ae) {
4917   Label DONE, SHORT_LOOP, SHORT_STRING, SHORT_LAST, TAIL, STUB,
4918       DIFF, NEXT_WORD, SHORT_LOOP_TAIL, SHORT_LAST2, SHORT_LAST_INIT,
4919       SHORT_LOOP_START, TAIL_CHECK;
4920 
4921   bool isLL = ae == StrIntrinsicNode::LL;
4922   bool isLU = ae == StrIntrinsicNode::LU;
4923   bool isUL = ae == StrIntrinsicNode::UL;
4924 
4925   // The stub threshold for LL strings is: 72 (64 + 8) chars
4926   // UU: 36 chars, or 72 bytes (valid for the 64-byte large loop with prefetch)
4927   // LU/UL: 24 chars, or 48 bytes (valid for the 16-character loop at least)
4928   const u1 stub_threshold = isLL ? 72 : ((isLU || isUL) ? 24 : 36);
4929 
4930   bool str1_isL = isLL || isLU;
4931   bool str2_isL = isLL || isUL;
4932 
4933   int str1_chr_shift = str1_isL ? 0 : 1;
4934   int str2_chr_shift = str2_isL ? 0 : 1;
4935   int str1_chr_size = str1_isL ? 1 : 2;
4936   int str2_chr_size = str2_isL ? 1 : 2;
4937   int minCharsInWord = isLL ? wordSize : wordSize/2;
4938 
4939   FloatRegister vtmpZ = vtmp1, vtmp = vtmp2;
4940   chr_insn str1_load_chr = str1_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
4941                                       (chr_insn)&amp;MacroAssembler::ldrh;
4942   chr_insn str2_load_chr = str2_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
4943                                       (chr_insn)&amp;MacroAssembler::ldrh;
4944   uxt_insn ext_chr = isLL ? (uxt_insn)&amp;MacroAssembler::uxtbw :
4945                             (uxt_insn)&amp;MacroAssembler::uxthw;
4946 
4947   BLOCK_COMMENT(&quot;string_compare {&quot;);
4948 
4949   // Bizzarely, the counts are passed in bytes, regardless of whether they
4950   // are L or U strings, however the result is always in characters.
4951   if (!str1_isL) asrw(cnt1, cnt1, 1);
4952   if (!str2_isL) asrw(cnt2, cnt2, 1);
4953 
4954   // Compute the minimum of the string lengths and save the difference.
4955   subsw(result, cnt1, cnt2);
4956   cselw(cnt2, cnt1, cnt2, Assembler::LE); // min
4957 
4958   // A very short string
4959   cmpw(cnt2, minCharsInWord);
4960   br(Assembler::LE, SHORT_STRING);
4961 
4962   // Compare longwords
4963   // load first parts of strings and finish initialization while loading
4964   {
4965     if (str1_isL == str2_isL) { // LL or UU
4966       ldr(tmp1, Address(str1));
4967       cmp(str1, str2);
4968       br(Assembler::EQ, DONE);
4969       ldr(tmp2, Address(str2));
4970       cmp(cnt2, stub_threshold);
4971       br(GE, STUB);
4972       subsw(cnt2, cnt2, minCharsInWord);
4973       br(EQ, TAIL_CHECK);
4974       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
4975       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
4976       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
4977     } else if (isLU) {
4978       ldrs(vtmp, Address(str1));
4979       cmp(str1, str2);
4980       br(Assembler::EQ, DONE);
4981       ldr(tmp2, Address(str2));
4982       cmp(cnt2, stub_threshold);
4983       br(GE, STUB);
4984       subw(cnt2, cnt2, 4);
4985       eor(vtmpZ, T16B, vtmpZ, vtmpZ);
4986       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
4987       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
4988       zip1(vtmp, T8B, vtmp, vtmpZ);
4989       sub(cnt1, zr, cnt2, LSL, str1_chr_shift);
4990       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
4991       add(cnt1, cnt1, 4);
4992       fmovd(tmp1, vtmp);
4993     } else { // UL case
4994       ldr(tmp1, Address(str1));
4995       cmp(str1, str2);
4996       br(Assembler::EQ, DONE);
4997       ldrs(vtmp, Address(str2));
4998       cmp(cnt2, stub_threshold);
4999       br(GE, STUB);
5000       subw(cnt2, cnt2, 4);
5001       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
5002       eor(vtmpZ, T16B, vtmpZ, vtmpZ);
5003       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
5004       sub(cnt1, zr, cnt2, LSL, str1_chr_shift);
5005       zip1(vtmp, T8B, vtmp, vtmpZ);
5006       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
5007       add(cnt1, cnt1, 8);
5008       fmovd(tmp2, vtmp);
5009     }
5010     adds(cnt2, cnt2, isUL ? 4 : 8);
5011     br(GE, TAIL);
5012     eor(rscratch2, tmp1, tmp2);
5013     cbnz(rscratch2, DIFF);
5014     // main loop
5015     bind(NEXT_WORD);
5016     if (str1_isL == str2_isL) {
5017       ldr(tmp1, Address(str1, cnt2));
5018       ldr(tmp2, Address(str2, cnt2));
5019       adds(cnt2, cnt2, 8);
5020     } else if (isLU) {
5021       ldrs(vtmp, Address(str1, cnt1));
5022       ldr(tmp2, Address(str2, cnt2));
5023       add(cnt1, cnt1, 4);
5024       zip1(vtmp, T8B, vtmp, vtmpZ);
5025       fmovd(tmp1, vtmp);
5026       adds(cnt2, cnt2, 8);
5027     } else { // UL
5028       ldrs(vtmp, Address(str2, cnt2));
5029       ldr(tmp1, Address(str1, cnt1));
5030       zip1(vtmp, T8B, vtmp, vtmpZ);
5031       add(cnt1, cnt1, 8);
5032       fmovd(tmp2, vtmp);
5033       adds(cnt2, cnt2, 4);
5034     }
5035     br(GE, TAIL);
5036 
5037     eor(rscratch2, tmp1, tmp2);
5038     cbz(rscratch2, NEXT_WORD);
5039     b(DIFF);
5040     bind(TAIL);
5041     eor(rscratch2, tmp1, tmp2);
5042     cbnz(rscratch2, DIFF);
5043     // Last longword.  In the case where length == 4 we compare the
5044     // same longword twice, but that&#39;s still faster than another
5045     // conditional branch.
5046     if (str1_isL == str2_isL) {
5047       ldr(tmp1, Address(str1));
5048       ldr(tmp2, Address(str2));
5049     } else if (isLU) {
5050       ldrs(vtmp, Address(str1));
5051       ldr(tmp2, Address(str2));
5052       zip1(vtmp, T8B, vtmp, vtmpZ);
5053       fmovd(tmp1, vtmp);
5054     } else { // UL
5055       ldrs(vtmp, Address(str2));
5056       ldr(tmp1, Address(str1));
5057       zip1(vtmp, T8B, vtmp, vtmpZ);
5058       fmovd(tmp2, vtmp);
5059     }
5060     bind(TAIL_CHECK);
5061     eor(rscratch2, tmp1, tmp2);
5062     cbz(rscratch2, DONE);
5063 
5064     // Find the first different characters in the longwords and
5065     // compute their difference.
5066     bind(DIFF);
5067     rev(rscratch2, rscratch2);
5068     clz(rscratch2, rscratch2);
5069     andr(rscratch2, rscratch2, isLL ? -8 : -16);
5070     lsrv(tmp1, tmp1, rscratch2);
5071     (this-&gt;*ext_chr)(tmp1, tmp1);
5072     lsrv(tmp2, tmp2, rscratch2);
5073     (this-&gt;*ext_chr)(tmp2, tmp2);
5074     subw(result, tmp1, tmp2);
5075     b(DONE);
5076   }
5077 
5078   bind(STUB);
5079     RuntimeAddress stub = NULL;
5080     switch(ae) {
5081       case StrIntrinsicNode::LL:
5082         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_LL());
5083         break;
5084       case StrIntrinsicNode::UU:
5085         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_UU());
5086         break;
5087       case StrIntrinsicNode::LU:
5088         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_LU());
5089         break;
5090       case StrIntrinsicNode::UL:
5091         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_UL());
5092         break;
5093       default:
5094         ShouldNotReachHere();
5095      }
5096     assert(stub.target() != NULL, &quot;compare_long_string stub has not been generated&quot;);
5097     trampoline_call(stub);
5098     b(DONE);
5099 
5100   bind(SHORT_STRING);
5101   // Is the minimum length zero?
5102   cbz(cnt2, DONE);
5103   // arrange code to do most branches while loading and loading next characters
5104   // while comparing previous
5105   (this-&gt;*str1_load_chr)(tmp1, Address(post(str1, str1_chr_size)));
5106   subs(cnt2, cnt2, 1);
5107   br(EQ, SHORT_LAST_INIT);
5108   (this-&gt;*str2_load_chr)(cnt1, Address(post(str2, str2_chr_size)));
5109   b(SHORT_LOOP_START);
5110   bind(SHORT_LOOP);
5111   subs(cnt2, cnt2, 1);
5112   br(EQ, SHORT_LAST);
5113   bind(SHORT_LOOP_START);
5114   (this-&gt;*str1_load_chr)(tmp2, Address(post(str1, str1_chr_size)));
5115   (this-&gt;*str2_load_chr)(rscratch1, Address(post(str2, str2_chr_size)));
5116   cmp(tmp1, cnt1);
5117   br(NE, SHORT_LOOP_TAIL);
5118   subs(cnt2, cnt2, 1);
5119   br(EQ, SHORT_LAST2);
5120   (this-&gt;*str1_load_chr)(tmp1, Address(post(str1, str1_chr_size)));
5121   (this-&gt;*str2_load_chr)(cnt1, Address(post(str2, str2_chr_size)));
5122   cmp(tmp2, rscratch1);
5123   br(EQ, SHORT_LOOP);
5124   sub(result, tmp2, rscratch1);
5125   b(DONE);
5126   bind(SHORT_LOOP_TAIL);
5127   sub(result, tmp1, cnt1);
5128   b(DONE);
5129   bind(SHORT_LAST2);
5130   cmp(tmp2, rscratch1);
5131   br(EQ, DONE);
5132   sub(result, tmp2, rscratch1);
5133 
5134   b(DONE);
5135   bind(SHORT_LAST_INIT);
5136   (this-&gt;*str2_load_chr)(cnt1, Address(post(str2, str2_chr_size)));
5137   bind(SHORT_LAST);
5138   cmp(tmp1, cnt1);
5139   br(EQ, DONE);
5140   sub(result, tmp1, cnt1);
5141 
5142   bind(DONE);
5143 
5144   BLOCK_COMMENT(&quot;} string_compare&quot;);
5145 }
5146 #endif // COMPILER2
5147 
5148 // This method checks if provided byte array contains byte with highest bit set.
5149 void MacroAssembler::has_negatives(Register ary1, Register len, Register result) {
5150     // Simple and most common case of aligned small array which is not at the
5151     // end of memory page is placed here. All other cases are in stub.
5152     Label LOOP, END, STUB, STUB_LONG, SET_RESULT, DONE;
5153     const uint64_t UPPER_BIT_MASK=0x8080808080808080;
5154     assert_different_registers(ary1, len, result);
5155 
5156     cmpw(len, 0);
5157     br(LE, SET_RESULT);
5158     cmpw(len, 4 * wordSize);
5159     br(GE, STUB_LONG); // size &gt; 32 then go to stub
5160 
5161     int shift = 64 - exact_log2(os::vm_page_size());
5162     lsl(rscratch1, ary1, shift);
5163     mov(rscratch2, (size_t)(4 * wordSize) &lt;&lt; shift);
5164     adds(rscratch2, rscratch1, rscratch2);  // At end of page?
5165     br(CS, STUB); // at the end of page then go to stub
5166     subs(len, len, wordSize);
5167     br(LT, END);
5168 
5169   BIND(LOOP);
5170     ldr(rscratch1, Address(post(ary1, wordSize)));
5171     tst(rscratch1, UPPER_BIT_MASK);
5172     br(NE, SET_RESULT);
5173     subs(len, len, wordSize);
5174     br(GE, LOOP);
5175     cmpw(len, -wordSize);
5176     br(EQ, SET_RESULT);
5177 
5178   BIND(END);
5179     ldr(result, Address(ary1));
5180     sub(len, zr, len, LSL, 3); // LSL 3 is to get bits from bytes
5181     lslv(result, result, len);
5182     tst(result, UPPER_BIT_MASK);
5183     b(SET_RESULT);
5184 
5185   BIND(STUB);
5186     RuntimeAddress has_neg =  RuntimeAddress(StubRoutines::aarch64::has_negatives());
5187     assert(has_neg.target() != NULL, &quot;has_negatives stub has not been generated&quot;);
5188     trampoline_call(has_neg);
5189     b(DONE);
5190 
5191   BIND(STUB_LONG);
5192     RuntimeAddress has_neg_long =  RuntimeAddress(
5193             StubRoutines::aarch64::has_negatives_long());
5194     assert(has_neg_long.target() != NULL, &quot;has_negatives stub has not been generated&quot;);
5195     trampoline_call(has_neg_long);
5196     b(DONE);
5197 
5198   BIND(SET_RESULT);
5199     cset(result, NE); // set true or false
5200 
5201   BIND(DONE);
5202 }
5203 
5204 void MacroAssembler::arrays_equals(Register a1, Register a2, Register tmp3,
5205                                    Register tmp4, Register tmp5, Register result,
5206                                    Register cnt1, int elem_size) {
5207   Label DONE, SAME;
5208   Register tmp1 = rscratch1;
5209   Register tmp2 = rscratch2;
5210   Register cnt2 = tmp2;  // cnt2 only used in array length compare
5211   int elem_per_word = wordSize/elem_size;
5212   int log_elem_size = exact_log2(elem_size);
5213   int length_offset = arrayOopDesc::length_offset_in_bytes();
5214   int base_offset
5215     = arrayOopDesc::base_offset_in_bytes(elem_size == 2 ? T_CHAR : T_BYTE);
5216   int stubBytesThreshold = 3 * 64 + (UseSIMDForArrayEquals ? 0 : 16);
5217 
5218   assert(elem_size == 1 || elem_size == 2, &quot;must be char or byte&quot;);
5219   assert_different_registers(a1, a2, result, cnt1, rscratch1, rscratch2);
5220 
5221 #ifndef PRODUCT
5222   {
5223     const char kind = (elem_size == 2) ? &#39;U&#39; : &#39;L&#39;;
5224     char comment[64];
5225     snprintf(comment, sizeof comment, &quot;array_equals%c{&quot;, kind);
5226     BLOCK_COMMENT(comment);
5227   }
5228 #endif
5229 
5230   // if (a1 == a2)
5231   //     return true;
5232   cmpoop(a1, a2); // May have read barriers for a1 and a2.
5233   br(EQ, SAME);
5234 
5235   if (UseSimpleArrayEquals) {
5236     Label NEXT_WORD, SHORT, TAIL03, TAIL01, A_MIGHT_BE_NULL, A_IS_NOT_NULL;
5237     // if (a1 == null || a2 == null)
5238     //     return false;
5239     // a1 &amp; a2 == 0 means (some-pointer is null) or
5240     // (very-rare-or-even-probably-impossible-pointer-values)
5241     // so, we can save one branch in most cases
5242     tst(a1, a2);
5243     mov(result, false);
5244     br(EQ, A_MIGHT_BE_NULL);
5245     // if (a1.length != a2.length)
5246     //      return false;
5247     bind(A_IS_NOT_NULL);
5248     ldrw(cnt1, Address(a1, length_offset));
5249     ldrw(cnt2, Address(a2, length_offset));
5250     eorw(tmp5, cnt1, cnt2);
5251     cbnzw(tmp5, DONE);
5252     lea(a1, Address(a1, base_offset));
5253     lea(a2, Address(a2, base_offset));
5254     // Check for short strings, i.e. smaller than wordSize.
5255     subs(cnt1, cnt1, elem_per_word);
5256     br(Assembler::LT, SHORT);
5257     // Main 8 byte comparison loop.
5258     bind(NEXT_WORD); {
5259       ldr(tmp1, Address(post(a1, wordSize)));
5260       ldr(tmp2, Address(post(a2, wordSize)));
5261       subs(cnt1, cnt1, elem_per_word);
5262       eor(tmp5, tmp1, tmp2);
5263       cbnz(tmp5, DONE);
5264     } br(GT, NEXT_WORD);
5265     // Last longword.  In the case where length == 4 we compare the
5266     // same longword twice, but that&#39;s still faster than another
5267     // conditional branch.
5268     // cnt1 could be 0, -1, -2, -3, -4 for chars; -4 only happens when
5269     // length == 4.
5270     if (log_elem_size &gt; 0)
5271       lsl(cnt1, cnt1, log_elem_size);
5272     ldr(tmp3, Address(a1, cnt1));
5273     ldr(tmp4, Address(a2, cnt1));
5274     eor(tmp5, tmp3, tmp4);
5275     cbnz(tmp5, DONE);
5276     b(SAME);
5277     bind(A_MIGHT_BE_NULL);
5278     // in case both a1 and a2 are not-null, proceed with loads
5279     cbz(a1, DONE);
5280     cbz(a2, DONE);
5281     b(A_IS_NOT_NULL);
5282     bind(SHORT);
5283 
5284     tbz(cnt1, 2 - log_elem_size, TAIL03); // 0-7 bytes left.
5285     {
5286       ldrw(tmp1, Address(post(a1, 4)));
5287       ldrw(tmp2, Address(post(a2, 4)));
5288       eorw(tmp5, tmp1, tmp2);
5289       cbnzw(tmp5, DONE);
5290     }
5291     bind(TAIL03);
5292     tbz(cnt1, 1 - log_elem_size, TAIL01); // 0-3 bytes left.
5293     {
5294       ldrh(tmp3, Address(post(a1, 2)));
5295       ldrh(tmp4, Address(post(a2, 2)));
5296       eorw(tmp5, tmp3, tmp4);
5297       cbnzw(tmp5, DONE);
5298     }
5299     bind(TAIL01);
5300     if (elem_size == 1) { // Only needed when comparing byte arrays.
5301       tbz(cnt1, 0, SAME); // 0-1 bytes left.
5302       {
5303         ldrb(tmp1, a1);
5304         ldrb(tmp2, a2);
5305         eorw(tmp5, tmp1, tmp2);
5306         cbnzw(tmp5, DONE);
5307       }
5308     }
5309   } else {
5310     Label NEXT_DWORD, SHORT, TAIL, TAIL2, STUB, EARLY_OUT,
5311         CSET_EQ, LAST_CHECK;
5312     mov(result, false);
5313     cbz(a1, DONE);
5314     ldrw(cnt1, Address(a1, length_offset));
5315     cbz(a2, DONE);
5316     ldrw(cnt2, Address(a2, length_offset));
5317     // on most CPUs a2 is still &quot;locked&quot;(surprisingly) in ldrw and it&#39;s
5318     // faster to perform another branch before comparing a1 and a2
5319     cmp(cnt1, (u1)elem_per_word);
5320     br(LE, SHORT); // short or same
5321     ldr(tmp3, Address(pre(a1, base_offset)));
5322     subs(zr, cnt1, stubBytesThreshold);
5323     br(GE, STUB);
5324     ldr(tmp4, Address(pre(a2, base_offset)));
5325     sub(tmp5, zr, cnt1, LSL, 3 + log_elem_size);
5326     cmp(cnt2, cnt1);
5327     br(NE, DONE);
5328 
5329     // Main 16 byte comparison loop with 2 exits
5330     bind(NEXT_DWORD); {
5331       ldr(tmp1, Address(pre(a1, wordSize)));
5332       ldr(tmp2, Address(pre(a2, wordSize)));
5333       subs(cnt1, cnt1, 2 * elem_per_word);
5334       br(LE, TAIL);
5335       eor(tmp4, tmp3, tmp4);
5336       cbnz(tmp4, DONE);
5337       ldr(tmp3, Address(pre(a1, wordSize)));
5338       ldr(tmp4, Address(pre(a2, wordSize)));
5339       cmp(cnt1, (u1)elem_per_word);
5340       br(LE, TAIL2);
5341       cmp(tmp1, tmp2);
5342     } br(EQ, NEXT_DWORD);
5343     b(DONE);
5344 
5345     bind(TAIL);
5346     eor(tmp4, tmp3, tmp4);
5347     eor(tmp2, tmp1, tmp2);
5348     lslv(tmp2, tmp2, tmp5);
5349     orr(tmp5, tmp4, tmp2);
5350     cmp(tmp5, zr);
5351     b(CSET_EQ);
5352 
5353     bind(TAIL2);
5354     eor(tmp2, tmp1, tmp2);
5355     cbnz(tmp2, DONE);
5356     b(LAST_CHECK);
5357 
5358     bind(STUB);
5359     ldr(tmp4, Address(pre(a2, base_offset)));
5360     cmp(cnt2, cnt1);
5361     br(NE, DONE);
5362     if (elem_size == 2) { // convert to byte counter
5363       lsl(cnt1, cnt1, 1);
5364     }
5365     eor(tmp5, tmp3, tmp4);
5366     cbnz(tmp5, DONE);
5367     RuntimeAddress stub = RuntimeAddress(StubRoutines::aarch64::large_array_equals());
5368     assert(stub.target() != NULL, &quot;array_equals_long stub has not been generated&quot;);
5369     trampoline_call(stub);
5370     b(DONE);
5371 
5372     bind(EARLY_OUT);
5373     // (a1 != null &amp;&amp; a2 == null) || (a1 != null &amp;&amp; a2 != null &amp;&amp; a1 == a2)
5374     // so, if a2 == null =&gt; return false(0), else return true, so we can return a2
5375     mov(result, a2);
5376     b(DONE);
5377     bind(SHORT);
5378     cmp(cnt2, cnt1);
5379     br(NE, DONE);
5380     cbz(cnt1, SAME);
5381     sub(tmp5, zr, cnt1, LSL, 3 + log_elem_size);
5382     ldr(tmp3, Address(a1, base_offset));
5383     ldr(tmp4, Address(a2, base_offset));
5384     bind(LAST_CHECK);
5385     eor(tmp4, tmp3, tmp4);
5386     lslv(tmp5, tmp4, tmp5);
5387     cmp(tmp5, zr);
5388     bind(CSET_EQ);
5389     cset(result, EQ);
5390     b(DONE);
5391   }
5392 
5393   bind(SAME);
5394   mov(result, true);
5395   // That&#39;s it.
5396   bind(DONE);
5397 
5398   BLOCK_COMMENT(&quot;} array_equals&quot;);
5399 }
5400 
5401 // Compare Strings
5402 
5403 // For Strings we&#39;re passed the address of the first characters in a1
5404 // and a2 and the length in cnt1.
5405 // elem_size is the element size in bytes: either 1 or 2.
5406 // There are two implementations.  For arrays &gt;= 8 bytes, all
5407 // comparisons (including the final one, which may overlap) are
5408 // performed 8 bytes at a time.  For strings &lt; 8 bytes, we compare a
5409 // halfword, then a short, and then a byte.
5410 
5411 void MacroAssembler::string_equals(Register a1, Register a2,
5412                                    Register result, Register cnt1, int elem_size)
5413 {
5414   Label SAME, DONE, SHORT, NEXT_WORD;
5415   Register tmp1 = rscratch1;
5416   Register tmp2 = rscratch2;
5417   Register cnt2 = tmp2;  // cnt2 only used in array length compare
5418 
5419   assert(elem_size == 1 || elem_size == 2, &quot;must be 2 or 1 byte&quot;);
5420   assert_different_registers(a1, a2, result, cnt1, rscratch1, rscratch2);
5421 
5422 #ifndef PRODUCT
5423   {
5424     const char kind = (elem_size == 2) ? &#39;U&#39; : &#39;L&#39;;
5425     char comment[64];
5426     snprintf(comment, sizeof comment, &quot;{string_equals%c&quot;, kind);
5427     BLOCK_COMMENT(comment);
5428   }
5429 #endif
5430 
5431   mov(result, false);
5432 
5433   // Check for short strings, i.e. smaller than wordSize.
5434   subs(cnt1, cnt1, wordSize);
5435   br(Assembler::LT, SHORT);
5436   // Main 8 byte comparison loop.
5437   bind(NEXT_WORD); {
5438     ldr(tmp1, Address(post(a1, wordSize)));
5439     ldr(tmp2, Address(post(a2, wordSize)));
5440     subs(cnt1, cnt1, wordSize);
5441     eor(tmp1, tmp1, tmp2);
5442     cbnz(tmp1, DONE);
5443   } br(GT, NEXT_WORD);
5444   // Last longword.  In the case where length == 4 we compare the
5445   // same longword twice, but that&#39;s still faster than another
5446   // conditional branch.
5447   // cnt1 could be 0, -1, -2, -3, -4 for chars; -4 only happens when
5448   // length == 4.
5449   ldr(tmp1, Address(a1, cnt1));
5450   ldr(tmp2, Address(a2, cnt1));
5451   eor(tmp2, tmp1, tmp2);
5452   cbnz(tmp2, DONE);
5453   b(SAME);
5454 
5455   bind(SHORT);
5456   Label TAIL03, TAIL01;
5457 
5458   tbz(cnt1, 2, TAIL03); // 0-7 bytes left.
5459   {
5460     ldrw(tmp1, Address(post(a1, 4)));
5461     ldrw(tmp2, Address(post(a2, 4)));
5462     eorw(tmp1, tmp1, tmp2);
5463     cbnzw(tmp1, DONE);
5464   }
5465   bind(TAIL03);
5466   tbz(cnt1, 1, TAIL01); // 0-3 bytes left.
5467   {
5468     ldrh(tmp1, Address(post(a1, 2)));
5469     ldrh(tmp2, Address(post(a2, 2)));
5470     eorw(tmp1, tmp1, tmp2);
5471     cbnzw(tmp1, DONE);
5472   }
5473   bind(TAIL01);
5474   if (elem_size == 1) { // Only needed when comparing 1-byte elements
5475     tbz(cnt1, 0, SAME); // 0-1 bytes left.
5476     {
5477       ldrb(tmp1, a1);
5478       ldrb(tmp2, a2);
5479       eorw(tmp1, tmp1, tmp2);
5480       cbnzw(tmp1, DONE);
5481     }
5482   }
5483   // Arrays are equal.
5484   bind(SAME);
5485   mov(result, true);
5486 
5487   // That&#39;s it.
5488   bind(DONE);
5489   BLOCK_COMMENT(&quot;} string_equals&quot;);
5490 }
5491 
5492 
5493 // The size of the blocks erased by the zero_blocks stub.  We must
5494 // handle anything smaller than this ourselves in zero_words().
5495 const int MacroAssembler::zero_words_block_size = 8;
5496 
5497 // zero_words() is used by C2 ClearArray patterns.  It is as small as
5498 // possible, handling small word counts locally and delegating
5499 // anything larger to the zero_blocks stub.  It is expanded many times
5500 // in compiled code, so it is important to keep it short.
5501 
5502 // ptr:   Address of a buffer to be zeroed.
5503 // cnt:   Count in HeapWords.
5504 //
5505 // ptr, cnt, rscratch1, and rscratch2 are clobbered.
5506 void MacroAssembler::zero_words(Register ptr, Register cnt)
5507 {
5508   assert(is_power_of_2(zero_words_block_size), &quot;adjust this&quot;);
5509   assert(ptr == r10 &amp;&amp; cnt == r11, &quot;mismatch in register usage&quot;);
5510 
5511   BLOCK_COMMENT(&quot;zero_words {&quot;);
5512   cmp(cnt, (u1)zero_words_block_size);
5513   Label around;
5514   br(LO, around);
5515   {
5516     RuntimeAddress zero_blocks =  RuntimeAddress(StubRoutines::aarch64::zero_blocks());
5517     assert(zero_blocks.target() != NULL, &quot;zero_blocks stub has not been generated&quot;);
5518     if (StubRoutines::aarch64::complete()) {
5519       trampoline_call(zero_blocks);
5520     } else {
5521       bl(zero_blocks);
5522     }
5523   }
5524   bind(around);
5525   for (int i = zero_words_block_size &gt;&gt; 1; i &gt; 1; i &gt;&gt;= 1) {
5526     Label l;
5527     tbz(cnt, exact_log2(i), l);
5528     for (int j = 0; j &lt; i; j += 2) {
5529       stp(zr, zr, post(ptr, 16));
5530     }
5531     bind(l);
5532   }
5533   {
5534     Label l;
5535     tbz(cnt, 0, l);
5536     str(zr, Address(ptr));
5537     bind(l);
5538   }
5539   BLOCK_COMMENT(&quot;} zero_words&quot;);
5540 }
5541 
5542 // base:         Address of a buffer to be zeroed, 8 bytes aligned.
5543 // cnt:          Immediate count in HeapWords.
5544 #define SmallArraySize (18 * BytesPerLong)
5545 void MacroAssembler::zero_words(Register base, uint64_t cnt)
5546 {
5547   BLOCK_COMMENT(&quot;zero_words {&quot;);
5548   int i = cnt &amp; 1;  // store any odd word to start
5549   if (i) str(zr, Address(base));
5550 
5551   if (cnt &lt;= SmallArraySize / BytesPerLong) {
5552     for (; i &lt; (int)cnt; i += 2)
5553       stp(zr, zr, Address(base, i * wordSize));
5554   } else {
5555     const int unroll = 4; // Number of stp(zr, zr) instructions we&#39;ll unroll
5556     int remainder = cnt % (2 * unroll);
5557     for (; i &lt; remainder; i += 2)
5558       stp(zr, zr, Address(base, i * wordSize));
5559 
5560     Label loop;
5561     Register cnt_reg = rscratch1;
5562     Register loop_base = rscratch2;
5563     cnt = cnt - remainder;
5564     mov(cnt_reg, cnt);
5565     // adjust base and prebias by -2 * wordSize so we can pre-increment
5566     add(loop_base, base, (remainder - 2) * wordSize);
5567     bind(loop);
5568     sub(cnt_reg, cnt_reg, 2 * unroll);
5569     for (i = 1; i &lt; unroll; i++)
5570       stp(zr, zr, Address(loop_base, 2 * i * wordSize));
5571     stp(zr, zr, Address(pre(loop_base, 2 * unroll * wordSize)));
5572     cbnz(cnt_reg, loop);
5573   }
5574   BLOCK_COMMENT(&quot;} zero_words&quot;);
5575 }
5576 
5577 // Zero blocks of memory by using DC ZVA.
5578 //
5579 // Aligns the base address first sufficently for DC ZVA, then uses
5580 // DC ZVA repeatedly for every full block.  cnt is the size to be
5581 // zeroed in HeapWords.  Returns the count of words left to be zeroed
5582 // in cnt.
5583 //
5584 // NOTE: This is intended to be used in the zero_blocks() stub.  If
5585 // you want to use it elsewhere, note that cnt must be &gt;= 2*zva_length.
5586 void MacroAssembler::zero_dcache_blocks(Register base, Register cnt) {
5587   Register tmp = rscratch1;
5588   Register tmp2 = rscratch2;
5589   int zva_length = VM_Version::zva_length();
5590   Label initial_table_end, loop_zva;
5591   Label fini;
5592 
5593   // Base must be 16 byte aligned. If not just return and let caller handle it
5594   tst(base, 0x0f);
5595   br(Assembler::NE, fini);
5596   // Align base with ZVA length.
5597   neg(tmp, base);
5598   andr(tmp, tmp, zva_length - 1);
5599 
5600   // tmp: the number of bytes to be filled to align the base with ZVA length.
5601   add(base, base, tmp);
5602   sub(cnt, cnt, tmp, Assembler::ASR, 3);
5603   adr(tmp2, initial_table_end);
5604   sub(tmp2, tmp2, tmp, Assembler::LSR, 2);
5605   br(tmp2);
5606 
5607   for (int i = -zva_length + 16; i &lt; 0; i += 16)
5608     stp(zr, zr, Address(base, i));
5609   bind(initial_table_end);
5610 
5611   sub(cnt, cnt, zva_length &gt;&gt; 3);
5612   bind(loop_zva);
5613   dc(Assembler::ZVA, base);
5614   subs(cnt, cnt, zva_length &gt;&gt; 3);
5615   add(base, base, zva_length);
5616   br(Assembler::GE, loop_zva);
5617   add(cnt, cnt, zva_length &gt;&gt; 3); // count not zeroed by DC ZVA
5618   bind(fini);
5619 }
5620 
5621 // base:   Address of a buffer to be filled, 8 bytes aligned.
5622 // cnt:    Count in 8-byte unit.
5623 // value:  Value to be filled with.
5624 // base will point to the end of the buffer after filling.
5625 void MacroAssembler::fill_words(Register base, Register cnt, Register value)
5626 {
5627 //  Algorithm:
5628 //
5629 //    scratch1 = cnt &amp; 7;
5630 //    cnt -= scratch1;
5631 //    p += scratch1;
5632 //    switch (scratch1) {
5633 //      do {
5634 //        cnt -= 8;
5635 //          p[-8] = v;
5636 //        case 7:
5637 //          p[-7] = v;
5638 //        case 6:
5639 //          p[-6] = v;
5640 //          // ...
5641 //        case 1:
5642 //          p[-1] = v;
5643 //        case 0:
5644 //          p += 8;
5645 //      } while (cnt);
5646 //    }
5647 
5648   assert_different_registers(base, cnt, value, rscratch1, rscratch2);
5649 
5650   Label fini, skip, entry, loop;
5651   const int unroll = 8; // Number of stp instructions we&#39;ll unroll
5652 
5653   cbz(cnt, fini);
5654   tbz(base, 3, skip);
5655   str(value, Address(post(base, 8)));
5656   sub(cnt, cnt, 1);
5657   bind(skip);
5658 
5659   andr(rscratch1, cnt, (unroll-1) * 2);
5660   sub(cnt, cnt, rscratch1);
5661   add(base, base, rscratch1, Assembler::LSL, 3);
5662   adr(rscratch2, entry);
5663   sub(rscratch2, rscratch2, rscratch1, Assembler::LSL, 1);
5664   br(rscratch2);
5665 
5666   bind(loop);
5667   add(base, base, unroll * 16);
5668   for (int i = -unroll; i &lt; 0; i++)
5669     stp(value, value, Address(base, i * 16));
5670   bind(entry);
5671   subs(cnt, cnt, unroll * 2);
5672   br(Assembler::GE, loop);
5673 
5674   tbz(cnt, 0, fini);
5675   str(value, Address(post(base, 8)));
5676   bind(fini);
5677 }
5678 
5679 // Intrinsic for sun/nio/cs/ISO_8859_1$Encoder.implEncodeISOArray and
5680 // java/lang/StringUTF16.compress.
5681 void MacroAssembler::encode_iso_array(Register src, Register dst,
5682                       Register len, Register result,
5683                       FloatRegister Vtmp1, FloatRegister Vtmp2,
5684                       FloatRegister Vtmp3, FloatRegister Vtmp4)
5685 {
5686     Label DONE, SET_RESULT, NEXT_32, NEXT_32_PRFM, LOOP_8, NEXT_8, LOOP_1, NEXT_1,
5687         NEXT_32_START, NEXT_32_PRFM_START;
5688     Register tmp1 = rscratch1, tmp2 = rscratch2;
5689 
5690       mov(result, len); // Save initial len
5691 
5692       cmp(len, (u1)8); // handle shortest strings first
5693       br(LT, LOOP_1);
5694       cmp(len, (u1)32);
5695       br(LT, NEXT_8);
5696       // The following code uses the SIMD &#39;uzp1&#39; and &#39;uzp2&#39; instructions
5697       // to convert chars to bytes
5698       if (SoftwarePrefetchHintDistance &gt;= 0) {
5699         ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5700         subs(tmp2, len, SoftwarePrefetchHintDistance/2 + 16);
5701         br(LE, NEXT_32_START);
5702         b(NEXT_32_PRFM_START);
5703         BIND(NEXT_32_PRFM);
5704           ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5705         BIND(NEXT_32_PRFM_START);
5706           prfm(Address(src, SoftwarePrefetchHintDistance));
5707           orr(v4, T16B, Vtmp1, Vtmp2);
5708           orr(v5, T16B, Vtmp3, Vtmp4);
5709           uzp1(Vtmp1, T16B, Vtmp1, Vtmp2);
5710           uzp1(Vtmp3, T16B, Vtmp3, Vtmp4);
5711           uzp2(v5, T16B, v4, v5); // high bytes
5712           umov(tmp2, v5, D, 1);
5713           fmovd(tmp1, v5);
5714           orr(tmp1, tmp1, tmp2);
5715           cbnz(tmp1, LOOP_8);
5716           stpq(Vtmp1, Vtmp3, dst);
5717           sub(len, len, 32);
5718           add(dst, dst, 32);
5719           add(src, src, 64);
5720           subs(tmp2, len, SoftwarePrefetchHintDistance/2 + 16);
5721           br(GE, NEXT_32_PRFM);
5722           cmp(len, (u1)32);
5723           br(LT, LOOP_8);
5724         BIND(NEXT_32);
5725           ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5726         BIND(NEXT_32_START);
5727       } else {
5728         BIND(NEXT_32);
5729           ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5730       }
5731       prfm(Address(src, SoftwarePrefetchHintDistance));
5732       uzp1(v4, T16B, Vtmp1, Vtmp2);
5733       uzp1(v5, T16B, Vtmp3, Vtmp4);
5734       orr(Vtmp1, T16B, Vtmp1, Vtmp2);
5735       orr(Vtmp3, T16B, Vtmp3, Vtmp4);
5736       uzp2(Vtmp1, T16B, Vtmp1, Vtmp3); // high bytes
5737       umov(tmp2, Vtmp1, D, 1);
5738       fmovd(tmp1, Vtmp1);
5739       orr(tmp1, tmp1, tmp2);
5740       cbnz(tmp1, LOOP_8);
5741       stpq(v4, v5, dst);
5742       sub(len, len, 32);
5743       add(dst, dst, 32);
5744       add(src, src, 64);
5745       cmp(len, (u1)32);
5746       br(GE, NEXT_32);
5747       cbz(len, DONE);
5748 
5749     BIND(LOOP_8);
5750       cmp(len, (u1)8);
5751       br(LT, LOOP_1);
5752     BIND(NEXT_8);
5753       ld1(Vtmp1, T8H, src);
5754       uzp1(Vtmp2, T16B, Vtmp1, Vtmp1); // low bytes
5755       uzp2(Vtmp3, T16B, Vtmp1, Vtmp1); // high bytes
5756       fmovd(tmp1, Vtmp3);
5757       cbnz(tmp1, NEXT_1);
5758       strd(Vtmp2, dst);
5759 
5760       sub(len, len, 8);
5761       add(dst, dst, 8);
5762       add(src, src, 16);
5763       cmp(len, (u1)8);
5764       br(GE, NEXT_8);
5765 
5766     BIND(LOOP_1);
5767 
5768     cbz(len, DONE);
5769     BIND(NEXT_1);
5770       ldrh(tmp1, Address(post(src, 2)));
5771       tst(tmp1, 0xff00);
5772       br(NE, SET_RESULT);
5773       strb(tmp1, Address(post(dst, 1)));
5774       subs(len, len, 1);
5775       br(GT, NEXT_1);
5776 
5777     BIND(SET_RESULT);
5778       sub(result, result, len); // Return index where we stopped
5779                                 // Return len == 0 if we processed all
5780                                 // characters
5781     BIND(DONE);
5782 }
5783 
5784 
5785 // Inflate byte[] array to char[].
5786 void MacroAssembler::byte_array_inflate(Register src, Register dst, Register len,
5787                                         FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3,
5788                                         Register tmp4) {
5789   Label big, done, after_init, to_stub;
5790 
5791   assert_different_registers(src, dst, len, tmp4, rscratch1);
5792 
5793   fmovd(vtmp1, zr);
5794   lsrw(tmp4, len, 3);
5795   bind(after_init);
5796   cbnzw(tmp4, big);
5797   // Short string: less than 8 bytes.
5798   {
5799     Label loop, tiny;
5800 
5801     cmpw(len, 4);
5802     br(LT, tiny);
5803     // Use SIMD to do 4 bytes.
5804     ldrs(vtmp2, post(src, 4));
5805     zip1(vtmp3, T8B, vtmp2, vtmp1);
5806     subw(len, len, 4);
5807     strd(vtmp3, post(dst, 8));
5808 
5809     cbzw(len, done);
5810 
5811     // Do the remaining bytes by steam.
5812     bind(loop);
5813     ldrb(tmp4, post(src, 1));
5814     strh(tmp4, post(dst, 2));
5815     subw(len, len, 1);
5816 
5817     bind(tiny);
5818     cbnz(len, loop);
5819 
5820     b(done);
5821   }
5822 
5823   if (SoftwarePrefetchHintDistance &gt;= 0) {
5824     bind(to_stub);
5825       RuntimeAddress stub =  RuntimeAddress(StubRoutines::aarch64::large_byte_array_inflate());
5826       assert(stub.target() != NULL, &quot;large_byte_array_inflate stub has not been generated&quot;);
5827       trampoline_call(stub);
5828       b(after_init);
5829   }
5830 
5831   // Unpack the bytes 8 at a time.
5832   bind(big);
5833   {
5834     Label loop, around, loop_last, loop_start;
5835 
5836     if (SoftwarePrefetchHintDistance &gt;= 0) {
5837       const int large_loop_threshold = (64 + 16)/8;
5838       ldrd(vtmp2, post(src, 8));
5839       andw(len, len, 7);
5840       cmp(tmp4, (u1)large_loop_threshold);
5841       br(GE, to_stub);
5842       b(loop_start);
5843 
5844       bind(loop);
5845       ldrd(vtmp2, post(src, 8));
5846       bind(loop_start);
5847       subs(tmp4, tmp4, 1);
5848       br(EQ, loop_last);
5849       zip1(vtmp2, T16B, vtmp2, vtmp1);
5850       ldrd(vtmp3, post(src, 8));
5851       st1(vtmp2, T8H, post(dst, 16));
5852       subs(tmp4, tmp4, 1);
5853       zip1(vtmp3, T16B, vtmp3, vtmp1);
5854       st1(vtmp3, T8H, post(dst, 16));
5855       br(NE, loop);
5856       b(around);
5857       bind(loop_last);
5858       zip1(vtmp2, T16B, vtmp2, vtmp1);
5859       st1(vtmp2, T8H, post(dst, 16));
5860       bind(around);
5861       cbz(len, done);
5862     } else {
5863       andw(len, len, 7);
5864       bind(loop);
5865       ldrd(vtmp2, post(src, 8));
5866       sub(tmp4, tmp4, 1);
5867       zip1(vtmp3, T16B, vtmp2, vtmp1);
5868       st1(vtmp3, T8H, post(dst, 16));
5869       cbnz(tmp4, loop);
5870     }
5871   }
5872 
5873   // Do the tail of up to 8 bytes.
5874   add(src, src, len);
5875   ldrd(vtmp3, Address(src, -8));
5876   add(dst, dst, len, ext::uxtw, 1);
5877   zip1(vtmp3, T16B, vtmp3, vtmp1);
5878   strq(vtmp3, Address(dst, -16));
5879 
5880   bind(done);
5881 }
5882 
5883 // Compress char[] array to byte[].
5884 void MacroAssembler::char_array_compress(Register src, Register dst, Register len,
5885                                          FloatRegister tmp1Reg, FloatRegister tmp2Reg,
5886                                          FloatRegister tmp3Reg, FloatRegister tmp4Reg,
5887                                          Register result) {
5888   encode_iso_array(src, dst, len, result,
5889                    tmp1Reg, tmp2Reg, tmp3Reg, tmp4Reg);
5890   cmp(len, zr);
5891   csel(result, result, zr, EQ);
5892 }
5893 
5894 // get_thread() can be called anywhere inside generated code so we
5895 // need to save whatever non-callee save context might get clobbered
5896 // by the call to JavaThread::aarch64_get_thread_helper() or, indeed,
5897 // the call setup code.
5898 //
5899 // aarch64_get_thread_helper() clobbers only r0, r1, and flags.
5900 //
5901 void MacroAssembler::get_thread(Register dst) {
5902   RegSet saved_regs = RegSet::range(r0, r1) + lr - dst;
5903   push(saved_regs, sp);
5904 
5905   mov(lr, CAST_FROM_FN_PTR(address, JavaThread::aarch64_get_thread_helper));
5906   blr(lr);
5907   if (dst != c_rarg0) {
5908     mov(dst, c_rarg0);
5909   }
5910 
5911   pop(saved_regs, sp);
5912 }
5913 
5914 void MacroAssembler::cache_wb(Address line) {
5915   assert(line.getMode() == Address::base_plus_offset, &quot;mode should be base_plus_offset&quot;);
5916   assert(line.index() == noreg, &quot;index should be noreg&quot;);
5917   assert(line.offset() == 0, &quot;offset should be 0&quot;);
5918   // would like to assert this
5919   // assert(line._ext.shift == 0, &quot;shift should be zero&quot;);
5920   if (VM_Version::supports_dcpop()) {
5921     // writeback using clear virtual address to point of persistence
5922     dc(Assembler::CVAP, line.base());
5923   } else {
5924     // no need to generate anything as Unsafe.writebackMemory should
5925     // never invoke this stub
5926   }
5927 }
5928 
5929 void MacroAssembler::cache_wbsync(bool is_pre) {
5930   // we only need a barrier post sync
5931   if (!is_pre) {
5932     membar(Assembler::AnyAny);
5933   }
5934 }
    </pre>
  </body>
</html>