<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src\hotspot\os_cpu\windows_x86\atomic_windows_x86.hpp</title>
    <link rel="stylesheet" href="..\..\..\..\style.css" />
  </head>
<body>
<center><a href="..\windows_aarch64\vm_version_windows_aarch64.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="..\..\..\..\index.html" target="_top">index</a> <a href="bytes_windows_x86.inline.hpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src\hotspot\os_cpu\windows_x86\atomic_windows_x86.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 23,10 ***</span>
<span class="line-new-header">--- 23,11 ---</span>
   */
  
  #ifndef OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
  #define OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
  
<span class="line-added">+ #include &lt;intrin.h&gt;</span>
  #include &quot;runtime/os.hpp&quot;
  
  // Note that in MSVC, volatile memory accesses are explicitly
  // guaranteed to have acquire release semantics (w.r.t. compiler
  // reordering) and therefore does not even need a compiler barrier
</pre>
<hr />
<pre>
<span class="line-old-header">*** 49,47 ***</span>
  //
  // Performance note: On uniprocessors, the &#39;lock&#39; prefixes are not
  // necessary (and expensive). We should generate separate cases if
  // this becomes a performance problem.
  
<span class="line-removed">- #pragma warning(disable: 4035) // Disables warnings reporting missing return statement</span>
  
  template&lt;size_t byte_size&gt;
  struct Atomic::PlatformAdd
    : Atomic::AddAndFetch&lt;Atomic::PlatformAdd&lt;byte_size&gt; &gt;
  {
    template&lt;typename D, typename I&gt;
    D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;
  };
  
<span class="line-removed">- #ifdef AMD64</span>
<span class="line-removed">- template&lt;&gt;</span>
<span class="line-removed">- template&lt;typename D, typename I&gt;</span>
<span class="line-removed">- inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="line-removed">-                                                atomic_memory_order order) const {</span>
<span class="line-removed">-   return add_using_helper&lt;int32_t&gt;(os::atomic_add_func, dest, add_value);</span>
<span class="line-removed">- }</span>
  
<span class="line-modified">! template&lt;&gt;</span>
<span class="line-modified">! template&lt;typename D, typename I&gt;</span>
<span class="line-modified">! inline D Atomic::PlatformAdd&lt;8&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="line-modified">!                                                atomic_memory_order order) const {</span>
<span class="line-modified">!   return add_using_helper&lt;int64_t&gt;(os::atomic_add_long_func, dest, add_value);</span>
<span class="line-modified">! }</span>
  
  #define DEFINE_STUB_XCHG(ByteSize, StubType, StubName)                  \
    template&lt;&gt;                                                            \
    template&lt;typename T&gt;                                                  \
    inline T Atomic::PlatformXchg&lt;ByteSize&gt;::operator()(T volatile* dest, \
                                                        T exchange_value, \
                                                        atomic_memory_order order) const { \
      STATIC_ASSERT(ByteSize == sizeof(T));                               \
<span class="line-modified">!     return xchg_using_helper&lt;StubType&gt;(StubName, dest, exchange_value); \</span>
    }
  
<span class="line-modified">! DEFINE_STUB_XCHG(4, int32_t, os::atomic_xchg_func)</span>
<span class="line-modified">! DEFINE_STUB_XCHG(8, int64_t, os::atomic_xchg_long_func)</span>
  
  #undef DEFINE_STUB_XCHG
  
  #define DEFINE_STUB_CMPXCHG(ByteSize, StubType, StubName)                  \
    template&lt;&gt;                                                               \
<span class="line-new-header">--- 50,51 ---</span>
  //
  // Performance note: On uniprocessors, the &#39;lock&#39; prefixes are not
  // necessary (and expensive). We should generate separate cases if
  // this becomes a performance problem.
  
  
  template&lt;size_t byte_size&gt;
  struct Atomic::PlatformAdd
    : Atomic::AddAndFetch&lt;Atomic::PlatformAdd&lt;byte_size&gt; &gt;
  {
    template&lt;typename D, typename I&gt;
    D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;
  };
  
  
<span class="line-modified">! #define DEFINE_STUB_ADD(ByteSize, StubType, StubName)                     \</span>
<span class="line-modified">!   template&lt;&gt;                                                              \</span>
<span class="line-modified">!   template&lt;typename D, typename I&gt;                                        \</span>
<span class="line-modified">!   inline D Atomic::PlatformAdd&lt;ByteSize&gt;::add_and_fetch(D volatile* dest, \</span>
<span class="line-modified">!                                                         I add_value,      \</span>
<span class="line-modified">!                                                         atomic_memory_order order) const { \</span>
<span class="line-added">+     STATIC_ASSERT(ByteSize == sizeof(D));                                 \</span>
<span class="line-added">+     return PrimitiveConversions::cast&lt;D&gt;(                                 \</span>
<span class="line-added">+       StubName(reinterpret_cast&lt;StubType volatile *&gt;(dest),               \</span>
<span class="line-added">+                PrimitiveConversions::cast&lt;StubType&gt;(add_value)));         \</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+ DEFINE_STUB_ADD(4, long,    InterlockedAdd)</span>
<span class="line-added">+ DEFINE_STUB_ADD(8, __int64, InterlockedAdd64)</span>
<span class="line-added">+ </span>
<span class="line-added">+ #undef DEFINE_STUB_ADD</span>
  
  #define DEFINE_STUB_XCHG(ByteSize, StubType, StubName)                  \
    template&lt;&gt;                                                            \
    template&lt;typename T&gt;                                                  \
    inline T Atomic::PlatformXchg&lt;ByteSize&gt;::operator()(T volatile* dest, \
                                                        T exchange_value, \
                                                        atomic_memory_order order) const { \
      STATIC_ASSERT(ByteSize == sizeof(T));                               \
<span class="line-modified">!     return PrimitiveConversions::cast&lt;T&gt;(                               \</span>
<span class="line-added">+       StubName(reinterpret_cast&lt;StubType volatile *&gt;(dest),             \</span>
<span class="line-added">+                PrimitiveConversions::cast&lt;StubType&gt;(exchange_value)));  \</span>
    }
  
<span class="line-modified">! DEFINE_STUB_XCHG(4, long,    InterlockedExchange)</span>
<span class="line-modified">! DEFINE_STUB_XCHG(8, __int64, InterlockedExchange64)</span>
  
  #undef DEFINE_STUB_XCHG
  
  #define DEFINE_STUB_CMPXCHG(ByteSize, StubType, StubName)                  \
    template&lt;&gt;                                                               \
</pre>
<hr />
<pre>
<span class="line-old-header">*** 97,106 ***</span>
    inline T Atomic::PlatformCmpxchg&lt;ByteSize&gt;::operator()(T volatile* dest, \
                                                           T compare_value,  \
                                                           T exchange_value, \
                                                           atomic_memory_order order) const { \
      STATIC_ASSERT(ByteSize == sizeof(T));                                  \
<span class="line-modified">!     return cmpxchg_using_helper&lt;StubType&gt;(StubName, dest, compare_value, exchange_value); \</span>
    }
  
<span class="line-modified">! DEFINE_STUB_CMPXCHG(1, int8_t,  os::atomic_cmpxchg_byte_func)</span>
<span class="line-modified">! DEFINE_STUB_CMPXCHG(4, int32_t, os::atomic_cmpxchg_func)</span>
<span class="line-modified">! DEFINE_STUB_CMPXCHG(8, int64_t, os::atomic_cmpxchg_long_func)</span>
  
  #undef DEFINE_STUB_CMPXCHG
  
<span class="line-removed">- #else // !AMD64</span>
  
<span class="line-modified">! template&lt;&gt;</span>
<span class="line-removed">- template&lt;typename D, typename I&gt;</span>
<span class="line-removed">- inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="line-removed">-                                                atomic_memory_order order) const {</span>
<span class="line-removed">-   STATIC_ASSERT(4 == sizeof(I));</span>
<span class="line-removed">-   STATIC_ASSERT(4 == sizeof(D));</span>
<span class="line-removed">-   __asm {</span>
<span class="line-removed">-     mov edx, dest;</span>
<span class="line-removed">-     mov eax, add_value;</span>
<span class="line-removed">-     mov ecx, eax;</span>
<span class="line-removed">-     lock xadd dword ptr [edx], eax;</span>
<span class="line-removed">-     add eax, ecx;</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- }</span>
<span class="line-removed">- </span>
<span class="line-removed">- template&lt;&gt;</span>
<span class="line-removed">- template&lt;typename T&gt;</span>
<span class="line-removed">- inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">-                                              T exchange_value,</span>
<span class="line-removed">-                                              atomic_memory_order order) const {</span>
<span class="line-removed">-   STATIC_ASSERT(4 == sizeof(T));</span>
<span class="line-removed">-   // alternative for InterlockedExchange</span>
<span class="line-removed">-   __asm {</span>
<span class="line-removed">-     mov eax, exchange_value;</span>
<span class="line-removed">-     mov ecx, dest;</span>
<span class="line-removed">-     xchg eax, dword ptr [ecx];</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- }</span>
<span class="line-removed">- </span>
<span class="line-removed">- template&lt;&gt;</span>
<span class="line-removed">- template&lt;typename T&gt;</span>
<span class="line-removed">- inline T Atomic::PlatformCmpxchg&lt;1&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">-                                                 T compare_value,</span>
<span class="line-removed">-                                                 T exchange_value,</span>
<span class="line-removed">-                                                 atomic_memory_order order) const {</span>
<span class="line-removed">-   STATIC_ASSERT(1 == sizeof(T));</span>
<span class="line-removed">-   // alternative for InterlockedCompareExchange</span>
<span class="line-removed">-   __asm {</span>
<span class="line-removed">-     mov edx, dest</span>
<span class="line-removed">-     mov cl, exchange_value</span>
<span class="line-removed">-     mov al, compare_value</span>
<span class="line-removed">-     lock cmpxchg byte ptr [edx], cl</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- }</span>
<span class="line-removed">- </span>
<span class="line-removed">- template&lt;&gt;</span>
<span class="line-removed">- template&lt;typename T&gt;</span>
<span class="line-removed">- inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">-                                                 T compare_value,</span>
<span class="line-removed">-                                                 T exchange_value,</span>
<span class="line-removed">-                                                 atomic_memory_order order) const {</span>
<span class="line-removed">-   STATIC_ASSERT(4 == sizeof(T));</span>
<span class="line-removed">-   // alternative for InterlockedCompareExchange</span>
<span class="line-removed">-   __asm {</span>
<span class="line-removed">-     mov edx, dest</span>
<span class="line-removed">-     mov ecx, exchange_value</span>
<span class="line-removed">-     mov eax, compare_value</span>
<span class="line-removed">-     lock cmpxchg dword ptr [edx], ecx</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- }</span>
  
<span class="line-modified">! template&lt;&gt;</span>
<span class="line-removed">- template&lt;typename T&gt;</span>
<span class="line-removed">- inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">-                                                 T compare_value,</span>
<span class="line-removed">-                                                 T exchange_value,</span>
<span class="line-removed">-                                                 atomic_memory_order order) const {</span>
<span class="line-removed">-   STATIC_ASSERT(8 == sizeof(T));</span>
<span class="line-removed">-   int32_t ex_lo  = (int32_t)exchange_value;</span>
<span class="line-removed">-   int32_t ex_hi  = *( ((int32_t*)&amp;exchange_value) + 1 );</span>
<span class="line-removed">-   int32_t cmp_lo = (int32_t)compare_value;</span>
<span class="line-removed">-   int32_t cmp_hi = *( ((int32_t*)&amp;compare_value) + 1 );</span>
<span class="line-removed">-   __asm {</span>
<span class="line-removed">-     push ebx</span>
<span class="line-removed">-     push edi</span>
<span class="line-removed">-     mov eax, cmp_lo</span>
<span class="line-removed">-     mov edx, cmp_hi</span>
<span class="line-removed">-     mov edi, dest</span>
<span class="line-removed">-     mov ebx, ex_lo</span>
<span class="line-removed">-     mov ecx, ex_hi</span>
<span class="line-removed">-     lock cmpxchg8b qword ptr [edi]</span>
<span class="line-removed">-     pop edi</span>
<span class="line-removed">-     pop ebx</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- }</span>
  
  template&lt;&gt;
  template&lt;typename T&gt;
  inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
    STATIC_ASSERT(8 == sizeof(T));
<span class="line-new-header">--- 102,26 ---</span>
    inline T Atomic::PlatformCmpxchg&lt;ByteSize&gt;::operator()(T volatile* dest, \
                                                           T compare_value,  \
                                                           T exchange_value, \
                                                           atomic_memory_order order) const { \
      STATIC_ASSERT(ByteSize == sizeof(T));                                  \
<span class="line-modified">!     return PrimitiveConversions::cast&lt;T&gt;(                                  \</span>
<span class="line-added">+       StubName(reinterpret_cast&lt;StubType volatile *&gt;(dest),                \</span>
<span class="line-added">+                PrimitiveConversions::cast&lt;StubType&gt;(exchange_value),       \</span>
<span class="line-added">+                PrimitiveConversions::cast&lt;StubType&gt;(compare_value)));      \</span>
    }
  
<span class="line-modified">! DEFINE_STUB_CMPXCHG(1, char,    _InterlockedCompareExchange8) // Use the intrinsic as InterlockedCompareExchange8 does not exist</span>
<span class="line-modified">! DEFINE_STUB_CMPXCHG(4, long,    InterlockedCompareExchange)</span>
<span class="line-modified">! DEFINE_STUB_CMPXCHG(8, __int64, InterlockedCompareExchange64)</span>
  
  #undef DEFINE_STUB_CMPXCHG
  
  
<span class="line-modified">! #ifndef AMD64</span>
  
<span class="line-modified">! #pragma warning(disable: 4035) // Disables warnings reporting missing return statement</span>
  
  template&lt;&gt;
  template&lt;typename T&gt;
  inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
    STATIC_ASSERT(8 == sizeof(T));
</pre>
<hr />
<pre>
<span class="line-old-header">*** 223,15 ***</span>
      mov eax, dest
      fistp    qword ptr [eax]
    }
  }
  
<span class="line-removed">- #endif // AMD64</span>
<span class="line-removed">- </span>
  #pragma warning(default: 4035) // Enables warnings reporting missing return statement
  
<span class="line-removed">- #ifndef AMD64</span>
  template&lt;&gt;
  struct Atomic::PlatformOrderedStore&lt;1, RELEASE_X_FENCE&gt;
  {
    template &lt;typename T&gt;
    void operator()(volatile T* p, T v) const {
<span class="line-new-header">--- 148,12 ---</span>
</pre>
<center><a href="..\windows_aarch64\vm_version_windows_aarch64.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="..\..\..\..\index.html" target="_top">index</a> <a href="bytes_windows_x86.inline.hpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>