<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src\hotspot\os_cpu\windows_aarch64\atomic_windows_aarch64.hpp</title>
    <link rel="stylesheet" href="..\..\..\..\style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.
  4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  5  *
  6  * This code is free software; you can redistribute it and/or modify it
  7  * under the terms of the GNU General Public License version 2 only, as
  8  * published by the Free Software Foundation.
  9  *
 10  * This code is distributed in the hope that it will be useful, but WITHOUT
 11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 13  * version 2 for more details (a copy is included in the LICENSE file that
 14  * accompanied this code).
 15  *
 16  * You should have received a copy of the GNU General Public License version
 17  * 2 along with this work; if not, write to the Free Software Foundation,
 18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 19  *
 20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 21  * or visit www.oracle.com if you need additional information or have any
 22  * questions.
 23  *
 24  */
 25 
 26 #ifndef OS_CPU_WINDOWS_AARCH64_ATOMIC_WINDOWS_AARCH64_HPP
 27 #define OS_CPU_WINDOWS_AARCH64_ATOMIC_WINDOWS_AARCH64_HPP
 28 
 29 #include &lt;intrin.h&gt;
 30 #include &quot;runtime/os.hpp&quot;
 31 #include &quot;runtime/vm_version.hpp&quot;
 32 
 33 
 34 // As per atomic.hpp all read-modify-write operations have to provide two-way
 35 // barriers semantics. The memory_order parameter is ignored - we always provide
 36 // the strongest/most-conservative ordering
 37 //
 38 // For AARCH64 we add explicit barriers in the stubs.
 39 
 40 template&lt;size_t byte_size&gt;
 41 struct Atomic::PlatformAdd
 42   : Atomic::AddAndFetch&lt;Atomic::PlatformAdd&lt;byte_size&gt; &gt;
 43 {
 44   template&lt;typename D, typename I&gt;
 45   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;
 46 };
 47 
 48 #define DEFINE_STUB_ADD(ByteSize, StubType, StubName)                     \
 49   template&lt;&gt;                                                              \
 50   template&lt;typename D, typename I&gt;                                        \
 51   inline D Atomic::PlatformAdd&lt;ByteSize&gt;::add_and_fetch(D volatile* dest, \
 52                                                         I add_value,      \
 53                                                         atomic_memory_order order) const { \
 54     STATIC_ASSERT(ByteSize == sizeof(D));                                 \
 55     return PrimitiveConversions::cast&lt;D&gt;(                                 \
 56       StubName(reinterpret_cast&lt;StubType volatile *&gt;(dest),               \
 57                PrimitiveConversions::cast&lt;StubType&gt;(add_value)));         \
 58   }
 59 
 60 DEFINE_STUB_ADD(4, long,    InterlockedAdd)
 61 DEFINE_STUB_ADD(8, __int64, InterlockedAdd64)
 62 
 63 #undef DEFINE_STUB_ADD
 64 
 65 #define DEFINE_STUB_XCHG(ByteSize, StubType, StubName)                  \
 66   template&lt;&gt;                                                            \
 67   template&lt;typename T&gt;                                                  \
 68   inline T Atomic::PlatformXchg&lt;ByteSize&gt;::operator()(T volatile* dest, \
 69                                                       T exchange_value, \
 70                                                       atomic_memory_order order) const { \
 71     STATIC_ASSERT(ByteSize == sizeof(T));                               \
 72     return PrimitiveConversions::cast&lt;T&gt;(                               \
 73       StubName(reinterpret_cast&lt;StubType volatile *&gt;(dest),             \
 74                PrimitiveConversions::cast&lt;StubType&gt;(exchange_value)));  \
 75   }
 76 
 77 DEFINE_STUB_XCHG(4, long,    InterlockedExchange)
 78 DEFINE_STUB_XCHG(8, __int64, InterlockedExchange64)
 79 
 80 #undef DEFINE_STUB_XCHG
 81 
 82 #define DEFINE_STUB_CMPXCHG(ByteSize, StubType, StubName)                  \
 83   template&lt;&gt;                                                               \
 84   template&lt;typename T&gt;                                                     \
 85   inline T Atomic::PlatformCmpxchg&lt;ByteSize&gt;::operator()(T volatile* dest, \
 86                                                          T compare_value,  \
 87                                                          T exchange_value, \
 88                                                          atomic_memory_order order) const { \
 89     STATIC_ASSERT(ByteSize == sizeof(T));                                  \
 90     return PrimitiveConversions::cast&lt;T&gt;(                                  \
 91       StubName(reinterpret_cast&lt;StubType volatile *&gt;(dest),                \
 92                PrimitiveConversions::cast&lt;StubType&gt;(exchange_value),       \
 93                PrimitiveConversions::cast&lt;StubType&gt;(compare_value)));      \
 94   }
 95 
 96 DEFINE_STUB_CMPXCHG(1, char,    _InterlockedCompareExchange8) // Use the intrinsic as InterlockedCompareExchange8 does not exist
 97 DEFINE_STUB_CMPXCHG(4, long,    InterlockedCompareExchange)
 98 DEFINE_STUB_CMPXCHG(8, __int64, InterlockedCompareExchange64)
 99 
100 #undef DEFINE_STUB_CMPXCHG
101 
102 #endif // OS_CPU_WINDOWS_AARCH64_ATOMIC_WINDOWS_AARCH64_HPP
    </pre>
  </body>
</html>