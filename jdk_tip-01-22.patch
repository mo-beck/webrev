diff a/src/hotspot/cpu/aarch64/aarch64.ad b/src/hotspot/cpu/aarch64/aarch64.ad
--- a/src/hotspot/cpu/aarch64/aarch64.ad
+++ b/src/hotspot/cpu/aarch64/aarch64.ad
@@ -530,17 +530,23 @@
   return _ANY_REG_mask;
 %}
 
 // Class for non-allocatable 32 bit registers
 reg_class non_allocatable_reg32(
+#ifdef _WIN64
+    R18,                        // tls on Windows
+#endif
     R28,                        // thread
     R30,                        // lr
     R31                         // sp
 );
 
 // Class for non-allocatable 64 bit registers
 reg_class non_allocatable_reg(
+#ifdef _WIN64
+    R18, R18_H,                 // tls on Windows
+#endif
     R28, R28_H,                 // thread
     R30, R30_H,                 // lr
     R31, R31_H                  // sp
 );
 
@@ -1623,11 +1629,11 @@
 void MachPrologNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {
   Compile* C = ra_->C;
   MacroAssembler _masm(&cbuf);
 
   // n.b. frame size includes space for return pc and rfp
-  const long framesize = C->frame_size_in_bytes();
+  const int64_t framesize = C->frame_size_in_bytes();
   assert(framesize%(2*wordSize) == 0, "must preserve 2*wordSize alignment");
 
   // insert a nop at the start of the prolog so we can patch in a
   // branch if we need to invalidate the method later
   __ nop();
@@ -3110,11 +3116,11 @@
 
   /// mov envcodings
 
   enc_class aarch64_enc_movw_imm(iRegI dst, immI src) %{
     MacroAssembler _masm(&cbuf);
-    u_int32_t con = (u_int32_t)$src$$constant;
+    uint32_t con = (uint32_t)$src$$constant;
     Register dst_reg = as_Register($dst$$reg);
     if (con == 0) {
       __ movw(dst_reg, zr);
     } else {
       __ movw(dst_reg, con);
@@ -3122,11 +3128,11 @@
   %}
 
   enc_class aarch64_enc_mov_imm(iRegL dst, immL src) %{
     MacroAssembler _masm(&cbuf);
     Register dst_reg = as_Register($dst$$reg);
-    u_int64_t con = (u_int64_t)$src$$constant;
+    uint64_t con = (uint64_t)$src$$constant;
     if (con == 0) {
       __ mov(dst_reg, zr);
     } else {
       __ mov(dst_reg, con);
     }
@@ -3147,11 +3153,11 @@
       } else {
         assert(rtype == relocInfo::none, "unexpected reloc type");
         if (con < (address)(uintptr_t)os::vm_page_size()) {
           __ mov(dst_reg, con);
         } else {
-          unsigned long offset;
+          uint64_t offset;
           __ adrp(dst_reg, con, offset);
           __ add(dst_reg, dst_reg, offset);
         }
       }
     }
@@ -3164,18 +3170,18 @@
   %}
 
   enc_class aarch64_enc_mov_p1(iRegP dst, immP_1 src) %{
     MacroAssembler _masm(&cbuf);
     Register dst_reg = as_Register($dst$$reg);
-    __ mov(dst_reg, (u_int64_t)1);
+    __ mov(dst_reg, (uint64_t)1);
   %}
 
   enc_class aarch64_enc_mov_poll_page(iRegP dst, immPollPage src) %{
     MacroAssembler _masm(&cbuf);
     address page = (address)$src$$constant;
     Register dst_reg = as_Register($dst$$reg);
-    unsigned long off;
+    uint64_t off;
     __ adrp(dst_reg, Address(page, relocInfo::poll_type), off);
     assert(off == 0, "assumed offset == 0");
   %}
 
   enc_class aarch64_enc_mov_byte_map_base(iRegP dst, immByteMapBase src) %{
@@ -3298,11 +3304,11 @@
   %}
 
   enc_class aarch64_enc_cmpw_imm(iRegI src1, immI src2) %{
     MacroAssembler _masm(&cbuf);
     Register reg1 = as_Register($src1$$reg);
-    u_int32_t val = (u_int32_t)$src2$$constant;
+    uint32_t val = (uint32_t)$src2$$constant;
     __ movw(rscratch1, val);
     __ cmpw(reg1, rscratch1);
   %}
 
   enc_class aarch64_enc_cmp(iRegL src1, iRegL src2) %{
@@ -3320,19 +3326,19 @@
       __ subs(zr, reg, val);
     } else if (val != -val) {
       __ adds(zr, reg, -val);
     } else {
     // aargh, Long.MIN_VALUE is a special case
-      __ orr(rscratch1, zr, (u_int64_t)val);
+      __ orr(rscratch1, zr, (uint64_t)val);
       __ subs(zr, reg, rscratch1);
     }
   %}
 
   enc_class aarch64_enc_cmp_imm(iRegL src1, immL src2) %{
     MacroAssembler _masm(&cbuf);
     Register reg1 = as_Register($src1$$reg);
-    u_int64_t val = (u_int64_t)$src2$$constant;
+    uint64_t val = (uint64_t)$src2$$constant;
     __ mov(rscratch1, val);
     __ cmp(reg1, rscratch1);
   %}
 
   enc_class aarch64_enc_cmpp(iRegP src1, iRegP src2) %{
@@ -4228,22 +4234,22 @@
 %}
 
 // 32 bit integer valid for add sub immediate
 operand immIAddSub()
 %{
-  predicate(Assembler::operand_valid_for_add_sub_immediate((long)n->get_int()));
+  predicate(Assembler::operand_valid_for_add_sub_immediate((int64_t)n->get_int()));
   match(ConI);
   op_cost(0);
   format %{ %}
   interface(CONST_INTER);
 %}
 
 // 32 bit unsigned integer valid for logical immediate
 // TODO -- check this is right when e.g the mask is 0x80000000
 operand immILog()
 %{
-  predicate(Assembler::operand_valid_for_logical_immediate(/*is32*/true, (unsigned long)n->get_int()));
+  predicate(Assembler::operand_valid_for_logical_immediate(/*is32*/true, (uint64_t)n->get_int()));
   match(ConI);
 
   op_cost(0);
   format %{ %}
   interface(CONST_INTER);
@@ -4317,11 +4323,11 @@
 %}
 
 // 64 bit integer valid for logical immediate
 operand immLLog()
 %{
-  predicate(Assembler::operand_valid_for_logical_immediate(/*is32*/false, (unsigned long)n->get_long()));
+  predicate(Assembler::operand_valid_for_logical_immediate(/*is32*/false, (uint64_t)n->get_long()));
   match(ConL);
   op_cost(0);
   format %{ %}
   interface(CONST_INTER);
 %}
@@ -5888,11 +5894,11 @@
 pipeline %{
 
 attributes %{
   // ARM instructions are of fixed length
   fixed_size_instructions;        // Fixed size instructions TODO does
-  max_instructions_per_bundle = 2;   // A53 = 2, A57 = 4
+  max_instructions_per_bundle = 4;   // A53 = 2, A57 = 4
   // ARM instructions come in 32-bit word units
   instruction_unit_size = 4;         // An instruction is 4 bytes long
   instruction_fetch_unit_size = 64;  // The processor fetches one line
   instruction_fetch_units = 1;       // of 64 bytes
 
@@ -7124,11 +7130,11 @@
 instruct loadConL(iRegLNoSp dst, immL src)
 %{
   match(Set dst src);
 
   ins_cost(INSN_COST);
-  format %{ "mov $dst, $src\t# long" %}
+  format %{ "mov $dst, $src\t# int64_t" %}
 
   ins_encode( aarch64_enc_mov_imm(dst, src) );
 
   ins_pipe(ialu_imm);
 %}
@@ -8276,11 +8282,11 @@
 
 instruct castX2P(iRegPNoSp dst, iRegL src) %{
   match(Set dst (CastX2P src));
 
   ins_cost(INSN_COST);
-  format %{ "mov $dst, $src\t# long -> ptr" %}
+  format %{ "mov $dst, $src\t# int64_t -> ptr" %}
 
   ins_encode %{
     if ($dst$$reg != $src$$reg) {
       __ mov(as_Register($dst$$reg), as_Register($src$$reg));
     }
@@ -8291,11 +8297,11 @@
 
 instruct castP2X(iRegLNoSp dst, iRegP src) %{
   match(Set dst (CastP2X src));
 
   ins_cost(INSN_COST);
-  format %{ "mov $dst, $src\t# ptr -> long" %}
+  format %{ "mov $dst, $src\t# ptr -> int64_t" %}
 
   ins_encode %{
     if ($dst$$reg != $src$$reg) {
       __ mov(as_Register($dst$$reg), as_Register($src$$reg));
     }
@@ -8640,11 +8646,11 @@
   ins_cost(2 * VOLATILE_REF_COST);
 
   effect(KILL cr);
 
  format %{
-    "cmpxchg $mem, $oldval, $newval\t# (long) if $mem == $oldval then $mem <-- $newval"
+    "cmpxchg $mem, $oldval, $newval\t# (int64_t) if $mem == $oldval then $mem <-- $newval"
     "cset $res, EQ\t# $res <-- (EQ ? 1 : 0)"
  %}
 
  ins_encode(aarch64_enc_cmpxchg(mem, oldval, newval),
             aarch64_enc_cset_eq(res));
@@ -8755,11 +8761,11 @@
   ins_cost(VOLATILE_REF_COST);
 
   effect(KILL cr);
 
  format %{
-    "cmpxchg_acq $mem, $oldval, $newval\t# (long) if $mem == $oldval then $mem <-- $newval"
+    "cmpxchg_acq $mem, $oldval, $newval\t# (int64_t) if $mem == $oldval then $mem <-- $newval"
     "cset $res, EQ\t# $res <-- (EQ ? 1 : 0)"
  %}
 
  ins_encode(aarch64_enc_cmpxchg_acq(mem, oldval, newval),
             aarch64_enc_cset_eq(res));
@@ -8872,11 +8878,11 @@
 instruct compareAndExchangeL(iRegLNoSp res, indirect mem, iRegL oldval, iRegL newval, rFlagsReg cr) %{
   match(Set res (CompareAndExchangeL mem (Binary oldval newval)));
   ins_cost(2 * VOLATILE_REF_COST);
   effect(TEMP_DEF res, KILL cr);
   format %{
-    "cmpxchg $res = $mem, $oldval, $newval\t# (long, weak) if $mem == $oldval then $mem <-- $newval"
+    "cmpxchg $res = $mem, $oldval, $newval\t# (int64_t, weak) if $mem == $oldval then $mem <-- $newval"
   %}
   ins_encode %{
     __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register,
                Assembler::xword, /*acquire*/ false, /*release*/ true,
                /*weak*/ false, $res$$Register);
@@ -8970,11 +8976,11 @@
   predicate(needs_acquiring_load_exclusive(n));
   match(Set res (CompareAndExchangeL mem (Binary oldval newval)));
   ins_cost(VOLATILE_REF_COST);
   effect(TEMP_DEF res, KILL cr);
   format %{
-    "cmpxchg_acq $res = $mem, $oldval, $newval\t# (long, weak) if $mem == $oldval then $mem <-- $newval"
+    "cmpxchg_acq $res = $mem, $oldval, $newval\t# (int64_t, weak) if $mem == $oldval then $mem <-- $newval"
   %}
   ins_encode %{
     __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register,
                Assembler::xword, /*acquire*/ true, /*release*/ true,
                /*weak*/ false, $res$$Register);
@@ -9069,11 +9075,11 @@
 instruct weakCompareAndSwapL(iRegINoSp res, indirect mem, iRegL oldval, iRegL newval, rFlagsReg cr) %{
   match(Set res (WeakCompareAndSwapL mem (Binary oldval newval)));
   ins_cost(2 * VOLATILE_REF_COST);
   effect(KILL cr);
   format %{
-    "cmpxchg $res = $mem, $oldval, $newval\t# (long, weak) if $mem == $oldval then $mem <-- $newval"
+    "cmpxchg $res = $mem, $oldval, $newval\t# (int64_t, weak) if $mem == $oldval then $mem <-- $newval"
     "csetw $res, EQ\t# $res <-- (EQ ? 1 : 0)"
   %}
   ins_encode %{
     __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register,
                Assembler::xword, /*acquire*/ false, /*release*/ true,
@@ -9176,11 +9182,11 @@
   predicate(needs_acquiring_load_exclusive(n));
   match(Set res (WeakCompareAndSwapL mem (Binary oldval newval)));
   ins_cost(VOLATILE_REF_COST);
   effect(KILL cr);
   format %{
-    "cmpxchg_acq $res = $mem, $oldval, $newval\t# (long, weak) if $mem == $oldval then $mem <-- $newval"
+    "cmpxchg_acq $res = $mem, $oldval, $newval\t# (int64_t, weak) if $mem == $oldval then $mem <-- $newval"
     "csetw $res, EQ\t# $res <-- (EQ ? 1 : 0)"
   %}
   ins_encode %{
     __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register,
                Assembler::xword, /*acquire*/ true, /*release*/ true,
@@ -9698,11 +9704,11 @@
 
 instruct cmovL_reg_reg(cmpOp cmp, rFlagsReg cr, iRegLNoSp dst, iRegL src1, iRegL src2) %{
   match(Set dst (CMoveL (Binary cmp cr) (Binary src1 src2)));
 
   ins_cost(INSN_COST * 2);
-  format %{ "csel $dst, $src2, $src1 $cmp\t# signed, long"  %}
+  format %{ "csel $dst, $src2, $src1 $cmp\t# signed, int64_t"  %}
 
   ins_encode %{
     __ csel(as_Register($dst$$reg),
             as_Register($src2$$reg),
             as_Register($src1$$reg),
@@ -9714,11 +9720,11 @@
 
 instruct cmovUL_reg_reg(cmpOpU cmp, rFlagsRegU cr, iRegLNoSp dst, iRegL src1, iRegL src2) %{
   match(Set dst (CMoveL (Binary cmp cr) (Binary src1 src2)));
 
   ins_cost(INSN_COST * 2);
-  format %{ "csel $dst, $src2, $src1 $cmp\t# unsigned, long"  %}
+  format %{ "csel $dst, $src2, $src1 $cmp\t# unsigned, int64_t"  %}
 
   ins_encode %{
     __ csel(as_Register($dst$$reg),
             as_Register($src2$$reg),
             as_Register($src1$$reg),
@@ -9732,11 +9738,11 @@
 
 instruct cmovL_reg_zero(cmpOp cmp, rFlagsReg cr, iRegLNoSp dst, iRegL src, immL0 zero) %{
   match(Set dst (CMoveL (Binary cmp cr) (Binary src zero)));
 
   ins_cost(INSN_COST * 2);
-  format %{ "csel $dst, zr, $src $cmp\t# signed, long"  %}
+  format %{ "csel $dst, zr, $src $cmp\t# signed, int64_t"  %}
 
   ins_encode %{
     __ csel(as_Register($dst$$reg),
             zr,
             as_Register($src$$reg),
@@ -9748,11 +9754,11 @@
 
 instruct cmovUL_reg_zero(cmpOpU cmp, rFlagsRegU cr, iRegLNoSp dst, iRegL src, immL0 zero) %{
   match(Set dst (CMoveL (Binary cmp cr) (Binary src zero)));
 
   ins_cost(INSN_COST * 2);
-  format %{ "csel $dst, zr, $src $cmp\t# unsigned, long"  %}
+  format %{ "csel $dst, zr, $src $cmp\t# unsigned, int64_t"  %}
 
   ins_encode %{
     __ csel(as_Register($dst$$reg),
             zr,
             as_Register($src$$reg),
@@ -9764,11 +9770,11 @@
 
 instruct cmovL_zero_reg(cmpOp cmp, rFlagsReg cr, iRegLNoSp dst, immL0 zero, iRegL src) %{
   match(Set dst (CMoveL (Binary cmp cr) (Binary zero src)));
 
   ins_cost(INSN_COST * 2);
-  format %{ "csel $dst, $src, zr $cmp\t# signed, long"  %}
+  format %{ "csel $dst, $src, zr $cmp\t# signed, int64_t"  %}
 
   ins_encode %{
     __ csel(as_Register($dst$$reg),
             as_Register($src$$reg),
             zr,
@@ -9780,11 +9786,11 @@
 
 instruct cmovUL_zero_reg(cmpOpU cmp, rFlagsRegU cr, iRegLNoSp dst, immL0 zero, iRegL src) %{
   match(Set dst (CMoveL (Binary cmp cr) (Binary zero src)));
 
   ins_cost(INSN_COST * 2);
-  format %{ "csel $dst, $src, zr $cmp\t# unsigned, long"  %}
+  format %{ "csel $dst, $src, zr $cmp\t# unsigned, int64_t"  %}
 
   ins_encode %{
     __ csel(as_Register($dst$$reg),
             as_Register($src$$reg),
             zr,
@@ -10185,11 +10191,11 @@
   format %{ "sbfiz $dst, $src, $scale & 63, -$scale & 63\t" %}
 
   ins_encode %{
     __ sbfiz(as_Register($dst$$reg),
           as_Register($src$$reg),
-          $scale$$constant & 63, MIN(32, (-$scale$$constant) & 63));
+          $scale$$constant & 63, MIN2((intptr_t)32, (-$scale$$constant) & 63));
   %}
 
   ins_pipe(ialu_reg_shift);
 %}
 
@@ -10325,11 +10331,11 @@
 
 instruct negL_reg(iRegLNoSp dst, iRegL src, immL0 zero, rFlagsReg cr) %{
   match(Set dst (SubL zero src));
 
   ins_cost(INSN_COST);
-  format %{ "neg $dst, $src\t# long" %}
+  format %{ "neg $dst, $src\t# int64_t" %}
 
   ins_encode %{
     __ neg(as_Register($dst$$reg),
            as_Register($src$$reg));
   %}
@@ -11991,11 +11997,11 @@
 
   ins_cost(INSN_COST);
   format %{ "ubfxw $dst, $src, $rshift, $mask" %}
   ins_encode %{
     int rshift = $rshift$$constant & 31;
-    long mask = $mask$$constant;
+    int64_t mask = $mask$$constant;
     int width = exact_log2(mask+1);
     __ ubfxw(as_Register($dst$$reg),
             as_Register($src$$reg), rshift, width);
   %}
   ins_pipe(ialu_reg_shift);
@@ -12008,11 +12014,11 @@
 
   ins_cost(INSN_COST);
   format %{ "ubfx $dst, $src, $rshift, $mask" %}
   ins_encode %{
     int rshift = $rshift$$constant & 63;
-    long mask = $mask$$constant;
+    int64_t mask = $mask$$constant;
     int width = exact_log2_long(mask+1);
     __ ubfx(as_Register($dst$$reg),
             as_Register($src$$reg), rshift, width);
   %}
   ins_pipe(ialu_reg_shift);
@@ -12028,11 +12034,11 @@
 
   ins_cost(INSN_COST * 2);
   format %{ "ubfx $dst, $src, $rshift, $mask" %}
   ins_encode %{
     int rshift = $rshift$$constant & 31;
-    long mask = $mask$$constant;
+    int64_t mask = $mask$$constant;
     int width = exact_log2(mask+1);
     __ ubfx(as_Register($dst$$reg),
             as_Register($src$$reg), rshift, width);
   %}
   ins_pipe(ialu_reg_shift);
@@ -12047,11 +12053,11 @@
 
   ins_cost(INSN_COST);
   format %{ "ubfizw $dst, $src, $lshift, $mask" %}
   ins_encode %{
     int lshift = $lshift$$constant & 31;
-    long mask = $mask$$constant;
+    int64_t mask = $mask$$constant;
     int width = exact_log2(mask+1);
     __ ubfizw(as_Register($dst$$reg),
           as_Register($src$$reg), lshift, width);
   %}
   ins_pipe(ialu_reg_shift);
@@ -12065,11 +12071,11 @@
 
   ins_cost(INSN_COST);
   format %{ "ubfiz $dst, $src, $lshift, $mask" %}
   ins_encode %{
     int lshift = $lshift$$constant & 63;
-    long mask = $mask$$constant;
+    int64_t mask = $mask$$constant;
     int width = exact_log2_long(mask+1);
     __ ubfiz(as_Register($dst$$reg),
           as_Register($src$$reg), lshift, width);
   %}
   ins_pipe(ialu_reg_shift);
@@ -12083,11 +12089,11 @@
 
   ins_cost(INSN_COST);
   format %{ "ubfiz $dst, $src, $lshift, $mask" %}
   ins_encode %{
     int lshift = $lshift$$constant & 63;
-    long mask = $mask$$constant;
+    int64_t mask = $mask$$constant;
     int width = exact_log2(mask+1);
     __ ubfiz(as_Register($dst$$reg),
              as_Register($src$$reg), lshift, width);
   %}
   ins_pipe(ialu_reg_shift);
@@ -13286,11 +13292,11 @@
 
   ins_cost(INSN_COST);
   ins_encode %{
     __ andw(as_Register($dst$$reg),
             as_Register($src1$$reg),
-            (unsigned long)($src2$$constant));
+            (uint64_t)($src2$$constant));
   %}
 
   ins_pipe(ialu_reg_imm);
 %}
 
@@ -13318,11 +13324,11 @@
 
   ins_cost(INSN_COST);
   ins_encode %{
     __ orrw(as_Register($dst$$reg),
             as_Register($src1$$reg),
-            (unsigned long)($src2$$constant));
+            (uint64_t)($src2$$constant));
   %}
 
   ins_pipe(ialu_reg_imm);
 %}
 
@@ -13350,11 +13356,11 @@
 
   ins_cost(INSN_COST);
   ins_encode %{
     __ eorw(as_Register($dst$$reg),
             as_Register($src1$$reg),
-            (unsigned long)($src2$$constant));
+            (uint64_t)($src2$$constant));
   %}
 
   ins_pipe(ialu_reg_imm);
 %}
 
@@ -13383,11 +13389,11 @@
 
   ins_cost(INSN_COST);
   ins_encode %{
     __ andr(as_Register($dst$$reg),
             as_Register($src1$$reg),
-            (unsigned long)($src2$$constant));
+            (uint64_t)($src2$$constant));
   %}
 
   ins_pipe(ialu_reg_imm);
 %}
 
@@ -13415,11 +13421,11 @@
 
   ins_cost(INSN_COST);
   ins_encode %{
     __ orr(as_Register($dst$$reg),
            as_Register($src1$$reg),
-           (unsigned long)($src2$$constant));
+           (uint64_t)($src2$$constant));
   %}
 
   ins_pipe(ialu_reg_imm);
 %}
 
@@ -13447,11 +13453,11 @@
   format %{ "eor  $dst, $src1, $src2\t# int" %}
 
   ins_encode %{
     __ eor(as_Register($dst$$reg),
            as_Register($src1$$reg),
-           (unsigned long)($src2$$constant));
+           (uint64_t)($src2$$constant));
   %}
 
   ins_pipe(ialu_reg_imm);
 %}
 
@@ -13896,20 +13902,20 @@
   ins_pipe(pipe_class_memory);
 %}
 
 instruct clearArray_imm_reg(immL cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)
 %{
-  predicate((u_int64_t)n->in(2)->get_long()
-            < (u_int64_t)(BlockZeroingLowLimit >> LogBytesPerWord));
+  predicate((uint64_t)n->in(2)->get_long()
+            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord));
   match(Set dummy (ClearArray cnt base));
   effect(USE_KILL base);
 
   ins_cost(4 * INSN_COST);
   format %{ "ClearArray $cnt, $base" %}
 
   ins_encode %{
-    __ zero_words($base$$Register, (u_int64_t)$cnt$$constant);
+    __ zero_words($base$$Register, (uint64_t)$cnt$$constant);
   %}
 
   ins_pipe(pipe_class_memory);
 %}
 
@@ -13944,11 +13950,11 @@
 
 instruct overflowAddL_reg_reg(rFlagsReg cr, iRegL op1, iRegL op2)
 %{
   match(Set cr (OverflowAddL op1 op2));
 
-  format %{ "cmn   $op1, $op2\t# overflow check long" %}
+  format %{ "cmn   $op1, $op2\t# overflow check int64_t" %}
   ins_cost(INSN_COST);
   ins_encode %{
     __ cmn($op1$$Register, $op2$$Register);
   %}
 
@@ -13957,11 +13963,11 @@
 
 instruct overflowAddL_reg_imm(rFlagsReg cr, iRegL op1, immLAddSub op2)
 %{
   match(Set cr (OverflowAddL op1 op2));
 
-  format %{ "cmn   $op1, $op2\t# overflow check long" %}
+  format %{ "cmn   $op1, $op2\t# overflow check int64_t" %}
   ins_cost(INSN_COST);
   ins_encode %{
     __ cmn($op1$$Register, $op2$$constant);
   %}
 
@@ -13996,11 +14002,11 @@
 
 instruct overflowSubL_reg_reg(rFlagsReg cr, iRegL op1, iRegL op2)
 %{
   match(Set cr (OverflowSubL op1 op2));
 
-  format %{ "cmp   $op1, $op2\t# overflow check long" %}
+  format %{ "cmp   $op1, $op2\t# overflow check int64_t" %}
   ins_cost(INSN_COST);
   ins_encode %{
     __ cmp($op1$$Register, $op2$$Register);
   %}
 
@@ -14009,11 +14015,11 @@
 
 instruct overflowSubL_reg_imm(rFlagsReg cr, iRegL op1, immLAddSub op2)
 %{
   match(Set cr (OverflowSubL op1 op2));
 
-  format %{ "cmp   $op1, $op2\t# overflow check long" %}
+  format %{ "cmp   $op1, $op2\t# overflow check int64_t" %}
   ins_cost(INSN_COST);
   ins_encode %{
     __ subs(zr, $op1$$Register, $op2$$constant);
   %}
 
@@ -14035,11 +14041,11 @@
 
 instruct overflowNegL_reg(rFlagsReg cr, immI0 zero, iRegL op1)
 %{
   match(Set cr (OverflowSubL zero op1));
 
-  format %{ "cmp   zr, $op1\t# overflow check long" %}
+  format %{ "cmp   zr, $op1\t# overflow check int64_t" %}
   ins_cost(INSN_COST);
   ins_encode %{
     __ cmp(zr, $op1$$Register);
   %}
 
@@ -14091,11 +14097,11 @@
 
 instruct overflowMulL_reg(rFlagsReg cr, iRegL op1, iRegL op2)
 %{
   match(Set cr (OverflowMulL op1 op2));
 
-  format %{ "mul   rscratch1, $op1, $op2\t#overflow check long\n\t"
+  format %{ "mul   rscratch1, $op1, $op2\t#overflow check int64_t\n\t"
             "smulh rscratch2, $op1, $op2\n\t"
             "cmp   rscratch2, rscratch1, ASR #63\n\t"
             "movw  rscratch1, #0x80000000\n\t"
             "cselw rscratch1, rscratch1, zr, NE\n\t"
             "cmpw  rscratch1, #1" %}
@@ -14117,11 +14123,11 @@
   match(If cmp (OverflowMulL op1 op2));
   predicate(n->in(1)->as_Bool()->_test._test == BoolTest::overflow
             || n->in(1)->as_Bool()->_test._test == BoolTest::no_overflow);
   effect(USE labl, KILL cr);
 
-  format %{ "mul   rscratch1, $op1, $op2\t#overflow check long\n\t"
+  format %{ "mul   rscratch1, $op1, $op2\t#overflow check int64_t\n\t"
             "smulh rscratch2, $op1, $op2\n\t"
             "cmp   rscratch2, rscratch1, ASR #63\n\t"
             "b$cmp $labl" %}
   ins_cost(4 * INSN_COST); // Branch is rare so treat as INSN_COST
   ins_encode %{
@@ -14881,11 +14887,11 @@
 instruct cmpL_branch_sign(cmpOpLtGe cmp, iRegL op1, immL0 op2, label labl) %{
   match(If cmp (CmpL op1 op2));
   effect(USE labl);
 
   ins_cost(BRANCH_COST);
-  format %{ "cb$cmp   $op1, $labl # long" %}
+  format %{ "cb$cmp   $op1, $labl # int64_t" %}
   ins_encode %{
     Label* L = $labl$$label;
     Assembler::Condition cond =
       ((Assembler::Condition)$cmp$$cmpcode == Assembler::LT) ? Assembler::NE : Assembler::EQ;
     __ tbr(cond, $op1$$Register, 63, *L);
@@ -14948,11 +14954,11 @@
 instruct far_cmpL_branch_sign(cmpOpLtGe cmp, iRegL op1, immL0 op2, label labl) %{
   match(If cmp (CmpL op1 op2));
   effect(USE labl);
 
   ins_cost(BRANCH_COST);
-  format %{ "cb$cmp   $op1, $labl # long" %}
+  format %{ "cb$cmp   $op1, $labl # int64_t" %}
   ins_encode %{
     Label* L = $labl$$label;
     Assembler::Condition cond =
       ((Assembler::Condition)$cmp$$cmpcode == Assembler::LT) ? Assembler::NE : Assembler::EQ;
     __ tbr(cond, $op1$$Register, 63, *L, /*far*/true);
@@ -15013,11 +15019,11 @@
   match(Set cr (CmpL (AndL op1 op2) op3));
   predicate(Assembler::operand_valid_for_logical_immediate
             (/*is_32*/false, n->in(1)->in(2)->get_long()));
 
   ins_cost(INSN_COST);
-  format %{ "tst $op1, $op2 # long" %}
+  format %{ "tst $op1, $op2 # int64_t" %}
   ins_encode %{
     __ tst($op1$$Register, $op2$$constant);
   %}
   ins_pipe(ialu_reg_reg);
 %}
@@ -15037,11 +15043,11 @@
 
 instruct cmpL_and_reg(cmpOp cmp, iRegL op1, iRegL op2, immL0 op3, rFlagsReg cr) %{
   match(Set cr (CmpL (AndL op1 op2) op3));
 
   ins_cost(INSN_COST);
-  format %{ "tst $op1, $op2 # long" %}
+  format %{ "tst $op1, $op2 # int64_t" %}
   ins_encode %{
     __ tst($op1$$Register, $op2$$Register);
   %}
   ins_pipe(ialu_reg_reg);
 %}
diff a/src/hotspot/cpu/aarch64/assembler_aarch64.cpp b/src/hotspot/cpu/aarch64/assembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/assembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/assembler_aarch64.cpp
@@ -29,11 +29,11 @@
 #include "asm/assembler.hpp"
 #include "asm/assembler.inline.hpp"
 #include "interpreter/interpreter.hpp"
 
 #ifndef PRODUCT
-const unsigned long Assembler::asm_bp = 0x00007fffee09ac88;
+const uint64_t Assembler::asm_bp = 0x00007fffee09ac88;
 #endif
 
 #include "compiler/disassembler.hpp"
 #include "memory/resourceArea.hpp"
 #include "runtime/interfaceSupport.inline.hpp"
@@ -51,10 +51,16 @@
 
 #define BIND(label) bind(label); __ BLOCK_COMMENT(#label ":")
 
 static float unpack(unsigned value);
 
+#ifdef _WIN64
+address Assembler::locate_next_instruction(address inst) {
+	return inst + Assembler::instruction_size;
+}
+#endif
+
 short Assembler::SIMD_Size_in_bytes[] = {
   // T8B, T16B, T4H, T8H, T2S, T4S, T1D, T2D, T1Q
        8,   16,   8,  16,   8,  16,   8,  16,  16
 };
 
@@ -1457,11 +1463,10 @@
        0x4cd17200, // ld1   {v0.16b}, [x16], x17
       };
     asm_check((unsigned int *)PC, vector_insns,
               sizeof vector_insns / sizeof vector_insns[0]);
   }
-
 #endif // ASSERT
 }
 
 #undef __
 
@@ -1494,19 +1499,20 @@
       Disassembler::decode((address)start + len, (address)start);
     else
       Disassembler::decode((address)start, (address)start + len);
   }
 
-  JNIEXPORT void das1(unsigned long insn) {
+  JNIEXPORT void das1(uint64_t insn) {
     das(insn, 1);
   }
 }
 
 #define gas_assert(ARG1) assert(ARG1, #ARG1)
 
 #define __ as->
 
+
 void Address::lea(MacroAssembler *as, Register r) const {
   Relocation* reloc = _rspec.reloc();
   relocInfo::relocType rtype = (relocInfo::relocType) reloc->type();
 
   switch(_mode) {
@@ -1518,11 +1524,11 @@
     else
       __ sub(r, _base, -_offset);
       break;
   }
   case base_plus_offset_reg: {
-    __ add(r, _base, _index, _ext.op(), MAX(_ext.shift(), 0));
+    __ add(r, _base, _index, _ext.op(), MAX2(_ext.shift(), 0));
     break;
   }
   case literal: {
     if (rtype == relocInfo::none)
       __ mov(r, target());
@@ -1533,31 +1539,31 @@
   default:
     ShouldNotReachHere();
   }
 }
 
-void Assembler::adrp(Register reg1, const Address &dest, unsigned long &byte_offset) {
+void Assembler::adrp(Register reg1, const Address &dest, uint64_t &byte_offset) {
   ShouldNotReachHere();
 }
 
 #undef __
 
 #define starti Instruction_aarch64 do_not_use(this); set_current(&do_not_use)
 
   void Assembler::adr(Register Rd, address adr) {
-    long offset = adr - pc();
+    int64_t offset = adr - pc();
     int offset_lo = offset & 3;
     offset >>= 2;
     starti;
     f(0, 31), f(offset_lo, 30, 29), f(0b10000, 28, 24), sf(offset, 23, 5);
     rf(Rd, 0);
   }
 
   void Assembler::_adrp(Register Rd, address adr) {
     uint64_t pc_page = (uint64_t)pc() >> 12;
     uint64_t adr_page = (uint64_t)adr >> 12;
-    long offset = adr_page - pc_page;
+    int64_t offset = adr_page - pc_page;
     int offset_lo = offset & 3;
     offset >>= 2;
     starti;
     f(1, 31), f(offset_lo, 30, 29), f(0b10000, 28, 24), sf(offset, 23, 5);
     rf(Rd, 0);
@@ -1702,13 +1708,13 @@
     srf(Rd, 0);
 
   srf(Rn, 5);
 }
 
-bool Assembler::operand_valid_for_add_sub_immediate(long imm) {
+bool Assembler::operand_valid_for_add_sub_immediate(int64_t imm) {
   bool shift = false;
-  unsigned long uimm = uabs(imm);
+  uint64_t uimm = uabs(imm);
   if (uimm < (1 << 12))
     return true;
   if (uimm < (1 << 24)
       && ((uimm >> 12) << 12 == uimm)) {
     return true;
diff a/src/hotspot/cpu/aarch64/assembler_aarch64.hpp b/src/hotspot/cpu/aarch64/assembler_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/assembler_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/assembler_aarch64.hpp
@@ -197,11 +197,11 @@
   static inline int32_t sextract(uint32_t val, int msb, int lsb) {
     uint32_t uval = extract(val, msb, lsb);
     return extend(uval, msb - lsb);
   }
 
-  static void patch(address a, int msb, int lsb, unsigned long val) {
+  static void patch(address a, int msb, int lsb, uint64_t val) {
     int nbits = msb - lsb + 1;
     guarantee(val < (1U << nbits), "Field too big for insn");
     assert_cond(msb >= lsb);
     unsigned mask = (1U << nbits) - 1;
     val <<= lsb;
@@ -210,13 +210,13 @@
     target &= ~mask;
     target |= val;
     *(unsigned *)a = target;
   }
 
-  static void spatch(address a, int msb, int lsb, long val) {
+  static void spatch(address a, int msb, int lsb, int64_t val) {
     int nbits = msb - lsb + 1;
-    long chk = val >> (nbits - 1);
+    int64_t chk = val >> (nbits - 1);
     guarantee (chk == -1 || chk == 0, "Field too big for insn");
     unsigned uval = val;
     unsigned mask = (1U << nbits) - 1;
     uval &= mask;
     uval <<= lsb;
@@ -243,13 +243,13 @@
 
   void f(unsigned val, int bit) {
     f(val, bit, bit);
   }
 
-  void sf(long val, int msb, int lsb) {
+  void sf(int64_t val, int msb, int lsb) {
     int nbits = msb - lsb + 1;
-    long chk = val >> (nbits - 1);
+    int64_t chk = val >> (nbits - 1);
     guarantee (chk == -1 || chk == 0, "Field too big for insn");
     unsigned uval = val;
     unsigned mask = (1U << nbits) - 1;
     uval &= mask;
     f(uval, lsb + nbits - 1, lsb);
@@ -355,11 +355,11 @@
   };
 
  private:
   Register _base;
   Register _index;
-  long _offset;
+  int64_t _offset;
   enum mode _mode;
   extend _ext;
 
   RelocationHolder _rspec;
 
@@ -378,13 +378,13 @@
     : _mode(no_mode) { }
   Address(Register r)
     : _base(r), _index(noreg), _offset(0), _mode(base_plus_offset), _target(0) { }
   Address(Register r, int o)
     : _base(r), _index(noreg), _offset(o), _mode(base_plus_offset), _target(0) { }
-  Address(Register r, long o)
+  Address(Register r, int64_t o)
     : _base(r), _index(noreg), _offset(o), _mode(base_plus_offset), _target(0) { }
-  Address(Register r, unsigned long o)
+  Address(Register r, uint64_t o)
     : _base(r), _index(noreg), _offset(o), _mode(base_plus_offset), _target(0) { }
 #ifdef ASSERT
   Address(Register r, ByteSize disp)
     : _base(r), _index(noreg), _offset(in_bytes(disp)), _mode(base_plus_offset), _target(0) { }
 #endif
@@ -415,16 +415,16 @@
       _offset = index.as_constant() << ext.shift();
     }
   }
 
   Register base() const {
-    guarantee((_mode == base_plus_offset | _mode == base_plus_offset_reg
-               | _mode == post | _mode == post_reg),
+    guarantee(((_mode == base_plus_offset) | (_mode == base_plus_offset_reg)
+               | (_mode == post) | (_mode == post_reg)),
               "wrong mode");
     return _base;
   }
-  long offset() const {
+  int64_t offset() const {
     return _offset;
   }
   Register index() const {
     return _index;
   }
@@ -552,11 +552,11 @@
               "Bad addressing mode for non-temporal op");
   }
 
   void lea(MacroAssembler *, Register) const;
 
-  static bool offset_ok_for_immed(long offset, int shift) {
+  static bool offset_ok_for_immed(int64_t offset, int shift) {
     unsigned mask = (1 << shift) - 1;
     if (offset < 0 || offset & mask) {
       return (uabs(offset) < (1 << (20 - 12))); // Unscaled offset
     } else {
       return ((offset >> shift) < (1 << (21 - 10 + 1))); // Scaled, unsigned offset
@@ -614,15 +614,20 @@
 } prfop;
 
 class Assembler : public AbstractAssembler {
 
 #ifndef PRODUCT
-  static const unsigned long asm_bp;
+  static const uint64_t asm_bp;
 
   void emit_long(jint x) {
-    if ((unsigned long)pc() == asm_bp)
+    if ((uint64_t)pc() == asm_bp) {
+#ifdef _WIN64
+      __nop();
+#else
       asm volatile ("nop");
+#endif
+    }
     AbstractAssembler::emit_int32(x);
   }
 #else
   void emit_long(jint x) {
     AbstractAssembler::emit_int32(x);
@@ -658,21 +663,23 @@
 
   Address post(Register base, Register idx) {
     return Address(Post(base, idx));
   }
 
+  static address locate_next_instruction(address inst);
+
   Instruction_aarch64* current;
 
   void set_current(Instruction_aarch64* i) { current = i; }
 
   void f(unsigned val, int msb, int lsb) {
     current->f(val, msb, lsb);
   }
   void f(unsigned val, int msb) {
     current->f(val, msb, msb);
   }
-  void sf(long val, int msb, int lsb) {
+  void sf(int64_t val, int msb, int lsb) {
     current->sf(val, msb, lsb);
   }
   void rf(Register reg, int lsb) {
     current->rf(reg, lsb);
   }
@@ -717,12 +724,11 @@
     wrap_label(Rd, L, &Assembler::Assembler::adr);
   }
   void _adrp(Register Rd, Label &L) {
     wrap_label(Rd, L, &Assembler::_adrp);
   }
-
-  void adrp(Register Rd, const Address &dest, unsigned long &offset);
+  void adrp(Register Rd, const Address &dest, uint64_t &offset);
 
 #undef INSN
 
   void add_sub_immediate(Register Rd, Register Rn, unsigned uimm, int op,
                          int negated_op);
@@ -844,21 +850,21 @@
 
   // The maximum range of a branch is fixed for the AArch64
   // architecture.  In debug mode we shrink it in order to test
   // trampolines, but not so small that branches in the interpreter
   // are out of range.
-  static const unsigned long branch_range = NOT_DEBUG(128 * M) DEBUG_ONLY(2 * M);
+  static const uint64_t branch_range = NOT_DEBUG(128 * M) DEBUG_ONLY(2 * M);
 
   static bool reachable_from_branch_at(address branch, address target) {
     return uabs(target - branch) < branch_range;
   }
 
   // Unconditional branch (immediate)
 #define INSN(NAME, opcode)                                              \
   void NAME(address dest) {                                             \
     starti;                                                             \
-    long offset = (dest - pc()) >> 2;                                   \
+    int64_t offset = (dest - pc()) >> 2;                                   \
     DEBUG_ONLY(assert(reachable_from_branch_at(pc(), dest), "debug only")); \
     f(opcode, 31), f(0b00101, 30, 26), sf(offset, 25, 0);               \
   }                                                                     \
   void NAME(Label &L) {                                                 \
     wrap_label(L, &Assembler::NAME);                                    \
@@ -871,11 +877,11 @@
 #undef INSN
 
   // Compare & branch (immediate)
 #define INSN(NAME, opcode)                              \
   void NAME(Register Rt, address dest) {                \
-    long offset = (dest - pc()) >> 2;                   \
+    int64_t offset = (dest - pc()) >> 2;                \
     starti;                                             \
     f(opcode, 31, 24), sf(offset, 23, 5), rf(Rt, 0);    \
   }                                                     \
   void NAME(Register Rt, Label &L) {                    \
     wrap_label(Rt, L, &Assembler::NAME);                \
@@ -889,11 +895,11 @@
 #undef INSN
 
   // Test & branch (immediate)
 #define INSN(NAME, opcode)                                              \
   void NAME(Register Rt, int bitpos, address dest) {                    \
-    long offset = (dest - pc()) >> 2;                                   \
+    int64_t offset = (dest - pc()) >> 2;                                \
     int b5 = bitpos >> 5;                                               \
     bitpos &= 0x1f;                                                     \
     starti;                                                             \
     f(b5, 31), f(opcode, 30, 24), f(bitpos, 23, 19), sf(offset, 18, 5); \
     rf(Rt, 0);                                                          \
@@ -910,11 +916,11 @@
   // Conditional branch (immediate)
   enum Condition
     {EQ, NE, HS, CS=HS, LO, CC=LO, MI, PL, VS, VC, HI, LS, GE, LT, GT, LE, AL, NV};
 
   void br(Condition  cond, address dest) {
-    long offset = (dest - pc()) >> 2;
+    int64_t offset = (dest - pc()) >> 2;
     starti;
     f(0b0101010, 31, 25), f(0, 24), sf(offset, 23, 5), f(0, 4), f(cond, 3, 0);
   }
 
 #define INSN(NAME, cond)                        \
@@ -1290,11 +1296,11 @@
 #undef INSN
 
   // Load register (literal)
 #define INSN(NAME, opc, V)                                              \
   void NAME(Register Rt, address dest) {                                \
-    long offset = (dest - pc()) >> 2;                                   \
+    int64_t offset = (dest - pc()) >> 2;                                \
     starti;                                                             \
     f(opc, 31, 30), f(0b011, 29, 27), f(V, 26), f(0b00, 25, 24),        \
       sf(offset, 23, 5);                                                \
     rf(Rt, 0);                                                          \
   }                                                                     \
@@ -1315,11 +1321,11 @@
 
 #undef INSN
 
 #define INSN(NAME, opc, V)                                              \
   void NAME(FloatRegister Rt, address dest) {                           \
-    long offset = (dest - pc()) >> 2;                                   \
+    int64_t offset = (dest - pc()) >> 2;                                \
     starti;                                                             \
     f(opc, 31, 30), f(0b011, 29, 27), f(V, 26), f(0b00, 25, 24),        \
       sf(offset, 23, 5);                                                \
     rf((Register)Rt, 0);                                                \
   }
@@ -1330,11 +1336,11 @@
 
 #undef INSN
 
 #define INSN(NAME, opc, V)                                              \
   void NAME(address dest, prfop op = PLDL1KEEP) {                       \
-    long offset = (dest - pc()) >> 2;                                   \
+    int64_t offset = (dest - pc()) >> 2;                                \
     starti;                                                             \
     f(opc, 31, 30), f(0b011, 29, 27), f(V, 26), f(0b00, 25, 24),        \
       sf(offset, 23, 5);                                                \
     f(op, 4, 0);                                                        \
   }                                                                     \
@@ -1406,11 +1412,11 @@
     // make it worth sharing.
     if (adr.getMode() == Address::literal) {
       assert(size == 0b10 || size == 0b11, "bad operand size in ldr");
       assert(op == 0b01, "literal form can only be used with loads");
       f(size & 0b01, 31, 30), f(0b011, 29, 27), f(0b00, 25, 24);
-      long offset = (adr.target() - pc()) >> 2;
+      int64_t offset = (adr.target() - pc()) >> 2;
       sf(offset, 23, 5);
       code_section()->relocate(pc(), adr.rspec());
       return;
     }
 
@@ -1527,15 +1533,17 @@
   INSN(eonw, 0, 0b10, 1);
   INSN(bicsw, 0, 0b11, 1);
 
 #undef INSN
 
+#ifndef _WIN64
   // Aliases for short forms of orn
 void mvn(Register Rd, Register Rm,
             enum shift_kind kind = LSL, unsigned shift = 0) {
   orn(Rd, zr, Rm, kind, shift);
 }
+#endif
 
 void mvnw(Register Rd, Register Rm,
             enum shift_kind kind = LSL, unsigned shift = 0) {
   ornw(Rd, zr, Rm, kind, shift);
 }
@@ -2677,11 +2685,11 @@
 
   // Stack overflow checking
   virtual void bang_stack_with_offset(int offset);
 
   static bool operand_valid_for_logical_immediate(bool is32, uint64_t imm);
-  static bool operand_valid_for_add_sub_immediate(long imm);
+  static bool operand_valid_for_add_sub_immediate(int64_t imm);
   static bool operand_valid_for_float_immediate(double imm);
 
   void emit_data64(jlong data, relocInfo::relocType rtype, int format = 0);
   void emit_data64(jlong data, RelocationHolder const& rspec, int format = 0);
 };
diff a/src/hotspot/cpu/aarch64/c1_Defs_aarch64.hpp b/src/hotspot/cpu/aarch64/c1_Defs_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/c1_Defs_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/c1_Defs_aarch64.hpp
@@ -42,29 +42,29 @@
 // registers
 enum {
   pd_nof_cpu_regs_frame_map = RegisterImpl::number_of_registers,       // number of registers used during code emission
   pd_nof_fpu_regs_frame_map = FloatRegisterImpl::number_of_registers,  // number of registers used during code emission
 
-  pd_nof_caller_save_cpu_regs_frame_map = 19 - 2,  // number of registers killed by calls
+  pd_nof_caller_save_cpu_regs_frame_map = 19 - 2 /* rscratch1 and rsractch2 */ WIN64_ONLY(- 1 /* r18 */),  // number of registers killed by calls
   pd_nof_caller_save_fpu_regs_frame_map = 32,  // number of registers killed by calls
 
-  pd_first_callee_saved_reg = 19 - 2,
-  pd_last_callee_saved_reg = 26 - 2,
+  pd_first_callee_saved_reg = 19 - 2 /* rscratch1 and rsractch2 */ WIN64_ONLY(- 1 /* r18 */),
+  pd_last_callee_saved_reg = 26 - 2 /* rscratch1 and rsractch2 */ WIN64_ONLY(- 1 /* r18 */),
 
-  pd_last_allocatable_cpu_reg = 16,
+  pd_last_allocatable_cpu_reg = 16 WIN64_ONLY(- 1 /* r18 */),
 
   pd_nof_cpu_regs_reg_alloc
     = pd_last_allocatable_cpu_reg + 1,  // number of registers that are visible to register allocator
   pd_nof_fpu_regs_reg_alloc = 8,  // number of registers that are visible to register allocator
 
   pd_nof_cpu_regs_linearscan = 32, // number of registers visible to linear scan
   pd_nof_fpu_regs_linearscan = pd_nof_fpu_regs_frame_map, // number of registers visible to linear scan
   pd_nof_xmm_regs_linearscan = 0, // like sparc we don't have any of these
   pd_first_cpu_reg = 0,
-  pd_last_cpu_reg = 16,
+  pd_last_cpu_reg = 16 WIN64_ONLY(- 1 /* r18 */),
   pd_first_byte_reg = 0,
-  pd_last_byte_reg = 16,
+  pd_last_byte_reg = 16 WIN64_ONLY(- 1 /* r18 */),
   pd_first_fpu_reg = pd_nof_cpu_regs_frame_map,
   pd_last_fpu_reg =  pd_first_fpu_reg + 31,
 
   pd_first_callee_saved_fpu_reg = 8 + pd_first_fpu_reg,
   pd_last_callee_saved_fpu_reg = 15 + pd_first_fpu_reg,
diff a/src/hotspot/cpu/aarch64/c1_FpuStackSim_aarch64.cpp b/src/hotspot/cpu/aarch64/c1_FpuStackSim_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/c1_FpuStackSim_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/c1_FpuStackSim_aarch64.cpp
@@ -26,5 +26,6 @@
 //--------------------------------------------------------
 //               FpuStackSim
 //--------------------------------------------------------
 
 // No FPU stack on AARCH64
+#include "precompiled.hpp"
diff a/src/hotspot/cpu/aarch64/c1_FrameMap_aarch64.cpp b/src/hotspot/cpu/aarch64/c1_FrameMap_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/c1_FrameMap_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/c1_FrameMap_aarch64.cpp
@@ -179,20 +179,25 @@
   map_register(i, r13); r13_opr = LIR_OprFact::single_cpu(i); i++;
   map_register(i, r14); r14_opr = LIR_OprFact::single_cpu(i); i++;
   map_register(i, r15); r15_opr = LIR_OprFact::single_cpu(i); i++;
   map_register(i, r16); r16_opr = LIR_OprFact::single_cpu(i); i++;
   map_register(i, r17); r17_opr = LIR_OprFact::single_cpu(i); i++;
+#ifndef _WIN64
   map_register(i, r18); r18_opr = LIR_OprFact::single_cpu(i); i++;
+#endif
   map_register(i, r19); r19_opr = LIR_OprFact::single_cpu(i); i++;
   map_register(i, r20); r20_opr = LIR_OprFact::single_cpu(i); i++;
   map_register(i, r21); r21_opr = LIR_OprFact::single_cpu(i); i++;
   map_register(i, r22); r22_opr = LIR_OprFact::single_cpu(i); i++;
   map_register(i, r23); r23_opr = LIR_OprFact::single_cpu(i); i++;
   map_register(i, r24); r24_opr = LIR_OprFact::single_cpu(i); i++;
   map_register(i, r25); r25_opr = LIR_OprFact::single_cpu(i); i++;
   map_register(i, r26); r26_opr = LIR_OprFact::single_cpu(i); i++;
 
+#ifdef _WIN64
+  map_register(i, r18); r18_opr = LIR_OprFact::single_cpu(i); i++; // TLS for Win
+#endif
   map_register(i, r27); r27_opr = LIR_OprFact::single_cpu(i); i++; // rheapbase
   map_register(i, r28); r28_opr = LIR_OprFact::single_cpu(i); i++; // rthread
   map_register(i, r29); r29_opr = LIR_OprFact::single_cpu(i); i++; // rfp
   map_register(i, r30); r30_opr = LIR_OprFact::single_cpu(i); i++; // lr
   map_register(i, r31_sp); sp_opr = LIR_OprFact::single_cpu(i); i++; // sp
@@ -225,11 +230,14 @@
   _caller_save_cpu_regs[11] = r13_opr;
   _caller_save_cpu_regs[12] = r14_opr;
   _caller_save_cpu_regs[13] = r15_opr;
   _caller_save_cpu_regs[14] = r16_opr;
   _caller_save_cpu_regs[15] = r17_opr;
+#ifndef _WIN64
+  // r18 used for TLS on Windows
   _caller_save_cpu_regs[16] = r18_opr;
+#endif
 
   for (int i = 0; i < 8; i++) {
     _caller_save_fpu_regs[i] = LIR_OprFact::single_fpu(i);
   }
 
diff a/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp
@@ -1368,11 +1368,11 @@
     // get object class
     // not a safepoint as obj null check happens earlier
     __ load_klass(klass_RInfo, obj);
     if (k->is_loaded()) {
       // See if we get an immediate positive hit
-      __ ldr(rscratch1, Address(klass_RInfo, long(k->super_check_offset())));
+      __ ldr(rscratch1, Address(klass_RInfo, int64_t(k->super_check_offset())));
       __ cmp(k_RInfo, rscratch1);
       if ((juint)in_bytes(Klass::secondary_super_cache_offset()) != k->super_check_offset()) {
         __ br(Assembler::NE, *failure_target);
         // successful cast, fall through to profile or jump
       } else {
@@ -1995,11 +1995,11 @@
           __ cmpw(reg1, imm);
         else
           __ subs(zr, reg1, imm);
         return;
       } else {
-        __ mov(rscratch1, imm);
+        __ mov(rscratch1,(uint64_t) imm);
         if (is_32bit)
           __ cmpw(reg1, rscratch1);
         else
           __ cmp(reg1, rscratch1);
         return;
@@ -2032,11 +2032,11 @@
       ShouldNotReachHere();
     }
   } else if (code == lir_cmp_l2i) {
     Label done;
     __ cmp(left->as_register_lo(), right->as_register_lo());
-    __ mov(dst->as_register(), (u_int64_t)-1L);
+    __ mov(dst->as_register(), (uint64_t)-1L);
     __ br(Assembler::LT, done);
     __ csinc(dst->as_register(), zr, zr, Assembler::EQ);
     __ bind(done);
   } else {
     ShouldNotReachHere();
@@ -2300,11 +2300,10 @@
   }
 
   assert(default_type != NULL && default_type->is_array_klass() && default_type->is_loaded(), "must be true at this point");
 
   int elem_size = type2aelembytes(basic_type);
-  int shift_amount;
   int scale = exact_log2(elem_size);
 
   Address src_length_addr = Address(src, arrayOopDesc::length_offset_in_bytes());
   Address dst_length_addr = Address(dst, arrayOopDesc::length_offset_in_bytes());
   Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());
@@ -2691,11 +2690,11 @@
   Register crc = op->crc()->as_register();
   Register val = op->val()->as_register();
   Register res = op->result_opr()->as_register();
 
   assert_different_registers(val, crc, res);
-  unsigned long offset;
+  uint64_t offset;
   __ adrp(res, ExternalAddress(StubRoutines::crc_table_addr()), offset);
   if (offset) __ add(res, res, offset);
 
   __ mvnw(crc, crc); // ~crc
   __ update_byte_crc32(crc, val, res);
diff a/src/hotspot/cpu/aarch64/c1_LIRGenerator_aarch64.cpp b/src/hotspot/cpu/aarch64/c1_LIRGenerator_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/c1_LIRGenerator_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/c1_LIRGenerator_aarch64.cpp
@@ -288,11 +288,11 @@
   __ load(generate_address(base, disp, type), reg1, info);
   __ cmp(condition, reg, reg1);
 }
 
 
-bool LIRGenerator::strength_reduce_multiply(LIR_Opr left, int c, LIR_Opr result, LIR_Opr tmp) {
+bool LIRGenerator::strength_reduce_multiply(LIR_Opr left, jint c, LIR_Opr result, LIR_Opr tmp) {
 
   if (is_power_of_2(c - 1)) {
     __ shift_left(left, exact_log2(c - 1), tmp);
     __ add(tmp, left, result);
     return true;
diff a/src/hotspot/cpu/aarch64/compiledIC_aot_aarch64.cpp b/src/hotspot/cpu/aarch64/compiledIC_aot_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/compiledIC_aot_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/compiledIC_aot_aarch64.cpp
@@ -20,10 +20,11 @@
  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  * or visit www.oracle.com if you need additional information or have any
  * questions.
  */
 
+#include "precompiled.hpp"
 #include "aot/compiledIC_aot.hpp"
 #include "code/codeCache.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 
diff a/src/hotspot/cpu/aarch64/frame_aarch64.cpp b/src/hotspot/cpu/aarch64/frame_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/frame_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/frame_aarch64.cpp
@@ -671,19 +671,18 @@
 
 #undef DESCRIBE_FP_OFFSET
 
 #define DESCRIBE_FP_OFFSET(name)                                        \
   {                                                                     \
-    unsigned long *p = (unsigned long *)fp;                             \
-    printf("0x%016lx 0x%016lx %s\n", (unsigned long)(p + frame::name##_offset), \
+    uint64_t *p = (uint64_t *)fp;                             \
+    printf("0x%016lx 0x%016lx %s\n", (uint64_t)(p + frame::name##_offset), \
            p[frame::name##_offset], #name);                             \
   }
-
-static __thread unsigned long nextfp;
-static __thread unsigned long nextpc;
-static __thread unsigned long nextsp;
-static __thread RegisterMap *reg_map;
+static THREAD_LOCAL uint64_t nextfp;
+static THREAD_LOCAL uint64_t nextpc;
+static THREAD_LOCAL uint64_t nextsp;
+static THREAD_LOCAL RegisterMap *reg_map;
 
 static void printbc(Method *m, intptr_t bcx) {
   const char *name;
   char buf[16];
   if (m->validate_bci_from_bcp((address)bcx) < 0
@@ -697,11 +696,11 @@
   }
   ResourceMark rm;
   printf("%s : %s ==> %s\n", m->name_and_sig_as_C_string(), buf, name);
 }
 
-void internal_pf(unsigned long sp, unsigned long fp, unsigned long pc, unsigned long bcx) {
+void internal_pf(uint64_t sp, uint64_t fp, uint64_t pc, uint64_t bcx) {
   if (! fp)
     return;
 
   DESCRIBE_FP_OFFSET(return_addr);
   DESCRIBE_FP_OFFSET(link);
@@ -711,30 +710,30 @@
   DESCRIBE_FP_OFFSET(interpreter_frame_mdp);
   DESCRIBE_FP_OFFSET(interpreter_frame_cache);
   DESCRIBE_FP_OFFSET(interpreter_frame_locals);
   DESCRIBE_FP_OFFSET(interpreter_frame_bcp);
   DESCRIBE_FP_OFFSET(interpreter_frame_initial_sp);
-  unsigned long *p = (unsigned long *)fp;
+  uint64_t *p = (uint64_t *)fp;
 
   // We want to see all frames, native and Java.  For compiled and
   // interpreted frames we have special information that allows us to
   // unwind them; for everything else we assume that the native frame
   // pointer chain is intact.
   frame this_frame((intptr_t*)sp, (intptr_t*)fp, (address)pc);
   if (this_frame.is_compiled_frame() ||
       this_frame.is_interpreted_frame()) {
     frame sender = this_frame.sender(reg_map);
-    nextfp = (unsigned long)sender.fp();
-    nextpc = (unsigned long)sender.pc();
-    nextsp = (unsigned long)sender.unextended_sp();
+    nextfp = (uint64_t)sender.fp();
+    nextpc = (uint64_t)sender.pc();
+    nextsp = (uint64_t)sender.unextended_sp();
   } else {
     nextfp = p[frame::link_offset];
     nextpc = p[frame::return_addr_offset];
-    nextsp = (unsigned long)&p[frame::sender_sp_offset];
+    nextsp = (uint64_t)&p[frame::sender_sp_offset];
   }
 
-  if (bcx == -1ul)
+  if (bcx == -1ull)
     bcx = p[frame::interpreter_frame_bcp_offset];
 
   if (Interpreter::contains((address)pc)) {
     Method* m = (Method*)p[frame::interpreter_frame_method_offset];
     if(m && m->is_method()) {
@@ -764,12 +763,12 @@
   if (cb && cb->frame_size())
     nextfp = nextsp + wordSize * (cb->frame_size() - 2);
   internal_pf (nextsp, nextfp, nextpc, -1);
 }
 
-extern "C" void pf(unsigned long sp, unsigned long fp, unsigned long pc,
-                   unsigned long bcx, unsigned long thread) {
+extern "C" void pf(uint64_t sp, uint64_t fp, uint64_t pc,
+                   uint64_t bcx, uint64_t thread) {
   if (!reg_map) {
     reg_map = NEW_C_HEAP_OBJ(RegisterMap, mtNone);
     ::new (reg_map) RegisterMap((JavaThread*)thread, false);
   } else {
     *reg_map = RegisterMap((JavaThread*)thread, false);
@@ -784,13 +783,13 @@
 }
 
 // support for printing out where we are in a Java method
 // needs to be passed current fp and bcp register values
 // prints method name, bc index and bytecode name
-extern "C" void pm(unsigned long fp, unsigned long bcx) {
+extern "C" void pm(uint64_t fp, uint64_t bcx) {
   DESCRIBE_FP_OFFSET(interpreter_frame_method);
-  unsigned long *p = (unsigned long *)fp;
+  uint64_t *p = (uint64_t *)fp;
   Method* m = (Method*)p[frame::interpreter_frame_method_offset];
   printbc(m, bcx);
 }
 
 #ifndef PRODUCT
diff a/src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.cpp
@@ -171,20 +171,20 @@
     Register end = t1;
     Register heap_end = rscratch2;
     Label retry;
     __ bind(retry);
     {
-      unsigned long offset;
+      uint64_t offset;
       __ adrp(rscratch1, ExternalAddress((address) Universe::heap()->end_addr()), offset);
       __ ldr(heap_end, Address(rscratch1, offset));
     }
 
     ExternalAddress heap_top((address) Universe::heap()->top_addr());
 
     // Get the current top of the heap
     {
-      unsigned long offset;
+      uint64_t offset;
       __ adrp(rscratch1, heap_top, offset);
       // Use add() here after ARDP, rather than lea().
       // lea() does not generate anything if its offset is zero.
       // However, relocs expect to find either an ADD or a load/store
       // insn after an ADRP.  add() always generates an ADD insn, even
diff a/src/hotspot/cpu/aarch64/icache_aarch64.hpp b/src/hotspot/cpu/aarch64/icache_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/icache_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/icache_aarch64.hpp
@@ -29,10 +29,16 @@
 // Interface for updating the instruction cache.  Whenever the VM
 // modifies code, part of the processor instruction cache potentially
 // has to be flushed.
 
 class ICache : public AbstractICache {
+ private:
+#ifdef _WIN64
+  static void __clear_cache(char *start, char *end) {
+    FlushInstructionCache((HANDLE)GetCurrentProcess(), start, (SIZE_T)(end - start));
+  }
+#endif
  public:
   static void initialize();
   static void invalidate_word(address addr) {
     __clear_cache((char *)addr, (char *)(addr + 3));
   }
diff a/src/hotspot/cpu/aarch64/immediate_aarch64.cpp b/src/hotspot/cpu/aarch64/immediate_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/immediate_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/immediate_aarch64.cpp
@@ -19,11 +19,11 @@
  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  * or visit www.oracle.com if you need additional information or have any
  * questions.
  *
  */
-
+#include "precompiled.hpp"
 #include <stdlib.h>
 #include "immediate_aarch64.hpp"
 
 // there are at most 2^13 possible logical immediate encodings
 // however, some combinations of immr and imms are invalid
@@ -32,18 +32,18 @@
 static int li_table_entry_count;
 
 // for forward lookup we just use a direct array lookup
 // and assume that the cient has supplied a valid encoding
 // table[encoding] = immediate
-static u_int64_t LITable[LI_TABLE_SIZE];
+static uint64_t LITable[LI_TABLE_SIZE];
 
 // for reverse lookup we need a sparse map so we store a table of
 // immediate and encoding pairs sorted by immediate value
 
 struct li_pair {
-  u_int64_t immediate;
-  u_int32_t encoding;
+  uint64_t immediate;
+  uint32_t encoding;
 };
 
 static struct li_pair InverseLITable[LI_TABLE_SIZE];
 
 // comparator to sort entries in the inverse table
@@ -61,75 +61,75 @@
 }
 
 // helper functions used by expandLogicalImmediate
 
 // for i = 1, ... N result<i-1> = 1 other bits are zero
-static inline u_int64_t ones(int N)
+static inline uint64_t ones(int N)
 {
-  return (N == 64 ? (u_int64_t)-1UL : ((1UL << N) - 1));
+  return (N == 64 ? (uint64_t)-1ULL : ((1ULL << N) - 1));
 }
 
 /*
  * bit twiddling helpers for instruction decode
  */
 
 // 32 bit mask with bits [hi,...,lo] set
-static inline u_int32_t mask32(int hi = 31, int lo = 0)
+static inline uint32_t mask32(int hi = 31, int lo = 0)
 {
   int nbits = (hi + 1) - lo;
   return ((1 << nbits) - 1) << lo;
 }
 
-static inline u_int64_t mask64(int hi = 63, int lo = 0)
+static inline uint64_t mask64(int hi = 63, int lo = 0)
 {
   int nbits = (hi + 1) - lo;
   return ((1L << nbits) - 1) << lo;
 }
 
 // pick bits [hi,...,lo] from val
-static inline u_int32_t pick32(u_int32_t val, int hi = 31, int lo = 0)
+static inline uint32_t pick32(uint32_t val, int hi = 31, int lo = 0)
 {
   return (val & mask32(hi, lo));
 }
 
 // pick bits [hi,...,lo] from val
-static inline u_int64_t pick64(u_int64_t val, int hi = 31, int lo = 0)
+static inline uint64_t pick64(uint64_t val, int hi = 31, int lo = 0)
 {
   return (val & mask64(hi, lo));
 }
 
 // mask [hi,lo] and shift down to start at bit 0
-static inline u_int32_t pickbits32(u_int32_t val, int hi = 31, int lo = 0)
+static inline uint32_t pickbits32(uint32_t val, int hi = 31, int lo = 0)
 {
   return (pick32(val, hi, lo) >> lo);
 }
 
 // mask [hi,lo] and shift down to start at bit 0
-static inline u_int64_t pickbits64(u_int64_t val, int hi = 63, int lo = 0)
+static inline uint64_t pickbits64(uint64_t val, int hi = 63, int lo = 0)
 {
   return (pick64(val, hi, lo) >> lo);
 }
 
 // result<0> to val<N>
-static inline u_int64_t pickbit(u_int64_t val, int N)
+static inline uint64_t pickbit(uint64_t val, int N)
 {
   return pickbits64(val, N, N);
 }
 
-static inline u_int32_t uimm(u_int32_t val, int hi, int lo)
+static inline uint32_t uimm(uint32_t val, int hi, int lo)
 {
   return pickbits32(val, hi, lo);
 }
 
 // SPEC bits(M*N) Replicate(bits(M) x, integer N);
 // this is just an educated guess
 
-u_int64_t replicate(u_int64_t bits, int nbits, int count)
+uint64_t replicate(uint64_t bits, int nbits, int count)
 {
-  u_int64_t result = 0;
+  uint64_t result = 0;
   // nbits may be 64 in which case we want mask to be -1
-  u_int64_t mask = ones(nbits);
+  uint64_t mask = ones(nbits);
   for (int i = 0; i < count ; i++) {
     result <<= nbits;
     result |= (bits & mask);
   }
   return result;
@@ -138,28 +138,28 @@
 // this function writes the supplied bimm reference and returns a
 // boolean to indicate success (1) or fail (0) because an illegal
 // encoding must be treated as an UNALLOC instruction
 
 // construct a 32 bit immediate value for a logical immediate operation
-int expandLogicalImmediate(u_int32_t immN, u_int32_t immr,
-                            u_int32_t imms, u_int64_t &bimm)
+int expandLogicalImmediate(uint32_t immN, uint32_t immr,
+                            uint32_t imms, uint64_t &bimm)
 {
   int len;                  // ought to be <= 6
-  u_int32_t levels;         // 6 bits
-  u_int32_t tmask_and;      // 6 bits
-  u_int32_t wmask_and;      // 6 bits
-  u_int32_t tmask_or;       // 6 bits
-  u_int32_t wmask_or;       // 6 bits
-  u_int64_t imm64;          // 64 bits
-  u_int64_t tmask, wmask;   // 64 bits
-  u_int32_t S, R, diff;     // 6 bits?
+  uint32_t levels;         // 6 bits
+  uint32_t tmask_and;      // 6 bits
+  uint32_t wmask_and;      // 6 bits
+  uint32_t tmask_or;       // 6 bits
+  uint32_t wmask_or;       // 6 bits
+  uint64_t imm64;          // 64 bits
+  uint64_t tmask, wmask;   // 64 bits
+  uint32_t S, R, diff;     // 6 bits?
 
   if (immN == 1) {
     len = 6; // looks like 7 given the spec above but this cannot be!
   } else {
     len = 0;
-    u_int32_t val = (~imms & 0x3f);
+    uint32_t val = (~imms & 0x3f);
     for (int i = 5; i > 0; i--) {
       if (val & (1 << i)) {
         len = i;
         break;
       }
@@ -168,11 +168,11 @@
       return 0;
     }
     // for valid inputs leading 1s in immr must be less than leading
     // zeros in imms
     int len2 = 0;                   // ought to be < len
-    u_int32_t val2 = (~immr & 0x3f);
+    uint32_t val2 = (~immr & 0x3f);
     for (int i = 5; i > 0; i--) {
       if (!(val2 & (1 << i))) {
         len2 = i;
         break;
       }
@@ -197,16 +197,16 @@
   tmask_or = (diff & levels) & 0x3f;
   tmask = 0xffffffffffffffffULL;
 
   for (int i = 0; i < 6; i++) {
     int nbits = 1 << i;
-    u_int64_t and_bit = pickbit(tmask_and, i);
-    u_int64_t or_bit = pickbit(tmask_or, i);
-    u_int64_t and_bits_sub = replicate(and_bit, 1, nbits);
-    u_int64_t or_bits_sub = replicate(or_bit, 1, nbits);
-    u_int64_t and_bits_top = (and_bits_sub << nbits) | ones(nbits);
-    u_int64_t or_bits_top = (0 << nbits) | or_bits_sub;
+    uint64_t and_bit = pickbit(tmask_and, i);
+    uint64_t or_bit = pickbit(tmask_or, i);
+    uint64_t and_bits_sub = replicate(and_bit, 1, nbits);
+    uint64_t or_bits_sub = replicate(or_bit, 1, nbits);
+    uint64_t and_bits_top = (and_bits_sub << nbits) | ones(nbits);
+    uint64_t or_bits_top = (0 << nbits) | or_bits_sub;
 
     tmask = ((tmask
               & (replicate(and_bits_top, 2 * nbits, 32 / nbits)))
              | replicate(or_bits_top, 2 * nbits, 32 / nbits));
   }
@@ -216,16 +216,16 @@
 
   wmask = 0;
 
   for (int i = 0; i < 6; i++) {
     int nbits = 1 << i;
-    u_int64_t and_bit = pickbit(wmask_and, i);
-    u_int64_t or_bit = pickbit(wmask_or, i);
-    u_int64_t and_bits_sub = replicate(and_bit, 1, nbits);
-    u_int64_t or_bits_sub = replicate(or_bit, 1, nbits);
-    u_int64_t and_bits_top = (ones(nbits) << nbits) | and_bits_sub;
-    u_int64_t or_bits_top = (or_bits_sub << nbits) | 0;
+    uint64_t and_bit = pickbit(wmask_and, i);
+    uint64_t or_bit = pickbit(wmask_or, i);
+    uint64_t and_bits_sub = replicate(and_bit, 1, nbits);
+    uint64_t or_bits_sub = replicate(or_bit, 1, nbits);
+    uint64_t and_bits_top = (ones(nbits) << nbits) | and_bits_sub;
+    uint64_t or_bits_top = (or_bits_sub << nbits) | 0;
 
     wmask = ((wmask
               & (replicate(and_bits_top, 2 * nbits, 32 / nbits)))
              | replicate(or_bits_top, 2 * nbits, 32 / nbits));
   }
@@ -241,18 +241,21 @@
   return 1;
 }
 
 // constructor to initialise the lookup tables
 
-static void initLITables() __attribute__ ((constructor));
+static void initLITables() NOT_WIN64(__attribute__ ((constructor)));
+#ifdef _WIN64
+static struct initLITables_t { initLITables_t(void) { initLITables(); } } _initLITables;
+#endif
 static void initLITables()
 {
   li_table_entry_count = 0;
   for (unsigned index = 0; index < LI_TABLE_SIZE; index++) {
-    u_int32_t N = uimm(index, 12, 12);
-    u_int32_t immr = uimm(index, 11, 6);
-    u_int32_t imms = uimm(index, 5, 0);
+    uint32_t N = uimm(index, 12, 12);
+    uint32_t immr = uimm(index, 11, 6);
+    uint32_t imms = uimm(index, 5, 0);
     if (expandLogicalImmediate(N, immr, imms, LITable[index])) {
       InverseLITable[li_table_entry_count].immediate = LITable[index];
       InverseLITable[li_table_entry_count].encoding = index;
       li_table_entry_count++;
     }
@@ -262,16 +265,16 @@
         sizeof(InverseLITable[0]), compare_immediate_pair);
 }
 
 // public APIs provided for logical immediate lookup and reverse lookup
 
-u_int64_t logical_immediate_for_encoding(u_int32_t encoding)
+uint64_t logical_immediate_for_encoding(uint32_t encoding)
 {
   return LITable[encoding];
 }
 
-u_int32_t encoding_for_logical_immediate(u_int64_t immediate)
+uint32_t encoding_for_logical_immediate(uint64_t immediate)
 {
   struct li_pair pair;
   struct li_pair *result;
 
   pair.immediate = immediate;
@@ -291,19 +294,19 @@
 // fpimm[7] = sign bit
 // fpimm[6:4] = signed exponent
 // fpimm[3:0] = fraction (assuming leading 1)
 // i.e. F = s * 1.f * 2^(e - b)
 
-u_int64_t fp_immediate_for_encoding(u_int32_t imm8, int is_dp)
+uint64_t fp_immediate_for_encoding(uint32_t imm8, int is_dp)
 {
   union {
     float fpval;
     double dpval;
-    u_int64_t val;
+    uint64_t val;
   };
 
-  u_int32_t s, e, f;
+  uint32_t s, e, f;
   s = (imm8 >> 7 ) & 0x1;
   e = (imm8 >> 4) & 0x7;
   f = imm8 & 0xf;
   // the fp value is s * n/16 * 2r where n is 16+e
   fpval = (16.0 + f) / 16.0;
@@ -327,11 +330,11 @@
     dpval = (double)fpval;
   }
   return val;
 }
 
-u_int32_t encoding_for_fp_immediate(float immediate)
+uint32_t encoding_for_fp_immediate(float immediate)
 {
   // given a float which is of the form
   //
   //     s * n/16 * 2r
   //
@@ -339,14 +342,14 @@
   // return the imm8 result [s:r:f]
   //
 
   union {
     float fpval;
-    u_int32_t val;
+    uint32_t val;
   };
   fpval = immediate;
-  u_int32_t s, r, f, res;
+  uint32_t s, r, f, res;
   // sign bit is 31
   s = (val >> 31) & 0x1;
   // exponent is bits 30-23 but we only want the bottom 3 bits
   // strictly we ought to check that the bits bits 30-25 are
   // either all 1s or all 0s
diff a/src/hotspot/cpu/aarch64/immediate_aarch64.hpp b/src/hotspot/cpu/aarch64/immediate_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/immediate_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/immediate_aarch64.hpp
@@ -44,11 +44,11 @@
  * logical immediate instruction. encodings are supplied and returned
  * as 32 bit values. if a given 13 bit immediate has no corresponding
  * encoding then a map lookup will return 0xffffffff.
  */
 
-u_int64_t logical_immediate_for_encoding(u_int32_t encoding);
-u_int32_t encoding_for_logical_immediate(u_int64_t immediate);
-u_int64_t fp_immediate_for_encoding(u_int32_t imm8, int is_dp);
-u_int32_t encoding_for_fp_immediate(float immediate);
+uint64_t logical_immediate_for_encoding(uint32_t encoding);
+uint32_t encoding_for_logical_immediate(uint64_t immediate);
+uint64_t fp_immediate_for_encoding(uint32_t imm8, int is_dp);
+uint32_t encoding_for_fp_immediate(float immediate);
 
 #endif // _IMMEDIATE_H
diff a/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp b/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp
@@ -166,11 +166,11 @@
   ldrh(reg, Address(rbcp, bcp_offset));
   rev16(reg, reg);
 }
 
 void InterpreterMacroAssembler::get_dispatch() {
-  unsigned long offset;
+  uint64_t offset;
   adrp(rdispatch, ExternalAddress((address)Interpreter::dispatch_table()), offset);
   lea(rdispatch, Address(rdispatch, offset));
 }
 
 void InterpreterMacroAssembler::get_cache_index_at_bcp(Register index,
@@ -280,11 +280,16 @@
 }
 
 void InterpreterMacroAssembler::load_resolved_klass_at_offset(
                              Register cpool, Register index, Register klass, Register temp) {
   add(temp, cpool, index, LSL, LogBytesPerWord);
+#ifdef _WIN64
+  Address a1(temp, (int)sizeof(ConstantPool));
+  ldrh(temp, a1); // temp = resolved_klass_index
+#else
   ldrh(temp, Address(temp, sizeof(ConstantPool))); // temp = resolved_klass_index
+#endif
   ldr(klass, Address(cpool,  ConstantPool::resolved_klasses_offset_in_bytes())); // klass = cpool->_resolved_klasses
   add(klass, klass, temp, LSL, LogBytesPerWord);
   ldr(klass, Address(klass, Array<Klass*>::base_offset_in_bytes()));
 }
 
@@ -764,11 +769,11 @@
     // NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg
     // NOTE2: aarch64 does not like to subtract sp from rn so take a
     // copy
     mov(rscratch1, sp);
     sub(swap_reg, swap_reg, rscratch1);
-    ands(swap_reg, swap_reg, (unsigned long)(7 - os::vm_page_size()));
+    ands(swap_reg, swap_reg, (uint64_t)(7 - os::vm_page_size()));
 
     // Save the test result, for recursive case, the result is zero
     str(swap_reg, Address(lock_reg, mark_offset));
 
     if (PrintBiasedLockingStatistics) {
diff a/src/hotspot/cpu/aarch64/interpreterRT_aarch64.cpp b/src/hotspot/cpu/aarch64/interpreterRT_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/interpreterRT_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/interpreterRT_aarch64.cpp
@@ -344,11 +344,11 @@
     intptr_t from_obj = *(intptr_t*)(_from+Interpreter::local_offset_in_bytes(1));
     _from -= 2*Interpreter::stackElementSize;
 
     if (_num_fp_args < Argument::n_float_register_parameters_c) {
       *_fp_args++ = from_obj;
-      *_fp_identifiers |= (1 << _num_fp_args); // mark as double
+      *_fp_identifiers |= (1ULL << _num_fp_args); // mark as double
       _num_fp_args++;
     } else {
       *_to++ = from_obj;
       _num_fp_args++;
     }
diff a/src/hotspot/cpu/aarch64/jniFastGetField_aarch64.cpp b/src/hotspot/cpu/aarch64/jniFastGetField_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/jniFastGetField_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/jniFastGetField_aarch64.cpp
@@ -71,11 +71,11 @@
   MacroAssembler* masm = new MacroAssembler(&cbuf);
   address fast_entry = __ pc();
 
   Label slow;
 
-  unsigned long offset;
+  uint64_t offset;
   __ adrp(rcounter_addr,
           SafepointSynchronize::safepoint_counter_addr(), offset);
   Address safepoint_counter_addr(rcounter_addr, offset);
   __ ldrw(rcounter, safepoint_counter_addr);
   __ tbnz(rcounter, 0, slow);
@@ -97,11 +97,11 @@
   }
 
   if (JvmtiExport::can_post_field_access()) {
     // Check to see if a field access watch has been set before we
     // take the fast path.
-    unsigned long offset2;
+    uint64_t offset2;
     __ adrp(result,
             ExternalAddress((address) JvmtiExport::get_field_access_count_addr()),
             offset2);
     __ ldrw(result, Address(result, offset2));
     __ cbnzw(result, slow);
diff a/src/hotspot/cpu/aarch64/jvmciCodeInstaller_aarch64.cpp b/src/hotspot/cpu/aarch64/jvmciCodeInstaller_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/jvmciCodeInstaller_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/jvmciCodeInstaller_aarch64.cpp
@@ -18,11 +18,11 @@
  *
  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  * or visit www.oracle.com if you need additional information or have any
  * questions.
  */
-
+#include "precompiled.hpp"
 #include "jvmci/jvmciCodeInstaller.hpp"
 #include "jvmci/jvmciRuntime.hpp"
 #include "jvmci/jvmciCompilerToVM.hpp"
 #include "jvmci/jvmciJavaClasses.hpp"
 #include "oops/oop.inline.hpp"
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
@@ -69,12 +69,12 @@
 
 // Patch any kind of instruction; there may be several instructions.
 // Return the total length (in bytes) of the instructions.
 int MacroAssembler::pd_patch_instruction_size(address branch, address target) {
   int instructions = 1;
-  assert((uint64_t)target < (1ul << 48), "48-bit overflow in address constant");
-  long offset = (target - branch) >> 2;
+  assert((uint64_t)target < ((uint64_t)1 << 48), "48-bit overflow in address constant");
+  int64_t offset = (target - branch) >> 2;
   unsigned insn = *(unsigned*)branch;
   if ((Instruction_aarch64::extract(insn, 29, 24) & 0b111011) == 0b011000) {
     // Load register (literal)
     Instruction_aarch64::spatch(branch, 23, 5, offset);
   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
@@ -92,11 +92,11 @@
   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
     // PC-rel. addressing
     offset = target-branch;
     int shift = Instruction_aarch64::extract(insn, 31, 31);
     if (shift) {
-      u_int64_t dest = (u_int64_t)target;
+      uint64_t dest = (uint64_t)target;
       uint64_t pc_page = (uint64_t)branch >> 12;
       uint64_t adr_page = (uint64_t)target >> 12;
       unsigned offset_lo = dest & 0xfff;
       offset = adr_page - pc_page;
 
@@ -133,23 +133,23 @@
       } else if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110 &&
                    Instruction_aarch64::extract(insn, 4, 0) ==
                      Instruction_aarch64::extract(insn2, 4, 0)) {
         // movk #imm16<<32
         Instruction_aarch64::patch(branch + 4, 20, 5, (uint64_t)target >> 32);
-        long dest = ((long)target & 0xffffffffL) | ((long)branch & 0xffff00000000L);
-        long pc_page = (long)branch >> 12;
-        long adr_page = (long)dest >> 12;
+        int64_t dest = ((int64_t)target & 0xffffffffL) | ((int64_t)branch & 0xffff00000000L);
+        int64_t pc_page = (int64_t)branch >> 12;
+        int64_t adr_page = (int64_t)dest >> 12;
         offset = adr_page - pc_page;
         instructions = 2;
       }
     }
     int offset_lo = offset & 3;
     offset >>= 2;
     Instruction_aarch64::spatch(branch, 23, 5, offset);
     Instruction_aarch64::patch(branch, 30, 29, offset_lo);
   } else if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010100) {
-    u_int64_t dest = (u_int64_t)target;
+    uint64_t dest = (uint64_t)target;
     // Move wide constant
     assert(nativeInstruction_at(branch+4)->is_movk(), "wrong insns in patch");
     assert(nativeInstruction_at(branch+8)->is_movk(), "wrong insns in patch");
     Instruction_aarch64::patch(branch, 20, 5, dest & 0xffff);
     Instruction_aarch64::patch(branch+4, 20, 5, (dest >>= 16) & 0xffff);
@@ -204,11 +204,11 @@
   Instruction_aarch64::patch(insn_addr+4, 20, 5, n & 0xffff);
   return 2 * NativeInstruction::instruction_size;
 }
 
 address MacroAssembler::target_addr_for_insn(address insn_addr, unsigned insn) {
-  long offset = 0;
+  int64_t offset = 0;
   if ((Instruction_aarch64::extract(insn, 29, 24) & 0b011011) == 0b00011000) {
     // Load register (literal)
     offset = Instruction_aarch64::sextract(insn, 23, 5);
     return address(((uint64_t)insn_addr + (offset << 2)));
   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
@@ -271,17 +271,17 @@
       }
     } else {
       ShouldNotReachHere();
     }
   } else if (Instruction_aarch64::extract(insn, 31, 23) == 0b110100101) {
-    u_int32_t *insns = (u_int32_t *)insn_addr;
+    uint32_t *insns = (uint32_t *)insn_addr;
     // Move wide constant: movz, movk, movk.  See movptr().
     assert(nativeInstruction_at(insns+1)->is_movk(), "wrong insns in patch");
     assert(nativeInstruction_at(insns+2)->is_movk(), "wrong insns in patch");
-    return address(u_int64_t(Instruction_aarch64::extract(insns[0], 20, 5))
-                   + (u_int64_t(Instruction_aarch64::extract(insns[1], 20, 5)) << 16)
-                   + (u_int64_t(Instruction_aarch64::extract(insns[2], 20, 5)) << 32));
+    return address(uint64_t(Instruction_aarch64::extract(insns[0], 20, 5))
+                   + (uint64_t(Instruction_aarch64::extract(insns[1], 20, 5)) << 16)
+                   + (uint64_t(Instruction_aarch64::extract(insns[2], 20, 5)) << 32));
   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &&
              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
     return 0;
   } else {
     ShouldNotReachHere();
@@ -292,11 +292,11 @@
 void MacroAssembler::safepoint_poll(Label& slow_path) {
   if (SafepointMechanism::uses_thread_local_poll()) {
     ldr(rscratch1, Address(rthread, Thread::polling_page_offset()));
     tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);
   } else {
-    unsigned long offset;
+    uint64_t offset;
     adrp(rscratch1, ExternalAddress(SafepointSynchronize::address_of_state()), offset);
     ldrw(rscratch1, Address(rscratch1, offset));
     assert(SafepointSynchronize::_not_synchronized == 0, "rewrite this code");
     cbnz(rscratch1, slow_path);
   }
@@ -400,11 +400,11 @@
 void MacroAssembler::far_call(Address entry, CodeBuffer *cbuf, Register tmp) {
   assert(ReservedCodeCacheSize < 4*G, "branch out of range");
   assert(CodeCache::find_blob(entry.target()) != NULL,
          "destination of far call not found in code cache");
   if (far_branches()) {
-    unsigned long offset;
+    uint64_t offset;
     // We can use ADRP here because we know that the total size of
     // the code cache cannot exceed 2Gb.
     adrp(tmp, entry, offset);
     add(tmp, tmp, offset);
     if (cbuf) cbuf->set_insts_mark();
@@ -418,11 +418,11 @@
 void MacroAssembler::far_jump(Address entry, CodeBuffer *cbuf, Register tmp) {
   assert(ReservedCodeCacheSize < 4*G, "branch out of range");
   assert(CodeCache::find_blob(entry.target()) != NULL,
          "destination of far call not found in code cache");
   if (far_branches()) {
-    unsigned long offset;
+    uint64_t offset;
     // We can use ADRP here because we know that the total size of
     // the code cache cannot exceed 2Gb.
     adrp(tmp, entry, offset);
     add(tmp, tmp, offset);
     if (cbuf) cbuf->set_insts_mark();
@@ -1400,12 +1400,12 @@
 #ifdef ASSERT
   int offset1 = Interpreter::expr_offset_in_bytes(extra_slot_offset+1);
   assert(offset1 - offset == stackElementSize, "correct arithmetic");
 #endif
   if (arg_slot.is_constant()) {
-    return Address(esp, arg_slot.as_constant() * stackElementSize
-                   + offset);
+    int calc_offset = arg_slot.as_constant() * stackElementSize + offset;
+    return Address(esp, calc_offset);
   } else {
     add(rscratch1, esp, arg_slot.as_register(),
         ext::uxtx, exact_log2(stackElementSize));
     return Address(rscratch1, offset);
   }
@@ -1502,11 +1502,11 @@
 // MacroAssembler protected routines needed to implement
 // public methods
 
 void MacroAssembler::mov(Register r, Address dest) {
   code_section()->relocate(pc(), dest.rspec());
-  u_int64_t imm64 = (u_int64_t)dest.target();
+  uint64_t imm64 = (uint64_t)dest.target();
   movptr(r, imm64);
 }
 
 // Move a constant pointer into r.  In AArch64 mode the virtual
 // address space is 48 bits in size, so we only need three
@@ -1518,11 +1518,11 @@
     char buffer[64];
     snprintf(buffer, sizeof(buffer), "0x%" PRIX64, imm64);
     block_comment(buffer);
   }
 #endif
-  assert(imm64 < (1ul << 48), "48-bit overflow in address constant");
+  assert(imm64 < (1ull << 48), "48-bit overflow in address constant");
   movz(r, imm64 & 0xffff);
   imm64 >>= 16;
   movk(r, imm64 & 0xffff, 16);
   imm64 >>= 16;
   movk(r, imm64 & 0xffff, 32);
@@ -1535,24 +1535,24 @@
 //   imm32 == hex 0000efgh  T4H:  Vd = efghefghefghefgh
 //   imm32 == hex 0000efgh  T8H:  Vd = efghefghefghefghefghefghefghefgh
 //   imm32 == hex abcdefgh  T2S:  Vd = abcdefghabcdefgh
 //   imm32 == hex abcdefgh  T4S:  Vd = abcdefghabcdefghabcdefghabcdefgh
 //   T1D/T2D: invalid
-void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, u_int32_t imm32) {
+void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, uint32_t imm32) {
   assert(T != T1D && T != T2D, "invalid arrangement");
   if (T == T8B || T == T16B) {
     assert((imm32 & ~0xff) == 0, "extraneous bits in unsigned imm32 (T8B/T16B)");
     movi(Vd, T, imm32 & 0xff, 0);
     return;
   }
-  u_int32_t nimm32 = ~imm32;
+  uint32_t nimm32 = ~imm32;
   if (T == T4H || T == T8H) {
     assert((imm32  & ~0xffff) == 0, "extraneous bits in unsigned imm32 (T4H/T8H)");
     imm32 &= 0xffff;
     nimm32 &= 0xffff;
   }
-  u_int32_t x = imm32;
+  uint32_t x = imm32;
   int movi_cnt = 0;
   int movn_cnt = 0;
   while (x) { if (x & 0xff) movi_cnt++; x >>= 8; }
   x = nimm32;
   while (x) { if (x & 0xff) movn_cnt++; x >>= 8; }
@@ -1572,11 +1572,11 @@
       orri(Vd, T, imm32 & 0xff, lsl);
     lsl += 8; imm32 >>= 8;
   }
 }
 
-void MacroAssembler::mov_immediate64(Register dst, u_int64_t imm64)
+void MacroAssembler::mov_immediate64(Register dst, uint64_t imm64)
 {
 #ifndef PRODUCT
   {
     char buffer[64];
     snprintf(buffer, sizeof(buffer), "0x%" PRIX64, imm64);
@@ -1586,11 +1586,11 @@
   if (operand_valid_for_logical_immediate(false, imm64)) {
     orr(dst, zr, imm64);
   } else {
     // we can use a combination of MOVZ or MOVN with
     // MOVK to build up the constant
-    u_int64_t imm_h[4];
+    uint64_t imm_h[4];
     int zero_count = 0;
     int neg_count = 0;
     int i;
     for (i = 0; i < 4; i++) {
       imm_h[i] = ((imm64 >> (i * 16)) & 0xffffL);
@@ -1607,89 +1607,89 @@
       // one MOVN will do
       movn(dst, 0);
     } else if (zero_count == 3) {
       for (i = 0; i < 4; i++) {
         if (imm_h[i] != 0L) {
-          movz(dst, (u_int32_t)imm_h[i], (i << 4));
+          movz(dst, (uint32_t)imm_h[i], (i << 4));
           break;
         }
       }
     } else if (neg_count == 3) {
       // one MOVN will do
       for (int i = 0; i < 4; i++) {
         if (imm_h[i] != 0xffffL) {
-          movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i << 4));
+          movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i << 4));
           break;
         }
       }
     } else if (zero_count == 2) {
       // one MOVZ and one MOVK will do
       for (i = 0; i < 3; i++) {
         if (imm_h[i] != 0L) {
-          movz(dst, (u_int32_t)imm_h[i], (i << 4));
+          movz(dst, (uint32_t)imm_h[i], (i << 4));
           i++;
           break;
         }
       }
       for (;i < 4; i++) {
         if (imm_h[i] != 0L) {
-          movk(dst, (u_int32_t)imm_h[i], (i << 4));
+          movk(dst, (uint32_t)imm_h[i], (i << 4));
         }
       }
     } else if (neg_count == 2) {
       // one MOVN and one MOVK will do
       for (i = 0; i < 4; i++) {
         if (imm_h[i] != 0xffffL) {
-          movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i << 4));
+          movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i << 4));
           i++;
           break;
         }
       }
       for (;i < 4; i++) {
         if (imm_h[i] != 0xffffL) {
-          movk(dst, (u_int32_t)imm_h[i], (i << 4));
+          movk(dst, (uint32_t)imm_h[i], (i << 4));
         }
       }
     } else if (zero_count == 1) {
       // one MOVZ and two MOVKs will do
       for (i = 0; i < 4; i++) {
         if (imm_h[i] != 0L) {
-          movz(dst, (u_int32_t)imm_h[i], (i << 4));
+          movz(dst, (uint32_t)imm_h[i], (i << 4));
           i++;
           break;
         }
       }
       for (;i < 4; i++) {
         if (imm_h[i] != 0x0L) {
-          movk(dst, (u_int32_t)imm_h[i], (i << 4));
+          movk(dst, (uint32_t)imm_h[i], (i << 4));
         }
       }
     } else if (neg_count == 1) {
       // one MOVN and two MOVKs will do
       for (i = 0; i < 4; i++) {
         if (imm_h[i] != 0xffffL) {
-          movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i << 4));
+          movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i << 4));
           i++;
           break;
         }
       }
       for (;i < 4; i++) {
         if (imm_h[i] != 0xffffL) {
-          movk(dst, (u_int32_t)imm_h[i], (i << 4));
+          movk(dst, (uint32_t)imm_h[i], (i << 4));
         }
       }
     } else {
       // use a MOVZ and 3 MOVKs (makes it easier to debug)
-      movz(dst, (u_int32_t)imm_h[0], 0);
+      movz(dst, (uint32_t)imm_h[0], 0);
       for (i = 1; i < 4; i++) {
-        movk(dst, (u_int32_t)imm_h[i], (i << 4));
+        movk(dst, (uint32_t)imm_h[i], (i << 4));
       }
     }
   }
 }
 
-void MacroAssembler::mov_immediate32(Register dst, u_int32_t imm32)
+void MacroAssembler::mov_immediate32(Register dst, uint32_t imm32)
 {
 #ifndef PRODUCT
     {
       char buffer[64];
       snprintf(buffer, sizeof(buffer), "0x%" PRIX32, imm32);
@@ -1699,11 +1699,11 @@
   if (operand_valid_for_logical_immediate(true, imm32)) {
     orrw(dst, zr, imm32);
   } else {
     // we can use MOVZ, MOVN or two calls to MOVK to build up the
     // constant
-    u_int32_t imm_h[2];
+    uint32_t imm_h[2];
     imm_h[0] = imm32 & 0xffff;
     imm_h[1] = ((imm32 >> 16) & 0xffff);
     if (imm_h[0] == 0) {
       movzw(dst, imm_h[1], 16);
     } else if (imm_h[0] == 0xffff) {
@@ -1722,11 +1722,11 @@
 
 // Form an address from base + offset in Rd.  Rd may or may
 // not actually be used: you must use the Address that is returned.
 // It is up to you to ensure that the shift provided matches the size
 // of your data.
-Address MacroAssembler::form_address(Register Rd, Register base, long byte_offset, int shift) {
+Address MacroAssembler::form_address(Register Rd, Register base, int64_t byte_offset, int shift) {
   if (Address::offset_ok_for_immed(byte_offset, shift))
     // It fits; no need for any heroics
     return Address(base, byte_offset);
 
   // Don't do anything clever with negative or misaligned offsets
@@ -1737,12 +1737,12 @@
     return Address(Rd);
   }
 
   // See if we can do this with two 12-bit offsets
   {
-    unsigned long word_offset = byte_offset >> shift;
-    unsigned long masked_offset = word_offset & 0xfff000;
+    uint64_t word_offset = byte_offset >> shift;
+    uint64_t masked_offset = word_offset & 0xfff000;
     if (Address::offset_ok_for_immed(word_offset - masked_offset, 0)
         && Assembler::operand_valid_for_add_sub_immediate(masked_offset << shift)) {
       add(Rd, base, masked_offset << shift);
       word_offset -= masked_offset;
       return Address(Rd, word_offset << shift);
@@ -1979,11 +1979,11 @@
   if (value < 0)  { increment(reg, -value);      return; }
   if (value == 0) {                              return; }
   if (value < (1 << 12)) { sub(reg, reg, value); return; }
   /* else */ {
     assert(reg != rscratch2, "invalid dst for register decrement");
-    mov(rscratch2, (unsigned long)value);
+    mov(rscratch2, (uint64_t) value);
     sub(reg, reg, rscratch2);
   }
 }
 
 void MacroAssembler::decrementw(Address dst, int value)
@@ -2498,11 +2498,11 @@
   if (UseLSE) {                                                         \
     prev = prev->is_valid() ? prev : zr;                                \
     if (incr.is_register()) {                                           \
       AOP(sz, incr.as_register(), prev, addr);                          \
     } else {                                                            \
-      mov(rscratch2, incr.as_constant());                               \
+      mov(rscratch2, (address)incr.as_constant());                      \
       AOP(sz, rscratch2, prev, addr);                                   \
     }                                                                   \
     return;                                                             \
   }                                                                     \
   Register result = rscratch2;                                          \
@@ -2620,11 +2620,11 @@
   fatal("DEBUG MESSAGE: %s", msg);
 }
 
 void MacroAssembler::push_call_clobbered_registers() {
   int step = 4 * wordSize;
-  push(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2), sp);
+  push(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2) WIN64_ONLY(- r18), sp);
   sub(sp, sp, step);
   mov(rscratch1, -step);
   // Push v0-v7, v16-v31.
   for (int i = 31; i>= 4; i -= 4) {
     if (i <= v7->encoding() || i >= v16->encoding())
@@ -2639,12 +2639,11 @@
   for (int i = 0; i < 32; i += 4) {
     if (i <= v7->encoding() || i >= v16->encoding())
       ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
           as_FloatRegister(i+3), T1D, Address(post(sp, 4 * wordSize)));
   }
-
-  pop(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2), sp);
+  pop(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2) WIN64_ONLY(- r18), sp);
 }
 
 void MacroAssembler::push_CPU_state(bool save_vectors) {
   int step = (save_vectors ? 8 : 4) * wordSize;
   push(0x3fffffff, sp);         // integer registers except lr & sp
@@ -2712,23 +2711,23 @@
 
 // Checks whether offset is aligned.
 // Returns true if it is, else false.
 bool MacroAssembler::merge_alignment_check(Register base,
                                            size_t size,
-                                           long cur_offset,
-                                           long prev_offset) const {
+                                           int64_t cur_offset,
+                                           int64_t prev_offset) const {
   if (AvoidUnalignedAccesses) {
     if (base == sp) {
       // Checks whether low offset if aligned to pair of registers.
-      long pair_mask = size * 2 - 1;
-      long offset = prev_offset > cur_offset ? cur_offset : prev_offset;
+      int64_t pair_mask = size * 2 - 1;
+      int64_t offset = prev_offset > cur_offset ? cur_offset : prev_offset;
       return (offset & pair_mask) == 0;
     } else { // If base is not sp, we can't guarantee the access is aligned.
       return false;
     }
   } else {
-    long mask = size - 1;
+    int64_t mask = size - 1;
     // Load/store pair instruction only supports element size aligned offset.
     return (cur_offset & mask) == 0 && (prev_offset & mask) == 0;
   }
 }
 
@@ -2757,22 +2756,22 @@
 
   if (cur_size_in_bytes != prev_size_in_bytes || is_store != prev_ldst->is_store()) {
     return false;
   }
 
-  long max_offset = 63 * prev_size_in_bytes;
-  long min_offset = -64 * prev_size_in_bytes;
+  int64_t max_offset = 63 * prev_size_in_bytes;
+  int64_t min_offset = -64 * prev_size_in_bytes;
 
   assert(prev_ldst->is_not_pre_post_index(), "pre-index or post-index is not supported to be merged.");
 
   // Only same base can be merged.
   if (adr.base() != prev_ldst->base()) {
     return false;
   }
 
-  long cur_offset = adr.offset();
-  long prev_offset = prev_ldst->offset();
+  int64_t cur_offset = adr.offset();
+  int64_t prev_offset = prev_ldst->offset();
   size_t diff = abs(cur_offset - prev_offset);
   if (diff != prev_size_in_bytes) {
     return false;
   }
 
@@ -2785,11 +2784,11 @@
   // If t1 and t2 is the same in "ldp t1, t2, [xn, #imm]", we'll get SIGILL.
   if (!is_store && (adr.base() == prev_ldst->target() || rt == prev_ldst->target())) {
     return false;
   }
 
-  long low_offset = prev_offset > cur_offset ? cur_offset : prev_offset;
+  int64_t low_offset = prev_offset > cur_offset ? cur_offset : prev_offset;
   // Offset range must be in ldp/stp instruction's range.
   if (low_offset > max_offset || low_offset < min_offset) {
     return false;
   }
 
@@ -2810,11 +2809,11 @@
 
   Register rt_low, rt_high;
   address prev = pc() - NativeInstruction::instruction_size;
   NativeLdSt* prev_ldst = NativeLdSt_at(prev);
 
-  long offset;
+  int64_t offset;
 
   if (adr.offset() < prev_ldst->offset()) {
     offset = adr.offset();
     rt_low = rt;
     rt_high = prev_ldst->target();
@@ -3356,11 +3355,11 @@
  */
 void MacroAssembler::kernel_crc32(Register crc, Register buf, Register len,
         Register table0, Register table1, Register table2, Register table3,
         Register tmp, Register tmp2, Register tmp3) {
   Label L_by16, L_by16_loop, L_by4, L_by4_loop, L_by1, L_by1_loop, L_exit;
-  unsigned long offset;
+  uint64_t offset;
 
   if (UseCRC32) {
       kernel_crc32_using_crc32(crc, buf, len, table0, table1, table2, table3);
       return;
   }
@@ -3658,11 +3657,11 @@
 
 
 SkipIfEqual::SkipIfEqual(
     MacroAssembler* masm, const bool* flag_addr, bool value) {
   _masm = masm;
-  unsigned long offset;
+  uint64_t offset;
   _masm->adrp(rscratch1, ExternalAddress((address)flag_addr), offset);
   _masm->ldrb(rscratch1, Address(rscratch1, offset));
   _masm->cbzw(rscratch1, _label);
 }
 
@@ -3687,11 +3686,11 @@
   add(rscratch1, rscratch1, src);
   str(rscratch1, adr);
 }
 
 void MacroAssembler::cmpptr(Register src1, Address src2) {
-  unsigned long offset;
+  uint64_t offset;
   adrp(rscratch1, src2, offset);
   ldr(rscratch1, Address(rscratch1, offset));
   cmp(src1, rscratch1);
 }
 
@@ -4308,11 +4307,11 @@
 // Move the address of the polling page into dest.
 void MacroAssembler::get_polling_page(Register dest, address page, relocInfo::relocType rtype) {
   if (SafepointMechanism::uses_thread_local_poll()) {
     ldr(dest, Address(rthread, Thread::polling_page_offset()));
   } else {
-    unsigned long off;
+    uint64_t off;
     adrp(dest, Address(page, rtype), off);
     assert(off == 0, "polling page must be page aligned");
   }
 }
 
@@ -4330,17 +4329,17 @@
   code_section()->relocate(inst_mark(), rtype);
   ldrw(zr, Address(r, 0));
   return inst_mark();
 }
 
-void MacroAssembler::adrp(Register reg1, const Address &dest, unsigned long &byte_offset) {
+void MacroAssembler::adrp(Register reg1, const Address &dest, uint64_t &byte_offset) {
   relocInfo::relocType rtype = dest.rspec().reloc()->type();
-  unsigned long low_page = (unsigned long)CodeCache::low_bound() >> 12;
-  unsigned long high_page = (unsigned long)(CodeCache::high_bound()-1) >> 12;
-  unsigned long dest_page = (unsigned long)dest.target() >> 12;
-  long offset_low = dest_page - low_page;
-  long offset_high = dest_page - high_page;
+  uint64_t low_page = (uint64_t)CodeCache::low_bound() >> 12;
+  uint64_t high_page = (uint64_t)(CodeCache::high_bound() - 1) >> 12;
+  uint64_t dest_page = (uint64_t)dest.target() >> 12;
+  int64_t offset_low = dest_page - low_page;
+  int64_t offset_high = dest_page - high_page;
 
   assert(is_valid_AArch64_address(dest.target()), "bad address");
   assert(dest.getMode() == Address::literal, "ADRP must be applied to a literal address");
 
   InstructionMark im(this);
@@ -4348,28 +4347,28 @@
   // 8143067: Ensure that the adrp can reach the dest from anywhere within
   // the code cache so that if it is relocated we know it will still reach
   if (offset_high >= -(1<<20) && offset_low < (1<<20)) {
     _adrp(reg1, dest.target());
   } else {
-    unsigned long target = (unsigned long)dest.target();
-    unsigned long adrp_target
-      = (target & 0xffffffffUL) | ((unsigned long)pc() & 0xffff00000000UL);
+    uint64_t target = (uint64_t)dest.target();
+    uint64_t adrp_target
+      = (target & 0xffffffffUL) | ((uint64_t)pc() & 0xffff00000000UL);
 
     _adrp(reg1, (address)adrp_target);
     movk(reg1, target >> 32, 32);
   }
-  byte_offset = (unsigned long)dest.target() & 0xfff;
+  byte_offset = (uint64_t)dest.target() & 0xfff;
 }
 
 void MacroAssembler::load_byte_map_base(Register reg) {
   CardTable::CardValue* byte_map_base =
     ((CardTableBarrierSet*)(BarrierSet::barrier_set()))->card_table()->byte_map_base();
 
   if (is_valid_AArch64_address((address)byte_map_base)) {
     // Strictly speaking the byte_map_base isn't an address at all,
     // and it might even be negative.
-    unsigned long offset;
+    uint64_t offset;
     adrp(reg, ExternalAddress((address)byte_map_base), offset);
     // We expect offset to be zero with most collectors.
     if (offset != 0) {
       add(reg, reg, offset);
     }
@@ -4796,11 +4795,11 @@
         cmp(cnt2, (u1)8);
         br(LT, DO1_SHORT);
 
         sub(result_tmp, cnt2, 8/str2_chr_size);
         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
-        mov(tmp3, str2_isL ? 0x0101010101010101 : 0x0001000100010001);
+        mov(tmp3, (uint64_t)(str2_isL ? 0x0101010101010101 : 0x0001000100010001));
         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
 
         if (str2_isL) {
           orr(ch1, ch1, ch1, LSL, 8);
         }
@@ -4868,11 +4867,11 @@
   sub(cnt1, cnt1, 4);
   mov(result_tmp, cnt1);
   lea(str1, Address(str1, cnt1, Address::uxtw(1)));
   sub(cnt1_neg, zr, cnt1, LSL, 1);
 
-  mov(tmp3, 0x0001000100010001);
+  mov(tmp3, (uint64_t)0x0001000100010001);
 
   BIND(CH1_LOOP);
     ldr(ch1, Address(str1, cnt1_neg));
     eor(ch1, ch, ch1);
     sub(tmp1, ch1, tmp3);
@@ -4914,11 +4913,11 @@
 // Compare strings.
 void MacroAssembler::string_compare(Register str1, Register str2,
     Register cnt1, Register cnt2, Register result, Register tmp1, Register tmp2,
     FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3, int ae) {
   Label DONE, SHORT_LOOP, SHORT_STRING, SHORT_LAST, TAIL, STUB,
-      DIFFERENCE, NEXT_WORD, SHORT_LOOP_TAIL, SHORT_LAST2, SHORT_LAST_INIT,
+      DIFF, NEXT_WORD, SHORT_LOOP_TAIL, SHORT_LAST2, SHORT_LAST_INIT,
       SHORT_LOOP_START, TAIL_CHECK;
 
   bool isLL = ae == StrIntrinsicNode::LL;
   bool isLU = ae == StrIntrinsicNode::LU;
   bool isUL = ae == StrIntrinsicNode::UL;
@@ -5009,11 +5008,11 @@
       fmovd(tmp2, vtmp);
     }
     adds(cnt2, cnt2, isUL ? 4 : 8);
     br(GE, TAIL);
     eor(rscratch2, tmp1, tmp2);
-    cbnz(rscratch2, DIFFERENCE);
+    cbnz(rscratch2, DIFF);
     // main loop
     bind(NEXT_WORD);
     if (str1_isL == str2_isL) {
       ldr(tmp1, Address(str1, cnt2));
       ldr(tmp2, Address(str2, cnt2));
@@ -5035,14 +5034,14 @@
     }
     br(GE, TAIL);
 
     eor(rscratch2, tmp1, tmp2);
     cbz(rscratch2, NEXT_WORD);
-    b(DIFFERENCE);
+    b(DIFF);
     bind(TAIL);
     eor(rscratch2, tmp1, tmp2);
-    cbnz(rscratch2, DIFFERENCE);
+    cbnz(rscratch2, DIFF);
     // Last longword.  In the case where length == 4 we compare the
     // same longword twice, but that's still faster than another
     // conditional branch.
     if (str1_isL == str2_isL) {
       ldr(tmp1, Address(str1));
@@ -5062,11 +5061,11 @@
     eor(rscratch2, tmp1, tmp2);
     cbz(rscratch2, DONE);
 
     // Find the first different characters in the longwords and
     // compute their difference.
-    bind(DIFFERENCE);
+    bind(DIFF);
     rev(rscratch2, rscratch2);
     clz(rscratch2, rscratch2);
     andr(rscratch2, rscratch2, isLL ? -8 : -16);
     lsrv(tmp1, tmp1, rscratch2);
     (this->*ext_chr)(tmp1, tmp1);
@@ -5541,11 +5540,11 @@
 }
 
 // base:         Address of a buffer to be zeroed, 8 bytes aligned.
 // cnt:          Immediate count in HeapWords.
 #define SmallArraySize (18 * BytesPerLong)
-void MacroAssembler::zero_words(Register base, u_int64_t cnt)
+void MacroAssembler::zero_words(Register base, uint64_t cnt)
 {
   BLOCK_COMMENT("zero_words {");
   int i = cnt & 1;  // store any odd word to start
   if (i) str(zr, Address(base));
 
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
@@ -187,19 +187,23 @@
 
   virtual void _call_Unimplemented(address call_site) {
     mov(rscratch2, call_site);
   }
 
+#ifdef _WIN64
+#define call_Unimplemented() _call_Unimplemented((address)__FUNCSIG__)
+#else
 #define call_Unimplemented() _call_Unimplemented((address)__PRETTY_FUNCTION__)
+#endif
 
   // aliases defined in AARCH64 spec
 
   template<class T>
   inline void cmpw(Register Rd, T imm)  { subsw(zr, Rd, imm); }
 
   inline void cmp(Register Rd, unsigned char imm8)  { subs(zr, Rd, imm8); }
-  inline void cmp(Register Rd, unsigned imm) __attribute__ ((deprecated));
+  inline __declspec(deprecated) void cmp(Register Rd, unsigned imm);
 
   inline void cmnw(Register Rd, unsigned imm) { addsw(zr, Rd, imm); }
   inline void cmn(Register Rd, unsigned imm) { adds(zr, Rd, imm); }
 
   void cset(Register Rd, Assembler::Condition cond) {
@@ -451,12 +455,12 @@
   // macro assembly operations needed for aarch64
 
   // first two private routines for loading 32 bit or 64 bit constants
 private:
 
-  void mov_immediate64(Register dst, u_int64_t imm64);
-  void mov_immediate32(Register dst, u_int32_t imm32);
+  void mov_immediate64(Register dst, uint64_t imm64);
+  void mov_immediate32(Register dst, uint32_t imm32);
 
   int push(unsigned int bitset, Register stack);
   int pop(unsigned int bitset, Register stack);
 
   int push_fp(unsigned int bitset, Register stack);
@@ -481,65 +485,65 @@
   // now mov instructions for loading absolute addresses and 32 or
   // 64 bit integers
 
   inline void mov(Register dst, address addr)
   {
-    mov_immediate64(dst, (u_int64_t)addr);
+    mov_immediate64(dst, (uint64_t)addr);
   }
 
-  inline void mov(Register dst, u_int64_t imm64)
+  inline void mov(Register dst, uint64_t imm64)
   {
     mov_immediate64(dst, imm64);
   }
 
-  inline void movw(Register dst, u_int32_t imm32)
+  inline void movw(Register dst, uint32_t imm32)
   {
     mov_immediate32(dst, imm32);
   }
 
-  inline void mov(Register dst, long l)
+  inline void mov(Register dst, int64_t l)
   {
-    mov(dst, (u_int64_t)l);
+    mov(dst, (uint64_t)l);
   }
 
   inline void mov(Register dst, int i)
   {
-    mov(dst, (long)i);
+    mov(dst, (int64_t)i);
   }
 
   void mov(Register dst, RegisterOrConstant src) {
     if (src.is_register())
       mov(dst, src.as_register());
     else
-      mov(dst, src.as_constant());
+      mov(dst, (uint64_t)src.as_constant());
   }
 
   void movptr(Register r, uintptr_t imm64);
 
-  void mov(FloatRegister Vd, SIMD_Arrangement T, u_int32_t imm32);
+  void mov(FloatRegister Vd, SIMD_Arrangement T, uint32_t imm32);
 
   void mov(FloatRegister Vd, SIMD_Arrangement T, FloatRegister Vn) {
     orr(Vd, T, Vn, Vn);
   }
 
 public:
 
   // Generalized Test Bit And Branch, including a "far" variety which
   // spans more than 32KiB.
-  void tbr(Condition cond, Register Rt, int bitpos, Label &dest, bool far = false) {
+  void tbr(Condition cond, Register Rt, int bitpos, Label &dest, bool isfar = false) {
     assert(cond == EQ || cond == NE, "must be");
 
-    if (far)
+    if (isfar)
       cond = ~cond;
 
     void (Assembler::* branch)(Register Rt, int bitpos, Label &L);
     if (cond == Assembler::EQ)
       branch = &Assembler::tbz;
     else
       branch = &Assembler::tbnz;
 
-    if (far) {
+    if (isfar) {
       Label L;
       (this->*branch)(Rt, bitpos, L);
       b(dest);
       bind(L);
     } else {
@@ -1166,11 +1170,11 @@
   void add(Register Rd, Register Rn, RegisterOrConstant increment);
   void addw(Register Rd, Register Rn, RegisterOrConstant increment);
   void sub(Register Rd, Register Rn, RegisterOrConstant decrement);
   void subw(Register Rd, Register Rn, RegisterOrConstant decrement);
 
-  void adrp(Register reg1, const Address &dest, unsigned long &byte_offset);
+  void adrp(Register reg1, const Address &dest, uint64_t &byte_offset);
 
   void tableswitch(Register index, jint lowbound, jint highbound,
                    Label &jumptable, Label &jumptable_end, int stride = 1) {
     adr(rscratch1, jumptable);
     subsw(rscratch2, index, lowbound);
@@ -1183,11 +1187,11 @@
 
   // Form an address from base + offset in Rd.  Rd may or may not
   // actually be used: you must use the Address that is returned.  It
   // is up to you to ensure that the shift provided matches the size
   // of your data.
-  Address form_address(Register Rd, Register base, long byte_offset, int shift);
+  Address form_address(Register Rd, Register base, int64_t byte_offset, int shift);
 
   // Return true iff an address is within the 48-bit AArch64 address
   // space.
   bool is_valid_AArch64_address(address a) {
     return ((uint64_t)a >> 48) == 0;
@@ -1208,11 +1212,11 @@
 
   void ldr_constant(Register dest, const Address &const_addr) {
     if (NearCpool) {
       ldr(dest, const_addr);
     } else {
-      unsigned long offset;
+      uint64_t offset;
       adrp(dest, InternalAddress(const_addr.target()), offset);
       ldr(dest, Address(dest, offset));
     }
   }
 
@@ -1238,11 +1242,11 @@
 
   void string_equals(Register a1, Register a2, Register result, Register cnt1,
                      int elem_size);
 
   void fill_words(Register base, Register cnt, Register value);
-  void zero_words(Register base, u_int64_t cnt);
+  void zero_words(Register base, uint64_t cnt);
   void zero_words(Register ptr, Register cnt);
   void zero_dcache_blocks(Register base, Register cnt);
 
   static const int zero_words_block_size;
 
@@ -1320,11 +1324,11 @@
 private:
   // Returns an address on the stack which is reachable with a ldr/str of size
   // Uses rscratch2 if the address is not directly reachable
   Address spill_address(int size, int offset, Register tmp=rscratch2);
 
-  bool merge_alignment_check(Register base, size_t size, long cur_offset, long prev_offset) const;
+  bool merge_alignment_check(Register base, size_t size, int64_t cur_offset, int64_t prev_offset) const;
 
   // Check whether two loads/stores can be merged into ldp/stp.
   bool ldst_can_merge(Register rx, const Address &adr, size_t cur_size_in_bytes, bool is_store) const;
 
   // Merge current load/store with previous load/store into ldp/stp.
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64_log.cpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64_log.cpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64_log.cpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64_log.cpp
@@ -63,11 +63,11 @@
 //
 /******************************************************************************/
 
 // Table with p(r) polynomial coefficients
 // and table representation of logarithm values (hi and low parts)
-__attribute__ ((aligned(64))) juint _L_tbl[] =
+ATTRIBUTE_ALIGNED(64) juint _L_tbl[]
 {
     // coefficients of p(r) polynomial:
     // _coeff[]
     0x00000000UL, 0xbfd00000UL, // C1_0 = -0.25
     0x92492492UL, 0x3fc24924UL, // C1_1 = 0.14285714285714285
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64_trig.cpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64_trig.cpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64_trig.cpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64_trig.cpp
@@ -199,13 +199,13 @@
 //     2. Return n in r2, y[0] == y0 == v4, y[1] == y1 == v5
 // NOTE: general purpose register names match local variable names in C code
 // NOTE: fpu registers are actively reused. See comments in code about their usage
 void MacroAssembler::generate__ieee754_rem_pio2(address npio2_hw,
     address two_over_pi, address pio2) {
-  const long PIO2_1t = 0x3DD0B4611A626331UL;
-  const long PIO2_2  = 0x3DD0B4611A600000UL;
-  const long PIO2_2t = 0x3BA3198A2E037073UL;
+  const int64_t PIO2_1t = 0x3DD0B4611A626331UL;
+  const int64_t PIO2_2  = 0x3DD0B4611A600000UL;
+  const int64_t PIO2_2t = 0x3BA3198A2E037073UL;
   Label X_IS_NEGATIVE, X_IS_MEDIUM_OR_LARGE, X_IS_POSITIVE_LONG_PI, LARGE_ELSE,
       REDUCTION_DONE, X_IS_MEDIUM_BRANCH_DONE, X_IS_LARGE, NX_SET,
       X_IS_NEGATIVE_LONG_PI;
   Register X = r0, n = r2, ix = r3, jv = r4, tmp5 = r5, jx = r6,
       tmp3 = r7, iqBase = r10, ih = r11, i = r17;
@@ -687,11 +687,11 @@
       CONVERTION_FOR, FW_Y0_NO_NEGATION, FW_FOR1_DONE, FW_FOR2, FW_FOR2_DONE,
       IH_FOR, SKIP_F_LOAD, RECOMP_FOR1, RECOMP_FIRST_FOR, INIT_F_COPY,
       RECOMP_FOR1_CHECK;
   Register tmp2 = r1, n = r2, jv = r4, tmp5 = r5, jx = r6,
       tmp3 = r7, iqBase = r10, ih = r11, tmp4 = r12, tmp1 = r13,
-      jz = r14, j = r15, twoOverPiBase = r16, i = r17, qBase = r18;
+      jz = r14, j = r15, twoOverPiBase = r16, i = r17, qBase = r19;
     // jp = jk == init_jk[prec] = init_jk[2] == {2,3,4,6}[2] == 4
     // jx = nx - 1
     lea(twoOverPiBase, ExternalAddress(two_over_pi));
     cmpw(jv, zr);
     addw(tmp4, jx, 4); // tmp4 = m = jx + jk = jx + 4. jx is in {0,1,2} so m is in [4,5,6]
@@ -1421,12 +1421,12 @@
   Label DONE, ARG_REDUCTION, TINY_X, RETURN_SIN, EARLY_CASE;
   Register X = r0, absX = r1, n = r2, ix = r3;
   FloatRegister y0 = v4, y1 = v5;
     block_comment("check |x| ~< pi/4, NaN, Inf and |x| < 2**-27 cases"); {
       fmovd(X, v0);
-      mov(rscratch2, 0x3e400000);
-      mov(rscratch1, 0x3fe921fb00000000);            // pi/4. shifted to reuse later
+      mov(rscratch2, (uint64_t)0x3e400000);
+      mov(rscratch1, (uint64_t)0x3fe921fb00000000);  // pi/4. shifted to reuse later
       ubfm(absX, X, 0, 62);                          // absX
       movz(r10, POSITIVE_INFINITY_OR_NAN_PREFIX, 48);
       cmp(rscratch2, absX, LSR, 32);
       lsr(ix, absX, 32);                             // set ix
       br(GT, TINY_X);                                // handle tiny x (|x| < 2^-27)
diff a/src/hotspot/cpu/aarch64/methodHandles_aarch64.cpp b/src/hotspot/cpu/aarch64/methodHandles_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/methodHandles_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/methodHandles_aarch64.cpp
@@ -75,11 +75,16 @@
   __ push(RegSet::of(temp, temp2), sp);
   __ load_klass(temp, obj);
   __ cmpptr(temp, ExternalAddress((address) klass_addr));
   __ br(Assembler::EQ, L_ok);
   intptr_t super_check_offset = klass->super_check_offset();
+#ifdef _WIN64
+  Address a1(temp, (uint64_t) super_check_offset);
+  __ ldr(temp, a1);
+#else
   __ ldr(temp, Address(temp, super_check_offset));
+#endif
   __ cmpptr(temp, ExternalAddress((address) klass_addr));
   __ br(Assembler::EQ, L_ok);
   __ pop(RegSet::of(temp, temp2), sp);
   __ bind(L_bad);
   __ stop(error_message);
diff a/src/hotspot/cpu/aarch64/nativeInst_aarch64.hpp b/src/hotspot/cpu/aarch64/nativeInst_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/nativeInst_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/nativeInst_aarch64.hpp
@@ -681,11 +681,11 @@
       // others like: pre-index or post-index.
       ShouldNotReachHere();
       return 0;
     }
   }
-  size_t size_in_bytes() { return 1 << size(); }
+  size_t size_in_bytes() { return 1ULL << size(); }
   bool is_not_pre_post_index() { return (is_ldst_ur() || is_ldst_unsigned_offset()); }
   bool is_load() {
     assert(Instruction_aarch64::extract(uint_at(0), 23, 22) == 0b01 ||
            Instruction_aarch64::extract(uint_at(0), 23, 22) == 0b00, "must be ldr or str");
 
diff a/src/hotspot/cpu/aarch64/register_aarch64.cpp b/src/hotspot/cpu/aarch64/register_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/register_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/register_aarch64.cpp
@@ -36,11 +36,11 @@
 const char* RegisterImpl::name() const {
   const char* names[number_of_registers] = {
     "c_rarg0", "c_rarg1", "c_rarg2", "c_rarg3", "c_rarg4", "c_rarg5", "c_rarg6", "c_rarg7",
     "rscratch1", "rscratch2",
     "r10", "r11", "r12", "r13", "r14", "r15", "r16",
-    "r17", "r18", "r19",
+   "r17", NOT_WIN64("r18") WIN64_ONLY("rtls"), "r19",
     "resp", "rdispatch", "rbcp", "r23", "rlocals", "rmonitors", "rcpool", "rheapbase",
     "rthread", "rfp", "lr", "sp"
   };
   return is_valid() ? names[encoding()] : "noreg";
 }
diff a/src/hotspot/cpu/aarch64/register_aarch64.hpp b/src/hotspot/cpu/aarch64/register_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/register_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/register_aarch64.hpp
@@ -63,11 +63,11 @@
   const char* name() const;
   int   encoding_nocheck() const                 { return (intptr_t)this; }
 
   // Return the bit which represents this register.  This is intended
   // to be ORed into a bitmask: for usage see class RegSet below.
-  unsigned long bit(bool should_set = true) const { return should_set ? 1 << encoding() : 0; }
+  uint64_t bit(bool should_set = true) const { return should_set ? 1 << encoding() : 0; }
 };
 
 // The integer registers of the aarch64 architecture
 
 CONSTANT_REGISTER_DECLARATION(Register, noreg, (-1));
diff a/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp b/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp
@@ -1704,11 +1704,11 @@
   Label native_return;
   __ set_last_Java_frame(sp, noreg, native_return, rscratch1);
 
   Label dtrace_method_entry, dtrace_method_entry_done;
   {
-    unsigned long offset;
+    uint64_t offset;
     __ adrp(rscratch1, ExternalAddress((address)&DTraceMethodProbes), offset);
     __ ldrb(rscratch1, Address(rscratch1, offset));
     __ cbnzw(rscratch1, dtrace_method_entry);
     __ bind(dtrace_method_entry_done);
   }
@@ -1944,11 +1944,11 @@
     __ bind(done);
   }
 
   Label dtrace_method_exit, dtrace_method_exit_done;
   {
-    unsigned long offset;
+    uint64_t offset;
     __ adrp(rscratch1, ExternalAddress((address)&DTraceMethodProbes), offset);
     __ ldrb(rscratch1, Address(rscratch1, offset));
     __ cbnzw(rscratch1, dtrace_method_exit);
     __ bind(dtrace_method_exit_done);
   }
diff a/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp b/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp
@@ -562,13 +562,13 @@
       __ cbnz(c_rarg2, error);
     }
 #endif
 
     // Check if the oop is in the right area of memory
-    __ mov(c_rarg3, (intptr_t) Universe::verify_oop_mask());
+    __ mov(c_rarg3, (address) Universe::verify_oop_mask());
     __ andr(c_rarg2, r0, c_rarg3);
-    __ mov(c_rarg3, (intptr_t) Universe::verify_oop_bits());
+    __ mov(c_rarg3, (address) Universe::verify_oop_bits());
 
     // Compare c_rarg2 and c_rarg3.  We don't use a compare
     // instruction here because the flags register is live.
     __ eor(c_rarg2, c_rarg2, c_rarg3);
     __ cbnz(c_rarg2, error);
@@ -694,11 +694,10 @@
   void generate_copy_longs(Label &start, Register s, Register d, Register count,
                            copy_direction direction) {
     int unit = wordSize * direction;
     int bias = (UseSIMDForMemoryOps ? 4:2) * wordSize;
 
-    int offset;
     const Register t0 = r3, t1 = r4, t2 = r5, t3 = r6,
       t4 = r7, t5 = r10, t6 = r11, t7 = r12;
     const Register stride = r13;
 
     assert_different_registers(rscratch1, t0, t1, t2, t3, t4, t5, t6, t7);
@@ -1085,11 +1084,11 @@
     // <= 96 bytes do inline. Direction doesn't matter because we always
     // load all the data before writing anything
     Label copy4, copy8, copy16, copy32, copy80, copy_big, finish;
     const Register t2 = r5, t3 = r6, t4 = r7, t5 = r8;
     const Register t6 = r9, t7 = r10, t8 = r11, t9 = r12;
-    const Register send = r17, dend = r18;
+    const Register send = r17, dend = r16;
 
     if (PrefetchCopyIntervalInBytes > 0)
       __ prfm(Address(s, 0), PLDL1KEEP);
     __ cmp(count, u1((UseSIMDForMemoryOps ? 96:80)/granularity));
     __ br(Assembler::HI, copy_big);
@@ -1277,18 +1276,19 @@
 
   void clobber_registers() {
 #ifdef ASSERT
     __ mov(rscratch1, (uint64_t)0xdeadbeef);
     __ orr(rscratch1, rscratch1, rscratch1, Assembler::LSL, 32);
-    for (Register r = r3; r <= r18; r++)
+    for (Register r = r3; r <= NOT_WIN64(r18) WIN64_ONLY(r17); r++)
       if (r != rscratch1) __ mov(r, rscratch1);
 #endif
+
   }
 
   // Scan over array at a for count oops, verifying each one.
   // Preserves a and count, clobbers rscratch1 and rscratch2.
-  void verify_oop_array (size_t size, Register a, Register count, Register temp) {
+  void verify_oop_array (unsigned int size, Register a, Register count, Register temp) {
     Label loop, end;
     __ mov(rscratch1, a);
     __ mov(rscratch2, zr);
     __ bind(loop);
     __ cmp(rscratch2, count);
@@ -1322,11 +1322,11 @@
   //
   // Side Effects:
   //   disjoint_int_copy_entry is set to the no-overlap entry point
   //   used by generate_conjoint_int_oop_copy().
   //
-  address generate_disjoint_copy(size_t size, bool aligned, bool is_oop, address *entry,
+  address generate_disjoint_copy(int size, bool aligned, bool is_oop, address *entry,
                                   const char *name, bool dest_uninitialized = false) {
     Register s = c_rarg0, d = c_rarg1, count = c_rarg2;
     RegSet saved_reg = RegSet::of(s, d, count);
     __ align(CodeEntryAlignment);
     StubCodeMark mark(this, "StubRoutines", name);
@@ -1388,11 +1388,11 @@
   //
   // If 'from' and/or 'to' are aligned on 4-byte boundaries, we let
   // the hardware handle it.  The two dwords within qwords that span
   // cache line boundaries will still be loaded and stored atomicly.
   //
-  address generate_conjoint_copy(size_t size, bool aligned, bool is_oop, address nooverlap_target,
+  address generate_conjoint_copy(int size, bool aligned, bool is_oop, address nooverlap_target,
                                  address *entry, const char *name,
                                  bool dest_uninitialized = false) {
     Register s = c_rarg0, d = c_rarg1, count = c_rarg2;
     RegSet saved_regs = RegSet::of(s, d, count);
     StubCodeMark mark(this, "StubRoutines", name);
@@ -1639,11 +1639,11 @@
   //   no-overlap entry point used by generate_conjoint_long_oop_copy().
   //
   address generate_disjoint_oop_copy(bool aligned, address *entry,
                                      const char *name, bool dest_uninitialized) {
     const bool is_oop = true;
-    const size_t size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);
+    const int size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);
     return generate_disjoint_copy(size, aligned, is_oop, entry, name, dest_uninitialized);
   }
 
   // Arguments:
   //   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes
@@ -1657,11 +1657,11 @@
   //
   address generate_conjoint_oop_copy(bool aligned,
                                      address nooverlap_target, address *entry,
                                      const char *name, bool dest_uninitialized) {
     const bool is_oop = true;
-    const size_t size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);
+    const int size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);
     return generate_conjoint_copy(size, aligned, is_oop, nooverlap_target, entry,
                                   name, dest_uninitialized);
   }
 
 
@@ -1712,14 +1712,14 @@
     const Register ckval       = c_rarg4;   // super_klass
 
     RegSet wb_pre_saved_regs = RegSet::range(c_rarg0, c_rarg4);
     RegSet wb_post_saved_regs = RegSet::of(count);
 
-    // Registers used as temps (r18, r19, r20 are save-on-entry)
+    // Registers used as temps (r19, r20, r21, r22 are save-on-entry)
+    const Register copied_oop  = r22;       // actual oop copied
     const Register count_save  = r21;       // orig elementscount
     const Register start_to    = r20;       // destination array start address
-    const Register copied_oop  = r18;       // actual oop copied
     const Register r19_klass   = r19;       // oop._klass
 
     //---------------------------------------------------------------
     // Assembler stub will be used for this call to arraycopy
     // if the two arrays are subtypes of Object[] but the
@@ -1752,12 +1752,11 @@
       BLOCK_COMMENT("Entry:");
     }
 
      // Empty array:  Nothing to do.
     __ cbz(count, L_done);
-
-    __ push(RegSet::of(r18, r19, r20, r21), sp);
+    __ push(RegSet::of(r19, r20, r21, r22), sp);
 
 #ifdef ASSERT
     BLOCK_COMMENT("assert consistent ckoff/ckval");
     // The ckoff and ckval must be mutually consistent,
     // even though caller generates both.
@@ -1822,11 +1821,11 @@
 
     __ BIND(L_do_card_marks);
     bs->arraycopy_epilogue(_masm, decorators, is_oop, start_to, count_save, rscratch1, wb_post_saved_regs);
 
     __ bind(L_done_pop);
-    __ pop(RegSet::of(r18, r19, r20, r21), sp);
+    __ pop(RegSet::of(r19, r20, r21, r22), sp);
     inc_counter_np(SharedRuntime::_checkcast_array_copy_ctr);
 
     __ bind(L_done);
     __ mov(r0, count);
     __ leave();
@@ -1999,11 +1998,11 @@
     __ tbnz(dst_pos, 31, L_failed);  // i.e. sign bit set
 
     // registers used as temp
     const Register scratch_length    = r16; // elements count to copy
     const Register scratch_src_klass = r17; // array klass
-    const Register lh                = r18; // layout helper
+    const Register lh                = r15; // layout helper
 
     //  if (length < 0) return -1;
     __ movw(scratch_length, length);        // length (elements count, 32-bits value)
     __ tbnz(scratch_length, 31, L_failed);  // i.e. sign bit set
 
@@ -2070,11 +2069,11 @@
     // src_addr = (src + array_header_in_bytes()) + (src_pos << log2elemsize);
     // dst_addr = (dst + array_header_in_bytes()) + (dst_pos << log2elemsize);
     //
 
     const Register rscratch1_offset = rscratch1;    // array offset
-    const Register r18_elsize = lh; // element size
+    const Register r15_elsize = lh; // element size
 
     __ ubfx(rscratch1_offset, lh, Klass::_lh_header_size_shift,
            exact_log2(Klass::_lh_header_size_mask+1));   // array_offset
     __ add(src, src, rscratch1_offset);           // src array offset
     __ add(dst, dst, rscratch1_offset);           // dst array offset
@@ -2091,12 +2090,12 @@
     assert(Klass::_lh_log2_element_size_shift == 0, "fix this code");
 
     // The possible values of elsize are 0-3, i.e. exact_log2(element
     // size in bytes).  We do a simple bitwise binary search.
   __ BIND(L_copy_bytes);
-    __ tbnz(r18_elsize, 1, L_copy_ints);
-    __ tbnz(r18_elsize, 0, L_copy_shorts);
+    __ tbnz(r15_elsize, 1, L_copy_ints);
+    __ tbnz(r15_elsize, 0, L_copy_shorts);
     __ lea(from, Address(src, src_pos));// src_addr
     __ lea(to,   Address(dst, dst_pos));// dst_addr
     __ movw(count, scratch_length); // length
     __ b(RuntimeAddress(byte_copy_entry));
 
@@ -2105,23 +2104,23 @@
     __ lea(to,   Address(dst, dst_pos, Address::lsl(1)));// dst_addr
     __ movw(count, scratch_length); // length
     __ b(RuntimeAddress(short_copy_entry));
 
   __ BIND(L_copy_ints);
-    __ tbnz(r18_elsize, 0, L_copy_longs);
+    __ tbnz(r15_elsize, 0, L_copy_longs);
     __ lea(from, Address(src, src_pos, Address::lsl(2)));// src_addr
     __ lea(to,   Address(dst, dst_pos, Address::lsl(2)));// dst_addr
     __ movw(count, scratch_length); // length
     __ b(RuntimeAddress(int_copy_entry));
 
   __ BIND(L_copy_longs);
 #ifdef ASSERT
     {
       BLOCK_COMMENT("assert long copy {");
       Label L;
-      __ andw(lh, lh, Klass::_lh_log2_element_size_mask); // lh -> r18_elsize
-      __ cmpw(r18_elsize, LogBytesPerLong);
+      __ andw(lh, lh, Klass::_lh_log2_element_size_mask); // lh -> r15_elsize
+      __ cmpw(r15_elsize, LogBytesPerLong);
       __ br(Assembler::EQ, L);
       __ stop("must be long copy, but elsize is wrong");
       __ bind(L);
       BLOCK_COMMENT("} assert long copy done");
     }
@@ -2135,12 +2134,12 @@
   __ BIND(L_objArray);
     // live at this point:  scratch_src_klass, scratch_length, src[_pos], dst[_pos]
 
     Label L_plain_copy, L_checkcast_copy;
     //  test array classes for subtyping
-    __ load_klass(r18, dst);
-    __ cmp(scratch_src_klass, r18); // usual case is exact equality
+    __ load_klass(r15, dst);
+    __ cmp(scratch_src_klass, r15); // usual case is exact equality
     __ br(Assembler::NE, L_checkcast_copy);
 
     // Identically typed arrays can be copied without element-wise checks.
     arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,
                            rscratch2, L_failed);
@@ -2152,21 +2151,21 @@
     __ movw(count, scratch_length); // length
   __ BIND(L_plain_copy);
     __ b(RuntimeAddress(oop_copy_entry));
 
   __ BIND(L_checkcast_copy);
-    // live at this point:  scratch_src_klass, scratch_length, r18 (dst_klass)
+    // live at this point:  scratch_src_klass, scratch_length, r15 (dst_klass)
     {
       // Before looking at dst.length, make sure dst is also an objArray.
-      __ ldrw(rscratch1, Address(r18, lh_offset));
+      __ ldrw(rscratch1, Address(r15, lh_offset));
       __ movw(rscratch2, objArray_lh);
       __ eorw(rscratch1, rscratch1, rscratch2);
       __ cbnzw(rscratch1, L_failed);
 
       // It is safe to examine both src.length and dst.length.
       arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,
-                             r18, L_failed);
+                             r15, L_failed);
 
       __ load_klass(dst_klass, dst); // reload
 
       // Marshal the base address arguments now, freeing registers.
       __ lea(from, Address(src, src_pos, Address::lsl(LogBytesPerHeapOop)));
@@ -3280,12 +3279,12 @@
     FloatRegister vs2acc = v2;
     FloatRegister vtable = v3;
 
     // Max number of bytes we can process before having to take the mod
     // 0x15B0 is 5552 in decimal, the largest n such that 255n(n+1)/2 + (n+1)(BASE-1) <= 2^32-1
-    unsigned long BASE = 0xfff1;
-    unsigned long NMAX = 0x15B0;
+    uint64_t BASE = 0xfff1;
+    uint64_t NMAX = 0x15B0;
 
     __ mov(base, BASE);
     __ mov(nmax, NMAX);
 
     // Load accumulation coefficients for the upper 16 bits
@@ -4058,11 +4057,11 @@
     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, cnt2 = r4,
         tmp1 = r10, tmp2 = r11, tmp3 = r12, tmp4 = r14;
     FloatRegister vtmpZ = v0, vtmp = v1, vtmp3 = v2;
     RegSet spilled_regs = RegSet::of(tmp3, tmp4);
 
-    int prefetchLoopExitCondition = MAX(64, SoftwarePrefetchHintDistance/2);
+    int prefetchLoopExitCondition = MAX2(64, SoftwarePrefetchHintDistance/2);
 
     __ eor(vtmpZ, __ T16B, vtmpZ, vtmpZ);
     // cnt2 == amount of characters left to compare
     // Check already loaded first 4 symbols(vtmp and tmp2(LU)/tmp1(UL))
     __ zip1(vtmp, __ T8B, vtmp, vtmpZ);
@@ -4172,11 +4171,11 @@
     Label SMALL_LOOP, LARGE_LOOP_PREFETCH, CHECK_LAST, DIFF2, TAIL,
         LENGTH_DIFF, DIFF, LAST_CHECK_AND_LENGTH_DIFF,
         DIFF_LAST_POSITION, DIFF_LAST_POSITION2;
     // exit from large loop when less than 64 bytes left to read or we're about
     // to prefetch memory behind array border
-    int largeLoopExitCondition = MAX(64, SoftwarePrefetchHintDistance)/(isLL ? 1 : 2);
+    int largeLoopExitCondition = MAX2(64, SoftwarePrefetchHintDistance)/(isLL ? 1 : 2);
     // cnt1/cnt2 contains amount of characters to compare. cnt1 can be re-used
     // update cnt2 counter with already loaded 8 bytes
     __ sub(cnt2, cnt2, wordSize/(isLL ? 1 : 2));
     // update pointers, because of previous read
     __ add(str1, str1, wordSize);
@@ -4329,11 +4328,11 @@
     __ sub(cnt2, cnt2, cnt1);
     __ andr(first, ch1, str1_isL ? 0xFF : 0xFFFF);
     if (str1_isL != str2_isL) {
       __ eor(v0, __ T16B, v0, v0);
     }
-    __ mov(tmp1, str2_isL ? 0x0101010101010101 : 0x0001000100010001);
+    __ mov(tmp1, (uint64_t)(str2_isL ? 0x0101010101010101 : 0x0001000100010001));
     __ mul(first, first, tmp1);
     // check if we have less than 1 register to check
     __ subs(cnt2, cnt2, wordSize/str2_chr_size - 1);
     if (str1_isL != str2_isL) {
       __ fmovd(v1, ch1);
@@ -4598,11 +4597,11 @@
     __ align(CodeEntryAlignment);
     StubCodeMark mark(this, "StubRoutines", "large_byte_array_inflate");
     address entry = __ pc();
     Label LOOP, LOOP_START, LOOP_PRFM, LOOP_PRFM_START, DONE;
     Register src = r0, dst = r1, len = r2, octetCounter = r3;
-    const int large_loop_threshold = MAX(64, SoftwarePrefetchHintDistance)/8 + 4;
+    const int large_loop_threshold = MAX2(64, SoftwarePrefetchHintDistance)/8 + 4;
 
     // do one more 8-byte read to have address 16-byte aligned in most cases
     // also use single store instruction
     __ ldrd(v2, __ post(src, 8));
     __ sub(octetCounter, octetCounter, 2);
@@ -4848,44 +4847,53 @@
       Register reg = c_rarg0;
       Pa_base = reg;       // Argument registers
       if (squaring)
         Pb_base = Pa_base;
       else
-        Pb_base = ++reg;
-      Pn_base = ++reg;
-      Rlen= ++reg;
-      inv = ++reg;
-      Pm_base = ++reg;
+        Pb_base = next_reg(reg);
+      Pn_base = next_reg(reg);
+      Rlen= next_reg(reg);
+      inv = next_reg(reg);
+      Pm_base = next_reg(reg);
 
                           // Working registers:
-      Ra =  ++reg;        // The current digit of a, b, n, and m.
-      Rb =  ++reg;
-      Rm =  ++reg;
-      Rn =  ++reg;
+      Ra =  next_reg(reg); // The current digit of a, b, n, and m.
+      Rb =  next_reg(reg);
+      Rm =  next_reg(reg);
+      Rn =  next_reg(reg);
 
-      Pa =  ++reg;        // Pointers to the current/next digit of a, b, n, and m.
-      Pb =  ++reg;
-      Pm =  ++reg;
-      Pn =  ++reg;
+      Pa =  next_reg(reg); // Pointers to the current/next digit of a, b, n, and m.
+      Pb =  next_reg(reg);
+      Pm =  next_reg(reg);
+      Pn =  next_reg(reg);
 
-      t0 =  ++reg;        // Three registers which form a
-      t1 =  ++reg;        // triple-precision accumuator.
-      t2 =  ++reg;
+      t0 =  next_reg(reg); // Three registers which form a
+      t1 =  next_reg(reg); // triple-precision accumuator.
+      t2 =  next_reg(reg);
 
-      Ri =  ++reg;        // Inner and outer loop indexes.
-      Rj =  ++reg;
+      Ri =  next_reg(reg); // Inner and outer loop indexes.
+      Rj =  next_reg(reg);
 
-      Rhi_ab = ++reg;     // Product registers: low and high parts
-      Rlo_ab = ++reg;     // of a*b and m*n.
-      Rhi_mn = ++reg;
-      Rlo_mn = ++reg;
+      Rhi_ab = next_reg(reg); // Product registers: low and high parts
+      Rlo_ab = next_reg(reg); // of a*b and m*n.
+      Rhi_mn = next_reg(reg);
+      Rlo_mn = next_reg(reg);
 
       // r19 and up are callee-saved.
       _toSave = RegSet::range(r19, reg) + Pm_base;
     }
 
   private:
+    Register next_reg(Register &reg) {
+#ifdef _WIN64
+      // skip r18 on Windows, it's used by native TLS
+      return ++reg == r18 ? ++reg : reg;
+#else
+      return ++reg;
+#endif
+    }
+
     void save_regs() {
       push(_toSave, sp);
     }
 
     void restore_regs() {
diff a/src/hotspot/cpu/aarch64/stubRoutines_aarch64.cpp b/src/hotspot/cpu/aarch64/stubRoutines_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/stubRoutines_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/stubRoutines_aarch64.cpp
@@ -59,11 +59,11 @@
 bool StubRoutines::aarch64::_completed = false;
 
 /**
  *  crc_table[] from jdk/src/share/native/java/util/zip/zlib-1.2.5/crc32.h
  */
-juint StubRoutines::aarch64::_crc_table[] ATTRIBUTE_ALIGNED(4096) =
+ATTRIBUTE_ALIGNED(4096) juint StubRoutines::aarch64::_crc_table[] =
 {
     // Table 0
     0x00000000UL, 0x77073096UL, 0xee0e612cUL, 0x990951baUL, 0x076dc419UL,
     0x706af48fUL, 0xe963a535UL, 0x9e6495a3UL, 0x0edb8832UL, 0x79dcb8a4UL,
     0xe0d5e91eUL, 0x97d2d988UL, 0x09b64c2bUL, 0x7eb17cbdUL, 0xe7b82d07UL,
@@ -286,15 +286,15 @@
     0x02D578EDUL, 0x7DAEED62UL,         // word swap
     0xD502ED78UL, 0xAE7D62EDUL,         // byte swap of word swap
 };
 
 // Accumulation coefficients for adler32 upper 16 bits
-jubyte StubRoutines::aarch64::_adler_table[] __attribute__ ((aligned(64))) = {
+ATTRIBUTE_ALIGNED(64) jubyte StubRoutines::aarch64::_adler_table[] = {
     16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1
 };
 
-juint StubRoutines::aarch64::_npio2_hw[] __attribute__ ((aligned(64))) = {
+ATTRIBUTE_ALIGNED(64) juint StubRoutines::aarch64::_npio2_hw[] = {
     // first, various coefficient values: 0.5, invpio2, pio2_1, pio2_1t, pio2_2,
     // pio2_2t, pio2_3, pio2_3t
     // This is a small optimization wich keeping double[8] values in int[] table
     // to have less address calculation instructions
     //
@@ -322,37 +322,37 @@
     0x404858EB, 0x404921FB
 };
 
 // Coefficients for sin(x) polynomial approximation: S1..S6.
 // See kernel_sin comments in macroAssembler_aarch64_trig.cpp for details
-jdouble StubRoutines::aarch64::_dsin_coef[] __attribute__ ((aligned(64))) = {
-    -1.66666666666666324348e-01, // 0xBFC5555555555549
-     8.33333333332248946124e-03, // 0x3F8111111110F8A6
-    -1.98412698298579493134e-04, // 0xBF2A01A019C161D5
-     2.75573137070700676789e-06, // 0x3EC71DE357B1FE7D
-    -2.50507602534068634195e-08, // 0xBE5AE5E68A2B9CEB
-     1.58969099521155010221e-10  // 0x3DE5D93A5ACFD57C
+ATTRIBUTE_ALIGNED(64) julong StubRoutines::aarch64::_dsin_coef[] = {
+    0xBFC5555555555549, //  1.66666666666666324348e-01
+    0x3F8111111110F8A6, //  8.33333333332248946124e-03
+    0xBF2A01A019C161D5, // -1.98412698298579493134e-04
+    0x3EC71DE357B1FE7D, //  2.75573137070700676789e-06
+    0xBE5AE5E68A2B9CEB, // -2.50507602534068634195e-08
+    0x3DE5D93A5ACFD57C  //  1.58969099521155010221e-10
 };
 
 // Coefficients for cos(x) polynomial approximation: C1..C6.
 // See kernel_cos comments in macroAssembler_aarch64_trig.cpp for details
-jdouble StubRoutines::aarch64::_dcos_coef[] __attribute__ ((aligned(64))) = {
-     4.16666666666666019037e-02, // c0x3FA555555555554C
-    -1.38888888888741095749e-03, // 0xBF56C16C16C15177
-     2.48015872894767294178e-05, // 0x3EFA01A019CB1590
-    -2.75573143513906633035e-07, // 0xBE927E4F809C52AD
-     2.08757232129817482790e-09, // 0x3E21EE9EBDB4B1C4
-    -1.13596475577881948265e-11  // 0xBDA8FAE9BE8838D4
+ATTRIBUTE_ALIGNED(64) julong StubRoutines::aarch64::_dcos_coef[] = {
+    0x3FA555555555554C, //  4.16666666666666019037e-02
+    0xBF56C16C16C15177, // -1.38888888888741095749e-03
+    0x3EFA01A019CB1590, //  2.48015872894767294178e-05
+    0xBE927E4F809C52AD, // -2.75573143513906633035e-07
+    0x3E21EE9EBDB4B1C4, //  2.08757232129817482790e-09
+    0xBDA8FAE9BE8838D4  // -1.13596475577881948265e-11
 };
 
 // Table of constants for 2/pi, 396 Hex digits (476 decimal) of 2/pi.
 // Used in cases of very large argument. 396 hex digits is enough to support
 // required precision.
 // Converted to double to avoid unnecessary conversion in code
 // NOTE: table looks like original int table: {0xA2F983, 0x6E4E44,...} with
 //       only (double) conversion added
-jdouble StubRoutines::aarch64::_two_over_pi[] __attribute__ ((aligned(64))) = {
+ATTRIBUTE_ALIGNED(64) jdouble StubRoutines::aarch64::_two_over_pi[] = {
   (double)0xA2F983, (double)0x6E4E44, (double)0x1529FC, (double)0x2757D1, (double)0xF534DD, (double)0xC0DB62,
   (double)0x95993C, (double)0x439041, (double)0xFE5163, (double)0xABDEBB, (double)0xC561B7, (double)0x246E3A,
   (double)0x424DD2, (double)0xE00649, (double)0x2EEA09, (double)0xD1921C, (double)0xFE1DEB, (double)0x1CB129,
   (double)0xA73EE8, (double)0x8235F5, (double)0x2EBB44, (double)0x84E99C, (double)0x7026B4, (double)0x5F7E41,
   (double)0x3991D6, (double)0x398353, (double)0x39F49C, (double)0x845F8B, (double)0xBDF928, (double)0x3B1FF8,
@@ -363,15 +363,15 @@
   (double)0x91615E, (double)0xE61B08, (double)0x659985, (double)0x5F14A0, (double)0x68408D, (double)0xFFD880,
   (double)0x4D7327, (double)0x310606, (double)0x1556CA, (double)0x73A8C9, (double)0x60E27B, (double)0xC08C6B,
 };
 
 // Pi over 2 value
-jdouble StubRoutines::aarch64::_pio2[] __attribute__ ((aligned(64))) = {
-  1.57079625129699707031e+00, // 0x3FF921FB40000000
-  7.54978941586159635335e-08, // 0x3E74442D00000000
-  5.39030252995776476554e-15, // 0x3CF8469880000000
-  3.28200341580791294123e-22, // 0x3B78CC5160000000
-  1.27065575308067607349e-29, // 0x39F01B8380000000
-  1.22933308981111328932e-36, // 0x387A252040000000
-  2.73370053816464559624e-44, // 0x36E3822280000000
-  2.16741683877804819444e-51, // 0x3569F31D00000000
+ATTRIBUTE_ALIGNED(64) julong StubRoutines::aarch64::_pio2[] = {
+  0x3FF921FB40000000, // 1.57079625129699707031e+00
+  0x3E74442D00000000, // 7.54978941586159635335e-08
+  0x3CF8469880000000, // 5.39030252995776476554e-15
+  0x3B78CC5160000000, // 3.28200341580791294123e-22
+  0x39F01B8380000000, // 1.27065575308067607349e-29
+  0x387A252040000000, // 1.22933308981111328932e-36
+  0x36E3822280000000, // 2.73370053816464559624e-44
+  0x3569F31D00000000  // 2.16741683877804819444e-51
 };
diff a/src/hotspot/cpu/aarch64/stubRoutines_aarch64.hpp b/src/hotspot/cpu/aarch64/stubRoutines_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/stubRoutines_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/stubRoutines_aarch64.hpp
@@ -183,12 +183,12 @@
   static juint    _crc_table[];
   static jubyte   _adler_table[];
   // begin trigonometric tables block. See comments in .cpp file
   static juint    _npio2_hw[];
   static jdouble   _two_over_pi[];
-  static jdouble   _pio2[];
-  static jdouble   _dsin_coef[];
-  static jdouble  _dcos_coef[];
+  static julong    _pio2[];
+  static julong    _dsin_coef[];
+  static julong    _dcos_coef[];
   // end trigonometric tables block
 };
 
 #endif // CPU_AARCH64_STUBROUTINES_AARCH64_HPP
diff a/src/hotspot/cpu/aarch64/templateInterpreterGenerator_aarch64.cpp b/src/hotspot/cpu/aarch64/templateInterpreterGenerator_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/templateInterpreterGenerator_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/templateInterpreterGenerator_aarch64.cpp
@@ -992,11 +992,11 @@
 
     // Arguments are reversed on java expression stack
     __ ldrw(val, Address(esp, 0));              // byte value
     __ ldrw(crc, Address(esp, wordSize));       // Initial CRC
 
-    unsigned long offset;
+    uint64_t offset;
     __ adrp(tbl, ExternalAddress(StubRoutines::crc_table_addr()), offset);
     __ add(tbl, tbl, offset);
 
     __ mvnw(crc, crc); // ~crc
     __ update_byte_crc32(crc, val, tbl);
diff a/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp b/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
@@ -1703,11 +1703,11 @@
 {
   transition(ltos, itos);
   Label done;
   __ pop_l(r1);
   __ cmp(r1, r0);
-  __ mov(r0, (u_int64_t)-1L);
+  __ mov(r0, (uint64_t)-1L);
   __ br(Assembler::LT, done);
   // __ mov(r0, 1UL);
   // __ csel(r0, r0, zr, Assembler::NE);
   // and here is a faster way
   __ csinc(r0, zr, zr, Assembler::EQ);
@@ -1727,11 +1727,11 @@
     __ fcmpd(v1, v0);
   }
   if (unordered_result < 0) {
     // we want -1 for unordered or less than, 0 for equal and 1 for
     // greater than.
-    __ mov(r0, (u_int64_t)-1L);
+    __ mov(r0, (uint64_t)-1L);
     // for FP LT tests less than or unordered
     __ br(Assembler::LT, done);
     // install 0 for EQ otherwise 1
     __ csinc(r0, zr, zr, Assembler::EQ);
   } else {
diff a/src/hotspot/cpu/aarch64/vm_version_aarch64.cpp b/src/hotspot/cpu/aarch64/vm_version_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/vm_version_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/vm_version_aarch64.cpp
@@ -30,38 +30,17 @@
 #include "runtime/java.hpp"
 #include "runtime/os.hpp"
 #include "runtime/stubCodeGenerator.hpp"
 #include "runtime/vm_version.hpp"
 #include "utilities/macros.hpp"
+#include "vm_version_aarch64.hpp"
 
 #include OS_HEADER_INLINE(os)
 
-#include <sys/auxv.h>
+#ifndef _WIN64
+#include <sys/_features.h>
 #include <asm/hwcap.h>
-
-#ifndef HWCAP_AES
-#define HWCAP_AES   (1<<3)
-#endif
-
-#ifndef HWCAP_PMULL
-#define HWCAP_PMULL (1<<4)
-#endif
-
-#ifndef HWCAP_SHA1
-#define HWCAP_SHA1  (1<<5)
-#endif
-
-#ifndef HWCAP_SHA2
-#define HWCAP_SHA2  (1<<6)
-#endif
-
-#ifndef HWCAP_CRC32
-#define HWCAP_CRC32 (1<<7)
-#endif
-
-#ifndef HWCAP_ATOMICS
-#define HWCAP_ATOMICS (1<<8)
 #endif
 
 int VM_Version::_cpu;
 int VM_Version::_model;
 int VM_Version::_model2;
@@ -97,12 +76,14 @@
     __ enter();
 
     __ get_dczid_el0(rscratch1);
     __ strw(rscratch1, Address(c_rarg0, in_bytes(VM_Version::dczid_el0_offset())));
 
+#ifndef _WIN64
     __ get_ctr_el0(rscratch1);
     __ strw(rscratch1, Address(c_rarg0, in_bytes(VM_Version::ctr_el0_offset())));
+#endif
 
     __ leave();
     __ ret(lr);
 
 #   undef __
@@ -159,17 +140,26 @@
        (SoftwarePrefetchHintDistance & 7)) {
     warning("SoftwarePrefetchHintDistance must be -1, or a multiple of 8");
     SoftwarePrefetchHintDistance &= ~7;
   }
 
-  unsigned long auxv = getauxval(AT_HWCAP);
+#ifndef _WIN64
+  _features = getauxval(AT_HWCAP);
+#else
+  if (IsProcessorFeaturePresent(PF_ARM_V8_CRC32_INSTRUCTIONS_AVAILABLE))   _features |= CPU_CRC32;
+  if (IsProcessorFeaturePresent(PF_ARM_V8_CRYPTO_INSTRUCTIONS_AVAILABLE))  _features |= CPU_AES | CPU_SHA1 | CPU_SHA2;
+#ifdef PF_ARM_V81_ATOMIC_INSTRUCTIONS_AVAILABLE
+  if (IsProcessorFeaturePresent(PF_ARM_V81_ATOMIC_INSTRUCTIONS_AVAILABLE)) _features |= CPU_DMB_ATOMICS;
+#endif
+  if (IsProcessorFeaturePresent(PF_ARM_VFP_32_REGISTERS_AVAILABLE))        _features |= CPU_ASIMD;
+  // No check for CPU_PMULL
+#endif // _WIN64
 
   char buf[512];
 
-  _features = auxv;
-
   int cpu_lines = 0;
+#ifndef _WIN64
   if (FILE *f = fopen("/proc/cpuinfo", "r")) {
     // need a large buffer as the flags line may include lots of text
     char buf[1024], *p;
     while (fgets(buf, sizeof (buf), f) != NULL) {
       if ((p = strchr(buf, ':')) != NULL) {
@@ -191,10 +181,32 @@
         }
       }
     }
     fclose(f);
   }
+#else
+  {
+    char* buf = ::getenv("PROCESSOR_IDENTIFIER");
+    if (buf && strstr(buf, "Ampere(TM)") != NULL) {
+      _cpu = CPU_AMCC;
+      cpu_lines++;
+    } else if (buf && strstr(buf, "Cavium Inc.") != NULL) {
+      _cpu = CPU_CAVIUM;
+      cpu_lines++;
+    } else {
+      log_info(os)("VM_Version: unknown CPU model");
+    }
+
+    if (_cpu) {
+      SYSTEM_INFO si;
+      GetSystemInfo(&si);
+      _model = si.wProcessorLevel;
+      _variant = si.wProcessorRevision / 0xFF;
+      _revision = si.wProcessorRevision & 0xFF;
+    }
+  }
+#endif // _WIN64
 
   if (os::supports_map_sync()) {
     // if dcpop is available publish data cache line flush size via
     // generic field, otherwise let if default to zero thereby
     // disabling writeback
@@ -233,10 +245,11 @@
   }
 
   // ThunderX2
   if ((_cpu == CPU_CAVIUM && (_model == 0xAF)) ||
       (_cpu == CPU_BROADCOM && (_model == 0x516))) {
+    _features |= CPU_DMB_ATOMICS;
     if (FLAG_IS_DEFAULT(AvoidUnalignedAccesses)) {
       FLAG_SET_DEFAULT(AvoidUnalignedAccesses, true);
     }
     if (FLAG_IS_DEFAULT(UseSIMDForMemoryOps)) {
       FLAG_SET_DEFAULT(UseSIMDForMemoryOps, true);
@@ -278,24 +291,25 @@
   // undisclosed A53 cores which we could be swapped to at any stage
   if (_cpu == CPU_ARM && cpu_lines == 1 && _model == 0xd07) _features |= CPU_A53MAC;
 
   sprintf(buf, "0x%02x:0x%x:0x%03x:%d", _cpu, _variant, _model, _revision);
   if (_model2) sprintf(buf+strlen(buf), "(0x%03x)", _model2);
-  if (auxv & HWCAP_ASIMD) strcat(buf, ", simd");
-  if (auxv & HWCAP_CRC32) strcat(buf, ", crc");
-  if (auxv & HWCAP_AES)   strcat(buf, ", aes");
-  if (auxv & HWCAP_SHA1)  strcat(buf, ", sha1");
-  if (auxv & HWCAP_SHA2)  strcat(buf, ", sha256");
-  if (auxv & HWCAP_ATOMICS) strcat(buf, ", lse");
+  if (_features & CPU_ASIMD) strcat(buf, ", simd");
+  if (_features & CPU_CRC32) strcat(buf, ", crc");
+  if (_features & CPU_AES)   strcat(buf, ", aes");
+  if (_features & CPU_SHA1)  strcat(buf, ", sha1");
+  if (_features & CPU_SHA2)  strcat(buf, ", sha256");
+  if (_features & CPU_LSE)   strcat(buf, ", lse");
+  if (_features & CPU_DMB_ATOMICS) strcat(buf, ", atomics");
 
   _features_string = os::strdup(buf);
 
   if (FLAG_IS_DEFAULT(UseCRC32)) {
-    UseCRC32 = (auxv & HWCAP_CRC32) != 0;
+    UseCRC32 = (_features & CPU_CRC32) != 0;
   }
 
-  if (UseCRC32 && (auxv & HWCAP_CRC32) == 0) {
+  if (UseCRC32 && (_features & CPU_CRC32) == 0) {
     warning("UseCRC32 specified, but not supported on this CPU");
     FLAG_SET_DEFAULT(UseCRC32, false);
   }
 
   if (FLAG_IS_DEFAULT(UseAdler32Intrinsics)) {
@@ -305,21 +319,21 @@
   if (UseVectorizedMismatchIntrinsic) {
     warning("UseVectorizedMismatchIntrinsic specified, but not available on this CPU.");
     FLAG_SET_DEFAULT(UseVectorizedMismatchIntrinsic, false);
   }
 
-  if (auxv & HWCAP_ATOMICS) {
+  if (_features & CPU_LSE) {
     if (FLAG_IS_DEFAULT(UseLSE))
       FLAG_SET_DEFAULT(UseLSE, true);
   } else {
     if (UseLSE) {
       warning("UseLSE specified, but not supported on this CPU");
       FLAG_SET_DEFAULT(UseLSE, false);
     }
   }
 
-  if (auxv & HWCAP_AES) {
+  if (_features & CPU_AES) {
     UseAES = UseAES || FLAG_IS_DEFAULT(UseAES);
     UseAESIntrinsics =
         UseAESIntrinsics || (UseAES && FLAG_IS_DEFAULT(UseAESIntrinsics));
     if (UseAESIntrinsics && !UseAES) {
       warning("UseAESIntrinsics enabled, but UseAES not, enabling");
@@ -343,11 +357,11 @@
 
   if (FLAG_IS_DEFAULT(UseCRC32Intrinsics)) {
     UseCRC32Intrinsics = true;
   }
 
-  if (auxv & HWCAP_CRC32) {
+  if (_features & CPU_CRC32) {
     if (FLAG_IS_DEFAULT(UseCRC32CIntrinsics)) {
       FLAG_SET_DEFAULT(UseCRC32CIntrinsics, true);
     }
   } else if (UseCRC32CIntrinsics) {
     warning("CRC32C is not available on the CPU");
@@ -356,29 +370,29 @@
 
   if (FLAG_IS_DEFAULT(UseFMA)) {
     FLAG_SET_DEFAULT(UseFMA, true);
   }
 
-  if (auxv & (HWCAP_SHA1 | HWCAP_SHA2)) {
+  if (_features & (CPU_SHA1 | CPU_SHA2)) {
     if (FLAG_IS_DEFAULT(UseSHA)) {
       FLAG_SET_DEFAULT(UseSHA, true);
     }
   } else if (UseSHA) {
     warning("SHA instructions are not available on this CPU");
     FLAG_SET_DEFAULT(UseSHA, false);
   }
 
-  if (UseSHA && (auxv & HWCAP_SHA1)) {
+  if (UseSHA && (_features & CPU_SHA1)) {
     if (FLAG_IS_DEFAULT(UseSHA1Intrinsics)) {
       FLAG_SET_DEFAULT(UseSHA1Intrinsics, true);
     }
   } else if (UseSHA1Intrinsics) {
     warning("Intrinsics for SHA-1 crypto hash functions not available on this CPU.");
     FLAG_SET_DEFAULT(UseSHA1Intrinsics, false);
   }
 
-  if (UseSHA && (auxv & HWCAP_SHA2)) {
+  if (UseSHA && (_features & CPU_SHA2)) {
     if (FLAG_IS_DEFAULT(UseSHA256Intrinsics)) {
       FLAG_SET_DEFAULT(UseSHA256Intrinsics, true);
     }
   } else if (UseSHA256Intrinsics) {
     warning("Intrinsics for SHA-224 and SHA-256 crypto hash functions not available on this CPU.");
@@ -392,11 +406,11 @@
 
   if (!(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics)) {
     FLAG_SET_DEFAULT(UseSHA, false);
   }
 
-  if (auxv & HWCAP_PMULL) {
+  if (_features & CPU_PMULL) {
     if (FLAG_IS_DEFAULT(UseGHASHIntrinsics)) {
       FLAG_SET_DEFAULT(UseGHASHIntrinsics, true);
     }
   } else if (UseGHASHIntrinsics) {
     warning("GHASH intrinsics are not available on this CPU");
diff a/src/hotspot/cpu/aarch64/vm_version_aarch64.hpp b/src/hotspot/cpu/aarch64/vm_version_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/vm_version_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/vm_version_aarch64.hpp
@@ -27,10 +27,11 @@
 #define CPU_AARCH64_VM_VERSION_AARCH64_HPP
 
 #include "runtime/abstract_vm_version.hpp"
 #include "runtime/globals_extension.hpp"
 #include "utilities/sizes.hpp"
+#include "runtime/java.hpp"
 
 class VM_Version : public Abstract_VM_Version {
   friend class JVMCIVMStructs;
 
 protected:
@@ -41,11 +42,13 @@
   static int _revision;
   static int _stepping;
   static bool _dcpop;
   struct PsrInfo {
     uint32_t dczid_el0;
+#ifndef _WIN64
     uint32_t ctr_el0;
+#endif
   };
   static PsrInfo _psr_info;
   static void get_processor_features();
 
 public:
@@ -105,12 +108,15 @@
   static int cpu_model()                      { return _model; }
   static int cpu_model2()                     { return _model2; }
   static int cpu_variant()                    { return _variant; }
   static int cpu_revision()                   { return _revision; }
   static bool supports_dcpop()                { return _dcpop; }
+
   static ByteSize dczid_el0_offset() { return byte_offset_of(PsrInfo, dczid_el0); }
+#ifndef _WIN64
   static ByteSize ctr_el0_offset()   { return byte_offset_of(PsrInfo, ctr_el0); }
+#endif
   static bool is_zva_enabled() {
     // Check the DZP bit (bit 4) of dczid_el0 is zero
     // and block size (bit 0~3) is not zero.
     return ((_psr_info.dczid_el0 & 0x10) == 0 &&
             (_psr_info.dczid_el0 & 0xf) != 0);
@@ -118,14 +124,22 @@
   static int zva_length() {
     assert(is_zva_enabled(), "ZVA not available");
     return 4 << (_psr_info.dczid_el0 & 0xf);
   }
   static int icache_line_size() {
+#ifndef _WIN64
     return (1 << (_psr_info.ctr_el0 & 0x0f)) * 4;
+#else
+    return os::win32::get_cacheline_size();
+#endif
   }
   static int dcache_line_size() {
+#ifndef _WIN64
     return (1 << ((_psr_info.ctr_el0 >> 16) & 0x0f)) * 4;
+#else
+    return os::win32::get_cacheline_size();
+#endif
   }
   static bool supports_fast_class_init_checks() { return true; }
 };
 
 #endif // CPU_AARCH64_VM_VERSION_AARCH64_HPP
diff a/src/hotspot/cpu/aarch64/vm_version_ext_aarch64.cpp b/src/hotspot/cpu/aarch64/vm_version_ext_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/vm_version_ext_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/vm_version_ext_aarch64.cpp
@@ -20,10 +20,11 @@
  * or visit www.oracle.com if you need additional information or have any
  * questions.
  *
  */
 
+#include "precompiled.hpp"
 #include "memory/allocation.hpp"
 #include "memory/allocation.inline.hpp"
 #include "runtime/os.inline.hpp"
 #include "vm_version_ext_aarch64.hpp"
 
diff a/src/hotspot/os/windows/osThread_windows.hpp b/src/hotspot/os/windows/osThread_windows.hpp
--- a/src/hotspot/os/windows/osThread_windows.hpp
+++ b/src/hotspot/os/windows/osThread_windows.hpp
@@ -25,11 +25,11 @@
 #ifndef OS_WINDOWS_OSTHREAD_WINDOWS_HPP
 #define OS_WINDOWS_OSTHREAD_WINDOWS_HPP
 
   typedef void* HANDLE;
  public:
-  typedef unsigned long thread_id_t;
+  typedef uint64_t thread_id_t;
 
  private:
   // Win32-specific thread information
   HANDLE _thread_handle;        // Win32 thread handle
   HANDLE _interrupt_event;      // Event signalled on thread interrupt for use by
diff a/src/hotspot/os/windows/os_windows.cpp b/src/hotspot/os/windows/os_windows.cpp
--- a/src/hotspot/os/windows/os_windows.cpp
+++ b/src/hotspot/os/windows/os_windows.cpp
@@ -29,10 +29,11 @@
 #include "jvm.h"
 #include "classfile/classLoader.hpp"
 #include "classfile/systemDictionary.hpp"
 #include "classfile/vmSymbols.hpp"
 #include "code/icBuffer.hpp"
+#include "code/nativeInst.hpp"
 #include "code/vtableStubs.hpp"
 #include "compiler/compileBroker.hpp"
 #include "compiler/disassembler.hpp"
 #include "interpreter/interpreter.hpp"
 #include "logging/log.hpp"
@@ -120,18 +121,17 @@
 static FILETIME process_user_time;
 static FILETIME process_kernel_time;
 
 #ifdef _M_AMD64
   #define __CPU__ amd64
+#elif defined _M_ARM64
+  #define __CPU__ aarch64
 #else
   #define __CPU__ i486
 #endif
 
-#if INCLUDE_AOT
 PVOID  topLevelVectoredExceptionHandler = NULL;
-LONG WINAPI topLevelVectoredExceptionFilter(struct _EXCEPTION_POINTERS* exceptionInfo);
-#endif
 
 // save DLL module handle, used by GetModuleFileName
 
 HINSTANCE vm_lib_handle;
 
@@ -147,16 +147,14 @@
     break;
   case DLL_PROCESS_DETACH:
     if (ForceTimeHighResolution) {
       timeEndPeriod(1L);
     }
-#if INCLUDE_AOT
     if (topLevelVectoredExceptionHandler != NULL) {
       RemoveVectoredExceptionHandler(topLevelVectoredExceptionHandler);
       topLevelVectoredExceptionHandler = NULL;
     }
-#endif
     break;
   default:
     break;
   }
   return true;
@@ -420,12 +418,10 @@
     return res;
   }
   return NULL;
 }
 
-LONG WINAPI topLevelExceptionFilter(struct _EXCEPTION_POINTERS* exceptionInfo);
-
 // Thread start routine for all newly created threads
 static unsigned __stdcall thread_native_entry(Thread* thread) {
 
   thread->record_stack_base_and_size();
 
@@ -456,19 +452,14 @@
     res = 20115;    // java thread
   }
 
   log_info(os, thread)("Thread is alive (tid: " UINTX_FORMAT ").", os::current_thread_id());
 
-  // Install a win32 structured exception handler around every thread created
-  // by VM, so VM can generate error dump when an exception occurred in non-
-  // Java thread (e.g. VM thread).
-  __try {
-    thread->call_run();
-  } __except(topLevelExceptionFilter(
-                                     (_EXCEPTION_POINTERS*)_exception_info())) {
-    // Nothing to do.
-  }
+  // Any exception is caught by the Vectored Exception Handler, so VM can
+  // generate error dump when an exception occurred in non-Java thread
+  // (e.g. VM thread).
+  thread->call_run();
 
   // Note: at this point the thread object may already have deleted itself.
   // Do not dereference it from here on out.
 
   log_info(os, thread)("Thread finished (tid: " UINTX_FORMAT ").", os::current_thread_id());
@@ -1427,19 +1418,22 @@
     char* arch_name;
   } arch_t;
 
   static const arch_t arch_array[] = {
     {IMAGE_FILE_MACHINE_I386,      (char*)"IA 32"},
-    {IMAGE_FILE_MACHINE_AMD64,     (char*)"AMD 64"}
+    {IMAGE_FILE_MACHINE_AMD64,     (char*)"AMD 64"},
+    {IMAGE_FILE_MACHINE_ARM64,	   (char*)"ARM 64"}
   };
 #if (defined _M_AMD64)
   static const uint16_t running_arch = IMAGE_FILE_MACHINE_AMD64;
 #elif (defined _M_IX86)
   static const uint16_t running_arch = IMAGE_FILE_MACHINE_I386;
+#elif (defined _M_ARM64)
+  static const uint16_t running_arch = IMAGE_FILE_MACHINE_ARM64;
 #else
   #error Method os::dll_load requires that one of following \
-         is defined :_M_AMD64 or _M_IX86
+         is defined :_M_AMD64 or _M_IX86 or _M_ARM64
 #endif
 
 
   // Obtain a string for printf operation
   // lib_arch_str shall contain string what platform this .dll was built for
@@ -1732,11 +1726,12 @@
   // Retrieve SYSTEM_INFO from GetNativeSystemInfo call so that we could
   // find out whether we are running on 64 bit processor or not
   SYSTEM_INFO si;
   ZeroMemory(&si, sizeof(SYSTEM_INFO));
   GetNativeSystemInfo(&si);
-  if (si.wProcessorArchitecture == PROCESSOR_ARCHITECTURE_AMD64) {
+  if ((si.wProcessorArchitecture == PROCESSOR_ARCHITECTURE_AMD64) ||
+      (si.wProcessorArchitecture == PROCESSOR_ARCHITECTURE_ARM64)) {
     st->print(" , 64 bit");
   }
 
   st->print(" Build %d", build_number);
   st->print(" (%d.%d.%d.%d)", major_version, minor_version, build_number, build_minor);
@@ -2147,10 +2142,17 @@
   if (thread) {
     thread->set_saved_exception_pc((address)(DWORD_PTR)exceptionInfo->ContextRecord->Rip);
   }
   // Set pc to handler
   exceptionInfo->ContextRecord->Rip = (DWORD64)handler;
+#elif defined(_M_ARM64)
+  // Do not blow up if no thread info available.
+  if (thread) {
+    thread->set_saved_exception_pc((address)(DWORD_PTR)exceptionInfo->ContextRecord->Pc);
+  }
+  // Set pc to handler
+  exceptionInfo->ContextRecord->Pc = (DWORD64)handler;
 #else
   // Do not blow up if no thread info available.
   if (thread) {
     thread->set_saved_exception_pc((address)(DWORD_PTR)exceptionInfo->ContextRecord->Eip);
   }
@@ -2254,10 +2256,20 @@
   // Do not set ctx->Rax as it already contains the correct value (either 32 or 64 bit, depending on the operation)
   // this is the case because the exception only happens for -MinValue/-1 and -MinValue is always in rax because of the
   // idiv opcode (0xF7).
   ctx->Rdx = (DWORD)0;             // remainder
   // Continue the execution
+#elif defined(_M_ARM64)
+  PCONTEXT ctx = exceptionInfo->ContextRecord;
+  address pc = (address)ctx->Sp;
+  assert(pc[0] == 0x83, "not an sdiv opcode"); //Fixme did i get the right opcode?
+  assert(ctx->X4 == min_jint, "unexpected idiv exception");
+  // set correct result values and continue after idiv instruction
+  ctx->Pc = (uint64_t)pc + 4;        // idiv reg, reg, reg  is 4 bytes
+  ctx->X4 = (uint64_t)min_jint;      // result
+  ctx->X5 = (uint64_t)0;             // remainder
+  // Continue the execution
 #else
   PCONTEXT ctx = exceptionInfo->ContextRecord;
   address pc = (address)ctx->Eip;
   assert(pc[0] == 0xF7, "not an idiv opcode");
   assert((pc[1] & ~0x7) == 0xF8, "cannot handle non-register operands");
@@ -2269,10 +2281,11 @@
   // Continue the execution
 #endif
   return EXCEPTION_CONTINUE_EXECUTION;
 }
 
+#if defined(_M_AMD64) || defined(_M_IX86)
 //-----------------------------------------------------------------------------
 LONG WINAPI Handle_FLT_Exception(struct _EXCEPTION_POINTERS* exceptionInfo) {
   PCONTEXT ctx = exceptionInfo->ContextRecord;
 #ifndef  _WIN64
   // handle exception caused by native method modifying control word
@@ -2314,78 +2327,31 @@
   }
 #endif // !_WIN64
 
   return EXCEPTION_CONTINUE_SEARCH;
 }
+#endif
 
-static inline void report_error(Thread* t, DWORD exception_code,
+void report_error(Thread* t, DWORD exception_code,
+                                address addr, void* siginfo, void* context);
+void report_error(Thread* t, DWORD exception_code,
                                 address addr, void* siginfo, void* context) {
   VMError::report_and_die(t, exception_code, addr, siginfo, context);
 
   // If UseOsErrorReporting, this will return here and save the error file
   // somewhere where we can find it in the minidump.
 }
 
-bool os::win32::get_frame_at_stack_banging_point(JavaThread* thread,
-        struct _EXCEPTION_POINTERS* exceptionInfo, address pc, frame* fr) {
-  PEXCEPTION_RECORD exceptionRecord = exceptionInfo->ExceptionRecord;
-  address addr = (address) exceptionRecord->ExceptionInformation[1];
-  if (Interpreter::contains(pc)) {
-    *fr = os::fetch_frame_from_context((void*)exceptionInfo->ContextRecord);
-    if (!fr->is_first_java_frame()) {
-      // get_frame_at_stack_banging_point() is only called when we
-      // have well defined stacks so java_sender() calls do not need
-      // to assert safe_for_sender() first.
-      *fr = fr->java_sender();
-    }
-  } else {
-    // more complex code with compiled code
-    assert(!Interpreter::contains(pc), "Interpreted methods should have been handled above");
-    CodeBlob* cb = CodeCache::find_blob(pc);
-    if (cb == NULL || !cb->is_nmethod() || cb->is_frame_complete_at(pc)) {
-      // Not sure where the pc points to, fallback to default
-      // stack overflow handling
-      return false;
-    } else {
-      *fr = os::fetch_frame_from_context((void*)exceptionInfo->ContextRecord);
-      // in compiled code, the stack banging is performed just after the return pc
-      // has been pushed on the stack
-      *fr = frame(fr->sp() + 1, fr->fp(), (address)*(fr->sp()));
-      if (!fr->is_java_frame()) {
-        // See java_sender() comment above.
-        *fr = fr->java_sender();
-      }
-    }
-  }
-  assert(fr->is_java_frame(), "Safety check");
-  return true;
-}
-
-#if INCLUDE_AOT
-LONG WINAPI topLevelVectoredExceptionFilter(struct _EXCEPTION_POINTERS* exceptionInfo) {
-  PEXCEPTION_RECORD exceptionRecord = exceptionInfo->ExceptionRecord;
-  address addr = (address) exceptionRecord->ExceptionInformation[1];
-  address pc = (address) exceptionInfo->ContextRecord->Rip;
-
-  // Handle the case where we get an implicit exception in AOT generated
-  // code.  AOT DLL's loaded are not registered for structured exceptions.
-  // If the exception occurred in the codeCache or AOT code, pass control
-  // to our normal exception handler.
-  CodeBlob* cb = CodeCache::find_blob(pc);
-  if (cb != NULL) {
-    return topLevelExceptionFilter(exceptionInfo);
-  }
-
-  return EXCEPTION_CONTINUE_SEARCH;
-}
-#endif
-
 //-----------------------------------------------------------------------------
 LONG WINAPI topLevelExceptionFilter(struct _EXCEPTION_POINTERS* exceptionInfo) {
   if (InterceptOSException) return EXCEPTION_CONTINUE_SEARCH;
   DWORD exception_code = exceptionInfo->ExceptionRecord->ExceptionCode;
-#ifdef _M_AMD64
+  PEXCEPTION_RECORD exception_record = exceptionInfo->ExceptionRecord;
+  address addr = (address) exception_record->ExceptionInformation[1];
+#if defined(_M_ARM64)
+  address pc = (address)exceptionInfo->ContextRecord->Pc;
+#elif defined(_M_AMD64)
   address pc = (address) exceptionInfo->ContextRecord->Rip;
 #else
   address pc = (address) exceptionInfo->ContextRecord->Eip;
 #endif
   Thread* t = Thread::current_or_null_safe();
@@ -2399,13 +2365,11 @@
   // Execution protection violation - win32 running on AMD64 only
   // Handled first to avoid misdiagnosis as a "normal" access violation;
   // This is safe to do because we have a new/unique ExceptionInformation
   // code for this condition.
   if (exception_code == EXCEPTION_ACCESS_VIOLATION) {
-    PEXCEPTION_RECORD exceptionRecord = exceptionInfo->ExceptionRecord;
-    int exception_subcode = (int) exceptionRecord->ExceptionInformation[0];
-    address addr = (address) exceptionRecord->ExceptionInformation[1];
+    int exception_subcode = (int) exception_record->ExceptionInformation[0];
 
     if (exception_subcode == EXCEPTION_INFO_EXEC_VIOLATION) {
       int page_size = os::vm_page_size();
 
       // Make sure the pc and the faulting address are sane.
@@ -2472,36 +2436,39 @@
       return EXCEPTION_CONTINUE_SEARCH;
     }
   }
 #endif // _WIN64
 
+#if defined(_M_AMD64) || defined(_M_IX86)
   if ((exception_code == EXCEPTION_ACCESS_VIOLATION) &&
       VM_Version::is_cpuinfo_segv_addr(pc)) {
     // Verify that OS save/restore AVX registers.
     return Handle_Exception(exceptionInfo, VM_Version::cpuinfo_cont_addr());
   }
+#endif
 
   if (t != NULL && t->is_Java_thread()) {
     JavaThread* thread = (JavaThread*) t;
     bool in_java = thread->thread_state() == _thread_in_Java;
+    bool in_native = thread->thread_state() == _thread_in_native;
+    bool in_vm = thread->thread_state() == _thread_in_vm;
+    CodeBlob* cb = in_java ? CodeCache::find_blob_unsafe(pc) : NULL;
 
     // Handle potential stack overflows up front.
     if (exception_code == EXCEPTION_STACK_OVERFLOW) {
       if (thread->stack_guards_enabled()) {
         if (in_java) {
           frame fr;
-          PEXCEPTION_RECORD exceptionRecord = exceptionInfo->ExceptionRecord;
-          address addr = (address) exceptionRecord->ExceptionInformation[1];
           if (os::win32::get_frame_at_stack_banging_point(thread, exceptionInfo, pc, &fr)) {
             assert(fr.is_java_frame(), "Must be a Java frame");
             SharedRuntime::look_for_reserved_stack_annotated_method(thread, fr);
           }
         }
         // Yellow zone violation.  The o/s has unprotected the first yellow
         // zone page for us.  Note:  must call disable_stack_yellow_zone to
         // update the enabled status, even if the zone contains only one page.
-        assert(thread->thread_state() != _thread_in_vm, "Undersized StackShadowPages");
+        assert(!in_vm, "Undersized StackShadowPages");
         thread->disable_stack_yellow_reserved_zone();
         // If not in java code, return and hope for the best.
         return in_java
             ? Handle_Exception(exceptionInfo, SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::STACK_OVERFLOW))
             :  EXCEPTION_CONTINUE_EXECUTION;
@@ -2512,14 +2479,12 @@
         report_error(t, exception_code, pc, exceptionInfo->ExceptionRecord,
                       exceptionInfo->ContextRecord);
         return EXCEPTION_CONTINUE_SEARCH;
       }
     } else if (exception_code == EXCEPTION_ACCESS_VIOLATION) {
-      // Either stack overflow or null pointer exception.
       if (in_java) {
-        PEXCEPTION_RECORD exceptionRecord = exceptionInfo->ExceptionRecord;
-        address addr = (address) exceptionRecord->ExceptionInformation[1];
+        // Either stack overflow or null pointer exception.
         address stack_end = thread->stack_end();
         if (addr < stack_end && addr >= stack_end - os::vm_page_size()) {
           // Stack overflow.
           assert(!os::uses_stack_guard_pages(),
                  "should be caught by red zone code above.");
@@ -2527,101 +2492,104 @@
                                   SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::STACK_OVERFLOW));
         }
         // Check for safepoint polling and implicit null
         // We only expect null pointers in the stubs (vtable)
         // the rest are checked explicitly now.
-        CodeBlob* cb = CodeCache::find_blob(pc);
         if (cb != NULL) {
           if (os::is_poll_address(addr)) {
             address stub = SharedRuntime::get_poll_stub(pc);
             return Handle_Exception(exceptionInfo, stub);
           }
         }
-        {
 #ifdef _WIN64
-          // If it's a legal stack address map the entire region in
-          //
-          PEXCEPTION_RECORD exceptionRecord = exceptionInfo->ExceptionRecord;
-          address addr = (address) exceptionRecord->ExceptionInformation[1];
-          if (addr > thread->stack_reserved_zone_base() && addr < thread->stack_base()) {
-            addr = (address)((uintptr_t)addr &
-                             (~((uintptr_t)os::vm_page_size() - (uintptr_t)1)));
-            os::commit_memory((char *)addr, thread->stack_base() - addr,
-                              !ExecMem);
-            return EXCEPTION_CONTINUE_EXECUTION;
-          } else
+        // If it's a legal stack address map the entire region in
+        //
+        if (addr > thread->stack_reserved_zone_base() && addr < thread->stack_base()) {
+          addr = (address)((uintptr_t)addr & (~((uintptr_t)os::vm_page_size() - (uintptr_t)1)));
+          os::commit_memory((char *)addr, thread->stack_base() - addr, !ExecMem);
+          return EXCEPTION_CONTINUE_EXECUTION;
+        }
 #endif
-          {
-            // Null pointer exception.
-            if (MacroAssembler::uses_implicit_null_check((void*)addr)) {
-              address stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::IMPLICIT_NULL);
-              if (stub != NULL) return Handle_Exception(exceptionInfo, stub);
-            }
-            report_error(t, exception_code, pc, exceptionInfo->ExceptionRecord,
-                         exceptionInfo->ContextRecord);
-            return EXCEPTION_CONTINUE_SEARCH;
-          }
+        // Null pointer exception.
+        if (MacroAssembler::uses_implicit_null_check((void*)addr)) {
+          address stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::IMPLICIT_NULL);
+          if (stub != NULL) return Handle_Exception(exceptionInfo, stub);
+        }
+      }
+
+      // Unsafe memory access
+      CompiledMethod* nm = cb != NULL ? cb->as_compiled_method_or_null() : NULL;
+      bool is_unsafe_arraycopy = (in_native || in_java) && UnsafeCopyMemory::contains_pc(pc);
+      if (is_unsafe_arraycopy ||
+          ((in_native || in_vm) && thread->doing_unsafe_access()) ||
+          (nm != NULL && nm->has_unsafe_access())) {
+        address next_pc =  Assembler::locate_next_instruction(pc);
+        if (is_unsafe_arraycopy) {
+          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);
         }
+        return Handle_Exception(exceptionInfo, SharedRuntime::handle_unsafe_access(thread, next_pc));
       }
 
 #ifdef _WIN64
       // Special care for fast JNI field accessors.
       // jni_fast_Get<Primitive>Field can trap at certain pc's if a GC kicks
       // in and the heap gets shrunk before the field access.
-      if (exception_code == EXCEPTION_ACCESS_VIOLATION) {
-        address addr = JNI_FastGetField::find_slowcase_pc(pc);
-        if (addr != (address)-1) {
-          return Handle_Exception(exceptionInfo, addr);
-        }
+      address slowcase_pc = JNI_FastGetField::find_slowcase_pc(pc);
+      if (slowcase_pc != (address)-1) {
+        return Handle_Exception(exceptionInfo, slowcase_pc);
       }
 #endif
 
       // Stack overflow or null pointer exception in native code.
       report_error(t, exception_code, pc, exceptionInfo->ExceptionRecord,
                    exceptionInfo->ContextRecord);
       return EXCEPTION_CONTINUE_SEARCH;
-    } // /EXCEPTION_ACCESS_VIOLATION
-    // - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
-
-    if (exception_code == EXCEPTION_IN_PAGE_ERROR) {
-      CompiledMethod* nm = NULL;
-      JavaThread* thread = (JavaThread*)t;
-      if (in_java) {
-        CodeBlob* cb = CodeCache::find_blob_unsafe(pc);
-        nm = (cb != NULL) ? cb->as_compiled_method_or_null() : NULL;
-      }
-
-      bool is_unsafe_arraycopy = (thread->thread_state() == _thread_in_native || in_java) && UnsafeCopyMemory::contains_pc(pc);
-      if (((thread->thread_state() == _thread_in_vm ||
-           thread->thread_state() == _thread_in_native ||
-           is_unsafe_arraycopy) &&
-          thread->doing_unsafe_access()) ||
+    } else if (exception_code == EXCEPTION_IN_PAGE_ERROR) {
+      CompiledMethod* nm = cb != NULL ? cb->as_compiled_method_or_null() : NULL;
+      bool is_unsafe_arraycopy = (in_native || in_java) && UnsafeCopyMemory::contains_pc(pc);
+      if (((in_vm || in_native || is_unsafe_arraycopy) && thread->doing_unsafe_access()) ||
           (nm != NULL && nm->has_unsafe_access())) {
         address next_pc =  Assembler::locate_next_instruction(pc);
         if (is_unsafe_arraycopy) {
           next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);
         }
         return Handle_Exception(exceptionInfo, SharedRuntime::handle_unsafe_access(thread, next_pc));
       }
     }
 
+#ifdef _M_ARM64
+    if (in_java &&
+        (exception_code == EXCEPTION_ILLEGAL_INSTRUCTION ||
+          exception_code == EXCEPTION_ILLEGAL_INSTRUCTION_2)) {
+      if (nativeInstruction_at(pc)->is_sigill_zombie_not_entrant()) {
+        if (TraceTraps) {
+          tty->print_cr("trap: zombie_not_entrant");
+        }
+        return Handle_Exception(exceptionInfo, SharedRuntime::get_handle_wrong_method_stub());
+      }
+    }
+#endif
+
     if (in_java) {
       switch (exception_code) {
       case EXCEPTION_INT_DIVIDE_BY_ZERO:
         return Handle_Exception(exceptionInfo, SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::IMPLICIT_DIVIDE_BY_ZERO));
 
       case EXCEPTION_INT_OVERFLOW:
         return Handle_IDiv_Exception(exceptionInfo);
 
       } // switch
     }
+
+#if defined(_M_AMD64) || defined(_M_IX86)
     if (((thread->thread_state() == _thread_in_Java) ||
          (thread->thread_state() == _thread_in_native)) &&
          exception_code != EXCEPTION_UNCAUGHT_CXX_EXCEPTION) {
       LONG result=Handle_FLT_Exception(exceptionInfo);
       if (result==EXCEPTION_CONTINUE_EXECUTION) return result;
     }
+#endif
   }
 
   if (exception_code != EXCEPTION_BREAKPOINT) {
     report_error(t, exception_code, pc, exceptionInfo->ExceptionRecord,
                  exceptionInfo->ContextRecord);
@@ -3469,11 +3437,16 @@
 
 char* os::non_memory_address_word() {
   // Must never look like an address returned by reserve_memory,
   // even in its subfields (as defined by the CPU immediate fields,
   // if the CPU splits constants across multiple instructions).
+#ifdef _M_ARM64
+  // AArch64 has a maximum addressable space of 48-bits
+  return (char*)((1ull << 48) - 1);
+#else
   return (char*)-1;
+#endif
 }
 
 #define MAX_ERROR_COUNT 100
 #define SYS_THREAD_ERROR 0xffffffffUL
 
@@ -3665,10 +3638,38 @@
          "stack size not a multiple of page size");
 
   initialize_performance_counter();
 }
 
+int os::win32::get_cacheline_size() {
+  PSYSTEM_LOGICAL_PROCESSOR_INFORMATION buffer = NULL;
+  DWORD returnLength = 0;
+
+  // See https://docs.microsoft.com/en-us/windows/win32/api/sysinfoapi/nf-sysinfoapi-getlogicalprocessorinformation
+
+  GetLogicalProcessorInformation(NULL, &returnLength);
+  assert(GetLastError() == ERROR_INSUFFICIENT_BUFFER, "Unexpected return from GetLogicalProcessorInformation");
+
+  buffer = (PSYSTEM_LOGICAL_PROCESSOR_INFORMATION)os::malloc(returnLength, mtInternal);
+  BOOL rc = GetLogicalProcessorInformation(buffer, &returnLength);
+  assert(rc, "Unexpected return from GetLogicalProcessorInformation");
+
+  int line_sz = -1;
+  for (PSYSTEM_LOGICAL_PROCESSOR_INFORMATION ptr = buffer; ptr < buffer + returnLength / sizeof(SYSTEM_LOGICAL_PROCESSOR_INFORMATION); ptr++) {
+    switch (ptr->Relationship) {
+    case RelationCache:
+      // Cache data is in ptr->Cache, one CACHE_DESCRIPTOR structure for each cache.
+      PCACHE_DESCRIPTOR Cache = &ptr->Cache;
+      if (Cache->Level == 1) {
+        line_sz = Cache->LineSize;
+      }
+      break;
+    }
+  }
+  os::free(buffer);
+  return line_sz;
+}
 
 HINSTANCE os::win32::load_Windows_dll(const char* name, char *ebuf,
                                       int ebuflen) {
   char path[MAX_PATH];
   DWORD size;
@@ -3992,20 +3993,11 @@
   // This could be set any time but all platforms
   // have to set it the same so we have to mirror Solaris.
   DEBUG_ONLY(os::set_mutex_init_done();)
 
   // Setup Windows Exceptions
-
-#if INCLUDE_AOT
-  // If AOT is enabled we need to install a vectored exception handler
-  // in order to forward implicit exceptions from code in AOT
-  // generated DLLs.  This is necessary since these DLLs are not
-  // registered for structured exceptions like codecache methods are.
-  if (AOTLibrary != NULL && (UseAOT || FLAG_IS_DEFAULT(UseAOT))) {
-    topLevelVectoredExceptionHandler = AddVectoredExceptionHandler( 1, topLevelVectoredExceptionFilter);
-  }
-#endif
+  topLevelVectoredExceptionHandler = AddVectoredExceptionHandler(1, topLevelExceptionFilter);
 
   // for debugging float code generation bugs
   if (ForceFloatExceptions) {
 #ifndef  _WIN64
     static long fp_control_word = 0;
@@ -4096,11 +4088,11 @@
 
   if (UseNUMA && !ForceNUMA) {
     UseNUMA = false; // We don't fully support this yet
   }
 
-  if (UseNUMAInterleaving) {
+  if (UseNUMA) {
     // first check whether this Windows OS supports VirtualAllocExNuma, if not ignore this flag
     bool success = numa_interleaving_init();
     if (!success) UseNUMAInterleaving = false;
   }
 
@@ -5444,11 +5436,11 @@
 }
 
 // WINDOWS CONTEXT Flags for THREAD_SAMPLING
 #if defined(IA32)
   #define sampling_context_flags (CONTEXT_FULL | CONTEXT_FLOATING_POINT | CONTEXT_EXTENDED_REGISTERS)
-#elif defined (AMD64)
+#elif defined(AMD64) || defined(_M_ARM64)
   #define sampling_context_flags (CONTEXT_FULL | CONTEXT_FLOATING_POINT)
 #endif
 
 // returns true if thread could be suspended,
 // false otherwise
diff a/src/hotspot/os/windows/os_windows.hpp b/src/hotspot/os/windows/os_windows.hpp
--- a/src/hotspot/os/windows/os_windows.hpp
+++ b/src/hotspot/os/windows/os_windows.hpp
@@ -68,10 +68,11 @@
     return _processor_level;
   }
   static julong available_memory();
   static julong physical_memory() { return _physical_memory; }
 
+  static int get_cacheline_size();
   // load dll from Windows system directory or Windows directory
   static HINSTANCE load_Windows_dll(const char* name, char *ebuf, int ebuflen);
 
  private:
   enum Ept { EPT_THREAD, EPT_PROCESS, EPT_PROCESS_DIE };
diff a/src/hotspot/os/windows/threadCritical_windows.cpp b/src/hotspot/os/windows/threadCritical_windows.cpp
--- a/src/hotspot/os/windows/threadCritical_windows.cpp
+++ b/src/hotspot/os/windows/threadCritical_windows.cpp
@@ -33,14 +33,14 @@
 
 //
 // See threadCritical.hpp for details of this class.
 //
 
-static bool initialized = false;
-static volatile int lock_count = -1;
+static INIT_ONCE initialized = INIT_ONCE_STATIC_INIT;
+static int lock_count = 0;
 static HANDLE lock_event;
-static DWORD lock_owner = -1;
+static DWORD lock_owner = 0;
 
 //
 // Note that Microsoft's critical region code contains a race
 // condition, and is not suitable for use. A thread holding the
 // critical section cannot safely suspend a thread attempting
@@ -49,50 +49,38 @@
 //
 // I experiemented with the use of ordinary windows mutex objects
 // and found them ~30 times slower than the critical region code.
 //
 
+static BOOL initialize(PINIT_ONCE InitOnce, PVOID Parameter, PVOID *Context) {
+  lock_event = CreateEvent(NULL, false, true, NULL);
+  assert(lock_event != NULL, "unexpected return value from CreateEvent");
+  return true;
+}
+
 ThreadCritical::ThreadCritical() {
-  DWORD current_thread = GetCurrentThreadId();
+  InitOnceExecuteOnce(&initialized, &initialize, NULL, NULL);
 
+  DWORD current_thread = GetCurrentThreadId();
   if (lock_owner != current_thread) {
     // Grab the lock before doing anything.
-    while (Atomic::cmpxchg(&lock_count, -1, 0) != -1) {
-      if (initialized) {
-        DWORD ret = WaitForSingleObject(lock_event,  INFINITE);
-        assert(ret == WAIT_OBJECT_0, "unexpected return value from WaitForSingleObject");
-      }
-    }
-
-    // Make sure the event object is allocated.
-    if (!initialized) {
-      // Locking will not work correctly unless this is autoreset.
-      lock_event = CreateEvent(NULL, false, false, NULL);
-      initialized = true;
-    }
-
-    assert(lock_owner == -1, "Lock acquired illegally.");
+    DWORD ret = WaitForSingleObject(lock_event,  INFINITE);
+    assert(ret == WAIT_OBJECT_0, "unexpected return value from WaitForSingleObject");
     lock_owner = current_thread;
-  } else {
-    // Atomicity isn't required. Bump the recursion count.
-    lock_count++;
   }
-
-  assert(lock_owner == GetCurrentThreadId(), "Lock acquired illegally.");
+  // Atomicity isn't required. Bump the recursion count.
+  lock_count++;
 }
 
 ThreadCritical::~ThreadCritical() {
   assert(lock_owner == GetCurrentThreadId(), "unlock attempt by wrong thread");
   assert(lock_count >= 0, "Attempt to unlock when already unlocked");
 
+  lock_count--;
   if (lock_count == 0) {
     // We're going to unlock
-    lock_owner = -1;
-    lock_count = -1;
+    lock_owner = 0;
     // No lost wakeups, lock_event stays signaled until reset.
     DWORD ret = SetEvent(lock_event);
     assert(ret != 0, "unexpected return value from SetEvent");
-  } else {
-    // Just unwinding a recursive lock;
-    lock_count--;
   }
 }
diff a/src/hotspot/os/windows/thread_windows.cpp b/src/hotspot/os/windows/thread_windows.cpp
--- /dev/null
+++ b/src/hotspot/os/windows/thread_windows.cpp
@@ -0,0 +1,100 @@
+/*
+ * Copyright (c) 2003, 2018, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "precompiled.hpp"
+#include "memory/metaspaceShared.hpp"
+#include "runtime/frame.inline.hpp"
+#include "runtime/thread.inline.hpp"
+
+frame JavaThread::pd_last_frame() {
+  assert(has_last_Java_frame(), "must have last_Java_sp() when suspended");
+  vmassert(_anchor.last_Java_pc() != NULL, "not walkable");
+  return frame(_anchor.last_Java_sp(), _anchor.last_Java_fp(), _anchor.last_Java_pc());
+}
+
+// For Forte Analyzer AsyncGetCallTrace profiling support - thread is
+// currently interrupted by SIGPROF
+bool JavaThread::pd_get_top_frame_for_signal_handler(frame* fr_addr,
+  void* ucontext, bool isInJava) {
+
+  assert(Thread::current() == this, "caller must be current thread");
+  return pd_get_top_frame(fr_addr, ucontext, isInJava);
+}
+
+bool JavaThread::pd_get_top_frame_for_profiling(frame* fr_addr, void* ucontext, bool isInJava) {
+  return pd_get_top_frame(fr_addr, ucontext, isInJava);
+}
+
+bool JavaThread::pd_get_top_frame(frame* fr_addr, void* ucontext, bool isInJava) {
+
+  assert(this->is_Java_thread(), "must be JavaThread");
+
+  JavaThread* jt = (JavaThread *)this;
+
+  // If we have a last_Java_frame, then we should use it even if
+  // isInJava == true.  It should be more reliable than CONTEXT info.
+  if (jt->has_last_Java_frame() && jt->frame_anchor()->walkable()) {
+    *fr_addr = jt->pd_last_frame();
+    return true;
+  }
+
+  // At this point, we don't have a last_Java_frame, so
+  // we try to glean some information out of the CONTEXT
+  // if we were running Java code when SIGPROF came in.
+  if (isInJava) {
+    frame ret_frame = os::fetch_frame_from_context(ucontext);
+    if (ret_frame.pc() == NULL || ret_frame.sp() == NULL ) {
+      // CONTEXT wasn't useful
+      return false;
+    }
+
+    if (MetaspaceShared::is_in_trampoline_frame(ret_frame.pc())) {
+      // In the middle of a trampoline call. Bail out for safety.
+      // This happens rarely so shouldn't affect profiling.
+      return false;
+    }
+
+    if (!ret_frame.safe_for_sender(jt)) {
+#if COMPILER2_OR_JVMCI
+      // C2 and JVMCI use ebp as a general register see if NULL fp helps
+      frame ret_frame2(ret_frame.sp(), NULL, ret_frame.pc());
+      if (!ret_frame2.safe_for_sender(jt)) {
+        // nothing else to try if the frame isn't good
+        return false;
+      }
+      ret_frame = ret_frame2;
+#else
+      // nothing else to try if the frame isn't good
+      return false;
+#endif // COMPILER2_OR_JVMCI
+    }
+    *fr_addr = ret_frame;
+    return true;
+  }
+
+  // nothing else to try
+  return false;
+}
+
+void JavaThread::cache_global_variables() { }
diff a/src/hotspot/os_cpu/aix_ppc/os_aix_ppc.hpp b/src/hotspot/os_cpu/aix_ppc/os_aix_ppc.hpp
--- a/src/hotspot/os_cpu/aix_ppc/os_aix_ppc.hpp
+++ b/src/hotspot/os_cpu/aix_ppc/os_aix_ppc.hpp
@@ -26,14 +26,10 @@
 #ifndef OS_CPU_AIX_PPC_OS_AIX_PPC_HPP
 #define OS_CPU_AIX_PPC_OS_AIX_PPC_HPP
 
   static void setup_fpu() {}
 
-  // Used to register dynamic code cache area with the OS
-  // Note: Currently only used in 64 bit Windows implementations
-  static bool register_code_area(char *low, char *high) { return true; }
-
 #define PLATFORM_PRINT_NATIVE_STACK 1
 static bool platform_print_native_stack(outputStream* st, void* context,
                                         char *buf, int buf_size);
 
 #endif // OS_CPU_AIX_PPC_OS_AIX_PPC_HPP
diff a/src/hotspot/os_cpu/bsd_x86/os_bsd_x86.hpp b/src/hotspot/os_cpu/bsd_x86/os_bsd_x86.hpp
--- a/src/hotspot/os_cpu/bsd_x86/os_bsd_x86.hpp
+++ b/src/hotspot/os_cpu/bsd_x86/os_bsd_x86.hpp
@@ -30,10 +30,6 @@
 
   static jlong rdtsc();
 
   static bool is_allocatable(size_t bytes);
 
-  // Used to register dynamic code cache area with the OS
-  // Note: Currently only used in 64 bit Windows implementations
-  static bool register_code_area(char *low, char *high) { return true; }
-
 #endif // OS_CPU_BSD_X86_OS_BSD_X86_HPP
diff a/src/hotspot/os_cpu/bsd_zero/os_bsd_zero.hpp b/src/hotspot/os_cpu/bsd_zero/os_bsd_zero.hpp
--- a/src/hotspot/os_cpu/bsd_zero/os_bsd_zero.hpp
+++ b/src/hotspot/os_cpu/bsd_zero/os_bsd_zero.hpp
@@ -28,14 +28,10 @@
 
   static void setup_fpu() {}
 
   static bool is_allocatable(size_t bytes);
 
-  // Used to register dynamic code cache area with the OS
-  // Note: Currently only used in 64 bit Windows implementations
-  static bool register_code_area(char *low, char *high) { return true; }
-
   // Atomically copy 64 bits of data
   static void atomic_copy64(const volatile void *src, volatile void *dst) {
 #if defined(PPC32)
     double tmp;
     asm volatile ("lfd  %0, 0(%1)\n"
diff a/src/hotspot/os_cpu/linux_aarch64/os_linux_aarch64.hpp b/src/hotspot/os_cpu/linux_aarch64/os_linux_aarch64.hpp
--- a/src/hotspot/os_cpu/linux_aarch64/os_linux_aarch64.hpp
+++ b/src/hotspot/os_cpu/linux_aarch64/os_linux_aarch64.hpp
@@ -31,14 +31,10 @@
 
   static jlong rdtsc();
 
   static bool is_allocatable(size_t bytes);
 
-  // Used to register dynamic code cache area with the OS
-  // Note: Currently only used in 64 bit Windows implementations
-  static bool register_code_area(char *low, char *high) { return true; }
-
   // Atomically copy 64 bits of data
   static void atomic_copy64(const volatile void *src, volatile void *dst) {
     *(jlong *) dst = *(const jlong *) src;
   }
 
diff a/src/hotspot/os_cpu/linux_arm/os_linux_arm.hpp b/src/hotspot/os_cpu/linux_arm/os_linux_arm.hpp
--- a/src/hotspot/os_cpu/linux_arm/os_linux_arm.hpp
+++ b/src/hotspot/os_cpu/linux_arm/os_linux_arm.hpp
@@ -34,14 +34,10 @@
 
   static void setup_fpu();
 
   static bool is_allocatable(size_t bytes);
 
-  // Used to register dynamic code cache area with the OS
-  // Note: Currently only used in 64 bit Windows implementations
-  static bool register_code_area(char *low, char *high) { return true; }
-
   static int64_t (*atomic_cmpxchg_long_func)(int64_t compare_value,
                                              int64_t exchange_value,
                                              volatile int64_t *dest);
 
   static int64_t (*atomic_load_long_func)(const volatile int64_t*);
diff a/src/hotspot/os_cpu/linux_ppc/os_linux_ppc.hpp b/src/hotspot/os_cpu/linux_ppc/os_linux_ppc.hpp
--- a/src/hotspot/os_cpu/linux_ppc/os_linux_ppc.hpp
+++ b/src/hotspot/os_cpu/linux_ppc/os_linux_ppc.hpp
@@ -26,10 +26,6 @@
 #ifndef OS_CPU_LINUX_PPC_OS_LINUX_PPC_HPP
 #define OS_CPU_LINUX_PPC_OS_LINUX_PPC_HPP
 
   static void setup_fpu() {}
 
-  // Used to register dynamic code cache area with the OS
-  // Note: Currently only used in 64 bit Windows implementations
-  static bool register_code_area(char *low, char *high) { return true; }
-
 #endif // OS_CPU_LINUX_PPC_OS_LINUX_PPC_HPP
diff a/src/hotspot/os_cpu/linux_s390/os_linux_s390.hpp b/src/hotspot/os_cpu/linux_s390/os_linux_s390.hpp
--- a/src/hotspot/os_cpu/linux_s390/os_linux_s390.hpp
+++ b/src/hotspot/os_cpu/linux_s390/os_linux_s390.hpp
@@ -26,9 +26,6 @@
 #ifndef OS_CPU_LINUX_S390_OS_LINUX_S390_HPP
 #define OS_CPU_LINUX_S390_OS_LINUX_S390_HPP
 
   static void setup_fpu() {}
 
-  // Used to register dynamic code cache area with the OS.
-  static bool register_code_area(char *low, char *high) { return true; }
-
 #endif // OS_CPU_LINUX_S390_OS_LINUX_S390_HPP
diff a/src/hotspot/os_cpu/linux_sparc/os_linux_sparc.hpp b/src/hotspot/os_cpu/linux_sparc/os_linux_sparc.hpp
--- a/src/hotspot/os_cpu/linux_sparc/os_linux_sparc.hpp
+++ b/src/hotspot/os_cpu/linux_sparc/os_linux_sparc.hpp
@@ -40,10 +40,6 @@
 
   static void setup_fpu() {}
 
   static bool is_allocatable(size_t bytes);
 
-  // Used to register dynamic code cache area with the OS
-  // Note: Currently only used in 64 bit Windows implementations
-  static bool register_code_area(char *low, char *high) { return true; }
-
 #endif // OS_CPU_LINUX_SPARC_OS_LINUX_SPARC_HPP
diff a/src/hotspot/os_cpu/linux_x86/os_linux_x86.hpp b/src/hotspot/os_cpu/linux_x86/os_linux_x86.hpp
--- a/src/hotspot/os_cpu/linux_x86/os_linux_x86.hpp
+++ b/src/hotspot/os_cpu/linux_x86/os_linux_x86.hpp
@@ -30,14 +30,10 @@
 
   static jlong rdtsc();
 
   static bool is_allocatable(size_t bytes);
 
-  // Used to register dynamic code cache area with the OS
-  // Note: Currently only used in 64 bit Windows implementations
-  static bool register_code_area(char *low, char *high) { return true; }
-
   /*
    * Work-around for broken NX emulation using CS limit, Red Hat patch "Exec-Shield"
    * (IA32 only).
    *
    * Map and execute at a high VA to prevent CS lazy updates race with SMP MM
diff a/src/hotspot/os_cpu/linux_zero/os_linux_zero.hpp b/src/hotspot/os_cpu/linux_zero/os_linux_zero.hpp
--- a/src/hotspot/os_cpu/linux_zero/os_linux_zero.hpp
+++ b/src/hotspot/os_cpu/linux_zero/os_linux_zero.hpp
@@ -28,14 +28,10 @@
 
   static void setup_fpu() {}
 
   static bool is_allocatable(size_t bytes);
 
-  // Used to register dynamic code cache area with the OS
-  // Note: Currently only used in 64 bit Windows implementations
-  static bool register_code_area(char *low, char *high) { return true; }
-
   // Atomically copy 64 bits of data
   static void atomic_copy64(const volatile void *src, volatile void *dst) {
 #if defined(PPC32) && !defined(__SPE__)
     double tmp;
     asm volatile ("lfd  %0, %2\n"
diff a/src/hotspot/os_cpu/solaris_sparc/os_solaris_sparc.hpp b/src/hotspot/os_cpu/solaris_sparc/os_solaris_sparc.hpp
--- a/src/hotspot/os_cpu/solaris_sparc/os_solaris_sparc.hpp
+++ b/src/hotspot/os_cpu/solaris_sparc/os_solaris_sparc.hpp
@@ -40,10 +40,6 @@
 
   static void setup_fpu() {}
 
   static bool is_allocatable(size_t bytes);
 
-  // Used to register dynamic code cache area with the OS
-  // Note: Currently only used in 64 bit Windows implementations
-  static bool register_code_area(char *low, char *high) { return true; }
-
 #endif // OS_CPU_SOLARIS_SPARC_OS_SOLARIS_SPARC_HPP
diff a/src/hotspot/os_cpu/solaris_x86/os_solaris_x86.hpp b/src/hotspot/os_cpu/solaris_x86/os_solaris_x86.hpp
--- a/src/hotspot/os_cpu/solaris_x86/os_solaris_x86.hpp
+++ b/src/hotspot/os_cpu/solaris_x86/os_solaris_x86.hpp
@@ -48,10 +48,6 @@
 
   static jlong rdtsc();
 
   static bool is_allocatable(size_t bytes);
 
-  // Used to register dynamic code cache area with the OS
-  // Note: Currently only used in 64 bit Windows implementations
-  static bool register_code_area(char *low, char *high) { return true; }
-
 #endif // OS_CPU_SOLARIS_X86_OS_SOLARIS_X86_HPP
diff a/src/hotspot/os_cpu/windows_aarch64/assembler_windows_aarch64.cpp b/src/hotspot/os_cpu/windows_aarch64/assembler_windows_aarch64.cpp
--- /dev/null
+++ b/src/hotspot/os_cpu/windows_aarch64/assembler_windows_aarch64.cpp
@@ -0,0 +1,30 @@
+/*
+ * Copyright (c) 1999, 2015, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+// nothing required here
+
+#include "precompiled.hpp"
+
+
diff a/src/hotspot/os_cpu/windows_aarch64/atomic_windows_aarch64.hpp b/src/hotspot/os_cpu/windows_aarch64/atomic_windows_aarch64.hpp
--- /dev/null
+++ b/src/hotspot/os_cpu/windows_aarch64/atomic_windows_aarch64.hpp
@@ -0,0 +1,102 @@
+/*
+ * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_WINDOWS_AARCH64_ATOMIC_WINDOWS_AARCH64_HPP
+#define OS_CPU_WINDOWS_AARCH64_ATOMIC_WINDOWS_AARCH64_HPP
+
+#include <intrin.h>
+#include "runtime/os.hpp"
+#include "runtime/vm_version.hpp"
+
+
+// As per atomic.hpp all read-modify-write operations have to provide two-way
+// barriers semantics. The memory_order parameter is ignored - we always provide
+// the strongest/most-conservative ordering
+//
+// For AARCH64 we add explicit barriers in the stubs.
+
+template<size_t byte_size>
+struct Atomic::PlatformAdd
+  : Atomic::AddAndFetch<Atomic::PlatformAdd<byte_size> >
+{
+  template<typename D, typename I>
+  D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;
+};
+
+#define DEFINE_STUB_ADD(ByteSize, StubType, StubName)                     \
+  template<>                                                              \
+  template<typename D, typename I>                                        \
+  inline D Atomic::PlatformAdd<ByteSize>::add_and_fetch(D volatile* dest, \
+                                                        I add_value,      \
+                                                        atomic_memory_order order) const { \
+    STATIC_ASSERT(ByteSize == sizeof(D));                                 \
+    return PrimitiveConversions::cast<D>(                                 \
+      StubName(reinterpret_cast<StubType volatile *>(dest),               \
+               PrimitiveConversions::cast<StubType>(add_value)));         \
+  }
+
+DEFINE_STUB_ADD(4, long,    InterlockedAdd)
+DEFINE_STUB_ADD(8, __int64, InterlockedAdd64)
+
+#undef DEFINE_STUB_ADD
+
+#define DEFINE_STUB_XCHG(ByteSize, StubType, StubName)                  \
+  template<>                                                            \
+  template<typename T>                                                  \
+  inline T Atomic::PlatformXchg<ByteSize>::operator()(T volatile* dest, \
+                                                      T exchange_value, \
+                                                      atomic_memory_order order) const { \
+    STATIC_ASSERT(ByteSize == sizeof(T));                               \
+    return PrimitiveConversions::cast<T>(                               \
+      StubName(reinterpret_cast<StubType volatile *>(dest),             \
+               PrimitiveConversions::cast<StubType>(exchange_value)));  \
+  }
+
+DEFINE_STUB_XCHG(4, long,    InterlockedExchange)
+DEFINE_STUB_XCHG(8, __int64, InterlockedExchange64)
+
+#undef DEFINE_STUB_XCHG
+
+#define DEFINE_STUB_CMPXCHG(ByteSize, StubType, StubName)                  \
+  template<>                                                               \
+  template<typename T>                                                     \
+  inline T Atomic::PlatformCmpxchg<ByteSize>::operator()(T volatile* dest, \
+                                                         T compare_value,  \
+                                                         T exchange_value, \
+                                                         atomic_memory_order order) const { \
+    STATIC_ASSERT(ByteSize == sizeof(T));                                  \
+    return PrimitiveConversions::cast<T>(                                  \
+      StubName(reinterpret_cast<StubType volatile *>(dest),                \
+               PrimitiveConversions::cast<StubType>(exchange_value),       \
+               PrimitiveConversions::cast<StubType>(compare_value)));      \
+  }
+
+DEFINE_STUB_CMPXCHG(1, char,    _InterlockedCompareExchange8) // Use the intrinsic as InterlockedCompareExchange8 does not exist
+DEFINE_STUB_CMPXCHG(4, long,    InterlockedCompareExchange)
+DEFINE_STUB_CMPXCHG(8, __int64, InterlockedCompareExchange64)
+
+#undef DEFINE_STUB_CMPXCHG
+
+#endif // OS_CPU_WINDOWS_AARCH64_ATOMIC_WINDOWS_AARCH64_HPP
diff a/src/hotspot/os_cpu/windows_aarch64/bytes_windows_aarch64.inline.hpp b/src/hotspot/os_cpu/windows_aarch64/bytes_windows_aarch64.inline.hpp
--- /dev/null
+++ b/src/hotspot/os_cpu/windows_aarch64/bytes_windows_aarch64.inline.hpp
@@ -0,0 +1,47 @@
+/*
+ * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_WINDOWS_AARCH64_BYTES_WINDOWS_AARCH64_INLINE_HPP
+#define OS_CPU_WINDOWS_AARCH64_BYTES_WINDOWS_AARCH64_INLINE_HPP
+
+#include <stdlib.h>
+
+// Efficient swapping of data bytes from Java byte
+// ordering to native byte ordering and vice versa.
+inline u2   Bytes::swap_u2(u2 x) {
+  return _byteswap_ushort(x);
+}
+
+inline u4   Bytes::swap_u4(u4 x) {
+  return _byteswap_ulong(x);
+}
+
+inline u8 Bytes::swap_u8(u8 x) {
+  return _byteswap_uint64(x);
+}
+
+#pragma warning(default: 4035) // Enable warning 4035: no return value
+
+#endif // OS_CPU_WINDOWS_AARCH64_BYTES_WINDOWS_AARCH64_INLINE_HPP
diff a/src/hotspot/os_cpu/windows_aarch64/copy_windows_aarch64.inline.hpp b/src/hotspot/os_cpu/windows_aarch64/copy_windows_aarch64.inline.hpp
--- /dev/null
+++ b/src/hotspot/os_cpu/windows_aarch64/copy_windows_aarch64.inline.hpp
@@ -0,0 +1,165 @@
+/*
+ * Copyright (c) 2003, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_WINDOWS_AARCH64_COPY_WINDOWS_AARCH64_INLINE_HPP
+#define OS_CPU_WINDOWS_AARCH64_COPY_WINDOWS_AARCH64_INLINE_HPP
+
+#include <string.h>
+
+static void pd_conjoint_words(const HeapWord* from, HeapWord* to, size_t count) {
+//  __asm volatile( "prfm pldl1strm, [%[s], #0];" :: [s]"r"(from) : "memory");
+//  if (__builtin_expect(count <= 8, 1)) {
+//    COPY_SMALL(from, to, count);
+//    return;
+  (void)memmove(to, from, count * HeapWordSize);
+  //}
+  //_Copy_conjoint_words(from, to, count);
+}
+
+static void pd_disjoint_words(const HeapWord* from, HeapWord* to, size_t count) {
+   switch (count) {
+  case 8:  to[7] = from[7];
+  case 7:  to[6] = from[6];
+  case 6:  to[5] = from[5];
+  case 5:  to[4] = from[4];
+  case 4:  to[3] = from[3];
+  case 3:  to[2] = from[2];
+  case 2:  to[1] = from[1];
+  case 1:  to[0] = from[0];
+  case 0:  break;
+  default:
+    (void)memcpy(to, from, count * HeapWordSize);
+    break;
+  }
+}  
+
+static void pd_disjoint_words_atomic(const HeapWord* from, HeapWord* to, size_t count) {
+  switch (count) {
+  case 8:  to[7] = from[7];
+  case 7:  to[6] = from[6];
+  case 6:  to[5] = from[5];
+  case 5:  to[4] = from[4];
+  case 4:  to[3] = from[3];
+  case 3:  to[2] = from[2];
+  case 2:  to[1] = from[1];
+  case 1:  to[0] = from[0];
+  case 0:  break;
+  default: while (count-- > 0) {
+             *to++ = *from++;
+           }
+           break;
+  }
+}
+
+static void pd_aligned_conjoint_words(const HeapWord* from, HeapWord* to, size_t count) {
+ // pd_conjoint_words(from, to, count);
+  (void)memmove(to, from, count * HeapWordSize);
+}
+
+static void pd_aligned_disjoint_words(const HeapWord* from, HeapWord* to, size_t count) {
+  pd_disjoint_words(from, to, count);
+}
+
+static void pd_conjoint_bytes(const void* from, void* to, size_t count) {
+  (void)memmove(to, from, count);
+}
+
+static void pd_conjoint_bytes_atomic(const void* from, void* to, size_t count) {
+  pd_conjoint_bytes(from, to, count);
+}
+
+static void pd_conjoint_jshorts_atomic(const jshort* from, jshort* to, size_t count) {
+    if (from > to) {
+    while (count-- > 0) {
+      // Copy forwards
+      *to++ = *from++;
+    }
+  } else {
+    from += count - 1;
+    to   += count - 1;
+    while (count-- > 0) {
+      // Copy backwards
+      *to-- = *from--;
+    }
+  }
+}
+
+static void pd_conjoint_jints_atomic(const jint* from, jint* to, size_t count) {
+    if (from > to) {
+    while (count-- > 0) {
+      // Copy forwards
+      *to++ = *from++;
+    }
+  } else {
+    from += count - 1;
+    to   += count - 1;
+    while (count-- > 0) {
+      // Copy backwards
+      *to-- = *from--;
+    }
+  }
+}
+
+static void pd_conjoint_jlongs_atomic(const jlong* from, jlong* to, size_t count) {
+  pd_conjoint_oops_atomic((const oop*)from, (oop*)to, count);
+}
+
+static void pd_conjoint_oops_atomic(const oop* from, oop* to, size_t count) {
+ if (from > to) {
+    while (count-- > 0) {
+      // Copy forwards
+      *to++ = *from++;
+    }
+  } else {
+    from += count - 1;
+    to   += count - 1;
+    while (count-- > 0) {
+      // Copy backwards
+      *to-- = *from--;
+    }
+  }
+}
+
+static void pd_arrayof_conjoint_bytes(const HeapWord* from, HeapWord* to, size_t count) {
+  pd_conjoint_bytes_atomic(from, to, count);
+}
+
+static void pd_arrayof_conjoint_jshorts(const HeapWord* from, HeapWord* to, size_t count) {
+  pd_conjoint_jshorts_atomic((const jshort*)from, (jshort*)to, count);
+}
+
+static void pd_arrayof_conjoint_jints(const HeapWord* from, HeapWord* to, size_t count) {
+   pd_conjoint_jints_atomic((const jint*)from, (jint*)to, count);
+}
+
+static void pd_arrayof_conjoint_jlongs(const HeapWord* from, HeapWord* to, size_t count) {
+  pd_conjoint_jlongs_atomic((const jlong*)from, (jlong*)to, count);
+}
+
+static void pd_arrayof_conjoint_oops(const HeapWord* from, HeapWord* to, size_t count) {
+ pd_conjoint_oops_atomic((const oop*)from, (oop*)to, count);
+}
+
+#endif // OS_CPU_WINDOWS_AARCH64_COPY_WINDOWS_AARCH64_INLINE_HPP
diff a/src/hotspot/os_cpu/windows_aarch64/globals_windows_aarch64.hpp b/src/hotspot/os_cpu/windows_aarch64/globals_windows_aarch64.hpp
--- /dev/null
+++ b/src/hotspot/os_cpu/windows_aarch64/globals_windows_aarch64.hpp
@@ -0,0 +1,55 @@
+/*
+ * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_WINDOWS_AARCH64_GLOBALS_WINDOWS_AARCH64_HPP
+#define OS_CPU_WINDOWS_AARCH64_GLOBALS_WINDOWS_AARCH64_HPP
+
+// Sets the default values for platform dependent flags used by the runtime system.
+// (see globals.hpp)
+
+define_pd_global(bool, DontYieldALot,            false);
+
+// Default stack size on Windows is determined by the executable (java.exe
+// has a default value of 320K/1MB [32bit/64bit]). Depending on Windows version, changing
+// ThreadStackSize to non-zero may have significant impact on memory usage.
+// See comments in os_windows.cpp.
+define_pd_global(intx, ThreadStackSize,          0); // 0 => use system default
+define_pd_global(intx, VMThreadStackSize,        0);
+
+#ifdef ASSERT
+define_pd_global(intx, CompilerThreadStackSize,  1024);
+#else
+define_pd_global(intx, CompilerThreadStackSize,  0);
+#endif
+
+define_pd_global(uintx,JVMInvokeMethodSlack,     8192);
+
+// Used on 64 bit platforms for UseCompressedOops base address
+define_pd_global(uintx,HeapBaseMinAddress,       2*G);
+/*
+class Thread;
+extern __thread Thread *aarch64_currentThread;
+*/
+#endif // OS_CPU_WINDOWS_AARCH64_GLOBALS_WINDOWS_AARCH64_HPP
diff a/src/hotspot/os_cpu/windows_aarch64/orderAccess_windows_aarch64.hpp b/src/hotspot/os_cpu/windows_aarch64/orderAccess_windows_aarch64.hpp
--- /dev/null
+++ b/src/hotspot/os_cpu/windows_aarch64/orderAccess_windows_aarch64.hpp
@@ -0,0 +1,59 @@
+/*
+ * Copyright (c) 2003, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_WINDOWS_AARCH64_ORDERACCESS_WINDOWS_AARCH64_HPP
+#define OS_CPU_WINDOWS_AARCH64_ORDERACCESS_WINDOWS_AARCH64_HPP
+
+// Included in orderAccess.hpp header file.
+
+#include <intrin.h>
+#include "vm_version_aarch64.hpp"
+#include "runtime/vm_version.hpp"
+
+// Implementation of class OrderAccess.
+
+inline void OrderAccess::loadload()   { acquire(); }
+inline void OrderAccess::storestore() { release(); }
+inline void OrderAccess::loadstore()  { acquire(); }
+inline void OrderAccess::storeload()  { fence(); }
+
+inline void OrderAccess::acquire() {
+  _ReadBarrier();
+  __dmb(_ARM64_BARRIER_ISHLD);
+}
+
+inline void OrderAccess::release() {
+  _WriteBarrier();
+  __dmb(_ARM64_BARRIER_ISHST);
+}
+
+inline void OrderAccess::fence() {
+  _ReadWriteBarrier();
+  __dmb(_ARM64_BARRIER_ISH);
+}
+
+inline void OrderAccess::cross_modify_fence() { }
+
+#endif // OS_CPU_WINDOWS_AARCH64_ORDERACCESS_WINDOWS_AARCH64_HPP
diff a/src/hotspot/os_cpu/windows_aarch64/os_windows_aarch64.cpp b/src/hotspot/os_cpu/windows_aarch64/os_windows_aarch64.cpp
--- /dev/null
+++ b/src/hotspot/os_cpu/windows_aarch64/os_windows_aarch64.cpp
@@ -0,0 +1,310 @@
+/*
+ * Copyright (c) 1999, 2018, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+// no precompiled headers
+#include "precompiled.hpp"
+#include "jvm.h"
+#include "asm/macroAssembler.hpp"
+#include "classfile/classLoader.hpp"
+#include "classfile/systemDictionary.hpp"
+#include "classfile/vmSymbols.hpp"
+#include "code/codeCache.hpp"
+#include "code/icBuffer.hpp"
+#include "code/vtableStubs.hpp"
+#include "code/nativeInst.hpp"
+#include "interpreter/interpreter.hpp"
+#include "memory/allocation.inline.hpp"
+#include "prims/jniFastGetField.hpp"
+#include "prims/jvm_misc.hpp"
+#include "runtime/arguments.hpp"
+#include "runtime/extendedPC.hpp"
+#include "runtime/frame.inline.hpp"
+#include "runtime/interfaceSupport.inline.hpp"
+#include "runtime/java.hpp"
+#include "runtime/javaCalls.hpp"
+#include "runtime/mutexLocker.hpp"
+#include "runtime/osThread.hpp"
+#include "runtime/sharedRuntime.hpp"
+#include "runtime/stubRoutines.hpp"
+#include "runtime/thread.inline.hpp"
+#include "runtime/timer.hpp"
+#include "unwind_windows_aarch64.hpp"
+#include "utilities/debug.hpp"
+#include "utilities/events.hpp"
+#include "utilities/vmError.hpp"
+
+
+// put OS-includes here
+# include <sys/types.h>
+# include <signal.h>
+# include <errno.h>
+# include <stdlib.h>
+# include <stdio.h>
+# include <intrin.h>
+
+void os::os_exception_wrapper(java_call_t f, JavaValue* value, const methodHandle& method, JavaCallArguments* args, Thread* thread) {
+  f(value, method, args, thread);
+}
+
+PRAGMA_DISABLE_MSVC_WARNING(4172)
+// Returns an estimate of the current stack pointer. Result must be guaranteed
+// to point into the calling threads stack, and be no lower than the current
+// stack pointer.
+address os::current_stack_pointer() {
+  int dummy;
+  address sp = (address)&dummy;
+  return sp;
+}
+
+ExtendedPC os::fetch_frame_from_context(const void* ucVoid,
+                    intptr_t** ret_sp, intptr_t** ret_fp) {
+  ExtendedPC  epc;
+  CONTEXT* uc = (CONTEXT*)ucVoid;
+
+  if (uc != NULL) {
+    epc = ExtendedPC((address)uc->Pc);
+    if (ret_sp) *ret_sp = (intptr_t*)uc->Sp;
+    if (ret_fp) *ret_fp = (intptr_t*)uc->Fp;
+  } else {
+    // construct empty ExtendedPC for return value checking
+    epc = ExtendedPC(NULL);
+    if (ret_sp) *ret_sp = (intptr_t *)NULL;
+    if (ret_fp) *ret_fp = (intptr_t *)NULL;
+  }
+  return epc;
+}
+
+frame os::fetch_frame_from_context(const void* ucVoid) {
+  intptr_t* sp;
+  intptr_t* fp;
+  ExtendedPC epc = fetch_frame_from_context(ucVoid, &sp, &fp);
+  return frame(sp, fp, epc.pc());
+}
+
+bool os::win32::get_frame_at_stack_banging_point(JavaThread* thread,
+        struct _EXCEPTION_POINTERS* exceptionInfo, address pc, frame* fr) {
+  PEXCEPTION_RECORD exceptionRecord = exceptionInfo->ExceptionRecord;
+  address addr = (address) exceptionRecord->ExceptionInformation[1];
+  if (Interpreter::contains(pc)) {
+    // interpreter performs stack banging after the fixed frame header has
+    // been generated while the compilers perform it before. To maintain
+    // semantic consistency between interpreted and compiled frames, the
+    // method returns the Java sender of the current frame.
+    *fr = os::fetch_frame_from_context((void*)exceptionInfo->ContextRecord);
+    if (!fr->is_first_java_frame()) {
+      assert(fr->safe_for_sender(thread), "Safety check");
+      *fr = fr->java_sender();
+    }
+  } else {
+    // more complex code with compiled code
+    assert(!Interpreter::contains(pc), "Interpreted methods should have been handled above");
+    CodeBlob* cb = CodeCache::find_blob(pc);
+    if (cb == NULL || !cb->is_nmethod() || cb->is_frame_complete_at(pc)) {
+      // Not sure where the pc points to, fallback to default
+      // stack overflow handling
+      return false;
+    } else {
+      // In compiled code, the stack banging is performed before LR
+      // has been saved in the frame.  LR is live, and SP and FP
+      // belong to the caller.
+      intptr_t* fp = (intptr_t*)exceptionInfo->ContextRecord->Fp;
+      intptr_t* sp = (intptr_t*)exceptionInfo->ContextRecord->Sp;
+      address pc = (address)(exceptionInfo->ContextRecord->Lr
+                         - NativeInstruction::instruction_size);
+      *fr = frame(sp, fp, pc);
+      if (!fr->is_java_frame()) {
+        assert(fr->safe_for_sender(thread), "Safety check");
+        assert(!fr->is_first_frame(), "Safety check");
+        *fr = fr->java_sender();
+      }
+    }
+  }
+  assert(fr->is_java_frame(), "Safety check");
+  return true;
+}
+
+// By default, gcc always saves frame pointer rfp on this stack. This
+// may get turned off by -fomit-frame-pointer.
+frame os::get_sender_for_C_frame(frame* fr) {
+  return frame(fr->link(), fr->link(), fr->sender_pc());
+}
+
+frame os::current_frame() {
+  typedef intptr_t*      get_fp_func           ();
+  get_fp_func* func = CAST_TO_FN_PTR(get_fp_func*,
+                                     StubRoutines::aarch64::get_previous_fp_entry());
+  if (func == NULL) return frame();
+  intptr_t* fp = (*func)();
+  if (fp == NULL) {
+    return frame();
+  }
+
+  frame myframe((intptr_t*)os::current_stack_pointer(),
+                (intptr_t*)fp,
+                CAST_FROM_FN_PTR(address, os::current_frame));
+  if (os::is_first_C_frame(&myframe)) {
+
+    // stack is not walkable
+    return frame();
+  } else {
+    return os::get_sender_for_C_frame(&myframe);
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// thread stack
+
+// Minimum usable stack sizes required to get to user code. Space for
+// HotSpot guard pages is added later.
+
+/////////////////////////////////////////////////////////////////////////////
+// helper functions for fatal error handler
+
+void os::print_context(outputStream *st, const void *context) {
+  if (context == NULL) return;
+
+  const CONTEXT* uc = (const CONTEXT*)context;
+
+  st->print_cr("Registers:");
+
+  st->print(  "X0=" INTPTR_FORMAT, uc->X0);
+  st->print(", X1=" INTPTR_FORMAT, uc->X1);
+  st->print(", X2=" INTPTR_FORMAT, uc->X2);
+  st->print(", X3=" INTPTR_FORMAT, uc->X3);
+  st->cr();
+  st->print(  "X4=" INTPTR_FORMAT, uc->X4);
+  st->print(", X5=" INTPTR_FORMAT, uc->X5);
+  st->print(", X6=" INTPTR_FORMAT, uc->X6);
+  st->print(", X7=" INTPTR_FORMAT, uc->X7);
+  st->cr();
+  st->print(  "X8 =" INTPTR_FORMAT, uc->X8);
+  st->print(", X9 =" INTPTR_FORMAT, uc->X9);
+  st->print(", X10=" INTPTR_FORMAT, uc->X10);
+  st->print(", X11=" INTPTR_FORMAT, uc->X11);
+  st->cr();
+  st->print(  "X12=" INTPTR_FORMAT, uc->X12);
+  st->print(", X13=" INTPTR_FORMAT, uc->X13);
+  st->print(", X14=" INTPTR_FORMAT, uc->X14);
+  st->print(", X15=" INTPTR_FORMAT, uc->X15);
+  st->cr();
+  st->print(  "X16=" INTPTR_FORMAT, uc->X16);
+  st->print(", X17=" INTPTR_FORMAT, uc->X17);
+  st->print(", X18=" INTPTR_FORMAT, uc->X18);
+  st->print(", X19=" INTPTR_FORMAT, uc->X19);
+  st->cr();
+  st->print(", X20=" INTPTR_FORMAT, uc->X20);
+  st->print(", X21=" INTPTR_FORMAT, uc->X21);
+  st->print(", X22=" INTPTR_FORMAT, uc->X22);
+  st->print(", X23=" INTPTR_FORMAT, uc->X23);
+  st->cr();
+  st->print(", X24=" INTPTR_FORMAT, uc->X24);
+  st->print(", X25=" INTPTR_FORMAT, uc->X25);
+  st->print(", X26=" INTPTR_FORMAT, uc->X26);
+  st->print(", X27=" INTPTR_FORMAT, uc->X27);
+  st->print(", X28=" INTPTR_FORMAT, uc->X28);
+  st->cr();
+  st->cr();
+
+  intptr_t *sp = (intptr_t *)uc->Sp;
+  st->print_cr("Top of Stack: (sp=" PTR_FORMAT ")", sp);
+  print_hex_dump(st, (address)sp, (address)(sp + 32), sizeof(intptr_t));
+  st->cr();
+
+  // Note: it may be unsafe to inspect memory near pc. For example, pc may
+  // point to garbage if entry point in an nmethod is corrupted. Leave
+  // this at the end, and hope for the best.
+  address pc = (address)uc->Pc;
+  st->print_cr("Instructions: (pc=" PTR_FORMAT ")", pc);
+  print_hex_dump(st, pc - 32, pc + 32, sizeof(char));
+  st->cr();
+
+}
+
+void os::print_register_info(outputStream *st, const void *context) {
+ if (context == NULL) return;
+
+  const CONTEXT* uc = (const CONTEXT*)context;
+
+  st->print_cr("Register to memory mapping:");
+  st->cr();
+  // this is only for the "general purpose" registers
+  st->print("X0="); print_location(st, uc->X0);
+  st->print("X1="); print_location(st, uc->X1);
+  st->print("X2="); print_location(st, uc->X2);
+  st->print("X3="); print_location(st, uc->X3);
+  st->cr();
+  st->print("X4="); print_location(st, uc->X4);
+  st->print("X5="); print_location(st, uc->X5);
+  st->print("X6="); print_location(st, uc->X6);
+  st->print("X7="); print_location(st, uc->X7);
+  st->cr();
+  st->print("X8="); print_location(st, uc->X8);
+  st->print("X9="); print_location(st, uc->X9);
+  st->print("X10="); print_location(st, uc->X10);
+  st->print("X11="); print_location(st, uc->X11);
+  st->cr();
+  st->print("X12="); print_location(st, uc->X12);
+  st->print("X13="); print_location(st, uc->X13);
+  st->print("X14="); print_location(st, uc->X14);
+  st->print("X15="); print_location(st, uc->X15);
+  st->cr();
+  st->print("X16="); print_location(st, uc->X16);
+  st->print("X17="); print_location(st, uc->X17);
+  st->print("X18="); print_location(st, uc->X18);
+  st->print("X19="); print_location(st, uc->X19);
+  st->cr();
+  st->print("X20="); print_location(st, uc->X20);
+  st->print("X21="); print_location(st, uc->X21);
+  st->print("X22="); print_location(st, uc->X22);
+  st->print("X23="); print_location(st, uc->X23);
+  st->cr();
+  st->print("X24="); print_location(st, uc->X24);
+  st->print("X25="); print_location(st, uc->X25);
+  st->print("X26="); print_location(st, uc->X26);
+  st->print("X27="); print_location(st, uc->X27);
+  st->print("X28="); print_location(st, uc->X28);
+
+  st->cr();
+}
+
+void os::setup_fpu() {
+}
+
+#ifndef PRODUCT
+void os::verify_stack_alignment() {
+  assert(((intptr_t)os::current_stack_pointer() & (StackAlignmentInBytes-1)) == 0, "incorrect stack alignment");
+}
+#endif
+
+int os::extra_bang_size_in_bytes() {
+  // AArch64 does not require the additional stack bang.
+  return 0;
+}
+
+extern "C" {
+  int SpinPause() {
+    return 0;
+  }
+};
diff a/src/hotspot/os_cpu/windows_aarch64/os_windows_aarch64.hpp b/src/hotspot/os_cpu/windows_aarch64/os_windows_aarch64.hpp
--- /dev/null
+++ b/src/hotspot/os_cpu/windows_aarch64/os_windows_aarch64.hpp
@@ -0,0 +1,62 @@
+/*
+ * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_WINDOWS_AARCH64_OS_WINDOWS_AARCH64_HPP
+#define OS_CPU_WINDOWS_AARCH64_OS_WINDOWS_AARCH64_HPP
+
+  static void setup_fpu();
+  static bool supports_sse();
+
+  //HighRes Timer on Windows: https://docs.microsoft.com/en-us/windows/win32/sysinfo/acquiring-high-resolution-time-stamps
+  //static jlong rdtsc();
+
+  static bool is_allocatable(size_t bytes);
+
+  // Atomically copy 64 bits of data
+  static void atomic_copy64(const volatile void *src, volatile void *dst) {
+    *(jlong *) dst = *(const jlong *) src;
+  }
+
+  static int32_t(*atomic_xchg_func)          (int, volatile int*);
+  static int64_t(*atomic_xchg_long_func)     (int64_t, volatile int64_t*);
+
+  static int8_t(*atomic_cmpxchg_byte_func)  (int8_t, volatile int8_t*, int8_t);
+  static int32_t(*atomic_cmpxchg_func)       (int32_t, volatile int32_t*, int32_t);
+  static int64_t(*atomic_cmpxchg_long_func)  (int64_t, volatile int64_t*, int64_t);
+
+  static int32_t(*atomic_add_func)           (int32_t, volatile int32_t*);
+  static int64_t(*atomic_add_long_func)      (int64_t, volatile int64_t*);
+
+  static int32_t   atomic_xchg_bootstrap(int32_t, volatile int32_t*);
+  static int64_t   atomic_xchg_long_bootstrap(int64_t, volatile int64_t*);
+
+  static int32_t   atomic_cmpxchg_bootstrap(int32_t, volatile int32_t*, int32_t);
+  static int8_t    atomic_cmpxchg_byte_bootstrap(int8_t, volatile int8_t*, int8_t);
+
+  static int32_t  atomic_add_bootstrap(int32_t, volatile int32_t*);
+  static int64_t  atomic_add_long_bootstrap(int64_t, volatile int64_t*);
+  static int64_t atomic_cmpxchg_long_bootstrap(int64_t, volatile int64_t*, int64_t);
+
+#endif // OS_CPU_WINDOWS_AARCH64_OS_WINDOWS_AARCH64_HPP
diff a/src/hotspot/os_cpu/windows_aarch64/os_windows_aarch64.inline.hpp b/src/hotspot/os_cpu/windows_aarch64/os_windows_aarch64.inline.hpp
--- /dev/null
+++ b/src/hotspot/os_cpu/windows_aarch64/os_windows_aarch64.inline.hpp
@@ -0,0 +1,31 @@
+/*
+ * Copyright (c) 2011, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_WINDOWS_AARCH64_OS_WINDOWS_AARCH64_INLINE_HPP
+#define OS_CPU_WINDOWS_AARCH64_OS_WINDOWS_AARCH64_INLINE_HPP
+
+#include "runtime/os.hpp"
+
+#endif // OS_CPU_WINDOWS_AARCH64_OS_WINDOWS_AARCH64_INLINE_HPP
diff a/src/hotspot/os_cpu/windows_aarch64/prefetch_windows_aarch64.inline.hpp b/src/hotspot/os_cpu/windows_aarch64/prefetch_windows_aarch64.inline.hpp
--- /dev/null
+++ b/src/hotspot/os_cpu/windows_aarch64/prefetch_windows_aarch64.inline.hpp
@@ -0,0 +1,38 @@
+/*
+ * Copyright (c) 2003, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_WINDOWS_AARCH64_PREFETCH_WINDOWS_AARCH64_INLINE_HPP
+#define OS_CPU_WINDOWS_AARCH64_PREFETCH_WINDOWS_AARCH64_INLINE_HPP
+
+#include "runtime/prefetch.hpp"
+
+
+inline void Prefetch::read (void *loc, intx interval) {
+}
+
+inline void Prefetch::write(void *loc, intx interval) {
+}
+
+#endif // OS_CPU_WINDOWS_AARCH64_PREFETCH_WINDOWS_AARCH64_INLINE_HPP
diff a/src/hotspot/os_cpu/windows_aarch64/thread_windows_aarch64.hpp b/src/hotspot/os_cpu/windows_aarch64/thread_windows_aarch64.hpp
--- /dev/null
+++ b/src/hotspot/os_cpu/windows_aarch64/thread_windows_aarch64.hpp
@@ -0,0 +1,83 @@
+/*
+ * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_WINDOWS_AARCH64_THREAD_WINDOWS_AARCH64_HPP
+#define OS_CPU_WINDOWS_AARCH64_THREAD_WINDOWS_AARCH64_HPP
+
+ private:
+
+#ifdef ASSERT
+  // spill stack holds N callee-save registers at each Java call and
+  // grows downwards towards limit
+  // we need limit to check we have space for a spill and base so we
+  // can identify all live spill frames at GC (eventually)
+  address          _spill_stack;
+  address          _spill_stack_base;
+  address          _spill_stack_limit;
+#endif // ASSERT
+
+  void pd_initialize() {
+    _anchor.clear();
+  }
+
+  frame pd_last_frame();
+
+ public:
+  // Mutators are highly dangerous....
+  intptr_t* last_Java_fp()                       { return _anchor.last_Java_fp(); }
+  void  set_last_Java_fp(intptr_t* fp)           { _anchor.set_last_Java_fp(fp);   }
+
+  void set_base_of_stack_pointer(intptr_t* base_sp) {
+  }
+
+  static ByteSize last_Java_fp_offset()          {
+    return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_fp_offset();
+  }
+
+  intptr_t* base_of_stack_pointer() {
+    return NULL;
+  }
+  void record_base_of_stack_pointer() {
+  }
+
+  bool pd_get_top_frame_for_signal_handler(frame* fr_addr, void* ucontext,
+    bool isInJava);
+
+  bool pd_get_top_frame_for_profiling(frame* fr_addr, void* ucontext, bool isInJava);
+private:
+  bool pd_get_top_frame(frame* fr_addr, void* ucontext, bool isInJava);
+public:
+
+  static Thread *aarch64_get_thread_helper() {
+     return Thread::current();
+  }
+
+  // These routines are only used on cpu architectures that
+  // have separate register stacks (Itanium).
+  static bool register_stack_overflow() { return false; }
+  static void enable_register_stack_guard() {}
+  static void disable_register_stack_guard() {}
+
+#endif // OS_CPU_WINDOWS_AARCH64_THREAD_WINDOWS_AARCH64_HPP
diff a/src/hotspot/os_cpu/windows_aarch64/unwind_windows_aarch64.hpp b/src/hotspot/os_cpu/windows_aarch64/unwind_windows_aarch64.hpp
--- /dev/null
+++ b/src/hotspot/os_cpu/windows_aarch64/unwind_windows_aarch64.hpp
@@ -0,0 +1,102 @@
+/*
+ * Copyright (c) 2004, 2019, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_WINDOWS_AARCH64_UNWIND_WINDOWS_AARCH64_HPP
+#define OS_CPU_WINDOWS_AARCH64_UNWIND_WINDOWS_AARCH64_HPP
+
+
+typedef unsigned char UBYTE;
+
+#if _MSC_VER < 1700
+
+/* Not needed for VS2012 compiler, comes from winnt.h. */
+#define UNW_FLAG_EHANDLER  0x01
+#define UNW_FLAG_UHANDLER  0x02
+#define UNW_FLAG_CHAININFO 0x04
+
+#endif
+
+// See https://docs.microsoft.com/en-us/cpp/build/arm64-exception-handling#xdata-records
+typedef struct _UNWIND_INFO_EH_ONLY {
+    DWORD FunctionLength : 18;
+    DWORD Version        : 2;
+    DWORD X              : 1; // = 1
+    DWORD E              : 1; // = 1
+    DWORD EpilogCount    : 5; // = 0
+    DWORD CodeWords      : 5; // = 1
+    DWORD UnwindCode0    : 8;
+    DWORD UnwindCode1    : 8;
+    DWORD UnwindCode2    : 8;
+    DWORD UnwindCode3    : 8;
+    DWORD ExceptionHandler;
+} UNWIND_INFO_EH_ONLY, *PUNWIND_INFO_EH_ONLY;
+
+/*
+typedef struct _RUNTIME_FUNCTION {
+    DWORD BeginAddress;
+    union {
+        DWORD UnwindData;
+        struct {
+            DWORD Flag : 2;
+            DWORD FunctionLength : 11;
+            DWORD RegF : 3;
+            DWORD RegI : 4;
+            DWORD H : 1;
+            DWORD CR : 2;
+            DWORD FrameSize : 9;
+        } DUMMYSTRUCTNAME;
+    } DUMMYUNIONNAME;
+} RUNTIME_FUNCTION, *PRUNTIME_FUNCTION;
+*/
+
+#if _MSC_VER < 1700
+
+/* Not needed for VS2012 compiler, comes from winnt.h. */
+typedef struct _DISPATCHER_CONTEXT {
+    ULONG64 ControlPc;
+    ULONG64 ImageBase;
+    PRUNTIME_FUNCTION FunctionEntry;
+    ULONG64 EstablisherFrame;
+    ULONG64 TargetIp;
+    PCONTEXT ContextRecord;
+//    PEXCEPTION_ROUTINE LanguageHandler;
+    char * LanguageHandler; // double dependency problem
+    PVOID HandlerData;
+} DISPATCHER_CONTEXT, *PDISPATCHER_CONTEXT;
+
+#endif
+
+#if _MSC_VER < 1500
+
+/* Not needed for VS2008 compiler, comes from winnt.h. */
+typedef EXCEPTION_DISPOSITION (*PEXCEPTION_ROUTINE) (
+    IN PEXCEPTION_RECORD ExceptionRecord,
+    IN ULONG64 EstablisherFrame,
+    IN OUT PCONTEXT ContextRecord,
+    IN OUT PDISPATCHER_CONTEXT DispatcherContext
+);
+
+#endif
+
+#endif // OS_CPU_WINDOWS_AARCH64_UNWIND_WINDOWS_AARCH64_HPP
diff a/src/hotspot/os_cpu/windows_aarch64/vmStructs_windows_aarch64.hpp b/src/hotspot/os_cpu/windows_aarch64/vmStructs_windows_aarch64.hpp
--- /dev/null
+++ b/src/hotspot/os_cpu/windows_aarch64/vmStructs_windows_aarch64.hpp
@@ -0,0 +1,50 @@
+/*
+ * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef OS_CPU_WINDOWS_AARCH64_VMSTRUCTS_WINDOWS_AARCH64_HPP
+#define OS_CPU_WINDOWS_AARCH64_VMSTRUCTS_WINDOWS_AARCH64_HPP
+
+// These are the OS and CPU-specific fields, types and integer
+// constants required by the Serviceability Agent. This file is
+// referenced by vmStructs.cpp.
+
+#define VM_STRUCTS_OS_CPU(nonstatic_field, static_field, unchecked_nonstatic_field, volatile_nonstatic_field, nonproduct_nonstatic_field, c2_nonstatic_field, unchecked_c1_static_field, unchecked_c2_static_field) \
+                                                                                                                                     \
+  /******************************/                                                                                                   \
+  /* Threads (NOTE: incomplete) */                                                                                                   \
+  /******************************/                                                                                                   \
+                                                                                                                                     \
+  nonstatic_field(OSThread,                    _thread_id,                                    OSThread::thread_id_t)                 \
+  unchecked_nonstatic_field(OSThread,          _thread_handle,                                sizeof(HANDLE)) /* NOTE: no type */
+
+#define VM_TYPES_OS_CPU(declare_type, declare_toplevel_type, declare_oop_type, declare_integer_type, declare_unsigned_integer_type, declare_c1_toplevel_type, declare_c2_type, declare_c2_toplevel_type) \
+                                                                          \
+  declare_unsigned_integer_type(OSThread::thread_id_t)
+
+#define VM_INT_CONSTANTS_OS_CPU(declare_constant, declare_preprocessor_constant, declare_c1_constant, declare_c2_constant, declare_c2_preprocessor_constant)
+
+#define VM_LONG_CONSTANTS_OS_CPU(declare_constant, declare_preprocessor_constant, declare_c1_constant, declare_c2_constant, declare_c2_preprocessor_constant)
+
+#endif // OS_CPU_WINDOWS_AARCH64_VMSTRUCTS_WINDOWS_AARCH64_HPP
diff a/src/hotspot/os_cpu/linux_aarch64/vm_version_linux_aarch64.cpp b/src/hotspot/os_cpu/windows_aarch64/vm_version_windows_aarch64.cpp
--- a/src/hotspot/os_cpu/linux_aarch64/vm_version_linux_aarch64.cpp
+++ b/src/hotspot/os_cpu/windows_aarch64/vm_version_windows_aarch64.cpp
diff a/src/hotspot/os_cpu/windows_x86/atomic_windows_x86.hpp b/src/hotspot/os_cpu/windows_x86/atomic_windows_x86.hpp
--- a/src/hotspot/os_cpu/windows_x86/atomic_windows_x86.hpp
+++ b/src/hotspot/os_cpu/windows_x86/atomic_windows_x86.hpp
@@ -23,10 +23,11 @@
  */
 
 #ifndef OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
 #define OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
 
+#include <intrin.h>
 #include "runtime/os.hpp"
 
 // Note that in MSVC, volatile memory accesses are explicitly
 // guaranteed to have acquire release semantics (w.r.t. compiler
 // reordering) and therefore does not even need a compiler barrier
@@ -49,47 +50,51 @@
 //
 // Performance note: On uniprocessors, the 'lock' prefixes are not
 // necessary (and expensive). We should generate separate cases if
 // this becomes a performance problem.
 
-#pragma warning(disable: 4035) // Disables warnings reporting missing return statement
 
 template<size_t byte_size>
 struct Atomic::PlatformAdd
   : Atomic::AddAndFetch<Atomic::PlatformAdd<byte_size> >
 {
   template<typename D, typename I>
   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;
 };
 
-#ifdef AMD64
-template<>
-template<typename D, typename I>
-inline D Atomic::PlatformAdd<4>::add_and_fetch(D volatile* dest, I add_value,
-                                               atomic_memory_order order) const {
-  return add_using_helper<int32_t>(os::atomic_add_func, dest, add_value);
-}
 
-template<>
-template<typename D, typename I>
-inline D Atomic::PlatformAdd<8>::add_and_fetch(D volatile* dest, I add_value,
-                                               atomic_memory_order order) const {
-  return add_using_helper<int64_t>(os::atomic_add_long_func, dest, add_value);
-}
+#define DEFINE_STUB_ADD(ByteSize, StubType, StubName)                     \
+  template<>                                                              \
+  template<typename D, typename I>                                        \
+  inline D Atomic::PlatformAdd<ByteSize>::add_and_fetch(D volatile* dest, \
+                                                        I add_value,      \
+                                                        atomic_memory_order order) const { \
+    STATIC_ASSERT(ByteSize == sizeof(D));                                 \
+    return PrimitiveConversions::cast<D>(                                 \
+      StubName(reinterpret_cast<StubType volatile *>(dest),               \
+               PrimitiveConversions::cast<StubType>(add_value)));         \
+  }
+
+DEFINE_STUB_ADD(4, long,    InterlockedAdd)
+DEFINE_STUB_ADD(8, __int64, InterlockedAdd64)
+
+#undef DEFINE_STUB_ADD
 
 #define DEFINE_STUB_XCHG(ByteSize, StubType, StubName)                  \
   template<>                                                            \
   template<typename T>                                                  \
   inline T Atomic::PlatformXchg<ByteSize>::operator()(T volatile* dest, \
                                                       T exchange_value, \
                                                       atomic_memory_order order) const { \
     STATIC_ASSERT(ByteSize == sizeof(T));                               \
-    return xchg_using_helper<StubType>(StubName, dest, exchange_value); \
+    return PrimitiveConversions::cast<T>(                               \
+      StubName(reinterpret_cast<StubType volatile *>(dest),             \
+               PrimitiveConversions::cast<StubType>(exchange_value)));  \
   }
 
-DEFINE_STUB_XCHG(4, int32_t, os::atomic_xchg_func)
-DEFINE_STUB_XCHG(8, int64_t, os::atomic_xchg_long_func)
+DEFINE_STUB_XCHG(4, long,    InterlockedExchange)
+DEFINE_STUB_XCHG(8, __int64, InterlockedExchange64)
 
 #undef DEFINE_STUB_XCHG
 
 #define DEFINE_STUB_CMPXCHG(ByteSize, StubType, StubName)                  \
   template<>                                                               \
@@ -97,106 +102,26 @@
   inline T Atomic::PlatformCmpxchg<ByteSize>::operator()(T volatile* dest, \
                                                          T compare_value,  \
                                                          T exchange_value, \
                                                          atomic_memory_order order) const { \
     STATIC_ASSERT(ByteSize == sizeof(T));                                  \
-    return cmpxchg_using_helper<StubType>(StubName, dest, compare_value, exchange_value); \
+    return PrimitiveConversions::cast<T>(                                  \
+      StubName(reinterpret_cast<StubType volatile *>(dest),                \
+               PrimitiveConversions::cast<StubType>(exchange_value),       \
+               PrimitiveConversions::cast<StubType>(compare_value)));      \
   }
 
-DEFINE_STUB_CMPXCHG(1, int8_t,  os::atomic_cmpxchg_byte_func)
-DEFINE_STUB_CMPXCHG(4, int32_t, os::atomic_cmpxchg_func)
-DEFINE_STUB_CMPXCHG(8, int64_t, os::atomic_cmpxchg_long_func)
+DEFINE_STUB_CMPXCHG(1, char,    _InterlockedCompareExchange8) // Use the intrinsic as InterlockedCompareExchange8 does not exist
+DEFINE_STUB_CMPXCHG(4, long,    InterlockedCompareExchange)
+DEFINE_STUB_CMPXCHG(8, __int64, InterlockedCompareExchange64)
 
 #undef DEFINE_STUB_CMPXCHG
 
-#else // !AMD64
 
-template<>
-template<typename D, typename I>
-inline D Atomic::PlatformAdd<4>::add_and_fetch(D volatile* dest, I add_value,
-                                               atomic_memory_order order) const {
-  STATIC_ASSERT(4 == sizeof(I));
-  STATIC_ASSERT(4 == sizeof(D));
-  __asm {
-    mov edx, dest;
-    mov eax, add_value;
-    mov ecx, eax;
-    lock xadd dword ptr [edx], eax;
-    add eax, ecx;
-  }
-}
-
-template<>
-template<typename T>
-inline T Atomic::PlatformXchg<4>::operator()(T volatile* dest,
-                                             T exchange_value,
-                                             atomic_memory_order order) const {
-  STATIC_ASSERT(4 == sizeof(T));
-  // alternative for InterlockedExchange
-  __asm {
-    mov eax, exchange_value;
-    mov ecx, dest;
-    xchg eax, dword ptr [ecx];
-  }
-}
-
-template<>
-template<typename T>
-inline T Atomic::PlatformCmpxchg<1>::operator()(T volatile* dest,
-                                                T compare_value,
-                                                T exchange_value,
-                                                atomic_memory_order order) const {
-  STATIC_ASSERT(1 == sizeof(T));
-  // alternative for InterlockedCompareExchange
-  __asm {
-    mov edx, dest
-    mov cl, exchange_value
-    mov al, compare_value
-    lock cmpxchg byte ptr [edx], cl
-  }
-}
-
-template<>
-template<typename T>
-inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,
-                                                T compare_value,
-                                                T exchange_value,
-                                                atomic_memory_order order) const {
-  STATIC_ASSERT(4 == sizeof(T));
-  // alternative for InterlockedCompareExchange
-  __asm {
-    mov edx, dest
-    mov ecx, exchange_value
-    mov eax, compare_value
-    lock cmpxchg dword ptr [edx], ecx
-  }
-}
+#ifndef AMD64
 
-template<>
-template<typename T>
-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,
-                                                T compare_value,
-                                                T exchange_value,
-                                                atomic_memory_order order) const {
-  STATIC_ASSERT(8 == sizeof(T));
-  int32_t ex_lo  = (int32_t)exchange_value;
-  int32_t ex_hi  = *( ((int32_t*)&exchange_value) + 1 );
-  int32_t cmp_lo = (int32_t)compare_value;
-  int32_t cmp_hi = *( ((int32_t*)&compare_value) + 1 );
-  __asm {
-    push ebx
-    push edi
-    mov eax, cmp_lo
-    mov edx, cmp_hi
-    mov edi, dest
-    mov ebx, ex_lo
-    mov ecx, ex_hi
-    lock cmpxchg8b qword ptr [edi]
-    pop edi
-    pop ebx
-  }
-}
+#pragma warning(disable: 4035) // Disables warnings reporting missing return statement
 
 template<>
 template<typename T>
 inline T Atomic::PlatformLoad<8>::operator()(T const volatile* src) const {
   STATIC_ASSERT(8 == sizeof(T));
@@ -223,15 +148,12 @@
     mov eax, dest
     fistp    qword ptr [eax]
   }
 }
 
-#endif // AMD64
-
 #pragma warning(default: 4035) // Enables warnings reporting missing return statement
 
-#ifndef AMD64
 template<>
 struct Atomic::PlatformOrderedStore<1, RELEASE_X_FENCE>
 {
   template <typename T>
   void operator()(volatile T* p, T v) const {
diff a/src/hotspot/os_cpu/windows_x86/bytes_windows_x86.inline.hpp b/src/hotspot/os_cpu/windows_x86/bytes_windows_x86.inline.hpp
--- a/src/hotspot/os_cpu/windows_x86/bytes_windows_x86.inline.hpp
+++ b/src/hotspot/os_cpu/windows_x86/bytes_windows_x86.inline.hpp
@@ -23,18 +23,21 @@
  */
 
 #ifndef OS_CPU_WINDOWS_X86_BYTES_WINDOWS_X86_INLINE_HPP
 #define OS_CPU_WINDOWS_X86_BYTES_WINDOWS_X86_INLINE_HPP
 
+#include <stdlib.h>
+
+#ifndef AMD64
 #pragma warning(disable: 4035) // Disable warning 4035: no return value
+#endif
 
 // Efficient swapping of data bytes from Java byte
 // ordering to native byte ordering and vice versa.
 inline u2 Bytes::swap_u2(u2 x) {
 #ifdef AMD64
-  address p = (address) &x;
-  return  ( (u2(p[0]) << 8 ) | ( u2(p[1])) );
+  return _byteswap_ushort(x);
 #else
   __asm {
     mov ax, x
     xchg al, ah
   }
@@ -43,12 +46,11 @@
 #endif // AMD64
 }
 
 inline u4 Bytes::swap_u4(u4 x) {
 #ifdef AMD64
-  address p = (address) &x;
-  return ( (u4(p[0]) << 24) | (u4(p[1]) << 16) | (u4(p[2]) << 8) | u4(p[3])) ;
+  return _byteswap_ulong(x);
 #else
   __asm {
     mov eax, x
     bswap eax
   }
@@ -57,13 +59,11 @@
 #endif // AMD64
 }
 
 #ifdef AMD64
 inline u8 Bytes::swap_u8(u8 x) {
-  address p = (address) &x;
-  return ( (u8(p[0]) << 56) | (u8(p[1]) << 48) | (u8(p[2]) << 40) | (u8(p[3]) << 32) |
-           (u8(p[4]) << 24) | (u8(p[5]) << 16) | (u8(p[6]) << 8)  | u8(p[7])) ;
+  return _byteswap_uint64(x);
 }
 
 #else
 // Helper function for swap_u8
 inline u8 Bytes::swap_u8_base(u4 x, u4 y) {
diff a/src/hotspot/os_cpu/windows_x86/os_windows_x86.cpp b/src/hotspot/os_cpu/windows_x86/os_windows_x86.cpp
--- a/src/hotspot/os_cpu/windows_x86/os_windows_x86.cpp
+++ b/src/hotspot/os_cpu/windows_x86/os_windows_x86.cpp
@@ -67,17 +67,14 @@
 #define REG_SP Esp
 #define REG_FP Ebp
 #define REG_PC Eip
 #endif // AMD64
 
-extern LONG WINAPI topLevelExceptionFilter(_EXCEPTION_POINTERS* );
-
 // Install a win32 structured exception handler around thread.
 void os::os_exception_wrapper(java_call_t f, JavaValue* value, const methodHandle& method, JavaCallArguments* args, Thread* thread) {
-  __try {
-
 #ifndef AMD64
+  __try {
     // We store the current thread in this wrapperthread location
     // and determine how far away this address is from the structured
     // execption pointer that FS:[0] points to.  This get_thread
     // code can then get the thread pointer via FS.
     //
@@ -111,241 +108,17 @@
     }
 #endif // ASSERT
 #endif // !AMD64
 
     f(value, method, args, thread);
-  } __except(topLevelExceptionFilter((_EXCEPTION_POINTERS*)_exception_info())) {
+#ifndef AMD64
+  } __except(EXCEPTION_CONTINUE_EXECUTION) {
       // Nothing to do.
   }
+#endif
 }
 
-#ifdef AMD64
-
-// This is the language specific handler for exceptions
-// originating from dynamically generated code.
-// We call the standard structured exception handler
-// We only expect Continued Execution since we cannot unwind
-// from generated code.
-LONG HandleExceptionFromCodeCache(
-  IN PEXCEPTION_RECORD ExceptionRecord,
-  IN ULONG64 EstablisherFrame,
-  IN OUT PCONTEXT ContextRecord,
-  IN OUT PDISPATCHER_CONTEXT DispatcherContext) {
-  EXCEPTION_POINTERS ep;
-  LONG result;
-
-  ep.ExceptionRecord = ExceptionRecord;
-  ep.ContextRecord = ContextRecord;
-
-  result = topLevelExceptionFilter(&ep);
-
-  // We better only get a CONTINUE_EXECUTION from our handler
-  // since we don't have unwind information registered.
-
-  guarantee( result == EXCEPTION_CONTINUE_EXECUTION,
-             "Unexpected result from topLevelExceptionFilter");
-
-  return(ExceptionContinueExecution);
-}
-
-
-// Structure containing the Windows Data Structures required
-// to register our Code Cache exception handler.
-// We put these in the CodeCache since the API requires
-// all addresses in these structures are relative to the Code
-// area registered with RtlAddFunctionTable.
-typedef struct {
-  char ExceptionHandlerInstr[16];  // jmp HandleExceptionFromCodeCache
-  RUNTIME_FUNCTION rt;
-  UNWIND_INFO_EH_ONLY unw;
-} DynamicCodeData, *pDynamicCodeData;
-
-#endif // AMD64
-//
-// Register our CodeCache area with the OS so it will dispatch exceptions
-// to our topLevelExceptionFilter when we take an exception in our
-// dynamically generated code.
-//
-// Arguments:  low and high are the address of the full reserved
-// codeCache area
-//
-bool os::register_code_area(char *low, char *high) {
-#ifdef AMD64
-
-  ResourceMark rm;
-
-  pDynamicCodeData pDCD;
-  PRUNTIME_FUNCTION prt;
-  PUNWIND_INFO_EH_ONLY punwind;
-
-  BufferBlob* blob = BufferBlob::create("CodeCache Exception Handler", sizeof(DynamicCodeData));
-  CodeBuffer cb(blob);
-  MacroAssembler* masm = new MacroAssembler(&cb);
-  pDCD = (pDynamicCodeData) masm->pc();
-
-  masm->jump(ExternalAddress((address)&HandleExceptionFromCodeCache));
-  masm->flush();
-
-  // Create an Unwind Structure specifying no unwind info
-  // other than an Exception Handler
-  punwind = &pDCD->unw;
-  punwind->Version = 1;
-  punwind->Flags = UNW_FLAG_EHANDLER;
-  punwind->SizeOfProlog = 0;
-  punwind->CountOfCodes = 0;
-  punwind->FrameRegister = 0;
-  punwind->FrameOffset = 0;
-  punwind->ExceptionHandler = (char *)(&(pDCD->ExceptionHandlerInstr[0])) -
-                              (char*)low;
-  punwind->ExceptionData[0] = 0;
-
-  // This structure describes the covered dynamic code area.
-  // Addresses are relative to the beginning on the code cache area
-  prt = &pDCD->rt;
-  prt->BeginAddress = 0;
-  prt->EndAddress = (ULONG)(high - low);
-  prt->UnwindData = ((char *)punwind - low);
-
-  guarantee(RtlAddFunctionTable(prt, 1, (ULONGLONG)low),
-            "Failed to register Dynamic Code Exception Handler with RtlAddFunctionTable");
-
-#endif // AMD64
-  return true;
-}
-
-// Atomics and Stub Functions
-
-typedef int32_t   xchg_func_t            (int32_t,  volatile int32_t*);
-typedef int64_t   xchg_long_func_t       (int64_t,  volatile int64_t*);
-typedef int32_t   cmpxchg_func_t         (int32_t,  volatile int32_t*, int32_t);
-typedef int8_t    cmpxchg_byte_func_t    (int8_t,   volatile int8_t*,  int8_t);
-typedef int64_t   cmpxchg_long_func_t    (int64_t,  volatile int64_t*, int64_t);
-typedef int32_t   add_func_t             (int32_t,  volatile int32_t*);
-typedef int64_t   add_long_func_t        (int64_t,  volatile int64_t*);
-
-#ifdef AMD64
-
-int32_t os::atomic_xchg_bootstrap(int32_t exchange_value, volatile int32_t* dest) {
-  // try to use the stub:
-  xchg_func_t* func = CAST_TO_FN_PTR(xchg_func_t*, StubRoutines::atomic_xchg_entry());
-
-  if (func != NULL) {
-    os::atomic_xchg_func = func;
-    return (*func)(exchange_value, dest);
-  }
-  assert(Threads::number_of_threads() == 0, "for bootstrap only");
-
-  int32_t old_value = *dest;
-  *dest = exchange_value;
-  return old_value;
-}
-
-int64_t os::atomic_xchg_long_bootstrap(int64_t exchange_value, volatile int64_t* dest) {
-  // try to use the stub:
-  xchg_long_func_t* func = CAST_TO_FN_PTR(xchg_long_func_t*, StubRoutines::atomic_xchg_long_entry());
-
-  if (func != NULL) {
-    os::atomic_xchg_long_func = func;
-    return (*func)(exchange_value, dest);
-  }
-  assert(Threads::number_of_threads() == 0, "for bootstrap only");
-
-  int64_t old_value = *dest;
-  *dest = exchange_value;
-  return old_value;
-}
-
-
-int32_t os::atomic_cmpxchg_bootstrap(int32_t exchange_value, volatile int32_t* dest, int32_t compare_value) {
-  // try to use the stub:
-  cmpxchg_func_t* func = CAST_TO_FN_PTR(cmpxchg_func_t*, StubRoutines::atomic_cmpxchg_entry());
-
-  if (func != NULL) {
-    os::atomic_cmpxchg_func = func;
-    return (*func)(exchange_value, dest, compare_value);
-  }
-  assert(Threads::number_of_threads() == 0, "for bootstrap only");
-
-  int32_t old_value = *dest;
-  if (old_value == compare_value)
-    *dest = exchange_value;
-  return old_value;
-}
-
-int8_t os::atomic_cmpxchg_byte_bootstrap(int8_t exchange_value, volatile int8_t* dest, int8_t compare_value) {
-  // try to use the stub:
-  cmpxchg_byte_func_t* func = CAST_TO_FN_PTR(cmpxchg_byte_func_t*, StubRoutines::atomic_cmpxchg_byte_entry());
-
-  if (func != NULL) {
-    os::atomic_cmpxchg_byte_func = func;
-    return (*func)(exchange_value, dest, compare_value);
-  }
-  assert(Threads::number_of_threads() == 0, "for bootstrap only");
-
-  int8_t old_value = *dest;
-  if (old_value == compare_value)
-    *dest = exchange_value;
-  return old_value;
-}
-
-#endif // AMD64
-
-int64_t os::atomic_cmpxchg_long_bootstrap(int64_t exchange_value, volatile int64_t* dest, int64_t compare_value) {
-  // try to use the stub:
-  cmpxchg_long_func_t* func = CAST_TO_FN_PTR(cmpxchg_long_func_t*, StubRoutines::atomic_cmpxchg_long_entry());
-
-  if (func != NULL) {
-    os::atomic_cmpxchg_long_func = func;
-    return (*func)(exchange_value, dest, compare_value);
-  }
-  assert(Threads::number_of_threads() == 0, "for bootstrap only");
-
-  int64_t old_value = *dest;
-  if (old_value == compare_value)
-    *dest = exchange_value;
-  return old_value;
-}
-
-#ifdef AMD64
-
-int32_t os::atomic_add_bootstrap(int32_t add_value, volatile int32_t* dest) {
-  // try to use the stub:
-  add_func_t* func = CAST_TO_FN_PTR(add_func_t*, StubRoutines::atomic_add_entry());
-
-  if (func != NULL) {
-    os::atomic_add_func = func;
-    return (*func)(add_value, dest);
-  }
-  assert(Threads::number_of_threads() == 0, "for bootstrap only");
-
-  return (*dest) += add_value;
-}
-
-int64_t os::atomic_add_long_bootstrap(int64_t add_value, volatile int64_t* dest) {
-  // try to use the stub:
-  add_long_func_t* func = CAST_TO_FN_PTR(add_long_func_t*, StubRoutines::atomic_add_long_entry());
-
-  if (func != NULL) {
-    os::atomic_add_long_func = func;
-    return (*func)(add_value, dest);
-  }
-  assert(Threads::number_of_threads() == 0, "for bootstrap only");
-
-  return (*dest) += add_value;
-}
-
-xchg_func_t*         os::atomic_xchg_func         = os::atomic_xchg_bootstrap;
-xchg_long_func_t*    os::atomic_xchg_long_func    = os::atomic_xchg_long_bootstrap;
-cmpxchg_func_t*      os::atomic_cmpxchg_func      = os::atomic_cmpxchg_bootstrap;
-cmpxchg_byte_func_t* os::atomic_cmpxchg_byte_func = os::atomic_cmpxchg_byte_bootstrap;
-add_func_t*          os::atomic_add_func          = os::atomic_add_bootstrap;
-add_long_func_t*     os::atomic_add_long_func     = os::atomic_add_long_bootstrap;
-
-#endif // AMD64
-
-cmpxchg_long_func_t* os::atomic_cmpxchg_long_func = os::atomic_cmpxchg_long_bootstrap;
-
 #ifdef AMD64
 /*
  * Windows/x64 does not use stack frames the way expected by Java:
  * [1] in most cases, there is no frame pointer. All locals are addressed via RSP
  * [2] in rare cases, when alloca() is used, a frame pointer is used, but this may
@@ -486,10 +259,45 @@
                                      StubRoutines::x86::get_previous_sp_entry());
   return (*func)();
 }
 #endif
 
+bool os::win32::get_frame_at_stack_banging_point(JavaThread* thread,
+        struct _EXCEPTION_POINTERS* exceptionInfo, address pc, frame* fr) {
+  PEXCEPTION_RECORD exceptionRecord = exceptionInfo->ExceptionRecord;
+  address addr = (address) exceptionRecord->ExceptionInformation[1];
+  if (Interpreter::contains(pc)) {
+    *fr = os::fetch_frame_from_context((void*)exceptionInfo->ContextRecord);
+    if (!fr->is_first_java_frame()) {
+      // get_frame_at_stack_banging_point() is only called when we
+      // have well defined stacks so java_sender() calls do not need
+      // to assert safe_for_sender() first.
+      *fr = fr->java_sender();
+    }
+  } else {
+    // more complex code with compiled code
+    assert(!Interpreter::contains(pc), "Interpreted methods should have been handled above");
+    CodeBlob* cb = CodeCache::find_blob(pc);
+    if (cb == NULL || !cb->is_nmethod() || cb->is_frame_complete_at(pc)) {
+      // Not sure where the pc points to, fallback to default
+      // stack overflow handling
+      return false;
+    } else {
+      // in compiled code, the stack banging is performed just after the return pc
+      // has been pushed on the stack
+      intptr_t* fp = (intptr_t*)exceptionInfo->ContextRecord->REG_FP;
+      intptr_t* sp = (intptr_t*)exceptionInfo->ContextRecord->REG_SP;
+      *fr = frame(sp + 1, fp, (address)*sp);
+      if (!fr->is_java_frame()) {
+        // See java_sender() comment above.
+        *fr = fr->java_sender();
+      }
+    }
+  }
+  assert(fr->is_java_frame(), "Safety check");
+  return true;
+}
 
 #ifndef AMD64
 intptr_t* _get_previous_fp() {
   intptr_t **frameptr;
   __asm {
diff a/src/hotspot/os_cpu/windows_x86/os_windows_x86.hpp b/src/hotspot/os_cpu/windows_x86/os_windows_x86.hpp
--- a/src/hotspot/os_cpu/windows_x86/os_windows_x86.hpp
+++ b/src/hotspot/os_cpu/windows_x86/os_windows_x86.hpp
@@ -60,12 +60,10 @@
   static void setup_fpu();
   static bool supports_sse() { return true; }
 
   static jlong rdtsc();
 
-  static bool      register_code_area(char *low, char *high);
-
 #ifdef AMD64
 #define PLATFORM_PRINT_NATIVE_STACK 1
 static bool platform_print_native_stack(outputStream* st, const void* context,
                                         char *buf, int buf_size);
 #endif
diff a/src/hotspot/os_cpu/windows_x86/thread_windows_x86.cpp b/src/hotspot/os_cpu/windows_x86/thread_windows_x86.cpp
--- a/src/hotspot/os_cpu/windows_x86/thread_windows_x86.cpp
+++ /dev/null
@@ -1,111 +0,0 @@
-/*
- * Copyright (c) 2003, 2018, Oracle and/or its affiliates. All rights reserved.
- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
- *
- * This code is free software; you can redistribute it and/or modify it
- * under the terms of the GNU General Public License version 2 only, as
- * published by the Free Software Foundation.
- *
- * This code is distributed in the hope that it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
- * version 2 for more details (a copy is included in the LICENSE file that
- * accompanied this code).
- *
- * You should have received a copy of the GNU General Public License version
- * 2 along with this work; if not, write to the Free Software Foundation,
- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
- *
- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
- * or visit www.oracle.com if you need additional information or have any
- * questions.
- *
- */
-
-#include "precompiled.hpp"
-#include "memory/metaspaceShared.hpp"
-#include "runtime/frame.inline.hpp"
-#include "runtime/thread.inline.hpp"
-
-frame JavaThread::pd_last_frame() {
-  assert(has_last_Java_frame(), "must have last_Java_sp() when suspended");
-  vmassert(_anchor.last_Java_pc() != NULL, "not walkable");
-  return frame(_anchor.last_Java_sp(), _anchor.last_Java_fp(), _anchor.last_Java_pc());
-}
-
-// For Forte Analyzer AsyncGetCallTrace profiling support - thread is
-// currently interrupted by SIGPROF
-bool JavaThread::pd_get_top_frame_for_signal_handler(frame* fr_addr,
-  void* ucontext, bool isInJava) {
-
-  assert(Thread::current() == this, "caller must be current thread");
-  return pd_get_top_frame(fr_addr, ucontext, isInJava);
-}
-
-bool JavaThread::pd_get_top_frame_for_profiling(frame* fr_addr, void* ucontext, bool isInJava) {
-  return pd_get_top_frame(fr_addr, ucontext, isInJava);
-}
-
-bool JavaThread::pd_get_top_frame(frame* fr_addr, void* ucontext, bool isInJava) {
-
-  assert(this->is_Java_thread(), "must be JavaThread");
-
-  JavaThread* jt = (JavaThread *)this;
-
-  // If we have a last_Java_frame, then we should use it even if
-  // isInJava == true.  It should be more reliable than CONTEXT info.
-  if (jt->has_last_Java_frame() && jt->frame_anchor()->walkable()) {
-    *fr_addr = jt->pd_last_frame();
-    return true;
-  }
-
-  // At this point, we don't have a last_Java_frame, so
-  // we try to glean some information out of the CONTEXT
-  // if we were running Java code when SIGPROF came in.
-  if (isInJava) {
-    CONTEXT* uc = (CONTEXT*)ucontext;
-
-#ifdef AMD64
-    intptr_t* ret_fp = (intptr_t*) uc->Rbp;
-    intptr_t* ret_sp = (intptr_t*) uc->Rsp;
-    ExtendedPC addr = ExtendedPC((address)uc->Rip);
-#else
-    intptr_t* ret_fp = (intptr_t*) uc->Ebp;
-    intptr_t* ret_sp = (intptr_t*) uc->Esp;
-    ExtendedPC addr = ExtendedPC((address)uc->Eip);
-#endif // AMD64
-    if (addr.pc() == NULL || ret_sp == NULL ) {
-      // CONTEXT wasn't useful
-      return false;
-    }
-
-    if (MetaspaceShared::is_in_trampoline_frame(addr.pc())) {
-      // In the middle of a trampoline call. Bail out for safety.
-      // This happens rarely so shouldn't affect profiling.
-      return false;
-    }
-
-    frame ret_frame(ret_sp, ret_fp, addr.pc());
-    if (!ret_frame.safe_for_sender(jt)) {
-#if COMPILER2_OR_JVMCI
-      // C2 and JVMCI use ebp as a general register see if NULL fp helps
-      frame ret_frame2(ret_sp, NULL, addr.pc());
-      if (!ret_frame2.safe_for_sender(jt)) {
-        // nothing else to try if the frame isn't good
-        return false;
-      }
-      ret_frame = ret_frame2;
-#else
-      // nothing else to try if the frame isn't good
-      return false;
-#endif // COMPILER2_OR_JVMCI
-    }
-    *fr_addr = ret_frame;
-    return true;
-  }
-
-  // nothing else to try
-  return false;
-}
-
-void JavaThread::cache_global_variables() { }
diff a/src/hotspot/share/adlc/adlc.hpp b/src/hotspot/share/adlc/adlc.hpp
--- a/src/hotspot/share/adlc/adlc.hpp
+++ b/src/hotspot/share/adlc/adlc.hpp
@@ -37,13 +37,16 @@
 #include <string.h>
 #include <ctype.h>
 #include <stdarg.h>
 #include <sys/types.h>
 
-/* Make sure that we have the intptr_t and uintptr_t definitions */
 #ifdef _WIN32
 
+/* Make sure that we don't define min/max */
+#define NOMINMAX
+
+/* Make sure that we have the intptr_t and uintptr_t definitions */
 #if _MSC_VER >= 1300
 using namespace std;
 #endif
 
 #if _MSC_VER >= 1400
diff a/src/hotspot/share/code/codeCache.cpp b/src/hotspot/share/code/codeCache.cpp
--- a/src/hotspot/share/code/codeCache.cpp
+++ b/src/hotspot/share/code/codeCache.cpp
@@ -949,17 +949,11 @@
     ReservedCodeSpace rs = reserve_heap_memory(ReservedCodeCacheSize);
     add_heap(rs, "CodeCache", CodeBlobType::All);
   }
 
   // Initialize ICache flush mechanism
-  // This service is needed for os::register_code_area
   icache_init();
-
-  // Give OS a chance to register generated code area.
-  // This is used on Windows 64 bit platforms to register
-  // Structured Exception Handlers for our generated code.
-  os::register_code_area((char*)low_bound(), (char*)high_bound());
 }
 
 void codeCache_init() {
   CodeCache::initialize();
   // Load AOT libraries and add AOT code heaps.
diff a/src/hotspot/share/prims/jni.cpp b/src/hotspot/share/prims/jni.cpp
--- a/src/hotspot/share/prims/jni.cpp
+++ b/src/hotspot/share/prims/jni.cpp
@@ -90,14 +90,10 @@
 #include "jvmci/jvmciCompiler.hpp"
 #endif
 
 static jint CurrentVersion = JNI_VERSION_10;
 
-#ifdef _WIN32
-extern LONG WINAPI topLevelExceptionFilter(_EXCEPTION_POINTERS* );
-#endif
-
 // The DT_RETURN_MARK macros create a scoped object to fire the dtrace
 // '-return' probe regardless of the return path is taken out of the function.
 // Methods that have multiple return paths use this to avoid having to
 // instrument each return path.  Methods that use CHECK or THROW must use this
 // since those macros can cause an immedate uninstrumented return.
@@ -3795,11 +3791,11 @@
 }
 
 DT_RETURN_MARK_DECL(CreateJavaVM, jint
                     , HOTSPOT_JNI_CREATEJAVAVM_RETURN(_ret_ref));
 
-static jint JNI_CreateJavaVM_inner(JavaVM **vm, void **penv, void *args) {
+_JNI_IMPORT_OR_EXPORT_ jint JNICALL JNI_CreateJavaVM(JavaVM **vm, void **penv, void *args) {
   HOTSPOT_JNI_CREATEJAVAVM_ENTRY((void **) vm, penv, args);
 
   jint result = JNI_ERR;
   DT_RETURN_MARK(CreateJavaVM, jint, (const jint&)result);
 
@@ -3924,25 +3920,10 @@
 
   return result;
 
 }
 
-_JNI_IMPORT_OR_EXPORT_ jint JNICALL JNI_CreateJavaVM(JavaVM **vm, void **penv, void *args) {
-  jint result = JNI_ERR;
-  // On Windows, let CreateJavaVM run with SEH protection
-#ifdef _WIN32
-  __try {
-#endif
-    result = JNI_CreateJavaVM_inner(vm, penv, args);
-#ifdef _WIN32
-  } __except(topLevelExceptionFilter((_EXCEPTION_POINTERS*)_exception_info())) {
-    // Nothing to do.
-  }
-#endif
-  return result;
-}
-
 _JNI_IMPORT_OR_EXPORT_ jint JNICALL JNI_GetCreatedJavaVMs(JavaVM **vm_buf, jsize bufLen, jsize *numVMs) {
   // See bug 4367188, the wrapper can sometimes cause VM crashes
   // JNIWrapper("GetCreatedJavaVMs");
 
   HOTSPOT_JNI_GETCREATEDJAVAVMS_ENTRY((void **) vm_buf, bufLen, (uintptr_t *) numVMs);
@@ -3960,11 +3941,11 @@
 extern "C" {
 
 DT_RETURN_MARK_DECL(DestroyJavaVM, jint
                     , HOTSPOT_JNI_DESTROYJAVAVM_RETURN(_ret_ref));
 
-static jint JNICALL jni_DestroyJavaVM_inner(JavaVM *vm) {
+jint JNICALL jni_DestroyJavaVM(JavaVM *vm) {
   HOTSPOT_JNI_DESTROYJAVAVM_ENTRY(vm);
   jint res = JNI_ERR;
   DT_RETURN_MARK(DestroyJavaVM, jint, (const jint&)res);
 
   if (vm_created == 0) {
@@ -3996,25 +3977,10 @@
     res = JNI_ERR;
     return res;
   }
 }
 
-jint JNICALL jni_DestroyJavaVM(JavaVM *vm) {
-  jint result = JNI_ERR;
-  // On Windows, we need SEH protection
-#ifdef _WIN32
-  __try {
-#endif
-    result = jni_DestroyJavaVM_inner(vm);
-#ifdef _WIN32
-  } __except(topLevelExceptionFilter((_EXCEPTION_POINTERS*)_exception_info())) {
-    // Nothing to do.
-  }
-#endif
-  return result;
-}
-
 static jint attach_current_thread(JavaVM *vm, void **penv, void *_args, bool daemon) {
   JavaVMAttachArgs *args = (JavaVMAttachArgs *) _args;
 
   // Check below commented out from JDK1.2fcs as well
   /*
diff a/src/hotspot/share/runtime/biasedLocking.cpp b/src/hotspot/share/runtime/biasedLocking.cpp
--- a/src/hotspot/share/runtime/biasedLocking.cpp
+++ b/src/hotspot/share/runtime/biasedLocking.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2005, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2005, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -620,11 +620,11 @@
   log_info(biasedlocking, handshake)("JavaThread " INTPTR_FORMAT " handshaking JavaThread "
                                      INTPTR_FORMAT " to revoke object " INTPTR_FORMAT, p2i(requester),
                                      p2i(biaser), p2i(obj()));
 
   RevokeOneBias revoke(obj, requester, biaser);
-  bool executed = Handshake::execute(&revoke, biaser);
+  bool executed = Handshake::execute_direct(&revoke, biaser);
   if (revoke.status_code() == NOT_REVOKED) {
     return NOT_REVOKED;
   }
   if (executed) {
     log_info(biasedlocking, handshake)("Handshake revocation for object " INTPTR_FORMAT " succeeded. Bias was %srevoked",
@@ -666,27 +666,28 @@
 
 // Caller should have instantiated a ResourceMark object before calling this method
 void BiasedLocking::walk_stack_and_revoke(oop obj, JavaThread* biased_locker) {
   assert(!SafepointSynchronize::is_at_safepoint() || !SafepointMechanism::uses_thread_local_poll(),
          "if SafepointMechanism::uses_thread_local_poll() is enabled this should always be executed outside safepoints");
-  assert(Thread::current() == biased_locker || Thread::current()->is_VM_thread(), "wrong thread");
+  Thread* cur = Thread::current();
+  assert(cur == biased_locker || cur == biased_locker->get_active_handshaker() || cur->is_VM_thread(), "wrong thread");
 
   markWord mark = obj->mark();
   assert(mark.biased_locker() == biased_locker &&
          obj->klass()->prototype_header().bias_epoch() == mark.bias_epoch(), "invariant");
 
   log_trace(biasedlocking)("%s(" INTPTR_FORMAT ") revoking object " INTPTR_FORMAT ", mark "
                            INTPTR_FORMAT ", type %s, prototype header " INTPTR_FORMAT
                            ", biaser " INTPTR_FORMAT " %s",
-                           Thread::current()->is_VM_thread() ? "VMThread" : "JavaThread",
-                           p2i(Thread::current()),
+                           cur->is_VM_thread() ? "VMThread" : "JavaThread",
+                           p2i(cur),
                            p2i(obj),
                            mark.value(),
                            obj->klass()->external_name(),
                            obj->klass()->prototype_header().value(),
                            p2i(biased_locker),
-                           Thread::current()->is_VM_thread() ? "" : "(walking own stack)");
+                           cur->is_VM_thread() ? "" : "(walking own stack)");
 
   markWord unbiased_prototype = markWord::prototype().set_age(obj->mark().age());
 
   GrowableArray<MonitorInfo*>* cached_monitor_info = get_or_compute_monitor_info(biased_locker);
   BasicLock* highest_lock = NULL;
diff a/src/hotspot/share/runtime/handshake.cpp b/src/hotspot/share/runtime/handshake.cpp
--- a/src/hotspot/share/runtime/handshake.cpp
+++ b/src/hotspot/share/runtime/handshake.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2017, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2017, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -35,55 +35,53 @@
 #include "runtime/thread.hpp"
 #include "runtime/vmThread.hpp"
 #include "utilities/formatBuffer.hpp"
 #include "utilities/preserveException.hpp"
 
-class HandshakeOperation: public StackObj {
-public:
-  virtual void do_handshake(JavaThread* thread) = 0;
-};
 
-class HandshakeThreadsOperation: public HandshakeOperation {
-  static Semaphore _done;
+class HandshakeOperation: public StackObj {
   HandshakeClosure* _handshake_cl;
+  int64_t _pending_threads;
   bool _executed;
+  bool _is_direct;
 public:
-  HandshakeThreadsOperation(HandshakeClosure* cl) : _handshake_cl(cl), _executed(false) {}
+  HandshakeOperation(HandshakeClosure* cl, bool is_direct = false) :
+    _handshake_cl(cl),
+    _pending_threads(1),
+    _executed(false),
+    _is_direct(is_direct) {}
+
   void do_handshake(JavaThread* thread);
-  bool thread_has_completed() { return _done.trywait(); }
+  bool is_completed() {
+    int64_t val = Atomic::load(&_pending_threads);
+    assert(val >= 0, "_pending_threads cannot be negative");
+    return val == 0;
+  }
+  void add_target_count(int count) { Atomic::add(&_pending_threads, count); }
   bool executed() const { return _executed; }
   const char* name() { return _handshake_cl->name(); }
 
+  bool is_direct() { return _is_direct; }
+
 #ifdef ASSERT
   void check_state() {
-    assert(!_done.trywait(), "Must be zero");
+    assert(_pending_threads == 0, "Must be zero");
   }
 #endif
 };
 
-Semaphore HandshakeThreadsOperation::_done(0);
-
 class VM_Handshake: public VM_Operation {
   const jlong _handshake_timeout;
  public:
   bool evaluate_at_safepoint() const { return false; }
 
  protected:
-  HandshakeThreadsOperation* const _op;
+  HandshakeOperation* const _op;
 
-  VM_Handshake(HandshakeThreadsOperation* op) :
+  VM_Handshake(HandshakeOperation* op) :
       _handshake_timeout(TimeHelper::millis_to_counter(HandshakeTimeout)), _op(op) {}
 
-  void set_handshake(JavaThread* target) {
-    target->set_handshake_operation(_op);
-  }
-
-  // This method returns true for threads completed their operation
-  // and true for threads canceled their operation.
-  // A cancellation can happen if the thread is exiting.
-  bool poll_for_completed_thread() { return _op->thread_has_completed(); }
-
   bool handshake_has_timed_out(jlong start_time);
   static void handle_timeout();
 };
 
 bool VM_Handshake::handshake_has_timed_out(jlong start_time) {
@@ -119,24 +117,22 @@
 }
 
 class VM_HandshakeOneThread: public VM_Handshake {
   JavaThread* _target;
  public:
-  VM_HandshakeOneThread(HandshakeThreadsOperation* op, JavaThread* target) :
+  VM_HandshakeOneThread(HandshakeOperation* op, JavaThread* target) :
     VM_Handshake(op), _target(target) {}
 
   void doit() {
-    DEBUG_ONLY(_op->check_state();)
-
     jlong start_time_ns = 0;
     if (log_is_enabled(Info, handshake)) {
       start_time_ns = os::javaTimeNanos();
     }
 
     ThreadsListHandle tlh;
     if (tlh.includes(_target)) {
-      set_handshake(_target);
+      _target->set_handshake_operation(_op);
     } else {
       log_handshake_info(start_time_ns, _op->name(), 0, 0, "(thread dead)");
       return;
     }
 
@@ -145,12 +141,12 @@
     bool by_vm_thread = false;
     do {
       if (handshake_has_timed_out(timeout_start_time)) {
         handle_timeout();
       }
-      by_vm_thread = _target->handshake_try_process_by_vmThread();
-    } while (!poll_for_completed_thread());
+      by_vm_thread = _target->handshake_try_process(_op);
+    } while (!_op->is_completed());
     DEBUG_ONLY(_op->check_state();)
     log_handshake_info(start_time_ns, _op->name(), 1, by_vm_thread ? 1 : 0);
   }
 
   VMOp_Type type() const { return VMOp_HandshakeOneThread; }
@@ -158,36 +154,35 @@
   bool executed() const { return _op->executed(); }
 };
 
 class VM_HandshakeAllThreads: public VM_Handshake {
  public:
-  VM_HandshakeAllThreads(HandshakeThreadsOperation* op) : VM_Handshake(op) {}
+  VM_HandshakeAllThreads(HandshakeOperation* op) : VM_Handshake(op) {}
 
   void doit() {
-    DEBUG_ONLY(_op->check_state();)
-
     jlong start_time_ns = 0;
     if (log_is_enabled(Info, handshake)) {
       start_time_ns = os::javaTimeNanos();
     }
     int handshake_executed_by_vm_thread = 0;
 
     JavaThreadIteratorWithHandle jtiwh;
     int number_of_threads_issued = 0;
     for (JavaThread *thr = jtiwh.next(); thr != NULL; thr = jtiwh.next()) {
-      set_handshake(thr);
+      thr->set_handshake_operation(_op);
       number_of_threads_issued++;
     }
 
     if (number_of_threads_issued < 1) {
       log_handshake_info(start_time_ns, _op->name(), 0, 0);
       return;
     }
+    // _op was created with a count == 1 so don't double count.
+    _op->add_target_count(number_of_threads_issued - 1);
 
     log_trace(handshake)("Threads signaled, begin processing blocked threads by VMThread");
     const jlong start_time = os::elapsed_counter();
-    int number_of_threads_completed = 0;
     do {
       // Check if handshake operation has timed out
       if (handshake_has_timed_out(start_time)) {
         handle_timeout();
       }
@@ -196,22 +191,16 @@
       // Observing a blocked state may of course be transient but the processing is guarded
       // by semaphores and we optimistically begin by working on the blocked threads
       jtiwh.rewind();
       for (JavaThread *thr = jtiwh.next(); thr != NULL; thr = jtiwh.next()) {
         // A new thread on the ThreadsList will not have an operation,
-        // hence it is skipped in handshake_process_by_vmthread.
-        if (thr->handshake_try_process_by_vmThread()) {
+        // hence it is skipped in handshake_try_process.
+        if (thr->handshake_try_process(_op)) {
           handshake_executed_by_vm_thread++;
         }
       }
-      while (poll_for_completed_thread()) {
-        // Includes canceled operations by exiting threads.
-        number_of_threads_completed++;
-      }
-
-    } while (number_of_threads_issued > number_of_threads_completed);
-    assert(number_of_threads_issued == number_of_threads_completed, "Must be the same");
+    } while (!_op->is_completed());
     DEBUG_ONLY(_op->check_state();)
 
     log_handshake_info(start_time_ns, _op->name(), number_of_threads_issued, handshake_executed_by_vm_thread);
   }
 
@@ -243,11 +232,11 @@
 
   VMOp_Type type() const { return VMOp_HandshakeFallback; }
   bool executed() const { return _executed; }
 };
 
-void HandshakeThreadsOperation::do_handshake(JavaThread* thread) {
+void HandshakeOperation::do_handshake(JavaThread* thread) {
   jlong start_time_ns = 0;
   if (log_is_enabled(Debug, handshake, task)) {
     start_time_ns = os::javaTimeNanos();
   }
 
@@ -261,148 +250,220 @@
     jlong completion_time = os::javaTimeNanos() - start_time_ns;
     log_debug(handshake, task)("Operation: %s for thread " PTR_FORMAT ", is_vm_thread: %s, completed in " JLONG_FORMAT " ns",
                                name(), p2i(thread), BOOL_TO_STR(Thread::current()->is_VM_thread()), completion_time);
   }
 
-  // Use the semaphore to inform the VM thread that we have completed the operation
-  _done.signal();
+  // Inform VMThread/Handshaker that we have completed the operation
+  Atomic::dec(&_pending_threads);
 
-  // It is no longer safe to refer to 'this' as the VMThread may have destroyed this operation
+  // It is no longer safe to refer to 'this' as the VMThread/Handshaker may have destroyed this operation
 }
 
 void Handshake::execute(HandshakeClosure* thread_cl) {
   if (SafepointMechanism::uses_thread_local_poll()) {
-    HandshakeThreadsOperation cto(thread_cl);
-    VM_HandshakeAllThreads handshake(&cto);
+    HandshakeOperation ho(thread_cl);
+    VM_HandshakeAllThreads handshake(&ho);
     VMThread::execute(&handshake);
   } else {
     VM_HandshakeFallbackOperation op(thread_cl);
     VMThread::execute(&op);
   }
 }
 
 bool Handshake::execute(HandshakeClosure* thread_cl, JavaThread* target) {
   if (SafepointMechanism::uses_thread_local_poll()) {
-    HandshakeThreadsOperation cto(thread_cl);
-    VM_HandshakeOneThread handshake(&cto, target);
+    HandshakeOperation ho(thread_cl);
+    VM_HandshakeOneThread handshake(&ho, target);
     VMThread::execute(&handshake);
     return handshake.executed();
   } else {
     VM_HandshakeFallbackOperation op(thread_cl, target);
     VMThread::execute(&op);
     return op.executed();
   }
 }
 
-HandshakeState::HandshakeState() : _operation(NULL), _semaphore(1), _thread_in_process_handshake(false) {
-  DEBUG_ONLY(_vmthread_processing_handshake = false;)
+bool Handshake::execute_direct(HandshakeClosure* thread_cl, JavaThread* target) {
+  if (!SafepointMechanism::uses_thread_local_poll()) {
+    VM_HandshakeFallbackOperation op(thread_cl, target);
+    VMThread::execute(&op);
+    return op.executed();
+  }
+  JavaThread* self = JavaThread::current();
+  HandshakeOperation op(thread_cl, /*is_direct*/ true);
+
+  jlong start_time_ns = 0;
+  if (log_is_enabled(Info, handshake)) {
+    start_time_ns = os::javaTimeNanos();
+  }
+
+  ThreadsListHandle tlh;
+  if (tlh.includes(target)) {
+    target->set_handshake_operation(&op);
+  } else {
+    log_handshake_info(start_time_ns, op.name(), 0, 0, "(thread dead)");
+    return false;
+  }
+
+  bool by_handshaker = false;
+  while (!op.is_completed()) {
+    by_handshaker = target->handshake_try_process(&op);
+    // Check for pending handshakes to avoid possible deadlocks where our
+    // target is trying to handshake us.
+    if (SafepointMechanism::should_block(self)) {
+      ThreadBlockInVM tbivm(self);
+    }
+  }
+  DEBUG_ONLY(op.check_state();)
+  log_handshake_info(start_time_ns, op.name(), 1, by_handshaker ? 1 : 0);
+
+  return op.executed();
+}
+
+HandshakeState::HandshakeState() :
+  _operation(NULL),
+  _operation_direct(NULL),
+  _handshake_turn_sem(1),
+  _processing_sem(1),
+  _thread_in_process_handshake(false)
+{
+  DEBUG_ONLY(_active_handshaker = NULL;)
 }
 
-void HandshakeState::set_operation(JavaThread* target, HandshakeOperation* op) {
-  _operation = op;
-  SafepointMechanism::arm_local_poll_release(target);
+void HandshakeState::set_operation(HandshakeOperation* op) {
+  if (!op->is_direct()) {
+    assert(Thread::current()->is_VM_thread(), "should be the VMThread");
+    _operation = op;
+  } else {
+    assert(Thread::current()->is_Java_thread(), "should be a JavaThread");
+    // Serialize direct handshakes so that only one proceeds at a time for a given target
+    _handshake_turn_sem.wait_with_safepoint_check(JavaThread::current());
+    _operation_direct = op;
+  }
+  SafepointMechanism::arm_local_poll_release(_handshakee);
 }
 
-void HandshakeState::clear_handshake(JavaThread* target) {
-  _operation = NULL;
-  SafepointMechanism::disarm_if_needed(target, true /* release */);
+void HandshakeState::clear_handshake(bool is_direct) {
+  if (!is_direct) {
+    _operation = NULL;
+  } else {
+    _operation_direct = NULL;
+    _handshake_turn_sem.signal();
+  }
 }
 
-void HandshakeState::process_self_inner(JavaThread* thread) {
-  assert(Thread::current() == thread, "should call from thread");
-  assert(!thread->is_terminated(), "should not be a terminated thread");
-  assert(thread->thread_state() != _thread_blocked, "should not be in a blocked state");
-  assert(thread->thread_state() != _thread_in_native, "should not be in native");
+void HandshakeState::process_self_inner() {
+  assert(Thread::current() == _handshakee, "should call from _handshakee");
+  assert(!_handshakee->is_terminated(), "should not be a terminated thread");
+  assert(_handshakee->thread_state() != _thread_blocked, "should not be in a blocked state");
+  assert(_handshakee->thread_state() != _thread_in_native, "should not be in native");
+  JavaThread* self = _handshakee;
 
   do {
-    ThreadInVMForHandshake tivm(thread);
-    if (!_semaphore.trywait()) {
-      _semaphore.wait_with_safepoint_check(thread);
+    ThreadInVMForHandshake tivm(self);
+    if (!_processing_sem.trywait()) {
+      _processing_sem.wait_with_safepoint_check(self);
     }
-    HandshakeOperation* op = Atomic::load_acquire(&_operation);
-    if (op != NULL) {
-      HandleMark hm(thread);
-      CautiouslyPreserveExceptionMark pem(thread);
-      // Disarm before execute the operation
-      clear_handshake(thread);
-      op->do_handshake(thread);
+    if (has_operation()) {
+      HandleMark hm(self);
+      CautiouslyPreserveExceptionMark pem(self);
+      HandshakeOperation * op = _operation;
+      if (op != NULL) {
+        // Disarm before executing the operation
+        clear_handshake(/*is_direct*/ false);
+        op->do_handshake(self);
+      }
+      op = _operation_direct;
+      if (op != NULL) {
+        // Disarm before executing the operation
+        clear_handshake(/*is_direct*/ true);
+        op->do_handshake(self);
+      }
     }
-    _semaphore.signal();
+    _processing_sem.signal();
   } while (has_operation());
 }
 
-bool HandshakeState::vmthread_can_process_handshake(JavaThread* target) {
+bool HandshakeState::can_process_handshake() {
   // handshake_safe may only be called with polls armed.
-  // VM thread controls this by first claiming the handshake via claim_handshake_for_vmthread.
-  return SafepointSynchronize::handshake_safe(target);
+  // Handshaker controls this by first claiming the handshake via claim_handshake().
+  return SafepointSynchronize::handshake_safe(_handshakee);
 }
 
-static bool possibly_vmthread_can_process_handshake(JavaThread* target) {
+bool HandshakeState::possibly_can_process_handshake() {
   // Note that this method is allowed to produce false positives.
-  if (target->is_ext_suspended()) {
+  if (_handshakee->is_ext_suspended()) {
     return true;
   }
-  if (target->is_terminated()) {
+  if (_handshakee->is_terminated()) {
     return true;
   }
-  switch (target->thread_state()) {
+  switch (_handshakee->thread_state()) {
   case _thread_in_native:
     // native threads are safe if they have no java stack or have walkable stack
-    return !target->has_last_Java_frame() || target->frame_anchor()->walkable();
+    return !_handshakee->has_last_Java_frame() || _handshakee->frame_anchor()->walkable();
 
   case _thread_blocked:
     return true;
 
   default:
     return false;
   }
 }
 
-bool HandshakeState::claim_handshake_for_vmthread() {
-  if (!_semaphore.trywait()) {
+bool HandshakeState::claim_handshake(bool is_direct) {
+  if (!_processing_sem.trywait()) {
     return false;
   }
-  if (has_operation()) {
+  if (has_specific_operation(is_direct)){
     return true;
   }
-  _semaphore.signal();
+  _processing_sem.signal();
   return false;
 }
 
-bool HandshakeState::try_process_by_vmThread(JavaThread* target) {
-  assert(Thread::current()->is_VM_thread(), "should call from vm thread");
+bool HandshakeState::try_process(HandshakeOperation* op) {
+  bool is_direct = op->is_direct();
 
-  if (!has_operation()) {
+  if (!has_specific_operation(is_direct)){
     // JT has already cleared its handshake
     return false;
   }
 
-  if (!possibly_vmthread_can_process_handshake(target)) {
+  if (!possibly_can_process_handshake()) {
     // JT is observed in an unsafe state, it must notice the handshake itself
     return false;
   }
 
   // Claim the semaphore if there still an operation to be executed.
-  if (!claim_handshake_for_vmthread()) {
+  if (!claim_handshake(is_direct)) {
+    return false;
+  }
+
+  // Check if the handshake operation is the same as the one we meant to execute. The
+  // handshake could have been already processed by the handshakee and a new handshake
+  // by another JavaThread might be in progress.
+  if ( (is_direct && op != _operation_direct)) {
+    _processing_sem.signal();
     return false;
   }
 
   // If we own the semaphore at this point and while owning the semaphore
   // can observe a safe state the thread cannot possibly continue without
   // getting caught by the semaphore.
   bool executed = false;
-  if (vmthread_can_process_handshake(target)) {
-    guarantee(!_semaphore.trywait(), "we should already own the semaphore");
-    log_trace(handshake)("Processing handshake by VMThtread");
-    DEBUG_ONLY(_vmthread_processing_handshake = true;)
-    _operation->do_handshake(target);
-    DEBUG_ONLY(_vmthread_processing_handshake = false;)
-    // Disarm after VM thread have executed the operation.
-    clear_handshake(target);
+  if (can_process_handshake()) {
+    guarantee(!_processing_sem.trywait(), "we should already own the semaphore");
+    log_trace(handshake)("Processing handshake by %s", Thread::current()->is_VM_thread() ? "VMThread" : "Handshaker");
+    DEBUG_ONLY(_active_handshaker = Thread::current();)
+    op->do_handshake(_handshakee);
+    DEBUG_ONLY(_active_handshaker = NULL;)
+    // Disarm after we have executed the operation.
+    clear_handshake(is_direct);
     executed = true;
   }
 
   // Release the thread
-  _semaphore.signal();
+  _processing_sem.signal();
 
   return executed;
 }
diff a/src/hotspot/share/runtime/handshake.hpp b/src/hotspot/share/runtime/handshake.hpp
--- a/src/hotspot/share/runtime/handshake.hpp
+++ b/src/hotspot/share/runtime/handshake.hpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2017, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2017, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -28,17 +28,21 @@
 #include "memory/allocation.hpp"
 #include "memory/iterator.hpp"
 #include "runtime/flags/flagSetting.hpp"
 #include "runtime/semaphore.hpp"
 
+class HandshakeOperation;
 class JavaThread;
 
 // A handshake closure is a callback that is executed for each JavaThread
 // while that thread is in a safepoint safe state. The callback is executed
-// either by the thread itself or by the VM thread while keeping the thread
-// in a blocked state. A handshake can be performed with a single
-// JavaThread as well.
+// either by the target JavaThread itself or by the VMThread while keeping
+// the target thread in a blocked state. A handshake can be performed with a
+// single JavaThread as well. In that case, the callback is executed either
+// by the target JavaThread itself or, depending on whether the operation is
+// a direct handshake or not, by the JavaThread that requested the handshake
+// or the VMThread respectively.
 class HandshakeClosure : public ThreadClosure {
   const char* const _name;
  public:
   HandshakeClosure(const char* name) : _name(name) {}
   const char* name() const {
@@ -50,51 +54,55 @@
 class Handshake : public AllStatic {
  public:
   // Execution of handshake operation
   static void execute(HandshakeClosure* hs_cl);
   static bool execute(HandshakeClosure* hs_cl, JavaThread* target);
+  static bool execute_direct(HandshakeClosure* hs_cl, JavaThread* target);
 };
 
-class HandshakeOperation;
-
-// The HandshakeState keep tracks of an ongoing handshake for one JavaThread.
-// VM thread and JavaThread are serialized with the semaphore making sure
-// the operation is only done by either VM thread on behalf of the JavaThread
-// or the JavaThread itself.
+// The HandshakeState keeps track of an ongoing handshake for this JavaThread.
+// VMThread/Handshaker and JavaThread are serialized with semaphore _processing_sem
+// making sure the operation is only done by either VMThread/Handshaker on behalf
+// of the JavaThread or by the target JavaThread itself.
 class HandshakeState {
+  JavaThread* _handshakee;
   HandshakeOperation* volatile _operation;
+  HandshakeOperation* volatile _operation_direct;
 
-  Semaphore _semaphore;
+  Semaphore _handshake_turn_sem;  // Used to serialize direct handshakes for this JavaThread.
+  Semaphore _processing_sem;
   bool _thread_in_process_handshake;
 
-  bool claim_handshake_for_vmthread();
-  bool vmthread_can_process_handshake(JavaThread* target);
+  bool claim_handshake(bool is_direct);
+  bool possibly_can_process_handshake();
+  bool can_process_handshake();
+  void clear_handshake(bool is_direct);
 
-  void clear_handshake(JavaThread* thread);
+  void process_self_inner();
 
-  void process_self_inner(JavaThread* thread);
 public:
   HandshakeState();
 
-  void set_operation(JavaThread* thread, HandshakeOperation* op);
+  void set_thread(JavaThread* thread) { _handshakee = thread; }
 
-  bool has_operation() const {
-    return _operation != NULL;
+  void set_operation(HandshakeOperation* op);
+  bool has_operation() const { return _operation != NULL || _operation_direct != NULL; }
+  bool has_specific_operation(bool is_direct) const {
+    return is_direct ? _operation_direct != NULL : _operation != NULL;
   }
 
-  void process_by_self(JavaThread* thread) {
+  void process_by_self() {
     if (!_thread_in_process_handshake) {
       FlagSetting fs(_thread_in_process_handshake, true);
-      process_self_inner(thread);
+      process_self_inner();
     }
   }
-
-  bool try_process_by_vmThread(JavaThread* target);
+  bool try_process(HandshakeOperation* op);
 
 #ifdef ASSERT
-  bool _vmthread_processing_handshake;
-  bool is_vmthread_processing_handshake() const { return _vmthread_processing_handshake; }
+  Thread* _active_handshaker;
+  Thread* get_active_handshaker() const { return _active_handshaker; }
 #endif
 
 };
 
 #endif // SHARE_RUNTIME_HANDSHAKE_HPP
diff a/src/hotspot/share/runtime/mutexLocker.cpp b/src/hotspot/share/runtime/mutexLocker.cpp
--- a/src/hotspot/share/runtime/mutexLocker.cpp
+++ b/src/hotspot/share/runtime/mutexLocker.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -187,11 +187,11 @@
   if (lock->owned_by_self()) return;
   fatal("must own lock %s", lock->name());
 }
 
 void assert_locked_or_safepoint_or_handshake(const Mutex* lock, const JavaThread* thread) {
-  if (Thread::current()->is_VM_thread() && thread->is_vmthread_processing_handshake()) return;
+  if (Thread::current() == thread->get_active_handshaker()) return;
   assert_locked_or_safepoint(lock);
 }
 #endif
 
 #define def(var, type, pri, vm_block, safepoint_check_allowed ) {      \
diff a/src/hotspot/share/runtime/safepoint.cpp b/src/hotspot/share/runtime/safepoint.cpp
--- a/src/hotspot/share/runtime/safepoint.cpp
+++ b/src/hotspot/share/runtime/safepoint.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -492,12 +492,10 @@
       DEBUG_ONLY(current->reset_visited_for_critical_count(active_safepoint_counter);)
       ThreadSafepointState* cur_state = current->safepoint_state();
       assert(!cur_state->is_running(), "Thread not suspended at safepoint");
       cur_state->restart(); // TSS _running
       assert(cur_state->is_running(), "safepoint state has not been reset");
-
-      SafepointMechanism::disarm_if_needed(current, false /* NO release */);
     }
   } // ~JavaThreadIteratorWithHandle
 
   // Release threads lock, so threads can be created/destroyed again.
   Threads_lock->unlock();
@@ -734,11 +732,10 @@
     return false;
   }
 }
 
 bool SafepointSynchronize::handshake_safe(JavaThread *thread) {
-  assert(Thread::current()->is_VM_thread(), "Must be VMThread");
   if (thread->is_ext_suspended() || thread->is_terminated()) {
     return true;
   }
   JavaThreadState stable_state;
   if (try_stable_load_state(&stable_state, thread, InactiveSafepointCounter)) {
diff a/src/hotspot/share/runtime/safepointMechanism.inline.hpp b/src/hotspot/share/runtime/safepointMechanism.inline.hpp
--- a/src/hotspot/share/runtime/safepointMechanism.inline.hpp
+++ b/src/hotspot/share/runtime/safepointMechanism.inline.hpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2017, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2017, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -68,23 +68,10 @@
 
 void SafepointMechanism::disarm_local_poll(JavaThread* thread) {
   thread->set_polling_page(poll_disarmed_value());
 }
 
-void SafepointMechanism::disarm_if_needed(JavaThread* thread, bool memory_order_release) {
-  JavaThreadState jts = thread->thread_state();
-  if (jts == _thread_in_native || jts == _thread_in_native_trans) {
-    // JavaThread will disarm itself and execute cross_modify_fence() before continuing
-    return;
-  }
-  if (memory_order_release) {
-    thread->set_polling_page_release(poll_disarmed_value());
-  } else {
-    thread->set_polling_page(poll_disarmed_value());
-  }
-}
-
 void SafepointMechanism::arm_local_poll_release(JavaThread* thread) {
   thread->set_polling_page_release(poll_armed_value());
 }
 
 void SafepointMechanism::disarm_local_poll_release(JavaThread* thread) {
diff a/src/hotspot/share/runtime/stubRoutines.cpp b/src/hotspot/share/runtime/stubRoutines.cpp
--- a/src/hotspot/share/runtime/stubRoutines.cpp
+++ b/src/hotspot/share/runtime/stubRoutines.cpp
@@ -390,11 +390,11 @@
   test_arraycopy_func(CAST_FROM_FN_PTR(address, Copy::aligned_conjoint_words), sizeof(jlong));
   test_arraycopy_func(CAST_FROM_FN_PTR(address, Copy::aligned_disjoint_words), sizeof(jlong));
 
   // test safefetch routines
   // Not on Windows 32bit until 8074860 is fixed
-#if ! (defined(_WIN32) && defined(_M_IX86))
+#if ! (defined(_WIN32) && defined(_M_IX86)) && !defined(_M_ARM64)
   test_safefetch32();
   test_safefetchN();
 #endif
 
 #endif
diff a/src/hotspot/share/runtime/thread.cpp b/src/hotspot/share/runtime/thread.cpp
--- a/src/hotspot/share/runtime/thread.cpp
+++ b/src/hotspot/share/runtime/thread.cpp
@@ -1704,10 +1704,11 @@
   _cached_monitor_info = NULL;
   _parker = Parker::Allocate(this);
   _SleepEvent = ParkEvent::Allocate(this);
   // Setup safepoint state info for this thread
   ThreadSafepointState::create(this);
+  _handshake.set_thread(this);
 
   debug_only(_java_call_counter = 0);
 
   // JVMTI PopFrame support
   _popframe_condition = popframe_inactive;
diff a/src/hotspot/share/runtime/thread.hpp b/src/hotspot/share/runtime/thread.hpp
--- a/src/hotspot/share/runtime/thread.hpp
+++ b/src/hotspot/share/runtime/thread.hpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -1316,28 +1316,28 @@
  private:
   // Support for thread handshake operations
   HandshakeState _handshake;
  public:
   void set_handshake_operation(HandshakeOperation* op) {
-    _handshake.set_operation(this, op);
+    _handshake.set_operation(op);
   }
 
   bool has_handshake() const {
     return _handshake.has_operation();
   }
 
   void handshake_process_by_self() {
-    _handshake.process_by_self(this);
+    _handshake.process_by_self();
   }
 
-  bool handshake_try_process_by_vmThread() {
-    return _handshake.try_process_by_vmThread(this);
+  bool handshake_try_process(HandshakeOperation* op) {
+    return _handshake.try_process(op);
   }
 
 #ifdef ASSERT
-  bool is_vmthread_processing_handshake() const {
-    return _handshake.is_vmthread_processing_handshake();
+  Thread* get_active_handshaker() const {
+    return _handshake.get_active_handshaker();
   }
 #endif
 
   // Suspend/resume support for JavaThread
  private:
diff a/src/hotspot/share/runtime/vmStructs.cpp b/src/hotspot/share/runtime/vmStructs.cpp
--- a/src/hotspot/share/runtime/vmStructs.cpp
+++ b/src/hotspot/share/runtime/vmStructs.cpp
@@ -100,19 +100,19 @@
 #include "runtime/vmStructs.hpp"
 #include "utilities/globalDefinitions.hpp"
 #include "utilities/hashtable.hpp"
 #include "utilities/macros.hpp"
 
+
 #include CPU_HEADER(vmStructs)
 #include OS_HEADER(vmStructs)
 #include OS_CPU_HEADER(vmStructs)
 
 #if INCLUDE_JVMCI
 # include "jvmci/vmStructs_jvmci.hpp"
 #endif
 
-
 #ifdef COMPILER2
 #include "opto/addnode.hpp"
 #include "opto/block.hpp"
 #include "opto/callnode.hpp"
 #include "opto/castnode.hpp"
@@ -166,10 +166,12 @@
 typedef HashtableEntry<intptr_t, mtInternal>  IntptrHashtableEntry;
 typedef Hashtable<intptr_t, mtInternal>       IntptrHashtable;
 typedef Hashtable<InstanceKlass*, mtClass>       KlassHashtable;
 typedef HashtableEntry<InstanceKlass*, mtClass>  KlassHashtableEntry;
 
+typedef PaddedEnd<ObjectMonitor>              PaddedObjectMonitor;
+
 //--------------------------------------------------------------------------------
 // VM_STRUCTS
 //
 // This list enumerates all of the fields the serviceability agent
 // needs to know about. Be sure to see also the type table below this one.
diff a/src/hotspot/share/runtime/vm_version.hpp b/src/hotspot/share/runtime/vm_version.hpp
--- a/src/hotspot/share/runtime/vm_version.hpp
+++ b/src/hotspot/share/runtime/vm_version.hpp
@@ -23,9 +23,11 @@
  */
 
 #ifndef SHARE_RUNTIME_VM_VERSION_HPP
 #define SHARE_RUNTIME_VM_VERSION_HPP
 
+#include "memory/allocation.hpp"
+#include "utilities/ostream.hpp"
 #include "utilities/macros.hpp"  // for CPU_HEADER() macro.
 #include CPU_HEADER(vm_version)
 
 #endif // SHARE_RUNTIME_VM_VERSION_HPP
diff a/src/java.base/windows/native/libjava/java_props_md.c b/src/java.base/windows/native/libjava/java_props_md.c
--- a/src/java.base/windows/native/libjava/java_props_md.c
+++ b/src/java.base/windows/native/libjava/java_props_md.c
@@ -566,10 +566,12 @@
         sprops.os_version = _strdup(buf);
 #if defined(_M_AMD64)
         sprops.os_arch = "amd64";
 #elif defined(_X86_)
         sprops.os_arch = "x86";
+#elif defined(_M_ARM64)
+        sprops.os_arch = "aarch64";
 #else
         sprops.os_arch = "unknown";
 #endif
     }
 
diff a/src/java.desktop/windows/native/libawt/windows/awt_DCHolder.cpp b/src/java.desktop/windows/native/libawt/windows/awt_DCHolder.cpp
--- a/src/java.desktop/windows/native/libawt/windows/awt_DCHolder.cpp
+++ b/src/java.desktop/windows/native/libawt/windows/awt_DCHolder.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2009, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2009, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.  Oracle designates this
@@ -21,11 +21,10 @@
  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  * or visit www.oracle.com if you need additional information or have any
  * questions.
  */
 
-#include "awt.h"
 #include "awt_ole.h"
 #include "awt_DCHolder.h"       // main symbols
 
 
 ////////////////////////
diff a/src/java.desktop/windows/native/libawt/windows/awt_DnDDT.cpp b/src/java.desktop/windows/native/libawt/windows/awt_DnDDT.cpp
--- a/src/java.desktop/windows/native/libawt/windows/awt_DnDDT.cpp
+++ b/src/java.desktop/windows/native/libawt/windows/awt_DnDDT.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2014, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.  Oracle designates this
@@ -21,22 +21,20 @@
  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  * or visit www.oracle.com if you need additional information or have any
  * questions.
  */
 
-#include "awt.h"
 #include <shlwapi.h>
 #include <shellapi.h>
 #include <memory.h>
 
 #include "awt_DataTransferer.h"
-#include "awt_Toolkit.h"
 #include "java_awt_dnd_DnDConstants.h"
 #include "sun_awt_windows_WDropTargetContextPeer.h"
 #include "awt_Container.h"
-#include "alloc.h"
 #include "awt_ole.h"
+#include "awt_Toolkit.h"
 #include "awt_DnDDT.h"
 #include "awt_DnDDS.h"
 
 
 // forwards
diff a/src/java.desktop/windows/native/libawt/windows/awt_ole.h b/src/java.desktop/windows/native/libawt/windows/awt_ole.h
--- a/src/java.desktop/windows/native/libawt/windows/awt_ole.h
+++ b/src/java.desktop/windows/native/libawt/windows/awt_ole.h
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2009, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2009, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.  Oracle designates this
@@ -24,14 +24,14 @@
  */
 
 #ifndef AWT_OLE_H
 #define AWT_OLE_H
 
-#include "awt.h"
 #include <ole2.h>
 #include <comdef.h>
 #include <comutil.h>
+#include "awt.h"
 
 #ifdef _DEBUG
     #define _SUN_DEBUG
 #endif
 
diff a/src/jdk.attach/windows/classes/sun/tools/attach/AttachProviderImpl.java b/src/jdk.attach/windows/classes/sun/tools/attach/AttachProviderImpl.java
--- a/src/jdk.attach/windows/classes/sun/tools/attach/AttachProviderImpl.java
+++ b/src/jdk.attach/windows/classes/sun/tools/attach/AttachProviderImpl.java
@@ -41,11 +41,11 @@
         if (os.startsWith("Windows 9") || os.equals("Windows Me")) {
             throw new RuntimeException(
                 "This provider is not supported on this version of Windows");
         }
         String arch = System.getProperty("os.arch");
-        if (!arch.equals("x86") && !arch.equals("amd64")) {
+        if (!arch.equals("x86") && !arch.equals("amd64") && !arch.equals("aarch64")) {
             throw new RuntimeException(
                 "This provider is not supported on this processor architecture");
         }
     }
 
diff a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/HotSpotAgent.java b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/HotSpotAgent.java
--- a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/HotSpotAgent.java
+++ b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/HotSpotAgent.java
@@ -553,12 +553,14 @@
 
         if (cpu.equals("x86")) {
             machDesc = new MachineDescriptionIntelX86();
         } else if (cpu.equals("amd64")) {
             machDesc = new MachineDescriptionAMD64();
+        } else if (cpu.equals("aarch64")) {
+            machDesc = new MachineDescriptionAArch64();
         } else {
-            throw new DebuggerException("Win32 supported under x86 and amd64 only");
+            throw new DebuggerException("Win32 supported under x86, amd64 and aarch64 only");
         }
 
         // Note we do not use a cache for the local debugger in server
         // mode; it will be taken care of on the client side (once remote
         // debugging is implemented).
diff a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/asm/Disassembler.java b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/asm/Disassembler.java
--- a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/asm/Disassembler.java
+++ b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/asm/Disassembler.java
@@ -69,10 +69,12 @@
          if (os.lastIndexOf("Windows", 0) != -1) {
             if (arch.equals("x86")) {
                libname +=  "-i386";
             } else if (arch.equals("amd64")) {
                libname +=  "-amd64";
+            } else if (arch.equals("aarch64")) {
+               libname +=  "-arm64";
             } else {
                libname +=  "-" + arch;
             }
             path.append(sep + "bin" + sep);
             libname += ".dll";
diff a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/debugger/windbg/WindbgDebuggerLocal.java b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/debugger/windbg/WindbgDebuggerLocal.java
--- a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/debugger/windbg/WindbgDebuggerLocal.java
+++ b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/debugger/windbg/WindbgDebuggerLocal.java
@@ -26,12 +26,14 @@
 
 import java.io.*;
 import java.net.*;
 import java.util.*;
 import sun.jvm.hotspot.debugger.*;
+import sun.jvm.hotspot.debugger.aarch64.*;
 import sun.jvm.hotspot.debugger.amd64.*;
 import sun.jvm.hotspot.debugger.x86.*;
+import sun.jvm.hotspot.debugger.windbg.aarch64.*;
 import sun.jvm.hotspot.debugger.windbg.amd64.*;
 import sun.jvm.hotspot.debugger.windbg.x86.*;
 import sun.jvm.hotspot.debugger.win32.coff.*;
 import sun.jvm.hotspot.debugger.cdbg.*;
 import sun.jvm.hotspot.debugger.cdbg.basic.BasicDebugEvent;
@@ -110,11 +112,13 @@
 
     String cpu = PlatformInfo.getCPU();
     if (cpu.equals("x86")) {
        threadFactory = new WindbgX86ThreadFactory(this);
     } else if (cpu.equals("amd64")) {
-       threadFactory = new WindbgAMD64ThreadFactory(this);
+      threadFactory = new WindbgAMD64ThreadFactory(this);
+    } else if (cpu.equals("aarch64")) {
+      threadFactory = new WindbgAARCH64ThreadFactory(this);
     }
 
     if (useCache) {
       // Cache portion of the remote process's address space.
       // Fetching data over the socket connection to dbx is slow.
diff a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/debugger/windbg/aarch64/WindbgAARCH64Thread.java b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/debugger/windbg/aarch64/WindbgAARCH64Thread.java
--- /dev/null
+++ b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/debugger/windbg/aarch64/WindbgAARCH64Thread.java
@@ -0,0 +1,93 @@
+/*
+ * Copyright (c) 2005, 2013, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+package sun.jvm.hotspot.debugger.windbg.aarch64;
+
+import sun.jvm.hotspot.debugger.*;
+import sun.jvm.hotspot.debugger.aarch64.*;
+import sun.jvm.hotspot.debugger.windbg.*;
+
+class WindbgAARCH64Thread implements ThreadProxy {
+  private WindbgDebugger debugger;
+  private long           sysId;
+  private boolean        gotID;
+  private long           id;
+
+  // The address argument must be the address of the OSThread::_thread_id
+  WindbgAARCH64Thread(WindbgDebugger debugger, Address addr) {
+    this.debugger = debugger;
+    this.sysId    = (long)addr.getCIntegerAt(0, 4, true);
+    gotID         = false;
+  }
+
+  WindbgAARCH64Thread(WindbgDebugger debugger, long sysId) {
+    this.debugger = debugger;
+    this.sysId    = sysId;
+    gotID         = false;
+  }
+
+  public ThreadContext getContext() throws IllegalThreadStateException {
+    long[] data = debugger.getThreadIntegerRegisterSet(getThreadID());
+    WindbgAARCH64ThreadContext context = new WindbgAARCH64ThreadContext(debugger);
+    for (int i = 0; i < data.length; i++) {
+      context.setRegister(i, data[i]);
+    }
+    return context;
+  }
+
+  public boolean canSetContext() throws DebuggerException {
+    return false;
+  }
+
+  public void setContext(ThreadContext thrCtx)
+    throws IllegalThreadStateException, DebuggerException {
+    throw new DebuggerException("Unimplemented");
+  }
+
+  public boolean equals(Object obj) {
+    if ((obj == null) || !(obj instanceof WindbgAARCH64Thread)) {
+      return false;
+    }
+
+    return (((WindbgAARCH64Thread) obj).getThreadID() == getThreadID());
+  }
+
+  public int hashCode() {
+    return (int) getThreadID();
+  }
+
+  public String toString() {
+    return Long.toString(getThreadID());
+  }
+
+  /** Retrieves the thread ID of this thread by examining the Thread
+      Information Block. */
+  private long getThreadID() {
+    if (!gotID) {
+       id = debugger.getThreadIdFromSysId(sysId);
+    }
+
+    return id;
+  }
+}
diff a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/debugger/windbg/aarch64/WindbgAARCH64ThreadContext.java b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/debugger/windbg/aarch64/WindbgAARCH64ThreadContext.java
--- /dev/null
+++ b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/debugger/windbg/aarch64/WindbgAARCH64ThreadContext.java
@@ -0,0 +1,46 @@
+/*
+ * Copyright (c) 2005, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+package sun.jvm.hotspot.debugger.windbg.aarch64;
+
+import sun.jvm.hotspot.debugger.*;
+import sun.jvm.hotspot.debugger.aarch64.*;
+import sun.jvm.hotspot.debugger.windbg.*;
+
+class WindbgAARCH64ThreadContext extends AARCH64ThreadContext {
+  private WindbgDebugger debugger;
+
+  public WindbgAARCH64ThreadContext(WindbgDebugger debugger) {
+    super();
+    this.debugger = debugger;
+  }
+
+  public void setRegisterAsAddress(int index, Address value) {
+    setRegister(index, debugger.getAddressValue(value));
+  }
+
+  public Address getRegisterAsAddress(int index) {
+    return debugger.newAddress(getRegister(index));
+  }
+}
diff a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/debugger/windbg/aarch64/WindbgAARCH64ThreadFactory.java b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/debugger/windbg/aarch64/WindbgAARCH64ThreadFactory.java
--- /dev/null
+++ b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/debugger/windbg/aarch64/WindbgAARCH64ThreadFactory.java
@@ -0,0 +1,44 @@
+/*
+ * Copyright (c) 2005, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+package sun.jvm.hotspot.debugger.windbg.aarch64;
+
+import sun.jvm.hotspot.debugger.*;
+import sun.jvm.hotspot.debugger.windbg.*;
+
+public class WindbgAARCH64ThreadFactory implements WindbgThreadFactory {
+  private WindbgDebugger debugger;
+
+  public WindbgAARCH64ThreadFactory(WindbgDebugger debugger) {
+    this.debugger = debugger;
+  }
+
+  public ThreadProxy createThreadWrapper(Address threadIdentifierAddr) {
+    return new WindbgAARCH64Thread(debugger, threadIdentifierAddr);
+  }
+
+  public ThreadProxy createThreadWrapper(long id) {
+    return new WindbgAARCH64Thread(debugger, id);
+  }
+}
diff a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/runtime/Threads.java b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/runtime/Threads.java
--- a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/runtime/Threads.java
+++ b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/runtime/Threads.java
@@ -29,10 +29,11 @@
 import sun.jvm.hotspot.debugger.*;
 import sun.jvm.hotspot.types.*;
 import sun.jvm.hotspot.runtime.solaris_sparc.SolarisSPARCJavaThreadPDAccess;
 import sun.jvm.hotspot.runtime.solaris_x86.SolarisX86JavaThreadPDAccess;
 import sun.jvm.hotspot.runtime.solaris_amd64.SolarisAMD64JavaThreadPDAccess;
+import sun.jvm.hotspot.runtime.win32_aarch64.Win32AARCH64JavaThreadPDAccess;
 import sun.jvm.hotspot.runtime.win32_amd64.Win32AMD64JavaThreadPDAccess;
 import sun.jvm.hotspot.runtime.win32_x86.Win32X86JavaThreadPDAccess;
 import sun.jvm.hotspot.runtime.linux_x86.LinuxX86JavaThreadPDAccess;
 import sun.jvm.hotspot.runtime.linux_amd64.LinuxAMD64JavaThreadPDAccess;
 import sun.jvm.hotspot.runtime.linux_aarch64.LinuxAARCH64JavaThreadPDAccess;
@@ -107,10 +108,12 @@
         } else if (os.equals("win32")) {
             if (cpu.equals("x86")) {
                 access =  new Win32X86JavaThreadPDAccess();
             } else if (cpu.equals("amd64")) {
                 access =  new Win32AMD64JavaThreadPDAccess();
+            } else if (cpu.equals("aarch64")) {
+                access =  new Win32AARCH64JavaThreadPDAccess();
             }
         } else if (os.equals("linux")) {
             if (cpu.equals("x86")) {
                 access = new LinuxX86JavaThreadPDAccess();
             } else if (cpu.equals("amd64")) {
diff a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/runtime/win32_aarch64/Win32AARCH64JavaThreadPDAccess.java b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/runtime/win32_aarch64/Win32AARCH64JavaThreadPDAccess.java
--- /dev/null
+++ b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/runtime/win32_aarch64/Win32AARCH64JavaThreadPDAccess.java
@@ -0,0 +1,137 @@
+/*
+ * Copyright (c) 2005, 2017, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+package sun.jvm.hotspot.runtime.win32_aarch64;
+
+import java.io.*;
+import java.util.*;
+import sun.jvm.hotspot.debugger.*;
+import sun.jvm.hotspot.debugger.aarch64.*;
+import sun.jvm.hotspot.runtime.*;
+import sun.jvm.hotspot.runtime.aarch64.*;
+import sun.jvm.hotspot.types.*;
+import sun.jvm.hotspot.utilities.*;
+
+/** This class is only public to allow using the VMObjectFactory to
+    instantiate these.
+*/
+
+public class Win32AARCH64JavaThreadPDAccess implements JavaThreadPDAccess {
+  private static AddressField  lastJavaFPField;
+  private static AddressField  osThreadField;
+
+  // Field from OSThread
+  private static Field         osThreadThreadIDField;
+
+  // This is currently unneeded but is being kept in case we change
+  // the currentFrameGuess algorithm
+  private static final long GUESS_SCAN_RANGE = 128 * 1024;
+
+  static {
+    VM.registerVMInitializedObserver(new Observer() {
+        public void update(Observable o, Object data) {
+          initialize(VM.getVM().getTypeDataBase());
+        }
+      });
+  }
+
+  private static synchronized void initialize(TypeDataBase db) {
+    Type type = db.lookupType("JavaThread");
+    osThreadField           = type.getAddressField("_osthread");
+
+    Type anchorType = db.lookupType("JavaFrameAnchor");
+    lastJavaFPField         = anchorType.getAddressField("_last_Java_fp");
+
+    Type osThreadType = db.lookupType("OSThread");
+    osThreadThreadIDField = osThreadType.getField("_thread_id");
+  }
+
+  public Address getLastJavaFP(Address addr) {
+    return lastJavaFPField.getValue(addr.addOffsetTo(sun.jvm.hotspot.runtime.JavaThread.getAnchorField().getOffset()));
+  }
+
+  public Address getLastJavaPC(Address addr) {
+    return null;
+  }
+
+  public Address getBaseOfStackPointer(Address addr) {
+    return null;
+  }
+
+  public Frame getLastFramePD(JavaThread thread, Address addr) {
+    Address fp = thread.getLastJavaFP();
+    if (fp == null) {
+      return null; // no information
+    }
+    Address pc =  thread.getLastJavaPC();
+    if ( pc != null ) {
+      return new AARCH64Frame(thread.getLastJavaSP(), fp, pc);
+    } else {
+      return new AARCH64Frame(thread.getLastJavaSP(), fp);
+    }
+  }
+
+  public RegisterMap newRegisterMap(JavaThread thread, boolean updateMap) {
+    return new AARCH64RegisterMap(thread, updateMap);
+  }
+
+  public Frame getCurrentFrameGuess(JavaThread thread, Address addr) {
+    ThreadProxy t = getThreadProxy(addr);
+    AARCH64ThreadContext context = (AARCH64ThreadContext) t.getContext();
+    AARCH64CurrentFrameGuess guesser = new AARCH64CurrentFrameGuess(context, thread);
+    if (!guesser.run(GUESS_SCAN_RANGE)) {
+      return null;
+    }
+    if (guesser.getPC() == null) {
+      return new AARCH64Frame(guesser.getSP(), guesser.getFP());
+    } else {
+      return new AARCH64Frame(guesser.getSP(), guesser.getFP(), guesser.getPC());
+    }
+  }
+
+  public void printThreadIDOn(Address addr, PrintStream tty) {
+    tty.print(getThreadProxy(addr));
+  }
+
+  public void printInfoOn(Address threadAddr, PrintStream tty) {
+  }
+
+  public Address getLastSP(Address addr) {
+    ThreadProxy t = getThreadProxy(addr);
+    AARCH64ThreadContext context = (AARCH64ThreadContext) t.getContext();
+    return context.getRegisterAsAddress(AARCH64ThreadContext.SP);
+  }
+
+  public ThreadProxy getThreadProxy(Address addr) {
+    // Addr is the address of the JavaThread.
+    // Fetch the OSThread (for now and for simplicity, not making a
+    // separate "OSThread" class in this package)
+    Address osThreadAddr = osThreadField.getValue(addr);
+    // Get the address of the thread_id within the OSThread
+    Address threadIdAddr = osThreadAddr.addOffsetTo(osThreadThreadIDField.getOffset());
+
+    JVMDebugger debugger = VM.getVM().getDebugger();
+    return debugger.getThreadForIdentifierAddress(threadIdAddr);
+  }
+}
diff a/src/jdk.hotspot.agent/windows/native/libsaproc/sawindbg.cpp b/src/jdk.hotspot.agent/windows/native/libsaproc/sawindbg.cpp
--- a/src/jdk.hotspot.agent/windows/native/libsaproc/sawindbg.cpp
+++ b/src/jdk.hotspot.agent/windows/native/libsaproc/sawindbg.cpp
@@ -34,10 +34,13 @@
   #include "sun_jvm_hotspot_debugger_x86_X86ThreadContext.h"
   #define NPRGREG sun_jvm_hotspot_debugger_x86_X86ThreadContext_NPRGREG
 #elif _M_AMD64
   #include "sun_jvm_hotspot_debugger_amd64_AMD64ThreadContext.h"
   #define NPRGREG sun_jvm_hotspot_debugger_amd64_AMD64ThreadContext_NPRGREG
+#elif _M_ARM64
+  #include "sun_jvm_hotspot_debugger_aarch64_AARCH64ThreadContext.h"
+  #define NPRGREG sun_jvm_hotspot_debugger_aarch64_AARCH64ThreadContext_NPRGREG
 #else
   #error "SA windbg back-end is not supported for your cpu!"
 #endif
 
 #include <limits.h>
diff a/test/hotspot/jtreg/runtime/handshake/HandshakeDirectTest.java b/test/hotspot/jtreg/runtime/handshake/HandshakeDirectTest.java
--- /dev/null
+++ b/test/hotspot/jtreg/runtime/handshake/HandshakeDirectTest.java
@@ -0,0 +1,130 @@
+/*
+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+/*
+ * @test HandshakeDirectTest
+ * @summary This test tries to stress direct handshakes between threads while suspending them.
+ * @library /testlibrary /test/lib
+ * @build HandshakeDirectTest
+ * @run main/othervm -XX:+UnlockDiagnosticVMOptions -XX:+SafepointALot -XX:BiasedLockingDecayTime=100000000 -XX:BiasedLockingBulkRebiasThreshold=1000000 -XX:BiasedLockingBulkRevokeThreshold=1000000 HandshakeDirectTest
+ * @run main/othervm -XX:+UnlockDiagnosticVMOptions -XX:GuaranteedSafepointInterval=10 -XX:+HandshakeALot -XX:+SafepointALot -XX:BiasedLockingDecayTime=100000000 -XX:BiasedLockingBulkRebiasThreshold=1000000 -XX:BiasedLockingBulkRevokeThreshold=1000000 HandshakeDirectTest
+ */
+
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.ThreadLocalRandom;
+import java.util.concurrent.Semaphore;
+import java.io.*;
+
+public class HandshakeDirectTest  implements Runnable {
+    static final int WORKING_THREADS = 32;
+    static final int DIRECT_HANDSHAKES_MARK = 50000;
+    static Thread[] workingThreads = new Thread[WORKING_THREADS];
+    static Semaphore[] handshakeSem = new Semaphore[WORKING_THREADS];
+    static Object[] locks = new Object[WORKING_THREADS];
+    static boolean[] isBiased = new boolean[WORKING_THREADS];
+    static AtomicInteger handshakeCount = new AtomicInteger(0);
+
+    @Override
+    public void run() {
+        int me = Integer.parseInt(Thread.currentThread().getName());
+
+        while (true) {
+            try {
+                if (!isBiased[me]) {
+                    handshakeSem[me].acquire();
+                    synchronized(locks[me]) {
+                        isBiased[me] = true;
+                    }
+                    handshakeSem[me].release();
+                }
+
+                // Handshake directly some other worker
+                int handshakee = ThreadLocalRandom.current().nextInt(0, WORKING_THREADS-1);
+                if (handshakee == me) {
+                    handshakee = handshakee != 0 ? handshakee - 1 : handshakee + 1;
+                }
+                handshakeSem[handshakee].acquire();
+                if (isBiased[handshakee]) {
+                    // Revoke biased lock
+                    synchronized(locks[handshakee]) {
+                        handshakeCount.incrementAndGet();
+                    }
+                    // Create new lock to be biased
+                    locks[handshakee] = new Object();
+                    isBiased[handshakee] = false;
+                }
+                handshakeSem[handshakee].release();
+                if (handshakeCount.get() >= DIRECT_HANDSHAKES_MARK) {
+                    break;
+                }
+            } catch(InterruptedException ie) {
+                throw new Error("Unexpected interrupt");
+            }
+        }
+    }
+
+    public static void main(String... args) throws Exception {
+        HandshakeDirectTest test = new HandshakeDirectTest();
+
+        // Initialize semaphores
+        for (int i = 0; i < WORKING_THREADS; i++) {
+            handshakeSem[i] = new Semaphore(1);
+        }
+
+        // Initialize locks
+        for (int i = 0; i < WORKING_THREADS; i++) {
+            locks[i] = new Object();
+        }
+
+        // Fire-up working threads.
+        for (int i = 0; i < WORKING_THREADS; i++) {
+            workingThreads[i] = new Thread(test, Integer.toString(i));
+            workingThreads[i].start();
+        }
+
+        // Fire-up suspend-resume thread
+        Thread suspendResumeThread = new Thread() {
+            @Override
+            public void run() {
+                while (true) {
+                    int i = ThreadLocalRandom.current().nextInt(0, WORKING_THREADS-1);
+                    workingThreads[i].suspend();
+                    try {
+                        Thread.sleep(1); // sleep for 1 ms
+                    } catch(InterruptedException ie) {
+                    }
+                    workingThreads[i].resume();
+                }
+            }
+        };
+        suspendResumeThread.setDaemon(true);
+        suspendResumeThread.start();
+
+        // Wait until the desired number of direct handshakes is reached
+        // and check that all workers exited
+        for (int i = 0; i < WORKING_THREADS; i++) {
+            workingThreads[i].join();
+        }
+    }
+}
