<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src\hotspot\cpu\aarch64\macroAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="..\..\..\..\style.css" />
  </head>
<body>
<center><a href="jvmciCodeInstaller_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="..\..\..\..\index.html" target="_top">index</a> <a href="macroAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src\hotspot\cpu\aarch64\macroAssembler_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  54 #include &quot;oops/oop.hpp&quot;
  55 #include &quot;opto/compile.hpp&quot;
  56 #include &quot;opto/intrinsicnode.hpp&quot;
  57 #include &quot;opto/node.hpp&quot;
  58 #endif
  59 
  60 #ifdef PRODUCT
  61 #define BLOCK_COMMENT(str) /* nothing */
  62 #define STOP(error) stop(error)
  63 #else
  64 #define BLOCK_COMMENT(str) block_comment(str)
  65 #define STOP(error) block_comment(error); stop(error)
  66 #endif
  67 
  68 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  69 
  70 // Patch any kind of instruction; there may be several instructions.
  71 // Return the total length (in bytes) of the instructions.
  72 int MacroAssembler::pd_patch_instruction_size(address branch, address target) {
  73   int instructions = 1;
<span class="line-modified">  74   assert((uint64_t)target &lt; (1ul &lt;&lt; 48), &quot;48-bit overflow in address constant&quot;);</span>
<span class="line-modified">  75   long offset = (target - branch) &gt;&gt; 2;</span>
  76   unsigned insn = *(unsigned*)branch;
  77   if ((Instruction_aarch64::extract(insn, 29, 24) &amp; 0b111011) == 0b011000) {
  78     // Load register (literal)
  79     Instruction_aarch64::spatch(branch, 23, 5, offset);
  80   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
  81     // Unconditional branch (immediate)
  82     Instruction_aarch64::spatch(branch, 25, 0, offset);
  83   } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {
  84     // Conditional branch (immediate)
  85     Instruction_aarch64::spatch(branch, 23, 5, offset);
  86   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {
  87     // Compare &amp; branch (immediate)
  88     Instruction_aarch64::spatch(branch, 23, 5, offset);
  89   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {
  90     // Test &amp; branch (immediate)
  91     Instruction_aarch64::spatch(branch, 18, 5, offset);
  92   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
  93     // PC-rel. addressing
  94     offset = target-branch;
  95     int shift = Instruction_aarch64::extract(insn, 31, 31);
  96     if (shift) {
<span class="line-modified">  97       u_int64_t dest = (u_int64_t)target;</span>
  98       uint64_t pc_page = (uint64_t)branch &gt;&gt; 12;
  99       uint64_t adr_page = (uint64_t)target &gt;&gt; 12;
 100       unsigned offset_lo = dest &amp; 0xfff;
 101       offset = adr_page - pc_page;
 102 
 103       // We handle 4 types of PC relative addressing
 104       //   1 - adrp    Rx, target_page
 105       //       ldr/str Ry, [Rx, #offset_in_page]
 106       //   2 - adrp    Rx, target_page
 107       //       add     Ry, Rx, #offset_in_page
 108       //   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 109       //       movk    Rx, #imm16&lt;&lt;32
 110       //   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 111       // In the first 3 cases we must check that Rx is the same in the adrp and the
 112       // subsequent ldr/str, add or movk instruction. Otherwise we could accidentally end
 113       // up treating a type 4 relocation as a type 1, 2 or 3 just because it happened
 114       // to be followed by a random unrelated ldr/str, add or movk instruction.
 115       //
 116       unsigned insn2 = ((unsigned*)branch)[1];
 117       if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &amp;&amp;
 118                 Instruction_aarch64::extract(insn, 4, 0) ==
 119                         Instruction_aarch64::extract(insn2, 9, 5)) {
 120         // Load/store register (unsigned immediate)
 121         unsigned size = Instruction_aarch64::extract(insn2, 31, 30);
 122         Instruction_aarch64::patch(branch + sizeof (unsigned),
 123                                     21, 10, offset_lo &gt;&gt; size);
 124         guarantee(((dest &gt;&gt; size) &lt;&lt; size) == dest, &quot;misaligned target&quot;);
 125         instructions = 2;
 126       } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &amp;&amp;
 127                 Instruction_aarch64::extract(insn, 4, 0) ==
 128                         Instruction_aarch64::extract(insn2, 4, 0)) {
 129         // add (immediate)
 130         Instruction_aarch64::patch(branch + sizeof (unsigned),
 131                                    21, 10, offset_lo);
 132         instructions = 2;
 133       } else if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110 &amp;&amp;
 134                    Instruction_aarch64::extract(insn, 4, 0) ==
 135                      Instruction_aarch64::extract(insn2, 4, 0)) {
 136         // movk #imm16&lt;&lt;32
 137         Instruction_aarch64::patch(branch + 4, 20, 5, (uint64_t)target &gt;&gt; 32);
<span class="line-modified"> 138         long dest = ((long)target &amp; 0xffffffffL) | ((long)branch &amp; 0xffff00000000L);</span>
<span class="line-modified"> 139         long pc_page = (long)branch &gt;&gt; 12;</span>
<span class="line-modified"> 140         long adr_page = (long)dest &gt;&gt; 12;</span>
 141         offset = adr_page - pc_page;
 142         instructions = 2;
 143       }
 144     }
 145     int offset_lo = offset &amp; 3;
 146     offset &gt;&gt;= 2;
 147     Instruction_aarch64::spatch(branch, 23, 5, offset);
 148     Instruction_aarch64::patch(branch, 30, 29, offset_lo);
 149   } else if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010100) {
<span class="line-modified"> 150     u_int64_t dest = (u_int64_t)target;</span>
 151     // Move wide constant
 152     assert(nativeInstruction_at(branch+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 153     assert(nativeInstruction_at(branch+8)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 154     Instruction_aarch64::patch(branch, 20, 5, dest &amp; 0xffff);
 155     Instruction_aarch64::patch(branch+4, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 156     Instruction_aarch64::patch(branch+8, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 157     assert(target_addr_for_insn(branch) == target, &quot;should be&quot;);
 158     instructions = 3;
 159   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 160              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 161     // nothing to do
 162     assert(target == 0, &quot;did not expect to relocate target for polling page load&quot;);
 163   } else {
 164     ShouldNotReachHere();
 165   }
 166   return instructions * NativeInstruction::instruction_size;
 167 }
 168 
 169 int MacroAssembler::patch_oop(address insn_addr, address o) {
 170   int instructions;
</pre>
<hr />
<pre>
 189     Instruction_aarch64::patch(insn_addr+8, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 190     instructions = 3;
 191   }
 192   return instructions * NativeInstruction::instruction_size;
 193 }
 194 
 195 int MacroAssembler::patch_narrow_klass(address insn_addr, narrowKlass n) {
 196   // Metatdata pointers are either narrow (32 bits) or wide (48 bits).
 197   // We encode narrow ones by setting the upper 16 bits in the first
 198   // instruction.
 199   NativeInstruction *insn = nativeInstruction_at(insn_addr);
 200   assert(Instruction_aarch64::extract(insn-&gt;encoding(), 31, 21) == 0b11010010101 &amp;&amp;
 201          nativeInstruction_at(insn_addr+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 202 
 203   Instruction_aarch64::patch(insn_addr, 20, 5, n &gt;&gt; 16);
 204   Instruction_aarch64::patch(insn_addr+4, 20, 5, n &amp; 0xffff);
 205   return 2 * NativeInstruction::instruction_size;
 206 }
 207 
 208 address MacroAssembler::target_addr_for_insn(address insn_addr, unsigned insn) {
<span class="line-modified"> 209   long offset = 0;</span>
 210   if ((Instruction_aarch64::extract(insn, 29, 24) &amp; 0b011011) == 0b00011000) {
 211     // Load register (literal)
 212     offset = Instruction_aarch64::sextract(insn, 23, 5);
 213     return address(((uint64_t)insn_addr + (offset &lt;&lt; 2)));
 214   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
 215     // Unconditional branch (immediate)
 216     offset = Instruction_aarch64::sextract(insn, 25, 0);
 217   } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {
 218     // Conditional branch (immediate)
 219     offset = Instruction_aarch64::sextract(insn, 23, 5);
 220   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {
 221     // Compare &amp; branch (immediate)
 222     offset = Instruction_aarch64::sextract(insn, 23, 5);
 223    } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {
 224     // Test &amp; branch (immediate)
 225     offset = Instruction_aarch64::sextract(insn, 18, 5);
 226   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
 227     // PC-rel. addressing
 228     offset = Instruction_aarch64::extract(insn, 30, 29);
 229     offset |= Instruction_aarch64::sextract(insn, 23, 5) &lt;&lt; 2;
</pre>
<hr />
<pre>
 256         return address(target_page + (byte_offset &lt;&lt; size));
 257       } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &amp;&amp;
 258                 Instruction_aarch64::extract(insn, 4, 0) ==
 259                         Instruction_aarch64::extract(insn2, 4, 0)) {
 260         // add (immediate)
 261         unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);
 262         return address(target_page + byte_offset);
 263       } else {
 264         if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110  &amp;&amp;
 265                Instruction_aarch64::extract(insn, 4, 0) ==
 266                  Instruction_aarch64::extract(insn2, 4, 0)) {
 267           target_page = (target_page &amp; 0xffffffff) |
 268                          ((uint64_t)Instruction_aarch64::extract(insn2, 20, 5) &lt;&lt; 32);
 269         }
 270         return (address)target_page;
 271       }
 272     } else {
 273       ShouldNotReachHere();
 274     }
 275   } else if (Instruction_aarch64::extract(insn, 31, 23) == 0b110100101) {
<span class="line-modified"> 276     u_int32_t *insns = (u_int32_t *)insn_addr;</span>
 277     // Move wide constant: movz, movk, movk.  See movptr().
 278     assert(nativeInstruction_at(insns+1)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 279     assert(nativeInstruction_at(insns+2)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
<span class="line-modified"> 280     return address(u_int64_t(Instruction_aarch64::extract(insns[0], 20, 5))</span>
<span class="line-modified"> 281                    + (u_int64_t(Instruction_aarch64::extract(insns[1], 20, 5)) &lt;&lt; 16)</span>
<span class="line-modified"> 282                    + (u_int64_t(Instruction_aarch64::extract(insns[2], 20, 5)) &lt;&lt; 32));</span>
 283   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 284              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 285     return 0;
 286   } else {
 287     ShouldNotReachHere();
 288   }
 289   return address(((uint64_t)insn_addr + (offset &lt;&lt; 2)));
 290 }
 291 
 292 void MacroAssembler::safepoint_poll(Label&amp; slow_path) {
 293   if (SafepointMechanism::uses_thread_local_poll()) {
 294     ldr(rscratch1, Address(rthread, Thread::polling_page_offset()));
 295     tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);
 296   } else {
<span class="line-modified"> 297     unsigned long offset;</span>
 298     adrp(rscratch1, ExternalAddress(SafepointSynchronize::address_of_state()), offset);
 299     ldrw(rscratch1, Address(rscratch1, offset));
 300     assert(SafepointSynchronize::_not_synchronized == 0, &quot;rewrite this code&quot;);
 301     cbnz(rscratch1, slow_path);
 302   }
 303 }
 304 
 305 // Just like safepoint_poll, but use an acquiring load for thread-
 306 // local polling.
 307 //
 308 // We need an acquire here to ensure that any subsequent load of the
 309 // global SafepointSynchronize::_state flag is ordered after this load
 310 // of the local Thread::_polling page.  We don&#39;t want this poll to
 311 // return false (i.e. not safepointing) and a later poll of the global
 312 // SafepointSynchronize::_state spuriously to return true.
 313 //
 314 // This is to avoid a race when we&#39;re in a native-&gt;Java transition
 315 // racing the code which wakes up from a safepoint.
 316 //
 317 void MacroAssembler::safepoint_poll_acquire(Label&amp; slow_path) {
</pre>
<hr />
<pre>
 385 }
 386 
 387 void MacroAssembler::set_last_Java_frame(Register last_java_sp,
 388                                          Register last_java_fp,
 389                                          Label &amp;L,
 390                                          Register scratch) {
 391   if (L.is_bound()) {
 392     set_last_Java_frame(last_java_sp, last_java_fp, target(L), scratch);
 393   } else {
 394     InstructionMark im(this);
 395     L.add_patch_at(code(), locator());
 396     set_last_Java_frame(last_java_sp, last_java_fp, pc() /* Patched later */, scratch);
 397   }
 398 }
 399 
 400 void MacroAssembler::far_call(Address entry, CodeBuffer *cbuf, Register tmp) {
 401   assert(ReservedCodeCacheSize &lt; 4*G, &quot;branch out of range&quot;);
 402   assert(CodeCache::find_blob(entry.target()) != NULL,
 403          &quot;destination of far call not found in code cache&quot;);
 404   if (far_branches()) {
<span class="line-modified"> 405     unsigned long offset;</span>
 406     // We can use ADRP here because we know that the total size of
 407     // the code cache cannot exceed 2Gb.
 408     adrp(tmp, entry, offset);
 409     add(tmp, tmp, offset);
 410     if (cbuf) cbuf-&gt;set_insts_mark();
 411     blr(tmp);
 412   } else {
 413     if (cbuf) cbuf-&gt;set_insts_mark();
 414     bl(entry);
 415   }
 416 }
 417 
 418 void MacroAssembler::far_jump(Address entry, CodeBuffer *cbuf, Register tmp) {
 419   assert(ReservedCodeCacheSize &lt; 4*G, &quot;branch out of range&quot;);
 420   assert(CodeCache::find_blob(entry.target()) != NULL,
 421          &quot;destination of far call not found in code cache&quot;);
 422   if (far_branches()) {
<span class="line-modified"> 423     unsigned long offset;</span>
 424     // We can use ADRP here because we know that the total size of
 425     // the code cache cannot exceed 2Gb.
 426     adrp(tmp, entry, offset);
 427     add(tmp, tmp, offset);
 428     if (cbuf) cbuf-&gt;set_insts_mark();
 429     br(tmp);
 430   } else {
 431     if (cbuf) cbuf-&gt;set_insts_mark();
 432     b(entry);
 433   }
 434 }
 435 
 436 void MacroAssembler::reserved_stack_check() {
 437     // testing if reserved zone needs to be enabled
 438     Label no_reserved_zone_enabling;
 439 
 440     ldr(rscratch1, Address(rthread, JavaThread::reserved_stack_activation_offset()));
 441     cmp(sp, rscratch1);
 442     br(Assembler::LO, no_reserved_zone_enabling);
 443 
</pre>
<hr />
<pre>
1385   lea(rscratch2, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
1386   ldr(rscratch2, Address(rscratch2));
1387   blr(rscratch2);
1388 
1389   ldp(rscratch2, lr, Address(post(sp, 2 * wordSize)));
1390   ldp(r0, rscratch1, Address(post(sp, 2 * wordSize)));
1391 
1392   BLOCK_COMMENT(&quot;} verify_oop_addr&quot;);
1393 }
1394 
1395 Address MacroAssembler::argument_address(RegisterOrConstant arg_slot,
1396                                          int extra_slot_offset) {
1397   // cf. TemplateTable::prepare_invoke(), if (load_receiver).
1398   int stackElementSize = Interpreter::stackElementSize;
1399   int offset = Interpreter::expr_offset_in_bytes(extra_slot_offset+0);
1400 #ifdef ASSERT
1401   int offset1 = Interpreter::expr_offset_in_bytes(extra_slot_offset+1);
1402   assert(offset1 - offset == stackElementSize, &quot;correct arithmetic&quot;);
1403 #endif
1404   if (arg_slot.is_constant()) {
<span class="line-modified">1405     return Address(esp, arg_slot.as_constant() * stackElementSize</span>
<span class="line-modified">1406                    + offset);</span>
1407   } else {
1408     add(rscratch1, esp, arg_slot.as_register(),
1409         ext::uxtx, exact_log2(stackElementSize));
1410     return Address(rscratch1, offset);
1411   }
1412 }
1413 
1414 void MacroAssembler::call_VM_leaf_base(address entry_point,
1415                                        int number_of_arguments,
1416                                        Label *retaddr) {
1417   Label E, L;
1418 
1419   stp(rscratch1, rmethod, Address(pre(sp, -2 * wordSize)));
1420 
1421   mov(rscratch1, entry_point);
1422   blr(rscratch1);
1423   if (retaddr)
1424     bind(*retaddr);
1425 
1426   ldp(rscratch1, rmethod, Address(post(sp, 2 * wordSize)));
</pre>
<hr />
<pre>
1487   MacroAssembler::call_VM_leaf_base(entry_point, 4);
1488 }
1489 
1490 void MacroAssembler::null_check(Register reg, int offset) {
1491   if (needs_explicit_null_check(offset)) {
1492     // provoke OS NULL exception if reg = NULL by
1493     // accessing M[reg] w/o changing any registers
1494     // NOTE: this is plenty to provoke a segv
1495     ldr(zr, Address(reg));
1496   } else {
1497     // nothing to do, (later) access of M[reg + offset]
1498     // will provoke OS NULL exception if reg = NULL
1499   }
1500 }
1501 
1502 // MacroAssembler protected routines needed to implement
1503 // public methods
1504 
1505 void MacroAssembler::mov(Register r, Address dest) {
1506   code_section()-&gt;relocate(pc(), dest.rspec());
<span class="line-modified">1507   u_int64_t imm64 = (u_int64_t)dest.target();</span>
1508   movptr(r, imm64);
1509 }
1510 
1511 // Move a constant pointer into r.  In AArch64 mode the virtual
1512 // address space is 48 bits in size, so we only need three
1513 // instructions to create a patchable instruction sequence that can
1514 // reach anywhere.
1515 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1516 #ifndef PRODUCT
1517   {
1518     char buffer[64];
1519     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1520     block_comment(buffer);
1521   }
1522 #endif
<span class="line-modified">1523   assert(imm64 &lt; (1ul &lt;&lt; 48), &quot;48-bit overflow in address constant&quot;);</span>
1524   movz(r, imm64 &amp; 0xffff);
1525   imm64 &gt;&gt;= 16;
1526   movk(r, imm64 &amp; 0xffff, 16);
1527   imm64 &gt;&gt;= 16;
1528   movk(r, imm64 &amp; 0xffff, 32);
1529 }
1530 
1531 // Macro to mov replicated immediate to vector register.
1532 //  Vd will get the following values for different arrangements in T
1533 //   imm32 == hex 000000gh  T8B:  Vd = ghghghghghghghgh
1534 //   imm32 == hex 000000gh  T16B: Vd = ghghghghghghghghghghghghghghghgh
1535 //   imm32 == hex 0000efgh  T4H:  Vd = efghefghefghefgh
1536 //   imm32 == hex 0000efgh  T8H:  Vd = efghefghefghefghefghefghefghefgh
1537 //   imm32 == hex abcdefgh  T2S:  Vd = abcdefghabcdefgh
1538 //   imm32 == hex abcdefgh  T4S:  Vd = abcdefghabcdefghabcdefghabcdefgh
1539 //   T1D/T2D: invalid
<span class="line-modified">1540 void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, u_int32_t imm32) {</span>
1541   assert(T != T1D &amp;&amp; T != T2D, &quot;invalid arrangement&quot;);
1542   if (T == T8B || T == T16B) {
1543     assert((imm32 &amp; ~0xff) == 0, &quot;extraneous bits in unsigned imm32 (T8B/T16B)&quot;);
1544     movi(Vd, T, imm32 &amp; 0xff, 0);
1545     return;
1546   }
<span class="line-modified">1547   u_int32_t nimm32 = ~imm32;</span>
1548   if (T == T4H || T == T8H) {
1549     assert((imm32  &amp; ~0xffff) == 0, &quot;extraneous bits in unsigned imm32 (T4H/T8H)&quot;);
1550     imm32 &amp;= 0xffff;
1551     nimm32 &amp;= 0xffff;
1552   }
<span class="line-modified">1553   u_int32_t x = imm32;</span>
1554   int movi_cnt = 0;
1555   int movn_cnt = 0;
1556   while (x) { if (x &amp; 0xff) movi_cnt++; x &gt;&gt;= 8; }
1557   x = nimm32;
1558   while (x) { if (x &amp; 0xff) movn_cnt++; x &gt;&gt;= 8; }
1559   if (movn_cnt &lt; movi_cnt) imm32 = nimm32;
1560   unsigned lsl = 0;
1561   while (imm32 &amp;&amp; (imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1562   if (movn_cnt &lt; movi_cnt)
1563     mvni(Vd, T, imm32 &amp; 0xff, lsl);
1564   else
1565     movi(Vd, T, imm32 &amp; 0xff, lsl);
1566   imm32 &gt;&gt;= 8; lsl += 8;
1567   while (imm32) {
1568     while ((imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1569     if (movn_cnt &lt; movi_cnt)
1570       bici(Vd, T, imm32 &amp; 0xff, lsl);
1571     else
1572       orri(Vd, T, imm32 &amp; 0xff, lsl);
1573     lsl += 8; imm32 &gt;&gt;= 8;
1574   }
1575 }
1576 
<span class="line-modified">1577 void MacroAssembler::mov_immediate64(Register dst, u_int64_t imm64)</span>
1578 {
1579 #ifndef PRODUCT
1580   {
1581     char buffer[64];
1582     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1583     block_comment(buffer);
1584   }
1585 #endif
1586   if (operand_valid_for_logical_immediate(false, imm64)) {
1587     orr(dst, zr, imm64);
1588   } else {
1589     // we can use a combination of MOVZ or MOVN with
1590     // MOVK to build up the constant
<span class="line-modified">1591     u_int64_t imm_h[4];</span>
1592     int zero_count = 0;
1593     int neg_count = 0;
1594     int i;
1595     for (i = 0; i &lt; 4; i++) {
1596       imm_h[i] = ((imm64 &gt;&gt; (i * 16)) &amp; 0xffffL);
1597       if (imm_h[i] == 0) {
1598         zero_count++;
1599       } else if (imm_h[i] == 0xffffL) {
1600         neg_count++;
1601       }
1602     }
1603     if (zero_count == 4) {
1604       // one MOVZ will do
1605       movz(dst, 0);
1606     } else if (neg_count == 4) {
1607       // one MOVN will do
1608       movn(dst, 0);
1609     } else if (zero_count == 3) {
1610       for (i = 0; i &lt; 4; i++) {
1611         if (imm_h[i] != 0L) {
<span class="line-modified">1612           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1613           break;
1614         }
1615       }
1616     } else if (neg_count == 3) {
1617       // one MOVN will do
1618       for (int i = 0; i &lt; 4; i++) {
1619         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1620           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1621           break;
1622         }
1623       }
1624     } else if (zero_count == 2) {
1625       // one MOVZ and one MOVK will do
1626       for (i = 0; i &lt; 3; i++) {
1627         if (imm_h[i] != 0L) {
<span class="line-modified">1628           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1629           i++;
1630           break;
1631         }
1632       }
1633       for (;i &lt; 4; i++) {
1634         if (imm_h[i] != 0L) {
<span class="line-modified">1635           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1636         }
1637       }
1638     } else if (neg_count == 2) {
1639       // one MOVN and one MOVK will do
1640       for (i = 0; i &lt; 4; i++) {
1641         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1642           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1643           i++;
1644           break;
1645         }
1646       }
1647       for (;i &lt; 4; i++) {
1648         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1649           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1650         }
1651       }
1652     } else if (zero_count == 1) {
1653       // one MOVZ and two MOVKs will do
1654       for (i = 0; i &lt; 4; i++) {
1655         if (imm_h[i] != 0L) {
<span class="line-modified">1656           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1657           i++;
1658           break;
1659         }
1660       }
1661       for (;i &lt; 4; i++) {
1662         if (imm_h[i] != 0x0L) {
<span class="line-modified">1663           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1664         }
1665       }
1666     } else if (neg_count == 1) {
1667       // one MOVN and two MOVKs will do
1668       for (i = 0; i &lt; 4; i++) {
1669         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1670           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1671           i++;
1672           break;
1673         }
1674       }
1675       for (;i &lt; 4; i++) {
1676         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1677           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1678         }
1679       }
1680     } else {
1681       // use a MOVZ and 3 MOVKs (makes it easier to debug)
<span class="line-modified">1682       movz(dst, (u_int32_t)imm_h[0], 0);</span>
1683       for (i = 1; i &lt; 4; i++) {
<span class="line-modified">1684         movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1685       }
1686     }
1687   }
1688 }
1689 
<span class="line-modified">1690 void MacroAssembler::mov_immediate32(Register dst, u_int32_t imm32)</span>
1691 {
1692 #ifndef PRODUCT
1693     {
1694       char buffer[64];
1695       snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX32, imm32);
1696       block_comment(buffer);
1697     }
1698 #endif
1699   if (operand_valid_for_logical_immediate(true, imm32)) {
1700     orrw(dst, zr, imm32);
1701   } else {
1702     // we can use MOVZ, MOVN or two calls to MOVK to build up the
1703     // constant
<span class="line-modified">1704     u_int32_t imm_h[2];</span>
1705     imm_h[0] = imm32 &amp; 0xffff;
1706     imm_h[1] = ((imm32 &gt;&gt; 16) &amp; 0xffff);
1707     if (imm_h[0] == 0) {
1708       movzw(dst, imm_h[1], 16);
1709     } else if (imm_h[0] == 0xffff) {
1710       movnw(dst, imm_h[1] ^ 0xffff, 16);
1711     } else if (imm_h[1] == 0) {
1712       movzw(dst, imm_h[0], 0);
1713     } else if (imm_h[1] == 0xffff) {
1714       movnw(dst, imm_h[0] ^ 0xffff, 0);
1715     } else {
1716       // use a MOVZ and MOVK (makes it easier to debug)
1717       movzw(dst, imm_h[0], 0);
1718       movkw(dst, imm_h[1], 16);
1719     }
1720   }
1721 }
1722 
1723 // Form an address from base + offset in Rd.  Rd may or may
1724 // not actually be used: you must use the Address that is returned.
1725 // It is up to you to ensure that the shift provided matches the size
1726 // of your data.
<span class="line-modified">1727 Address MacroAssembler::form_address(Register Rd, Register base, long byte_offset, int shift) {</span>
1728   if (Address::offset_ok_for_immed(byte_offset, shift))
1729     // It fits; no need for any heroics
1730     return Address(base, byte_offset);
1731 
1732   // Don&#39;t do anything clever with negative or misaligned offsets
1733   unsigned mask = (1 &lt;&lt; shift) - 1;
1734   if (byte_offset &lt; 0 || byte_offset &amp; mask) {
1735     mov(Rd, byte_offset);
1736     add(Rd, base, Rd);
1737     return Address(Rd);
1738   }
1739 
1740   // See if we can do this with two 12-bit offsets
1741   {
<span class="line-modified">1742     unsigned long word_offset = byte_offset &gt;&gt; shift;</span>
<span class="line-modified">1743     unsigned long masked_offset = word_offset &amp; 0xfff000;</span>
1744     if (Address::offset_ok_for_immed(word_offset - masked_offset, 0)
1745         &amp;&amp; Assembler::operand_valid_for_add_sub_immediate(masked_offset &lt;&lt; shift)) {
1746       add(Rd, base, masked_offset &lt;&lt; shift);
1747       word_offset -= masked_offset;
1748       return Address(Rd, word_offset &lt;&lt; shift);
1749     }
1750   }
1751 
1752   // Do it the hard way
1753   mov(Rd, byte_offset);
1754   add(Rd, base, Rd);
1755   return Address(Rd);
1756 }
1757 
1758 void MacroAssembler::atomic_incw(Register counter_addr, Register tmp, Register tmp2) {
1759   if (UseLSE) {
1760     mov(tmp, 1);
1761     ldadd(Assembler::word, tmp, zr, counter_addr);
1762     return;
1763   }
</pre>
<hr />
<pre>
1964 
1965 void MacroAssembler::decrementw(Register reg, int value)
1966 {
1967   if (value &lt; 0)  { incrementw(reg, -value);      return; }
1968   if (value == 0) {                               return; }
1969   if (value &lt; (1 &lt;&lt; 12)) { subw(reg, reg, value); return; }
1970   /* else */ {
1971     guarantee(reg != rscratch2, &quot;invalid dst for register decrement&quot;);
1972     movw(rscratch2, (unsigned)value);
1973     subw(reg, reg, rscratch2);
1974   }
1975 }
1976 
1977 void MacroAssembler::decrement(Register reg, int value)
1978 {
1979   if (value &lt; 0)  { increment(reg, -value);      return; }
1980   if (value == 0) {                              return; }
1981   if (value &lt; (1 &lt;&lt; 12)) { sub(reg, reg, value); return; }
1982   /* else */ {
1983     assert(reg != rscratch2, &quot;invalid dst for register decrement&quot;);
<span class="line-modified">1984     mov(rscratch2, (unsigned long)value);</span>
1985     sub(reg, reg, rscratch2);
1986   }
1987 }
1988 
1989 void MacroAssembler::decrementw(Address dst, int value)
1990 {
1991   assert(!dst.uses(rscratch1), &quot;invalid dst for address decrement&quot;);
1992   if (dst.getMode() == Address::literal) {
1993     assert(abs(value) &lt; (1 &lt;&lt; 12), &quot;invalid value and address mode combination&quot;);
1994     lea(rscratch2, dst);
1995     dst = Address(rscratch2);
1996   }
1997   ldrw(rscratch1, dst);
1998   decrementw(rscratch1, value);
1999   strw(rscratch1, dst);
2000 }
2001 
2002 void MacroAssembler::decrement(Address dst, int value)
2003 {
2004   assert(!dst.uses(rscratch1), &quot;invalid address for decrement&quot;);
</pre>
<hr />
<pre>
2483   } else {
2484     ShouldNotReachHere();
2485   }
2486 }
2487 
2488 
2489 static bool different(Register a, RegisterOrConstant b, Register c) {
2490   if (b.is_constant())
2491     return a != c;
2492   else
2493     return a != b.as_register() &amp;&amp; a != c &amp;&amp; b.as_register() != c;
2494 }
2495 
2496 #define ATOMIC_OP(NAME, LDXR, OP, IOP, AOP, STXR, sz)                   \
2497 void MacroAssembler::atomic_##NAME(Register prev, RegisterOrConstant incr, Register addr) { \
2498   if (UseLSE) {                                                         \
2499     prev = prev-&gt;is_valid() ? prev : zr;                                \
2500     if (incr.is_register()) {                                           \
2501       AOP(sz, incr.as_register(), prev, addr);                          \
2502     } else {                                                            \
<span class="line-modified">2503       mov(rscratch2, incr.as_constant());                               \</span>
2504       AOP(sz, rscratch2, prev, addr);                                   \
2505     }                                                                   \
2506     return;                                                             \
2507   }                                                                     \
2508   Register result = rscratch2;                                          \
2509   if (prev-&gt;is_valid())                                                 \
2510     result = different(prev, incr, addr) ? prev : rscratch2;            \
2511                                                                         \
2512   Label retry_load;                                                     \
2513   if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))         \
2514     prfm(Address(addr), PSTL1STRM);                                     \
2515   bind(retry_load);                                                     \
2516   LDXR(result, addr);                                                   \
2517   OP(rscratch1, result, incr);                                          \
2518   STXR(rscratch2, rscratch1, addr);                                     \
2519   cbnzw(rscratch2, retry_load);                                         \
2520   if (prev-&gt;is_valid() &amp;&amp; prev != result) {                             \
2521     IOP(prev, rscratch1, incr);                                         \
2522   }                                                                     \
2523 }
</pre>
<hr />
<pre>
2605       tty-&gt;print_cr(&quot;r19 = 0x%016lx&quot;, regs[19]);
2606       tty-&gt;print_cr(&quot;r20 = 0x%016lx&quot;, regs[20]);
2607       tty-&gt;print_cr(&quot;r21 = 0x%016lx&quot;, regs[21]);
2608       tty-&gt;print_cr(&quot;r22 = 0x%016lx&quot;, regs[22]);
2609       tty-&gt;print_cr(&quot;r23 = 0x%016lx&quot;, regs[23]);
2610       tty-&gt;print_cr(&quot;r24 = 0x%016lx&quot;, regs[24]);
2611       tty-&gt;print_cr(&quot;r25 = 0x%016lx&quot;, regs[25]);
2612       tty-&gt;print_cr(&quot;r26 = 0x%016lx&quot;, regs[26]);
2613       tty-&gt;print_cr(&quot;r27 = 0x%016lx&quot;, regs[27]);
2614       tty-&gt;print_cr(&quot;r28 = 0x%016lx&quot;, regs[28]);
2615       tty-&gt;print_cr(&quot;r30 = 0x%016lx&quot;, regs[30]);
2616       tty-&gt;print_cr(&quot;r31 = 0x%016lx&quot;, regs[31]);
2617       BREAKPOINT;
2618     }
2619   }
2620   fatal(&quot;DEBUG MESSAGE: %s&quot;, msg);
2621 }
2622 
2623 void MacroAssembler::push_call_clobbered_registers() {
2624   int step = 4 * wordSize;
<span class="line-modified">2625   push(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2), sp);</span>
2626   sub(sp, sp, step);
2627   mov(rscratch1, -step);
2628   // Push v0-v7, v16-v31.
2629   for (int i = 31; i&gt;= 4; i -= 4) {
2630     if (i &lt;= v7-&gt;encoding() || i &gt;= v16-&gt;encoding())
2631       st1(as_FloatRegister(i-3), as_FloatRegister(i-2), as_FloatRegister(i-1),
2632           as_FloatRegister(i), T1D, Address(post(sp, rscratch1)));
2633   }
2634   st1(as_FloatRegister(0), as_FloatRegister(1), as_FloatRegister(2),
2635       as_FloatRegister(3), T1D, Address(sp));
2636 }
2637 
2638 void MacroAssembler::pop_call_clobbered_registers() {
2639   for (int i = 0; i &lt; 32; i += 4) {
2640     if (i &lt;= v7-&gt;encoding() || i &gt;= v16-&gt;encoding())
2641       ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2642           as_FloatRegister(i+3), T1D, Address(post(sp, 4 * wordSize)));
2643   }
<span class="line-modified">2644 </span>
<span class="line-removed">2645   pop(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2), sp);</span>
2646 }
2647 
2648 void MacroAssembler::push_CPU_state(bool save_vectors) {
2649   int step = (save_vectors ? 8 : 4) * wordSize;
2650   push(0x3fffffff, sp);         // integer registers except lr &amp; sp
2651   mov(rscratch1, -step);
2652   sub(sp, sp, step);
2653   for (int i = 28; i &gt;= 4; i -= 4) {
2654     st1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2655         as_FloatRegister(i+3), save_vectors ? T2D : T1D, Address(post(sp, rscratch1)));
2656   }
2657   st1(v0, v1, v2, v3, save_vectors ? T2D : T1D, sp);
2658 }
2659 
2660 void MacroAssembler::pop_CPU_state(bool restore_vectors) {
2661   int step = (restore_vectors ? 8 : 4) * wordSize;
2662   for (int i = 0; i &lt;= 28; i += 4)
2663     ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2664         as_FloatRegister(i+3), restore_vectors ? T2D : T1D, Address(post(sp, step)));
2665   pop(0x3fffffff, sp);         // integer registers except lr &amp; sp
</pre>
<hr />
<pre>
2697   Register base = sp;
2698   if ((offset &amp; (size-1)) &amp;&amp; offset &gt;= (1&lt;&lt;8)) {
2699     add(tmp, base, offset &amp; ((1&lt;&lt;12)-1));
2700     base = tmp;
2701     offset &amp;= -1u&lt;&lt;12;
2702   }
2703 
2704   if (offset &gt;= (1&lt;&lt;12) * size) {
2705     add(tmp, base, offset &amp; (((1&lt;&lt;12)-1)&lt;&lt;12));
2706     base = tmp;
2707     offset &amp;= ~(((1&lt;&lt;12)-1)&lt;&lt;12);
2708   }
2709 
2710   return Address(base, offset);
2711 }
2712 
2713 // Checks whether offset is aligned.
2714 // Returns true if it is, else false.
2715 bool MacroAssembler::merge_alignment_check(Register base,
2716                                            size_t size,
<span class="line-modified">2717                                            long cur_offset,</span>
<span class="line-modified">2718                                            long prev_offset) const {</span>
2719   if (AvoidUnalignedAccesses) {
2720     if (base == sp) {
2721       // Checks whether low offset if aligned to pair of registers.
<span class="line-modified">2722       long pair_mask = size * 2 - 1;</span>
<span class="line-modified">2723       long offset = prev_offset &gt; cur_offset ? cur_offset : prev_offset;</span>
2724       return (offset &amp; pair_mask) == 0;
2725     } else { // If base is not sp, we can&#39;t guarantee the access is aligned.
2726       return false;
2727     }
2728   } else {
<span class="line-modified">2729     long mask = size - 1;</span>
2730     // Load/store pair instruction only supports element size aligned offset.
2731     return (cur_offset &amp; mask) == 0 &amp;&amp; (prev_offset &amp; mask) == 0;
2732   }
2733 }
2734 
2735 // Checks whether current and previous loads/stores can be merged.
2736 // Returns true if it can be merged, else false.
2737 bool MacroAssembler::ldst_can_merge(Register rt,
2738                                     const Address &amp;adr,
2739                                     size_t cur_size_in_bytes,
2740                                     bool is_store) const {
2741   address prev = pc() - NativeInstruction::instruction_size;
2742   address last = code()-&gt;last_insn();
2743 
2744   if (last == NULL || !nativeInstruction_at(last)-&gt;is_Imm_LdSt()) {
2745     return false;
2746   }
2747 
2748   if (adr.getMode() != Address::base_plus_offset || prev != last) {
2749     return false;
2750   }
2751 
2752   NativeLdSt* prev_ldst = NativeLdSt_at(prev);
2753   size_t prev_size_in_bytes = prev_ldst-&gt;size_in_bytes();
2754 
2755   assert(prev_size_in_bytes == 4 || prev_size_in_bytes == 8, &quot;only supports 64/32bit merging.&quot;);
2756   assert(cur_size_in_bytes == 4 || cur_size_in_bytes == 8, &quot;only supports 64/32bit merging.&quot;);
2757 
2758   if (cur_size_in_bytes != prev_size_in_bytes || is_store != prev_ldst-&gt;is_store()) {
2759     return false;
2760   }
2761 
<span class="line-modified">2762   long max_offset = 63 * prev_size_in_bytes;</span>
<span class="line-modified">2763   long min_offset = -64 * prev_size_in_bytes;</span>
2764 
2765   assert(prev_ldst-&gt;is_not_pre_post_index(), &quot;pre-index or post-index is not supported to be merged.&quot;);
2766 
2767   // Only same base can be merged.
2768   if (adr.base() != prev_ldst-&gt;base()) {
2769     return false;
2770   }
2771 
<span class="line-modified">2772   long cur_offset = adr.offset();</span>
<span class="line-modified">2773   long prev_offset = prev_ldst-&gt;offset();</span>
2774   size_t diff = abs(cur_offset - prev_offset);
2775   if (diff != prev_size_in_bytes) {
2776     return false;
2777   }
2778 
2779   // Following cases can not be merged:
2780   // ldr x2, [x2, #8]
2781   // ldr x3, [x2, #16]
2782   // or:
2783   // ldr x2, [x3, #8]
2784   // ldr x2, [x3, #16]
2785   // If t1 and t2 is the same in &quot;ldp t1, t2, [xn, #imm]&quot;, we&#39;ll get SIGILL.
2786   if (!is_store &amp;&amp; (adr.base() == prev_ldst-&gt;target() || rt == prev_ldst-&gt;target())) {
2787     return false;
2788   }
2789 
<span class="line-modified">2790   long low_offset = prev_offset &gt; cur_offset ? cur_offset : prev_offset;</span>
2791   // Offset range must be in ldp/stp instruction&#39;s range.
2792   if (low_offset &gt; max_offset || low_offset &lt; min_offset) {
2793     return false;
2794   }
2795 
2796   if (merge_alignment_check(adr.base(), prev_size_in_bytes, cur_offset, prev_offset)) {
2797     return true;
2798   }
2799 
2800   return false;
2801 }
2802 
2803 // Merge current load/store with previous load/store into ldp/stp.
2804 void MacroAssembler::merge_ldst(Register rt,
2805                                 const Address &amp;adr,
2806                                 size_t cur_size_in_bytes,
2807                                 bool is_store) {
2808 
2809   assert(ldst_can_merge(rt, adr, cur_size_in_bytes, is_store) == true, &quot;cur and prev must be able to be merged.&quot;);
2810 
2811   Register rt_low, rt_high;
2812   address prev = pc() - NativeInstruction::instruction_size;
2813   NativeLdSt* prev_ldst = NativeLdSt_at(prev);
2814 
<span class="line-modified">2815   long offset;</span>
2816 
2817   if (adr.offset() &lt; prev_ldst-&gt;offset()) {
2818     offset = adr.offset();
2819     rt_low = rt;
2820     rt_high = prev_ldst-&gt;target();
2821   } else {
2822     offset = prev_ldst-&gt;offset();
2823     rt_low = prev_ldst-&gt;target();
2824     rt_high = rt;
2825   }
2826 
2827   Address adr_p = Address(prev_ldst-&gt;base(), offset);
2828   // Overwrite previous generated binary.
2829   code_section()-&gt;set_end(prev);
2830 
2831   const int sz = prev_ldst-&gt;size_in_bytes();
2832   assert(sz == 8 || sz == 4, &quot;only supports 64/32bit merging.&quot;);
2833   if (!is_store) {
2834     BLOCK_COMMENT(&quot;merged ldr pair&quot;);
2835     if (sz == 8) {
</pre>
<hr />
<pre>
3341 
3342     sub(len, len, 64);
3343     add(buf, buf, 8);
3344     cmn(len, 128);
3345     br(Assembler::NE, CRC_less64);
3346   BIND(L_exit);
3347     mvnw(crc, crc);
3348 }
3349 
3350 /**
3351  * @param crc   register containing existing CRC (32-bit)
3352  * @param buf   register pointing to input byte buffer (byte*)
3353  * @param len   register containing number of bytes
3354  * @param table register that will contain address of CRC table
3355  * @param tmp   scratch register
3356  */
3357 void MacroAssembler::kernel_crc32(Register crc, Register buf, Register len,
3358         Register table0, Register table1, Register table2, Register table3,
3359         Register tmp, Register tmp2, Register tmp3) {
3360   Label L_by16, L_by16_loop, L_by4, L_by4_loop, L_by1, L_by1_loop, L_exit;
<span class="line-modified">3361   unsigned long offset;</span>
3362 
3363   if (UseCRC32) {
3364       kernel_crc32_using_crc32(crc, buf, len, table0, table1, table2, table3);
3365       return;
3366   }
3367 
3368     mvnw(crc, crc);
3369 
3370     adrp(table0, ExternalAddress(StubRoutines::crc_table_addr()), offset);
3371     if (offset) add(table0, table0, offset);
3372     add(table1, table0, 1*256*sizeof(juint));
3373     add(table2, table0, 2*256*sizeof(juint));
3374     add(table3, table0, 3*256*sizeof(juint));
3375 
3376   if (UseNeon) {
3377       cmp(len, (u1)64);
3378       br(Assembler::LT, L_by16);
3379       eor(v16, T16B, v16, v16);
3380 
3381     Label L_fold;
</pre>
<hr />
<pre>
3643   BIND(L_exit);
3644 }
3645 
3646 /**
3647  * @param crc   register containing existing CRC (32-bit)
3648  * @param buf   register pointing to input byte buffer (byte*)
3649  * @param len   register containing number of bytes
3650  * @param table register that will contain address of CRC table
3651  * @param tmp   scratch register
3652  */
3653 void MacroAssembler::kernel_crc32c(Register crc, Register buf, Register len,
3654         Register table0, Register table1, Register table2, Register table3,
3655         Register tmp, Register tmp2, Register tmp3) {
3656   kernel_crc32c_using_crc32c(crc, buf, len, table0, table1, table2, table3);
3657 }
3658 
3659 
3660 SkipIfEqual::SkipIfEqual(
3661     MacroAssembler* masm, const bool* flag_addr, bool value) {
3662   _masm = masm;
<span class="line-modified">3663   unsigned long offset;</span>
3664   _masm-&gt;adrp(rscratch1, ExternalAddress((address)flag_addr), offset);
3665   _masm-&gt;ldrb(rscratch1, Address(rscratch1, offset));
3666   _masm-&gt;cbzw(rscratch1, _label);
3667 }
3668 
3669 SkipIfEqual::~SkipIfEqual() {
3670   _masm-&gt;bind(_label);
3671 }
3672 
3673 void MacroAssembler::addptr(const Address &amp;dst, int32_t src) {
3674   Address adr;
3675   switch(dst.getMode()) {
3676   case Address::base_plus_offset:
3677     // This is the expected mode, although we allow all the other
3678     // forms below.
3679     adr = form_address(rscratch2, dst.base(), dst.offset(), LogBytesPerWord);
3680     break;
3681   default:
3682     lea(rscratch2, dst);
3683     adr = Address(rscratch2);
3684     break;
3685   }
3686   ldr(rscratch1, adr);
3687   add(rscratch1, rscratch1, src);
3688   str(rscratch1, adr);
3689 }
3690 
3691 void MacroAssembler::cmpptr(Register src1, Address src2) {
<span class="line-modified">3692   unsigned long offset;</span>
3693   adrp(rscratch1, src2, offset);
3694   ldr(rscratch1, Address(rscratch1, offset));
3695   cmp(src1, rscratch1);
3696 }
3697 
3698 void MacroAssembler::cmpoop(Register obj1, Register obj2) {
3699   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3700   bs-&gt;obj_equals(this, obj1, obj2);
3701 }
3702 
3703 void MacroAssembler::load_method_holder(Register holder, Register method) {
3704   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
3705   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
3706   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
3707 }
3708 
3709 void MacroAssembler::load_klass(Register dst, Register src) {
3710   if (UseCompressedClassPointers) {
3711     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3712     decode_klass_not_null(dst);
</pre>
<hr />
<pre>
4293   // Bang down shadow pages too.
4294   // At this point, (tmp-0) is the last address touched, so don&#39;t
4295   // touch it again.  (It was touched as (tmp-pagesize) but then tmp
4296   // was post-decremented.)  Skip this address by starting at i=1, and
4297   // touch a few more pages below.  N.B.  It is important to touch all
4298   // the way down to and including i=StackShadowPages.
4299   for (int i = 0; i &lt; (int)(JavaThread::stack_shadow_zone_size() / os::vm_page_size()) - 1; i++) {
4300     // this could be any sized move but this is can be a debugging crumb
4301     // so the bigger the better.
4302     lea(tmp, Address(tmp, -os::vm_page_size()));
4303     str(size, Address(tmp));
4304   }
4305 }
4306 
4307 
4308 // Move the address of the polling page into dest.
4309 void MacroAssembler::get_polling_page(Register dest, address page, relocInfo::relocType rtype) {
4310   if (SafepointMechanism::uses_thread_local_poll()) {
4311     ldr(dest, Address(rthread, Thread::polling_page_offset()));
4312   } else {
<span class="line-modified">4313     unsigned long off;</span>
4314     adrp(dest, Address(page, rtype), off);
4315     assert(off == 0, &quot;polling page must be page aligned&quot;);
4316   }
4317 }
4318 
4319 // Move the address of the polling page into r, then read the polling
4320 // page.
4321 address MacroAssembler::read_polling_page(Register r, address page, relocInfo::relocType rtype) {
4322   get_polling_page(r, page, rtype);
4323   return read_polling_page(r, rtype);
4324 }
4325 
4326 // Read the polling page.  The address of the polling page must
4327 // already be in r.
4328 address MacroAssembler::read_polling_page(Register r, relocInfo::relocType rtype) {
4329   InstructionMark im(this);
4330   code_section()-&gt;relocate(inst_mark(), rtype);
4331   ldrw(zr, Address(r, 0));
4332   return inst_mark();
4333 }
4334 
<span class="line-modified">4335 void MacroAssembler::adrp(Register reg1, const Address &amp;dest, unsigned long &amp;byte_offset) {</span>
4336   relocInfo::relocType rtype = dest.rspec().reloc()-&gt;type();
<span class="line-modified">4337   unsigned long low_page = (unsigned long)CodeCache::low_bound() &gt;&gt; 12;</span>
<span class="line-modified">4338   unsigned long high_page = (unsigned long)(CodeCache::high_bound()-1) &gt;&gt; 12;</span>
<span class="line-modified">4339   unsigned long dest_page = (unsigned long)dest.target() &gt;&gt; 12;</span>
<span class="line-modified">4340   long offset_low = dest_page - low_page;</span>
<span class="line-modified">4341   long offset_high = dest_page - high_page;</span>
4342 
4343   assert(is_valid_AArch64_address(dest.target()), &quot;bad address&quot;);
4344   assert(dest.getMode() == Address::literal, &quot;ADRP must be applied to a literal address&quot;);
4345 
4346   InstructionMark im(this);
4347   code_section()-&gt;relocate(inst_mark(), dest.rspec());
4348   // 8143067: Ensure that the adrp can reach the dest from anywhere within
4349   // the code cache so that if it is relocated we know it will still reach
4350   if (offset_high &gt;= -(1&lt;&lt;20) &amp;&amp; offset_low &lt; (1&lt;&lt;20)) {
4351     _adrp(reg1, dest.target());
4352   } else {
<span class="line-modified">4353     unsigned long target = (unsigned long)dest.target();</span>
<span class="line-modified">4354     unsigned long adrp_target</span>
<span class="line-modified">4355       = (target &amp; 0xffffffffUL) | ((unsigned long)pc() &amp; 0xffff00000000UL);</span>
4356 
4357     _adrp(reg1, (address)adrp_target);
4358     movk(reg1, target &gt;&gt; 32, 32);
4359   }
<span class="line-modified">4360   byte_offset = (unsigned long)dest.target() &amp; 0xfff;</span>
4361 }
4362 
4363 void MacroAssembler::load_byte_map_base(Register reg) {
4364   CardTable::CardValue* byte_map_base =
4365     ((CardTableBarrierSet*)(BarrierSet::barrier_set()))-&gt;card_table()-&gt;byte_map_base();
4366 
4367   if (is_valid_AArch64_address((address)byte_map_base)) {
4368     // Strictly speaking the byte_map_base isn&#39;t an address at all,
4369     // and it might even be negative.
<span class="line-modified">4370     unsigned long offset;</span>
4371     adrp(reg, ExternalAddress((address)byte_map_base), offset);
4372     // We expect offset to be zero with most collectors.
4373     if (offset != 0) {
4374       add(reg, reg, offset);
4375     }
4376   } else {
4377     mov(reg, (uint64_t)byte_map_base);
4378   }
4379 }
4380 
4381 void MacroAssembler::build_frame(int framesize) {
4382   assert(framesize &gt; 0, &quot;framesize must be &gt; 0&quot;);
4383   if (framesize &lt; ((1 &lt;&lt; 9) + 2 * wordSize)) {
4384     sub(sp, sp, framesize);
4385     stp(rfp, lr, Address(sp, framesize - 2 * wordSize));
4386     if (PreserveFramePointer) add(rfp, sp, framesize - 2 * wordSize);
4387   } else {
4388     stp(rfp, lr, Address(pre(sp, -2 * wordSize)));
4389     if (PreserveFramePointer) mov(rfp, sp);
4390     if (framesize &lt; ((1 &lt;&lt; 12) + 2 * wordSize))
</pre>
<hr />
<pre>
4781         b(NOMATCH);
4782 
4783       BIND(STR1_LOOP);
4784         add(cnt2tmp, cnt2_neg, 2*str2_chr_size);
4785         (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt2tmp));
4786         cmp(ch1, ch2);
4787         br(NE, STR2_NEXT);
4788         b(MATCH);
4789     }
4790 
4791     if (icnt1 == -1 || icnt1 == 1) {
4792       Label CH1_LOOP, HAS_ZERO, DO1_SHORT, DO1_LOOP;
4793 
4794       BIND(DO1);
4795         (this-&gt;*str1_load_1chr)(ch1, str1);
4796         cmp(cnt2, (u1)8);
4797         br(LT, DO1_SHORT);
4798 
4799         sub(result_tmp, cnt2, 8/str2_chr_size);
4800         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
<span class="line-modified">4801         mov(tmp3, str2_isL ? 0x0101010101010101 : 0x0001000100010001);</span>
4802         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4803 
4804         if (str2_isL) {
4805           orr(ch1, ch1, ch1, LSL, 8);
4806         }
4807         orr(ch1, ch1, ch1, LSL, 16);
4808         orr(ch1, ch1, ch1, LSL, 32);
4809       BIND(CH1_LOOP);
4810         ldr(ch2, Address(str2, cnt2_neg));
4811         eor(ch2, ch1, ch2);
4812         sub(tmp1, ch2, tmp3);
4813         orr(tmp2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4814         bics(tmp1, tmp1, tmp2);
4815         br(NE, HAS_ZERO);
4816         adds(cnt2_neg, cnt2_neg, 8);
4817         br(LT, CH1_LOOP);
4818 
4819         cmp(cnt2_neg, (u1)8);
4820         mov(cnt2_neg, 0);
4821         br(LT, CH1_LOOP);
</pre>
<hr />
<pre>
4853 void MacroAssembler::string_indexof_char(Register str1, Register cnt1,
4854                                          Register ch, Register result,
4855                                          Register tmp1, Register tmp2, Register tmp3)
4856 {
4857   Label CH1_LOOP, HAS_ZERO, DO1_SHORT, DO1_LOOP, MATCH, NOMATCH, DONE;
4858   Register cnt1_neg = cnt1;
4859   Register ch1 = rscratch1;
4860   Register result_tmp = rscratch2;
4861 
4862   cmp(cnt1, (u1)4);
4863   br(LT, DO1_SHORT);
4864 
4865   orr(ch, ch, ch, LSL, 16);
4866   orr(ch, ch, ch, LSL, 32);
4867 
4868   sub(cnt1, cnt1, 4);
4869   mov(result_tmp, cnt1);
4870   lea(str1, Address(str1, cnt1, Address::uxtw(1)));
4871   sub(cnt1_neg, zr, cnt1, LSL, 1);
4872 
<span class="line-modified">4873   mov(tmp3, 0x0001000100010001);</span>
4874 
4875   BIND(CH1_LOOP);
4876     ldr(ch1, Address(str1, cnt1_neg));
4877     eor(ch1, ch, ch1);
4878     sub(tmp1, ch1, tmp3);
4879     orr(tmp2, ch1, 0x7fff7fff7fff7fff);
4880     bics(tmp1, tmp1, tmp2);
4881     br(NE, HAS_ZERO);
4882     adds(cnt1_neg, cnt1_neg, 8);
4883     br(LT, CH1_LOOP);
4884 
4885     cmp(cnt1_neg, (u1)8);
4886     mov(cnt1_neg, 0);
4887     br(LT, CH1_LOOP);
4888     b(NOMATCH);
4889 
4890   BIND(HAS_ZERO);
4891     rev(tmp1, tmp1);
4892     clz(tmp1, tmp1);
4893     add(cnt1_neg, cnt1_neg, tmp1, LSR, 3);
</pre>
<hr />
<pre>
4899     sub(cnt1_neg, zr, cnt1, LSL, 1);
4900   BIND(DO1_LOOP);
4901     ldrh(ch1, Address(str1, cnt1_neg));
4902     cmpw(ch, ch1);
4903     br(EQ, MATCH);
4904     adds(cnt1_neg, cnt1_neg, 2);
4905     br(LT, DO1_LOOP);
4906   BIND(NOMATCH);
4907     mov(result, -1);
4908     b(DONE);
4909   BIND(MATCH);
4910     add(result, result_tmp, cnt1_neg, ASR, 1);
4911   BIND(DONE);
4912 }
4913 
4914 // Compare strings.
4915 void MacroAssembler::string_compare(Register str1, Register str2,
4916     Register cnt1, Register cnt2, Register result, Register tmp1, Register tmp2,
4917     FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3, int ae) {
4918   Label DONE, SHORT_LOOP, SHORT_STRING, SHORT_LAST, TAIL, STUB,
<span class="line-modified">4919       DIFFERENCE, NEXT_WORD, SHORT_LOOP_TAIL, SHORT_LAST2, SHORT_LAST_INIT,</span>
4920       SHORT_LOOP_START, TAIL_CHECK;
4921 
4922   bool isLL = ae == StrIntrinsicNode::LL;
4923   bool isLU = ae == StrIntrinsicNode::LU;
4924   bool isUL = ae == StrIntrinsicNode::UL;
4925 
4926   // The stub threshold for LL strings is: 72 (64 + 8) chars
4927   // UU: 36 chars, or 72 bytes (valid for the 64-byte large loop with prefetch)
4928   // LU/UL: 24 chars, or 48 bytes (valid for the 16-character loop at least)
4929   const u1 stub_threshold = isLL ? 72 : ((isLU || isUL) ? 24 : 36);
4930 
4931   bool str1_isL = isLL || isLU;
4932   bool str2_isL = isLL || isUL;
4933 
4934   int str1_chr_shift = str1_isL ? 0 : 1;
4935   int str2_chr_shift = str2_isL ? 0 : 1;
4936   int str1_chr_size = str1_isL ? 1 : 2;
4937   int str2_chr_size = str2_isL ? 1 : 2;
4938   int minCharsInWord = isLL ? wordSize : wordSize/2;
4939 
</pre>
<hr />
<pre>
4994     } else { // UL case
4995       ldr(tmp1, Address(str1));
4996       cmp(str1, str2);
4997       br(Assembler::EQ, DONE);
4998       ldrs(vtmp, Address(str2));
4999       cmp(cnt2, stub_threshold);
5000       br(GE, STUB);
5001       subw(cnt2, cnt2, 4);
5002       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
5003       eor(vtmpZ, T16B, vtmpZ, vtmpZ);
5004       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
5005       sub(cnt1, zr, cnt2, LSL, str1_chr_shift);
5006       zip1(vtmp, T8B, vtmp, vtmpZ);
5007       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
5008       add(cnt1, cnt1, 8);
5009       fmovd(tmp2, vtmp);
5010     }
5011     adds(cnt2, cnt2, isUL ? 4 : 8);
5012     br(GE, TAIL);
5013     eor(rscratch2, tmp1, tmp2);
<span class="line-modified">5014     cbnz(rscratch2, DIFFERENCE);</span>
5015     // main loop
5016     bind(NEXT_WORD);
5017     if (str1_isL == str2_isL) {
5018       ldr(tmp1, Address(str1, cnt2));
5019       ldr(tmp2, Address(str2, cnt2));
5020       adds(cnt2, cnt2, 8);
5021     } else if (isLU) {
5022       ldrs(vtmp, Address(str1, cnt1));
5023       ldr(tmp2, Address(str2, cnt2));
5024       add(cnt1, cnt1, 4);
5025       zip1(vtmp, T8B, vtmp, vtmpZ);
5026       fmovd(tmp1, vtmp);
5027       adds(cnt2, cnt2, 8);
5028     } else { // UL
5029       ldrs(vtmp, Address(str2, cnt2));
5030       ldr(tmp1, Address(str1, cnt1));
5031       zip1(vtmp, T8B, vtmp, vtmpZ);
5032       add(cnt1, cnt1, 8);
5033       fmovd(tmp2, vtmp);
5034       adds(cnt2, cnt2, 4);
5035     }
5036     br(GE, TAIL);
5037 
5038     eor(rscratch2, tmp1, tmp2);
5039     cbz(rscratch2, NEXT_WORD);
<span class="line-modified">5040     b(DIFFERENCE);</span>
5041     bind(TAIL);
5042     eor(rscratch2, tmp1, tmp2);
<span class="line-modified">5043     cbnz(rscratch2, DIFFERENCE);</span>
5044     // Last longword.  In the case where length == 4 we compare the
5045     // same longword twice, but that&#39;s still faster than another
5046     // conditional branch.
5047     if (str1_isL == str2_isL) {
5048       ldr(tmp1, Address(str1));
5049       ldr(tmp2, Address(str2));
5050     } else if (isLU) {
5051       ldrs(vtmp, Address(str1));
5052       ldr(tmp2, Address(str2));
5053       zip1(vtmp, T8B, vtmp, vtmpZ);
5054       fmovd(tmp1, vtmp);
5055     } else { // UL
5056       ldrs(vtmp, Address(str2));
5057       ldr(tmp1, Address(str1));
5058       zip1(vtmp, T8B, vtmp, vtmpZ);
5059       fmovd(tmp2, vtmp);
5060     }
5061     bind(TAIL_CHECK);
5062     eor(rscratch2, tmp1, tmp2);
5063     cbz(rscratch2, DONE);
5064 
5065     // Find the first different characters in the longwords and
5066     // compute their difference.
<span class="line-modified">5067     bind(DIFFERENCE);</span>
5068     rev(rscratch2, rscratch2);
5069     clz(rscratch2, rscratch2);
5070     andr(rscratch2, rscratch2, isLL ? -8 : -16);
5071     lsrv(tmp1, tmp1, rscratch2);
5072     (this-&gt;*ext_chr)(tmp1, tmp1);
5073     lsrv(tmp2, tmp2, rscratch2);
5074     (this-&gt;*ext_chr)(tmp2, tmp2);
5075     subw(result, tmp1, tmp2);
5076     b(DONE);
5077   }
5078 
5079   bind(STUB);
5080     RuntimeAddress stub = NULL;
5081     switch(ae) {
5082       case StrIntrinsicNode::LL:
5083         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_LL());
5084         break;
5085       case StrIntrinsicNode::UU:
5086         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_UU());
5087         break;
</pre>
<hr />
<pre>
5526   for (int i = zero_words_block_size &gt;&gt; 1; i &gt; 1; i &gt;&gt;= 1) {
5527     Label l;
5528     tbz(cnt, exact_log2(i), l);
5529     for (int j = 0; j &lt; i; j += 2) {
5530       stp(zr, zr, post(ptr, 16));
5531     }
5532     bind(l);
5533   }
5534   {
5535     Label l;
5536     tbz(cnt, 0, l);
5537     str(zr, Address(ptr));
5538     bind(l);
5539   }
5540   BLOCK_COMMENT(&quot;} zero_words&quot;);
5541 }
5542 
5543 // base:         Address of a buffer to be zeroed, 8 bytes aligned.
5544 // cnt:          Immediate count in HeapWords.
5545 #define SmallArraySize (18 * BytesPerLong)
<span class="line-modified">5546 void MacroAssembler::zero_words(Register base, u_int64_t cnt)</span>
5547 {
5548   BLOCK_COMMENT(&quot;zero_words {&quot;);
5549   int i = cnt &amp; 1;  // store any odd word to start
5550   if (i) str(zr, Address(base));
5551 
5552   if (cnt &lt;= SmallArraySize / BytesPerLong) {
5553     for (; i &lt; (int)cnt; i += 2)
5554       stp(zr, zr, Address(base, i * wordSize));
5555   } else {
5556     const int unroll = 4; // Number of stp(zr, zr) instructions we&#39;ll unroll
5557     int remainder = cnt % (2 * unroll);
5558     for (; i &lt; remainder; i += 2)
5559       stp(zr, zr, Address(base, i * wordSize));
5560 
5561     Label loop;
5562     Register cnt_reg = rscratch1;
5563     Register loop_base = rscratch2;
5564     cnt = cnt - remainder;
5565     mov(cnt_reg, cnt);
5566     // adjust base and prebias by -2 * wordSize so we can pre-increment
</pre>
</td>
<td>
<hr />
<pre>
  54 #include &quot;oops/oop.hpp&quot;
  55 #include &quot;opto/compile.hpp&quot;
  56 #include &quot;opto/intrinsicnode.hpp&quot;
  57 #include &quot;opto/node.hpp&quot;
  58 #endif
  59 
  60 #ifdef PRODUCT
  61 #define BLOCK_COMMENT(str) /* nothing */
  62 #define STOP(error) stop(error)
  63 #else
  64 #define BLOCK_COMMENT(str) block_comment(str)
  65 #define STOP(error) block_comment(error); stop(error)
  66 #endif
  67 
  68 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  69 
  70 // Patch any kind of instruction; there may be several instructions.
  71 // Return the total length (in bytes) of the instructions.
  72 int MacroAssembler::pd_patch_instruction_size(address branch, address target) {
  73   int instructions = 1;
<span class="line-modified">  74   assert((uint64_t)target &lt; ((uint64_t)1 &lt;&lt; 48), &quot;48-bit overflow in address constant&quot;);</span>
<span class="line-modified">  75   int64_t offset = (target - branch) &gt;&gt; 2;</span>
  76   unsigned insn = *(unsigned*)branch;
  77   if ((Instruction_aarch64::extract(insn, 29, 24) &amp; 0b111011) == 0b011000) {
  78     // Load register (literal)
  79     Instruction_aarch64::spatch(branch, 23, 5, offset);
  80   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
  81     // Unconditional branch (immediate)
  82     Instruction_aarch64::spatch(branch, 25, 0, offset);
  83   } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {
  84     // Conditional branch (immediate)
  85     Instruction_aarch64::spatch(branch, 23, 5, offset);
  86   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {
  87     // Compare &amp; branch (immediate)
  88     Instruction_aarch64::spatch(branch, 23, 5, offset);
  89   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {
  90     // Test &amp; branch (immediate)
  91     Instruction_aarch64::spatch(branch, 18, 5, offset);
  92   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
  93     // PC-rel. addressing
  94     offset = target-branch;
  95     int shift = Instruction_aarch64::extract(insn, 31, 31);
  96     if (shift) {
<span class="line-modified">  97       uint64_t dest = (uint64_t)target;</span>
  98       uint64_t pc_page = (uint64_t)branch &gt;&gt; 12;
  99       uint64_t adr_page = (uint64_t)target &gt;&gt; 12;
 100       unsigned offset_lo = dest &amp; 0xfff;
 101       offset = adr_page - pc_page;
 102 
 103       // We handle 4 types of PC relative addressing
 104       //   1 - adrp    Rx, target_page
 105       //       ldr/str Ry, [Rx, #offset_in_page]
 106       //   2 - adrp    Rx, target_page
 107       //       add     Ry, Rx, #offset_in_page
 108       //   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 109       //       movk    Rx, #imm16&lt;&lt;32
 110       //   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 111       // In the first 3 cases we must check that Rx is the same in the adrp and the
 112       // subsequent ldr/str, add or movk instruction. Otherwise we could accidentally end
 113       // up treating a type 4 relocation as a type 1, 2 or 3 just because it happened
 114       // to be followed by a random unrelated ldr/str, add or movk instruction.
 115       //
 116       unsigned insn2 = ((unsigned*)branch)[1];
 117       if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &amp;&amp;
 118                 Instruction_aarch64::extract(insn, 4, 0) ==
 119                         Instruction_aarch64::extract(insn2, 9, 5)) {
 120         // Load/store register (unsigned immediate)
 121         unsigned size = Instruction_aarch64::extract(insn2, 31, 30);
 122         Instruction_aarch64::patch(branch + sizeof (unsigned),
 123                                     21, 10, offset_lo &gt;&gt; size);
 124         guarantee(((dest &gt;&gt; size) &lt;&lt; size) == dest, &quot;misaligned target&quot;);
 125         instructions = 2;
 126       } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &amp;&amp;
 127                 Instruction_aarch64::extract(insn, 4, 0) ==
 128                         Instruction_aarch64::extract(insn2, 4, 0)) {
 129         // add (immediate)
 130         Instruction_aarch64::patch(branch + sizeof (unsigned),
 131                                    21, 10, offset_lo);
 132         instructions = 2;
 133       } else if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110 &amp;&amp;
 134                    Instruction_aarch64::extract(insn, 4, 0) ==
 135                      Instruction_aarch64::extract(insn2, 4, 0)) {
 136         // movk #imm16&lt;&lt;32
 137         Instruction_aarch64::patch(branch + 4, 20, 5, (uint64_t)target &gt;&gt; 32);
<span class="line-modified"> 138         int64_t dest = ((int64_t)target &amp; 0xffffffffL) | ((int64_t)branch &amp; 0xffff00000000L);</span>
<span class="line-modified"> 139         int64_t pc_page = (int64_t)branch &gt;&gt; 12;</span>
<span class="line-modified"> 140         int64_t adr_page = (int64_t)dest &gt;&gt; 12;</span>
 141         offset = adr_page - pc_page;
 142         instructions = 2;
 143       }
 144     }
 145     int offset_lo = offset &amp; 3;
 146     offset &gt;&gt;= 2;
 147     Instruction_aarch64::spatch(branch, 23, 5, offset);
 148     Instruction_aarch64::patch(branch, 30, 29, offset_lo);
 149   } else if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010100) {
<span class="line-modified"> 150     uint64_t dest = (uint64_t)target;</span>
 151     // Move wide constant
 152     assert(nativeInstruction_at(branch+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 153     assert(nativeInstruction_at(branch+8)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 154     Instruction_aarch64::patch(branch, 20, 5, dest &amp; 0xffff);
 155     Instruction_aarch64::patch(branch+4, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 156     Instruction_aarch64::patch(branch+8, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 157     assert(target_addr_for_insn(branch) == target, &quot;should be&quot;);
 158     instructions = 3;
 159   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 160              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 161     // nothing to do
 162     assert(target == 0, &quot;did not expect to relocate target for polling page load&quot;);
 163   } else {
 164     ShouldNotReachHere();
 165   }
 166   return instructions * NativeInstruction::instruction_size;
 167 }
 168 
 169 int MacroAssembler::patch_oop(address insn_addr, address o) {
 170   int instructions;
</pre>
<hr />
<pre>
 189     Instruction_aarch64::patch(insn_addr+8, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 190     instructions = 3;
 191   }
 192   return instructions * NativeInstruction::instruction_size;
 193 }
 194 
 195 int MacroAssembler::patch_narrow_klass(address insn_addr, narrowKlass n) {
 196   // Metatdata pointers are either narrow (32 bits) or wide (48 bits).
 197   // We encode narrow ones by setting the upper 16 bits in the first
 198   // instruction.
 199   NativeInstruction *insn = nativeInstruction_at(insn_addr);
 200   assert(Instruction_aarch64::extract(insn-&gt;encoding(), 31, 21) == 0b11010010101 &amp;&amp;
 201          nativeInstruction_at(insn_addr+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 202 
 203   Instruction_aarch64::patch(insn_addr, 20, 5, n &gt;&gt; 16);
 204   Instruction_aarch64::patch(insn_addr+4, 20, 5, n &amp; 0xffff);
 205   return 2 * NativeInstruction::instruction_size;
 206 }
 207 
 208 address MacroAssembler::target_addr_for_insn(address insn_addr, unsigned insn) {
<span class="line-modified"> 209   int64_t offset = 0;</span>
 210   if ((Instruction_aarch64::extract(insn, 29, 24) &amp; 0b011011) == 0b00011000) {
 211     // Load register (literal)
 212     offset = Instruction_aarch64::sextract(insn, 23, 5);
 213     return address(((uint64_t)insn_addr + (offset &lt;&lt; 2)));
 214   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
 215     // Unconditional branch (immediate)
 216     offset = Instruction_aarch64::sextract(insn, 25, 0);
 217   } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {
 218     // Conditional branch (immediate)
 219     offset = Instruction_aarch64::sextract(insn, 23, 5);
 220   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {
 221     // Compare &amp; branch (immediate)
 222     offset = Instruction_aarch64::sextract(insn, 23, 5);
 223    } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {
 224     // Test &amp; branch (immediate)
 225     offset = Instruction_aarch64::sextract(insn, 18, 5);
 226   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
 227     // PC-rel. addressing
 228     offset = Instruction_aarch64::extract(insn, 30, 29);
 229     offset |= Instruction_aarch64::sextract(insn, 23, 5) &lt;&lt; 2;
</pre>
<hr />
<pre>
 256         return address(target_page + (byte_offset &lt;&lt; size));
 257       } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &amp;&amp;
 258                 Instruction_aarch64::extract(insn, 4, 0) ==
 259                         Instruction_aarch64::extract(insn2, 4, 0)) {
 260         // add (immediate)
 261         unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);
 262         return address(target_page + byte_offset);
 263       } else {
 264         if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110  &amp;&amp;
 265                Instruction_aarch64::extract(insn, 4, 0) ==
 266                  Instruction_aarch64::extract(insn2, 4, 0)) {
 267           target_page = (target_page &amp; 0xffffffff) |
 268                          ((uint64_t)Instruction_aarch64::extract(insn2, 20, 5) &lt;&lt; 32);
 269         }
 270         return (address)target_page;
 271       }
 272     } else {
 273       ShouldNotReachHere();
 274     }
 275   } else if (Instruction_aarch64::extract(insn, 31, 23) == 0b110100101) {
<span class="line-modified"> 276     uint32_t *insns = (uint32_t *)insn_addr;</span>
 277     // Move wide constant: movz, movk, movk.  See movptr().
 278     assert(nativeInstruction_at(insns+1)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 279     assert(nativeInstruction_at(insns+2)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
<span class="line-modified"> 280     return address(uint64_t(Instruction_aarch64::extract(insns[0], 20, 5))</span>
<span class="line-modified"> 281                    + (uint64_t(Instruction_aarch64::extract(insns[1], 20, 5)) &lt;&lt; 16)</span>
<span class="line-modified"> 282                    + (uint64_t(Instruction_aarch64::extract(insns[2], 20, 5)) &lt;&lt; 32));</span>
 283   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 284              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 285     return 0;
 286   } else {
 287     ShouldNotReachHere();
 288   }
 289   return address(((uint64_t)insn_addr + (offset &lt;&lt; 2)));
 290 }
 291 
 292 void MacroAssembler::safepoint_poll(Label&amp; slow_path) {
 293   if (SafepointMechanism::uses_thread_local_poll()) {
 294     ldr(rscratch1, Address(rthread, Thread::polling_page_offset()));
 295     tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);
 296   } else {
<span class="line-modified"> 297     uint64_t offset;</span>
 298     adrp(rscratch1, ExternalAddress(SafepointSynchronize::address_of_state()), offset);
 299     ldrw(rscratch1, Address(rscratch1, offset));
 300     assert(SafepointSynchronize::_not_synchronized == 0, &quot;rewrite this code&quot;);
 301     cbnz(rscratch1, slow_path);
 302   }
 303 }
 304 
 305 // Just like safepoint_poll, but use an acquiring load for thread-
 306 // local polling.
 307 //
 308 // We need an acquire here to ensure that any subsequent load of the
 309 // global SafepointSynchronize::_state flag is ordered after this load
 310 // of the local Thread::_polling page.  We don&#39;t want this poll to
 311 // return false (i.e. not safepointing) and a later poll of the global
 312 // SafepointSynchronize::_state spuriously to return true.
 313 //
 314 // This is to avoid a race when we&#39;re in a native-&gt;Java transition
 315 // racing the code which wakes up from a safepoint.
 316 //
 317 void MacroAssembler::safepoint_poll_acquire(Label&amp; slow_path) {
</pre>
<hr />
<pre>
 385 }
 386 
 387 void MacroAssembler::set_last_Java_frame(Register last_java_sp,
 388                                          Register last_java_fp,
 389                                          Label &amp;L,
 390                                          Register scratch) {
 391   if (L.is_bound()) {
 392     set_last_Java_frame(last_java_sp, last_java_fp, target(L), scratch);
 393   } else {
 394     InstructionMark im(this);
 395     L.add_patch_at(code(), locator());
 396     set_last_Java_frame(last_java_sp, last_java_fp, pc() /* Patched later */, scratch);
 397   }
 398 }
 399 
 400 void MacroAssembler::far_call(Address entry, CodeBuffer *cbuf, Register tmp) {
 401   assert(ReservedCodeCacheSize &lt; 4*G, &quot;branch out of range&quot;);
 402   assert(CodeCache::find_blob(entry.target()) != NULL,
 403          &quot;destination of far call not found in code cache&quot;);
 404   if (far_branches()) {
<span class="line-modified"> 405     uint64_t offset;</span>
 406     // We can use ADRP here because we know that the total size of
 407     // the code cache cannot exceed 2Gb.
 408     adrp(tmp, entry, offset);
 409     add(tmp, tmp, offset);
 410     if (cbuf) cbuf-&gt;set_insts_mark();
 411     blr(tmp);
 412   } else {
 413     if (cbuf) cbuf-&gt;set_insts_mark();
 414     bl(entry);
 415   }
 416 }
 417 
 418 void MacroAssembler::far_jump(Address entry, CodeBuffer *cbuf, Register tmp) {
 419   assert(ReservedCodeCacheSize &lt; 4*G, &quot;branch out of range&quot;);
 420   assert(CodeCache::find_blob(entry.target()) != NULL,
 421          &quot;destination of far call not found in code cache&quot;);
 422   if (far_branches()) {
<span class="line-modified"> 423     uint64_t offset;</span>
 424     // We can use ADRP here because we know that the total size of
 425     // the code cache cannot exceed 2Gb.
 426     adrp(tmp, entry, offset);
 427     add(tmp, tmp, offset);
 428     if (cbuf) cbuf-&gt;set_insts_mark();
 429     br(tmp);
 430   } else {
 431     if (cbuf) cbuf-&gt;set_insts_mark();
 432     b(entry);
 433   }
 434 }
 435 
 436 void MacroAssembler::reserved_stack_check() {
 437     // testing if reserved zone needs to be enabled
 438     Label no_reserved_zone_enabling;
 439 
 440     ldr(rscratch1, Address(rthread, JavaThread::reserved_stack_activation_offset()));
 441     cmp(sp, rscratch1);
 442     br(Assembler::LO, no_reserved_zone_enabling);
 443 
</pre>
<hr />
<pre>
1385   lea(rscratch2, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
1386   ldr(rscratch2, Address(rscratch2));
1387   blr(rscratch2);
1388 
1389   ldp(rscratch2, lr, Address(post(sp, 2 * wordSize)));
1390   ldp(r0, rscratch1, Address(post(sp, 2 * wordSize)));
1391 
1392   BLOCK_COMMENT(&quot;} verify_oop_addr&quot;);
1393 }
1394 
1395 Address MacroAssembler::argument_address(RegisterOrConstant arg_slot,
1396                                          int extra_slot_offset) {
1397   // cf. TemplateTable::prepare_invoke(), if (load_receiver).
1398   int stackElementSize = Interpreter::stackElementSize;
1399   int offset = Interpreter::expr_offset_in_bytes(extra_slot_offset+0);
1400 #ifdef ASSERT
1401   int offset1 = Interpreter::expr_offset_in_bytes(extra_slot_offset+1);
1402   assert(offset1 - offset == stackElementSize, &quot;correct arithmetic&quot;);
1403 #endif
1404   if (arg_slot.is_constant()) {
<span class="line-modified">1405     int calc_offset = arg_slot.as_constant() * stackElementSize + offset;</span>
<span class="line-modified">1406     return Address(esp, calc_offset);</span>
1407   } else {
1408     add(rscratch1, esp, arg_slot.as_register(),
1409         ext::uxtx, exact_log2(stackElementSize));
1410     return Address(rscratch1, offset);
1411   }
1412 }
1413 
1414 void MacroAssembler::call_VM_leaf_base(address entry_point,
1415                                        int number_of_arguments,
1416                                        Label *retaddr) {
1417   Label E, L;
1418 
1419   stp(rscratch1, rmethod, Address(pre(sp, -2 * wordSize)));
1420 
1421   mov(rscratch1, entry_point);
1422   blr(rscratch1);
1423   if (retaddr)
1424     bind(*retaddr);
1425 
1426   ldp(rscratch1, rmethod, Address(post(sp, 2 * wordSize)));
</pre>
<hr />
<pre>
1487   MacroAssembler::call_VM_leaf_base(entry_point, 4);
1488 }
1489 
1490 void MacroAssembler::null_check(Register reg, int offset) {
1491   if (needs_explicit_null_check(offset)) {
1492     // provoke OS NULL exception if reg = NULL by
1493     // accessing M[reg] w/o changing any registers
1494     // NOTE: this is plenty to provoke a segv
1495     ldr(zr, Address(reg));
1496   } else {
1497     // nothing to do, (later) access of M[reg + offset]
1498     // will provoke OS NULL exception if reg = NULL
1499   }
1500 }
1501 
1502 // MacroAssembler protected routines needed to implement
1503 // public methods
1504 
1505 void MacroAssembler::mov(Register r, Address dest) {
1506   code_section()-&gt;relocate(pc(), dest.rspec());
<span class="line-modified">1507   uint64_t imm64 = (uint64_t)dest.target();</span>
1508   movptr(r, imm64);
1509 }
1510 
1511 // Move a constant pointer into r.  In AArch64 mode the virtual
1512 // address space is 48 bits in size, so we only need three
1513 // instructions to create a patchable instruction sequence that can
1514 // reach anywhere.
1515 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1516 #ifndef PRODUCT
1517   {
1518     char buffer[64];
1519     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1520     block_comment(buffer);
1521   }
1522 #endif
<span class="line-modified">1523   assert(imm64 &lt; (1ull &lt;&lt; 48), &quot;48-bit overflow in address constant&quot;);</span>
1524   movz(r, imm64 &amp; 0xffff);
1525   imm64 &gt;&gt;= 16;
1526   movk(r, imm64 &amp; 0xffff, 16);
1527   imm64 &gt;&gt;= 16;
1528   movk(r, imm64 &amp; 0xffff, 32);
1529 }
1530 
1531 // Macro to mov replicated immediate to vector register.
1532 //  Vd will get the following values for different arrangements in T
1533 //   imm32 == hex 000000gh  T8B:  Vd = ghghghghghghghgh
1534 //   imm32 == hex 000000gh  T16B: Vd = ghghghghghghghghghghghghghghghgh
1535 //   imm32 == hex 0000efgh  T4H:  Vd = efghefghefghefgh
1536 //   imm32 == hex 0000efgh  T8H:  Vd = efghefghefghefghefghefghefghefgh
1537 //   imm32 == hex abcdefgh  T2S:  Vd = abcdefghabcdefgh
1538 //   imm32 == hex abcdefgh  T4S:  Vd = abcdefghabcdefghabcdefghabcdefgh
1539 //   T1D/T2D: invalid
<span class="line-modified">1540 void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, uint32_t imm32) {</span>
1541   assert(T != T1D &amp;&amp; T != T2D, &quot;invalid arrangement&quot;);
1542   if (T == T8B || T == T16B) {
1543     assert((imm32 &amp; ~0xff) == 0, &quot;extraneous bits in unsigned imm32 (T8B/T16B)&quot;);
1544     movi(Vd, T, imm32 &amp; 0xff, 0);
1545     return;
1546   }
<span class="line-modified">1547   uint32_t nimm32 = ~imm32;</span>
1548   if (T == T4H || T == T8H) {
1549     assert((imm32  &amp; ~0xffff) == 0, &quot;extraneous bits in unsigned imm32 (T4H/T8H)&quot;);
1550     imm32 &amp;= 0xffff;
1551     nimm32 &amp;= 0xffff;
1552   }
<span class="line-modified">1553   uint32_t x = imm32;</span>
1554   int movi_cnt = 0;
1555   int movn_cnt = 0;
1556   while (x) { if (x &amp; 0xff) movi_cnt++; x &gt;&gt;= 8; }
1557   x = nimm32;
1558   while (x) { if (x &amp; 0xff) movn_cnt++; x &gt;&gt;= 8; }
1559   if (movn_cnt &lt; movi_cnt) imm32 = nimm32;
1560   unsigned lsl = 0;
1561   while (imm32 &amp;&amp; (imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1562   if (movn_cnt &lt; movi_cnt)
1563     mvni(Vd, T, imm32 &amp; 0xff, lsl);
1564   else
1565     movi(Vd, T, imm32 &amp; 0xff, lsl);
1566   imm32 &gt;&gt;= 8; lsl += 8;
1567   while (imm32) {
1568     while ((imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1569     if (movn_cnt &lt; movi_cnt)
1570       bici(Vd, T, imm32 &amp; 0xff, lsl);
1571     else
1572       orri(Vd, T, imm32 &amp; 0xff, lsl);
1573     lsl += 8; imm32 &gt;&gt;= 8;
1574   }
1575 }
1576 
<span class="line-modified">1577 void MacroAssembler::mov_immediate64(Register dst, uint64_t imm64)</span>
1578 {
1579 #ifndef PRODUCT
1580   {
1581     char buffer[64];
1582     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1583     block_comment(buffer);
1584   }
1585 #endif
1586   if (operand_valid_for_logical_immediate(false, imm64)) {
1587     orr(dst, zr, imm64);
1588   } else {
1589     // we can use a combination of MOVZ or MOVN with
1590     // MOVK to build up the constant
<span class="line-modified">1591     uint64_t imm_h[4];</span>
1592     int zero_count = 0;
1593     int neg_count = 0;
1594     int i;
1595     for (i = 0; i &lt; 4; i++) {
1596       imm_h[i] = ((imm64 &gt;&gt; (i * 16)) &amp; 0xffffL);
1597       if (imm_h[i] == 0) {
1598         zero_count++;
1599       } else if (imm_h[i] == 0xffffL) {
1600         neg_count++;
1601       }
1602     }
1603     if (zero_count == 4) {
1604       // one MOVZ will do
1605       movz(dst, 0);
1606     } else if (neg_count == 4) {
1607       // one MOVN will do
1608       movn(dst, 0);
1609     } else if (zero_count == 3) {
1610       for (i = 0; i &lt; 4; i++) {
1611         if (imm_h[i] != 0L) {
<span class="line-modified">1612           movz(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1613           break;
1614         }
1615       }
1616     } else if (neg_count == 3) {
1617       // one MOVN will do
1618       for (int i = 0; i &lt; 4; i++) {
1619         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1620           movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1621           break;
1622         }
1623       }
1624     } else if (zero_count == 2) {
1625       // one MOVZ and one MOVK will do
1626       for (i = 0; i &lt; 3; i++) {
1627         if (imm_h[i] != 0L) {
<span class="line-modified">1628           movz(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1629           i++;
1630           break;
1631         }
1632       }
1633       for (;i &lt; 4; i++) {
1634         if (imm_h[i] != 0L) {
<span class="line-modified">1635           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1636         }
1637       }
1638     } else if (neg_count == 2) {
1639       // one MOVN and one MOVK will do
1640       for (i = 0; i &lt; 4; i++) {
1641         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1642           movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1643           i++;
1644           break;
1645         }
1646       }
1647       for (;i &lt; 4; i++) {
1648         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1649           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1650         }
1651       }
1652     } else if (zero_count == 1) {
1653       // one MOVZ and two MOVKs will do
1654       for (i = 0; i &lt; 4; i++) {
1655         if (imm_h[i] != 0L) {
<span class="line-modified">1656           movz(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1657           i++;
1658           break;
1659         }
1660       }
1661       for (;i &lt; 4; i++) {
1662         if (imm_h[i] != 0x0L) {
<span class="line-modified">1663           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1664         }
1665       }
1666     } else if (neg_count == 1) {
1667       // one MOVN and two MOVKs will do
1668       for (i = 0; i &lt; 4; i++) {
1669         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1670           movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1671           i++;
1672           break;
1673         }
1674       }
1675       for (;i &lt; 4; i++) {
1676         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1677           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1678         }
1679       }
1680     } else {
1681       // use a MOVZ and 3 MOVKs (makes it easier to debug)
<span class="line-modified">1682       movz(dst, (uint32_t)imm_h[0], 0);</span>
1683       for (i = 1; i &lt; 4; i++) {
<span class="line-modified">1684         movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1685       }
1686     }
1687   }
1688 }
1689 
<span class="line-modified">1690 void MacroAssembler::mov_immediate32(Register dst, uint32_t imm32)</span>
1691 {
1692 #ifndef PRODUCT
1693     {
1694       char buffer[64];
1695       snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX32, imm32);
1696       block_comment(buffer);
1697     }
1698 #endif
1699   if (operand_valid_for_logical_immediate(true, imm32)) {
1700     orrw(dst, zr, imm32);
1701   } else {
1702     // we can use MOVZ, MOVN or two calls to MOVK to build up the
1703     // constant
<span class="line-modified">1704     uint32_t imm_h[2];</span>
1705     imm_h[0] = imm32 &amp; 0xffff;
1706     imm_h[1] = ((imm32 &gt;&gt; 16) &amp; 0xffff);
1707     if (imm_h[0] == 0) {
1708       movzw(dst, imm_h[1], 16);
1709     } else if (imm_h[0] == 0xffff) {
1710       movnw(dst, imm_h[1] ^ 0xffff, 16);
1711     } else if (imm_h[1] == 0) {
1712       movzw(dst, imm_h[0], 0);
1713     } else if (imm_h[1] == 0xffff) {
1714       movnw(dst, imm_h[0] ^ 0xffff, 0);
1715     } else {
1716       // use a MOVZ and MOVK (makes it easier to debug)
1717       movzw(dst, imm_h[0], 0);
1718       movkw(dst, imm_h[1], 16);
1719     }
1720   }
1721 }
1722 
1723 // Form an address from base + offset in Rd.  Rd may or may
1724 // not actually be used: you must use the Address that is returned.
1725 // It is up to you to ensure that the shift provided matches the size
1726 // of your data.
<span class="line-modified">1727 Address MacroAssembler::form_address(Register Rd, Register base, int64_t byte_offset, int shift) {</span>
1728   if (Address::offset_ok_for_immed(byte_offset, shift))
1729     // It fits; no need for any heroics
1730     return Address(base, byte_offset);
1731 
1732   // Don&#39;t do anything clever with negative or misaligned offsets
1733   unsigned mask = (1 &lt;&lt; shift) - 1;
1734   if (byte_offset &lt; 0 || byte_offset &amp; mask) {
1735     mov(Rd, byte_offset);
1736     add(Rd, base, Rd);
1737     return Address(Rd);
1738   }
1739 
1740   // See if we can do this with two 12-bit offsets
1741   {
<span class="line-modified">1742     uint64_t word_offset = byte_offset &gt;&gt; shift;</span>
<span class="line-modified">1743     uint64_t masked_offset = word_offset &amp; 0xfff000;</span>
1744     if (Address::offset_ok_for_immed(word_offset - masked_offset, 0)
1745         &amp;&amp; Assembler::operand_valid_for_add_sub_immediate(masked_offset &lt;&lt; shift)) {
1746       add(Rd, base, masked_offset &lt;&lt; shift);
1747       word_offset -= masked_offset;
1748       return Address(Rd, word_offset &lt;&lt; shift);
1749     }
1750   }
1751 
1752   // Do it the hard way
1753   mov(Rd, byte_offset);
1754   add(Rd, base, Rd);
1755   return Address(Rd);
1756 }
1757 
1758 void MacroAssembler::atomic_incw(Register counter_addr, Register tmp, Register tmp2) {
1759   if (UseLSE) {
1760     mov(tmp, 1);
1761     ldadd(Assembler::word, tmp, zr, counter_addr);
1762     return;
1763   }
</pre>
<hr />
<pre>
1964 
1965 void MacroAssembler::decrementw(Register reg, int value)
1966 {
1967   if (value &lt; 0)  { incrementw(reg, -value);      return; }
1968   if (value == 0) {                               return; }
1969   if (value &lt; (1 &lt;&lt; 12)) { subw(reg, reg, value); return; }
1970   /* else */ {
1971     guarantee(reg != rscratch2, &quot;invalid dst for register decrement&quot;);
1972     movw(rscratch2, (unsigned)value);
1973     subw(reg, reg, rscratch2);
1974   }
1975 }
1976 
1977 void MacroAssembler::decrement(Register reg, int value)
1978 {
1979   if (value &lt; 0)  { increment(reg, -value);      return; }
1980   if (value == 0) {                              return; }
1981   if (value &lt; (1 &lt;&lt; 12)) { sub(reg, reg, value); return; }
1982   /* else */ {
1983     assert(reg != rscratch2, &quot;invalid dst for register decrement&quot;);
<span class="line-modified">1984     mov(rscratch2, (uint64_t) value);</span>
1985     sub(reg, reg, rscratch2);
1986   }
1987 }
1988 
1989 void MacroAssembler::decrementw(Address dst, int value)
1990 {
1991   assert(!dst.uses(rscratch1), &quot;invalid dst for address decrement&quot;);
1992   if (dst.getMode() == Address::literal) {
1993     assert(abs(value) &lt; (1 &lt;&lt; 12), &quot;invalid value and address mode combination&quot;);
1994     lea(rscratch2, dst);
1995     dst = Address(rscratch2);
1996   }
1997   ldrw(rscratch1, dst);
1998   decrementw(rscratch1, value);
1999   strw(rscratch1, dst);
2000 }
2001 
2002 void MacroAssembler::decrement(Address dst, int value)
2003 {
2004   assert(!dst.uses(rscratch1), &quot;invalid address for decrement&quot;);
</pre>
<hr />
<pre>
2483   } else {
2484     ShouldNotReachHere();
2485   }
2486 }
2487 
2488 
2489 static bool different(Register a, RegisterOrConstant b, Register c) {
2490   if (b.is_constant())
2491     return a != c;
2492   else
2493     return a != b.as_register() &amp;&amp; a != c &amp;&amp; b.as_register() != c;
2494 }
2495 
2496 #define ATOMIC_OP(NAME, LDXR, OP, IOP, AOP, STXR, sz)                   \
2497 void MacroAssembler::atomic_##NAME(Register prev, RegisterOrConstant incr, Register addr) { \
2498   if (UseLSE) {                                                         \
2499     prev = prev-&gt;is_valid() ? prev : zr;                                \
2500     if (incr.is_register()) {                                           \
2501       AOP(sz, incr.as_register(), prev, addr);                          \
2502     } else {                                                            \
<span class="line-modified">2503       mov(rscratch2, (address)incr.as_constant());                      \</span>
2504       AOP(sz, rscratch2, prev, addr);                                   \
2505     }                                                                   \
2506     return;                                                             \
2507   }                                                                     \
2508   Register result = rscratch2;                                          \
2509   if (prev-&gt;is_valid())                                                 \
2510     result = different(prev, incr, addr) ? prev : rscratch2;            \
2511                                                                         \
2512   Label retry_load;                                                     \
2513   if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))         \
2514     prfm(Address(addr), PSTL1STRM);                                     \
2515   bind(retry_load);                                                     \
2516   LDXR(result, addr);                                                   \
2517   OP(rscratch1, result, incr);                                          \
2518   STXR(rscratch2, rscratch1, addr);                                     \
2519   cbnzw(rscratch2, retry_load);                                         \
2520   if (prev-&gt;is_valid() &amp;&amp; prev != result) {                             \
2521     IOP(prev, rscratch1, incr);                                         \
2522   }                                                                     \
2523 }
</pre>
<hr />
<pre>
2605       tty-&gt;print_cr(&quot;r19 = 0x%016lx&quot;, regs[19]);
2606       tty-&gt;print_cr(&quot;r20 = 0x%016lx&quot;, regs[20]);
2607       tty-&gt;print_cr(&quot;r21 = 0x%016lx&quot;, regs[21]);
2608       tty-&gt;print_cr(&quot;r22 = 0x%016lx&quot;, regs[22]);
2609       tty-&gt;print_cr(&quot;r23 = 0x%016lx&quot;, regs[23]);
2610       tty-&gt;print_cr(&quot;r24 = 0x%016lx&quot;, regs[24]);
2611       tty-&gt;print_cr(&quot;r25 = 0x%016lx&quot;, regs[25]);
2612       tty-&gt;print_cr(&quot;r26 = 0x%016lx&quot;, regs[26]);
2613       tty-&gt;print_cr(&quot;r27 = 0x%016lx&quot;, regs[27]);
2614       tty-&gt;print_cr(&quot;r28 = 0x%016lx&quot;, regs[28]);
2615       tty-&gt;print_cr(&quot;r30 = 0x%016lx&quot;, regs[30]);
2616       tty-&gt;print_cr(&quot;r31 = 0x%016lx&quot;, regs[31]);
2617       BREAKPOINT;
2618     }
2619   }
2620   fatal(&quot;DEBUG MESSAGE: %s&quot;, msg);
2621 }
2622 
2623 void MacroAssembler::push_call_clobbered_registers() {
2624   int step = 4 * wordSize;
<span class="line-modified">2625   push(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2) WIN64_ONLY(- r18), sp);</span>
2626   sub(sp, sp, step);
2627   mov(rscratch1, -step);
2628   // Push v0-v7, v16-v31.
2629   for (int i = 31; i&gt;= 4; i -= 4) {
2630     if (i &lt;= v7-&gt;encoding() || i &gt;= v16-&gt;encoding())
2631       st1(as_FloatRegister(i-3), as_FloatRegister(i-2), as_FloatRegister(i-1),
2632           as_FloatRegister(i), T1D, Address(post(sp, rscratch1)));
2633   }
2634   st1(as_FloatRegister(0), as_FloatRegister(1), as_FloatRegister(2),
2635       as_FloatRegister(3), T1D, Address(sp));
2636 }
2637 
2638 void MacroAssembler::pop_call_clobbered_registers() {
2639   for (int i = 0; i &lt; 32; i += 4) {
2640     if (i &lt;= v7-&gt;encoding() || i &gt;= v16-&gt;encoding())
2641       ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2642           as_FloatRegister(i+3), T1D, Address(post(sp, 4 * wordSize)));
2643   }
<span class="line-modified">2644   pop(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2) WIN64_ONLY(- r18), sp);</span>

2645 }
2646 
2647 void MacroAssembler::push_CPU_state(bool save_vectors) {
2648   int step = (save_vectors ? 8 : 4) * wordSize;
2649   push(0x3fffffff, sp);         // integer registers except lr &amp; sp
2650   mov(rscratch1, -step);
2651   sub(sp, sp, step);
2652   for (int i = 28; i &gt;= 4; i -= 4) {
2653     st1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2654         as_FloatRegister(i+3), save_vectors ? T2D : T1D, Address(post(sp, rscratch1)));
2655   }
2656   st1(v0, v1, v2, v3, save_vectors ? T2D : T1D, sp);
2657 }
2658 
2659 void MacroAssembler::pop_CPU_state(bool restore_vectors) {
2660   int step = (restore_vectors ? 8 : 4) * wordSize;
2661   for (int i = 0; i &lt;= 28; i += 4)
2662     ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2663         as_FloatRegister(i+3), restore_vectors ? T2D : T1D, Address(post(sp, step)));
2664   pop(0x3fffffff, sp);         // integer registers except lr &amp; sp
</pre>
<hr />
<pre>
2696   Register base = sp;
2697   if ((offset &amp; (size-1)) &amp;&amp; offset &gt;= (1&lt;&lt;8)) {
2698     add(tmp, base, offset &amp; ((1&lt;&lt;12)-1));
2699     base = tmp;
2700     offset &amp;= -1u&lt;&lt;12;
2701   }
2702 
2703   if (offset &gt;= (1&lt;&lt;12) * size) {
2704     add(tmp, base, offset &amp; (((1&lt;&lt;12)-1)&lt;&lt;12));
2705     base = tmp;
2706     offset &amp;= ~(((1&lt;&lt;12)-1)&lt;&lt;12);
2707   }
2708 
2709   return Address(base, offset);
2710 }
2711 
2712 // Checks whether offset is aligned.
2713 // Returns true if it is, else false.
2714 bool MacroAssembler::merge_alignment_check(Register base,
2715                                            size_t size,
<span class="line-modified">2716                                            int64_t cur_offset,</span>
<span class="line-modified">2717                                            int64_t prev_offset) const {</span>
2718   if (AvoidUnalignedAccesses) {
2719     if (base == sp) {
2720       // Checks whether low offset if aligned to pair of registers.
<span class="line-modified">2721       int64_t pair_mask = size * 2 - 1;</span>
<span class="line-modified">2722       int64_t offset = prev_offset &gt; cur_offset ? cur_offset : prev_offset;</span>
2723       return (offset &amp; pair_mask) == 0;
2724     } else { // If base is not sp, we can&#39;t guarantee the access is aligned.
2725       return false;
2726     }
2727   } else {
<span class="line-modified">2728     int64_t mask = size - 1;</span>
2729     // Load/store pair instruction only supports element size aligned offset.
2730     return (cur_offset &amp; mask) == 0 &amp;&amp; (prev_offset &amp; mask) == 0;
2731   }
2732 }
2733 
2734 // Checks whether current and previous loads/stores can be merged.
2735 // Returns true if it can be merged, else false.
2736 bool MacroAssembler::ldst_can_merge(Register rt,
2737                                     const Address &amp;adr,
2738                                     size_t cur_size_in_bytes,
2739                                     bool is_store) const {
2740   address prev = pc() - NativeInstruction::instruction_size;
2741   address last = code()-&gt;last_insn();
2742 
2743   if (last == NULL || !nativeInstruction_at(last)-&gt;is_Imm_LdSt()) {
2744     return false;
2745   }
2746 
2747   if (adr.getMode() != Address::base_plus_offset || prev != last) {
2748     return false;
2749   }
2750 
2751   NativeLdSt* prev_ldst = NativeLdSt_at(prev);
2752   size_t prev_size_in_bytes = prev_ldst-&gt;size_in_bytes();
2753 
2754   assert(prev_size_in_bytes == 4 || prev_size_in_bytes == 8, &quot;only supports 64/32bit merging.&quot;);
2755   assert(cur_size_in_bytes == 4 || cur_size_in_bytes == 8, &quot;only supports 64/32bit merging.&quot;);
2756 
2757   if (cur_size_in_bytes != prev_size_in_bytes || is_store != prev_ldst-&gt;is_store()) {
2758     return false;
2759   }
2760 
<span class="line-modified">2761   int64_t max_offset = 63 * prev_size_in_bytes;</span>
<span class="line-modified">2762   int64_t min_offset = -64 * prev_size_in_bytes;</span>
2763 
2764   assert(prev_ldst-&gt;is_not_pre_post_index(), &quot;pre-index or post-index is not supported to be merged.&quot;);
2765 
2766   // Only same base can be merged.
2767   if (adr.base() != prev_ldst-&gt;base()) {
2768     return false;
2769   }
2770 
<span class="line-modified">2771   int64_t cur_offset = adr.offset();</span>
<span class="line-modified">2772   int64_t prev_offset = prev_ldst-&gt;offset();</span>
2773   size_t diff = abs(cur_offset - prev_offset);
2774   if (diff != prev_size_in_bytes) {
2775     return false;
2776   }
2777 
2778   // Following cases can not be merged:
2779   // ldr x2, [x2, #8]
2780   // ldr x3, [x2, #16]
2781   // or:
2782   // ldr x2, [x3, #8]
2783   // ldr x2, [x3, #16]
2784   // If t1 and t2 is the same in &quot;ldp t1, t2, [xn, #imm]&quot;, we&#39;ll get SIGILL.
2785   if (!is_store &amp;&amp; (adr.base() == prev_ldst-&gt;target() || rt == prev_ldst-&gt;target())) {
2786     return false;
2787   }
2788 
<span class="line-modified">2789   int64_t low_offset = prev_offset &gt; cur_offset ? cur_offset : prev_offset;</span>
2790   // Offset range must be in ldp/stp instruction&#39;s range.
2791   if (low_offset &gt; max_offset || low_offset &lt; min_offset) {
2792     return false;
2793   }
2794 
2795   if (merge_alignment_check(adr.base(), prev_size_in_bytes, cur_offset, prev_offset)) {
2796     return true;
2797   }
2798 
2799   return false;
2800 }
2801 
2802 // Merge current load/store with previous load/store into ldp/stp.
2803 void MacroAssembler::merge_ldst(Register rt,
2804                                 const Address &amp;adr,
2805                                 size_t cur_size_in_bytes,
2806                                 bool is_store) {
2807 
2808   assert(ldst_can_merge(rt, adr, cur_size_in_bytes, is_store) == true, &quot;cur and prev must be able to be merged.&quot;);
2809 
2810   Register rt_low, rt_high;
2811   address prev = pc() - NativeInstruction::instruction_size;
2812   NativeLdSt* prev_ldst = NativeLdSt_at(prev);
2813 
<span class="line-modified">2814   int64_t offset;</span>
2815 
2816   if (adr.offset() &lt; prev_ldst-&gt;offset()) {
2817     offset = adr.offset();
2818     rt_low = rt;
2819     rt_high = prev_ldst-&gt;target();
2820   } else {
2821     offset = prev_ldst-&gt;offset();
2822     rt_low = prev_ldst-&gt;target();
2823     rt_high = rt;
2824   }
2825 
2826   Address adr_p = Address(prev_ldst-&gt;base(), offset);
2827   // Overwrite previous generated binary.
2828   code_section()-&gt;set_end(prev);
2829 
2830   const int sz = prev_ldst-&gt;size_in_bytes();
2831   assert(sz == 8 || sz == 4, &quot;only supports 64/32bit merging.&quot;);
2832   if (!is_store) {
2833     BLOCK_COMMENT(&quot;merged ldr pair&quot;);
2834     if (sz == 8) {
</pre>
<hr />
<pre>
3340 
3341     sub(len, len, 64);
3342     add(buf, buf, 8);
3343     cmn(len, 128);
3344     br(Assembler::NE, CRC_less64);
3345   BIND(L_exit);
3346     mvnw(crc, crc);
3347 }
3348 
3349 /**
3350  * @param crc   register containing existing CRC (32-bit)
3351  * @param buf   register pointing to input byte buffer (byte*)
3352  * @param len   register containing number of bytes
3353  * @param table register that will contain address of CRC table
3354  * @param tmp   scratch register
3355  */
3356 void MacroAssembler::kernel_crc32(Register crc, Register buf, Register len,
3357         Register table0, Register table1, Register table2, Register table3,
3358         Register tmp, Register tmp2, Register tmp3) {
3359   Label L_by16, L_by16_loop, L_by4, L_by4_loop, L_by1, L_by1_loop, L_exit;
<span class="line-modified">3360   uint64_t offset;</span>
3361 
3362   if (UseCRC32) {
3363       kernel_crc32_using_crc32(crc, buf, len, table0, table1, table2, table3);
3364       return;
3365   }
3366 
3367     mvnw(crc, crc);
3368 
3369     adrp(table0, ExternalAddress(StubRoutines::crc_table_addr()), offset);
3370     if (offset) add(table0, table0, offset);
3371     add(table1, table0, 1*256*sizeof(juint));
3372     add(table2, table0, 2*256*sizeof(juint));
3373     add(table3, table0, 3*256*sizeof(juint));
3374 
3375   if (UseNeon) {
3376       cmp(len, (u1)64);
3377       br(Assembler::LT, L_by16);
3378       eor(v16, T16B, v16, v16);
3379 
3380     Label L_fold;
</pre>
<hr />
<pre>
3642   BIND(L_exit);
3643 }
3644 
3645 /**
3646  * @param crc   register containing existing CRC (32-bit)
3647  * @param buf   register pointing to input byte buffer (byte*)
3648  * @param len   register containing number of bytes
3649  * @param table register that will contain address of CRC table
3650  * @param tmp   scratch register
3651  */
3652 void MacroAssembler::kernel_crc32c(Register crc, Register buf, Register len,
3653         Register table0, Register table1, Register table2, Register table3,
3654         Register tmp, Register tmp2, Register tmp3) {
3655   kernel_crc32c_using_crc32c(crc, buf, len, table0, table1, table2, table3);
3656 }
3657 
3658 
3659 SkipIfEqual::SkipIfEqual(
3660     MacroAssembler* masm, const bool* flag_addr, bool value) {
3661   _masm = masm;
<span class="line-modified">3662   uint64_t offset;</span>
3663   _masm-&gt;adrp(rscratch1, ExternalAddress((address)flag_addr), offset);
3664   _masm-&gt;ldrb(rscratch1, Address(rscratch1, offset));
3665   _masm-&gt;cbzw(rscratch1, _label);
3666 }
3667 
3668 SkipIfEqual::~SkipIfEqual() {
3669   _masm-&gt;bind(_label);
3670 }
3671 
3672 void MacroAssembler::addptr(const Address &amp;dst, int32_t src) {
3673   Address adr;
3674   switch(dst.getMode()) {
3675   case Address::base_plus_offset:
3676     // This is the expected mode, although we allow all the other
3677     // forms below.
3678     adr = form_address(rscratch2, dst.base(), dst.offset(), LogBytesPerWord);
3679     break;
3680   default:
3681     lea(rscratch2, dst);
3682     adr = Address(rscratch2);
3683     break;
3684   }
3685   ldr(rscratch1, adr);
3686   add(rscratch1, rscratch1, src);
3687   str(rscratch1, adr);
3688 }
3689 
3690 void MacroAssembler::cmpptr(Register src1, Address src2) {
<span class="line-modified">3691   uint64_t offset;</span>
3692   adrp(rscratch1, src2, offset);
3693   ldr(rscratch1, Address(rscratch1, offset));
3694   cmp(src1, rscratch1);
3695 }
3696 
3697 void MacroAssembler::cmpoop(Register obj1, Register obj2) {
3698   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3699   bs-&gt;obj_equals(this, obj1, obj2);
3700 }
3701 
3702 void MacroAssembler::load_method_holder(Register holder, Register method) {
3703   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
3704   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
3705   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
3706 }
3707 
3708 void MacroAssembler::load_klass(Register dst, Register src) {
3709   if (UseCompressedClassPointers) {
3710     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3711     decode_klass_not_null(dst);
</pre>
<hr />
<pre>
4292   // Bang down shadow pages too.
4293   // At this point, (tmp-0) is the last address touched, so don&#39;t
4294   // touch it again.  (It was touched as (tmp-pagesize) but then tmp
4295   // was post-decremented.)  Skip this address by starting at i=1, and
4296   // touch a few more pages below.  N.B.  It is important to touch all
4297   // the way down to and including i=StackShadowPages.
4298   for (int i = 0; i &lt; (int)(JavaThread::stack_shadow_zone_size() / os::vm_page_size()) - 1; i++) {
4299     // this could be any sized move but this is can be a debugging crumb
4300     // so the bigger the better.
4301     lea(tmp, Address(tmp, -os::vm_page_size()));
4302     str(size, Address(tmp));
4303   }
4304 }
4305 
4306 
4307 // Move the address of the polling page into dest.
4308 void MacroAssembler::get_polling_page(Register dest, address page, relocInfo::relocType rtype) {
4309   if (SafepointMechanism::uses_thread_local_poll()) {
4310     ldr(dest, Address(rthread, Thread::polling_page_offset()));
4311   } else {
<span class="line-modified">4312     uint64_t off;</span>
4313     adrp(dest, Address(page, rtype), off);
4314     assert(off == 0, &quot;polling page must be page aligned&quot;);
4315   }
4316 }
4317 
4318 // Move the address of the polling page into r, then read the polling
4319 // page.
4320 address MacroAssembler::read_polling_page(Register r, address page, relocInfo::relocType rtype) {
4321   get_polling_page(r, page, rtype);
4322   return read_polling_page(r, rtype);
4323 }
4324 
4325 // Read the polling page.  The address of the polling page must
4326 // already be in r.
4327 address MacroAssembler::read_polling_page(Register r, relocInfo::relocType rtype) {
4328   InstructionMark im(this);
4329   code_section()-&gt;relocate(inst_mark(), rtype);
4330   ldrw(zr, Address(r, 0));
4331   return inst_mark();
4332 }
4333 
<span class="line-modified">4334 void MacroAssembler::adrp(Register reg1, const Address &amp;dest, uint64_t &amp;byte_offset) {</span>
4335   relocInfo::relocType rtype = dest.rspec().reloc()-&gt;type();
<span class="line-modified">4336   uint64_t low_page = (uint64_t)CodeCache::low_bound() &gt;&gt; 12;</span>
<span class="line-modified">4337   uint64_t high_page = (uint64_t)(CodeCache::high_bound() - 1) &gt;&gt; 12;</span>
<span class="line-modified">4338   uint64_t dest_page = (uint64_t)dest.target() &gt;&gt; 12;</span>
<span class="line-modified">4339   int64_t offset_low = dest_page - low_page;</span>
<span class="line-modified">4340   int64_t offset_high = dest_page - high_page;</span>
4341 
4342   assert(is_valid_AArch64_address(dest.target()), &quot;bad address&quot;);
4343   assert(dest.getMode() == Address::literal, &quot;ADRP must be applied to a literal address&quot;);
4344 
4345   InstructionMark im(this);
4346   code_section()-&gt;relocate(inst_mark(), dest.rspec());
4347   // 8143067: Ensure that the adrp can reach the dest from anywhere within
4348   // the code cache so that if it is relocated we know it will still reach
4349   if (offset_high &gt;= -(1&lt;&lt;20) &amp;&amp; offset_low &lt; (1&lt;&lt;20)) {
4350     _adrp(reg1, dest.target());
4351   } else {
<span class="line-modified">4352     uint64_t target = (uint64_t)dest.target();</span>
<span class="line-modified">4353     uint64_t adrp_target</span>
<span class="line-modified">4354       = (target &amp; 0xffffffffUL) | ((uint64_t)pc() &amp; 0xffff00000000UL);</span>
4355 
4356     _adrp(reg1, (address)adrp_target);
4357     movk(reg1, target &gt;&gt; 32, 32);
4358   }
<span class="line-modified">4359   byte_offset = (uint64_t)dest.target() &amp; 0xfff;</span>
4360 }
4361 
4362 void MacroAssembler::load_byte_map_base(Register reg) {
4363   CardTable::CardValue* byte_map_base =
4364     ((CardTableBarrierSet*)(BarrierSet::barrier_set()))-&gt;card_table()-&gt;byte_map_base();
4365 
4366   if (is_valid_AArch64_address((address)byte_map_base)) {
4367     // Strictly speaking the byte_map_base isn&#39;t an address at all,
4368     // and it might even be negative.
<span class="line-modified">4369     uint64_t offset;</span>
4370     adrp(reg, ExternalAddress((address)byte_map_base), offset);
4371     // We expect offset to be zero with most collectors.
4372     if (offset != 0) {
4373       add(reg, reg, offset);
4374     }
4375   } else {
4376     mov(reg, (uint64_t)byte_map_base);
4377   }
4378 }
4379 
4380 void MacroAssembler::build_frame(int framesize) {
4381   assert(framesize &gt; 0, &quot;framesize must be &gt; 0&quot;);
4382   if (framesize &lt; ((1 &lt;&lt; 9) + 2 * wordSize)) {
4383     sub(sp, sp, framesize);
4384     stp(rfp, lr, Address(sp, framesize - 2 * wordSize));
4385     if (PreserveFramePointer) add(rfp, sp, framesize - 2 * wordSize);
4386   } else {
4387     stp(rfp, lr, Address(pre(sp, -2 * wordSize)));
4388     if (PreserveFramePointer) mov(rfp, sp);
4389     if (framesize &lt; ((1 &lt;&lt; 12) + 2 * wordSize))
</pre>
<hr />
<pre>
4780         b(NOMATCH);
4781 
4782       BIND(STR1_LOOP);
4783         add(cnt2tmp, cnt2_neg, 2*str2_chr_size);
4784         (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt2tmp));
4785         cmp(ch1, ch2);
4786         br(NE, STR2_NEXT);
4787         b(MATCH);
4788     }
4789 
4790     if (icnt1 == -1 || icnt1 == 1) {
4791       Label CH1_LOOP, HAS_ZERO, DO1_SHORT, DO1_LOOP;
4792 
4793       BIND(DO1);
4794         (this-&gt;*str1_load_1chr)(ch1, str1);
4795         cmp(cnt2, (u1)8);
4796         br(LT, DO1_SHORT);
4797 
4798         sub(result_tmp, cnt2, 8/str2_chr_size);
4799         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
<span class="line-modified">4800         mov(tmp3, (uint64_t)(str2_isL ? 0x0101010101010101 : 0x0001000100010001));</span>
4801         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4802 
4803         if (str2_isL) {
4804           orr(ch1, ch1, ch1, LSL, 8);
4805         }
4806         orr(ch1, ch1, ch1, LSL, 16);
4807         orr(ch1, ch1, ch1, LSL, 32);
4808       BIND(CH1_LOOP);
4809         ldr(ch2, Address(str2, cnt2_neg));
4810         eor(ch2, ch1, ch2);
4811         sub(tmp1, ch2, tmp3);
4812         orr(tmp2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4813         bics(tmp1, tmp1, tmp2);
4814         br(NE, HAS_ZERO);
4815         adds(cnt2_neg, cnt2_neg, 8);
4816         br(LT, CH1_LOOP);
4817 
4818         cmp(cnt2_neg, (u1)8);
4819         mov(cnt2_neg, 0);
4820         br(LT, CH1_LOOP);
</pre>
<hr />
<pre>
4852 void MacroAssembler::string_indexof_char(Register str1, Register cnt1,
4853                                          Register ch, Register result,
4854                                          Register tmp1, Register tmp2, Register tmp3)
4855 {
4856   Label CH1_LOOP, HAS_ZERO, DO1_SHORT, DO1_LOOP, MATCH, NOMATCH, DONE;
4857   Register cnt1_neg = cnt1;
4858   Register ch1 = rscratch1;
4859   Register result_tmp = rscratch2;
4860 
4861   cmp(cnt1, (u1)4);
4862   br(LT, DO1_SHORT);
4863 
4864   orr(ch, ch, ch, LSL, 16);
4865   orr(ch, ch, ch, LSL, 32);
4866 
4867   sub(cnt1, cnt1, 4);
4868   mov(result_tmp, cnt1);
4869   lea(str1, Address(str1, cnt1, Address::uxtw(1)));
4870   sub(cnt1_neg, zr, cnt1, LSL, 1);
4871 
<span class="line-modified">4872   mov(tmp3, (uint64_t)0x0001000100010001);</span>
4873 
4874   BIND(CH1_LOOP);
4875     ldr(ch1, Address(str1, cnt1_neg));
4876     eor(ch1, ch, ch1);
4877     sub(tmp1, ch1, tmp3);
4878     orr(tmp2, ch1, 0x7fff7fff7fff7fff);
4879     bics(tmp1, tmp1, tmp2);
4880     br(NE, HAS_ZERO);
4881     adds(cnt1_neg, cnt1_neg, 8);
4882     br(LT, CH1_LOOP);
4883 
4884     cmp(cnt1_neg, (u1)8);
4885     mov(cnt1_neg, 0);
4886     br(LT, CH1_LOOP);
4887     b(NOMATCH);
4888 
4889   BIND(HAS_ZERO);
4890     rev(tmp1, tmp1);
4891     clz(tmp1, tmp1);
4892     add(cnt1_neg, cnt1_neg, tmp1, LSR, 3);
</pre>
<hr />
<pre>
4898     sub(cnt1_neg, zr, cnt1, LSL, 1);
4899   BIND(DO1_LOOP);
4900     ldrh(ch1, Address(str1, cnt1_neg));
4901     cmpw(ch, ch1);
4902     br(EQ, MATCH);
4903     adds(cnt1_neg, cnt1_neg, 2);
4904     br(LT, DO1_LOOP);
4905   BIND(NOMATCH);
4906     mov(result, -1);
4907     b(DONE);
4908   BIND(MATCH);
4909     add(result, result_tmp, cnt1_neg, ASR, 1);
4910   BIND(DONE);
4911 }
4912 
4913 // Compare strings.
4914 void MacroAssembler::string_compare(Register str1, Register str2,
4915     Register cnt1, Register cnt2, Register result, Register tmp1, Register tmp2,
4916     FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3, int ae) {
4917   Label DONE, SHORT_LOOP, SHORT_STRING, SHORT_LAST, TAIL, STUB,
<span class="line-modified">4918       DIFF, NEXT_WORD, SHORT_LOOP_TAIL, SHORT_LAST2, SHORT_LAST_INIT,</span>
4919       SHORT_LOOP_START, TAIL_CHECK;
4920 
4921   bool isLL = ae == StrIntrinsicNode::LL;
4922   bool isLU = ae == StrIntrinsicNode::LU;
4923   bool isUL = ae == StrIntrinsicNode::UL;
4924 
4925   // The stub threshold for LL strings is: 72 (64 + 8) chars
4926   // UU: 36 chars, or 72 bytes (valid for the 64-byte large loop with prefetch)
4927   // LU/UL: 24 chars, or 48 bytes (valid for the 16-character loop at least)
4928   const u1 stub_threshold = isLL ? 72 : ((isLU || isUL) ? 24 : 36);
4929 
4930   bool str1_isL = isLL || isLU;
4931   bool str2_isL = isLL || isUL;
4932 
4933   int str1_chr_shift = str1_isL ? 0 : 1;
4934   int str2_chr_shift = str2_isL ? 0 : 1;
4935   int str1_chr_size = str1_isL ? 1 : 2;
4936   int str2_chr_size = str2_isL ? 1 : 2;
4937   int minCharsInWord = isLL ? wordSize : wordSize/2;
4938 
</pre>
<hr />
<pre>
4993     } else { // UL case
4994       ldr(tmp1, Address(str1));
4995       cmp(str1, str2);
4996       br(Assembler::EQ, DONE);
4997       ldrs(vtmp, Address(str2));
4998       cmp(cnt2, stub_threshold);
4999       br(GE, STUB);
5000       subw(cnt2, cnt2, 4);
5001       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
5002       eor(vtmpZ, T16B, vtmpZ, vtmpZ);
5003       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
5004       sub(cnt1, zr, cnt2, LSL, str1_chr_shift);
5005       zip1(vtmp, T8B, vtmp, vtmpZ);
5006       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
5007       add(cnt1, cnt1, 8);
5008       fmovd(tmp2, vtmp);
5009     }
5010     adds(cnt2, cnt2, isUL ? 4 : 8);
5011     br(GE, TAIL);
5012     eor(rscratch2, tmp1, tmp2);
<span class="line-modified">5013     cbnz(rscratch2, DIFF);</span>
5014     // main loop
5015     bind(NEXT_WORD);
5016     if (str1_isL == str2_isL) {
5017       ldr(tmp1, Address(str1, cnt2));
5018       ldr(tmp2, Address(str2, cnt2));
5019       adds(cnt2, cnt2, 8);
5020     } else if (isLU) {
5021       ldrs(vtmp, Address(str1, cnt1));
5022       ldr(tmp2, Address(str2, cnt2));
5023       add(cnt1, cnt1, 4);
5024       zip1(vtmp, T8B, vtmp, vtmpZ);
5025       fmovd(tmp1, vtmp);
5026       adds(cnt2, cnt2, 8);
5027     } else { // UL
5028       ldrs(vtmp, Address(str2, cnt2));
5029       ldr(tmp1, Address(str1, cnt1));
5030       zip1(vtmp, T8B, vtmp, vtmpZ);
5031       add(cnt1, cnt1, 8);
5032       fmovd(tmp2, vtmp);
5033       adds(cnt2, cnt2, 4);
5034     }
5035     br(GE, TAIL);
5036 
5037     eor(rscratch2, tmp1, tmp2);
5038     cbz(rscratch2, NEXT_WORD);
<span class="line-modified">5039     b(DIFF);</span>
5040     bind(TAIL);
5041     eor(rscratch2, tmp1, tmp2);
<span class="line-modified">5042     cbnz(rscratch2, DIFF);</span>
5043     // Last longword.  In the case where length == 4 we compare the
5044     // same longword twice, but that&#39;s still faster than another
5045     // conditional branch.
5046     if (str1_isL == str2_isL) {
5047       ldr(tmp1, Address(str1));
5048       ldr(tmp2, Address(str2));
5049     } else if (isLU) {
5050       ldrs(vtmp, Address(str1));
5051       ldr(tmp2, Address(str2));
5052       zip1(vtmp, T8B, vtmp, vtmpZ);
5053       fmovd(tmp1, vtmp);
5054     } else { // UL
5055       ldrs(vtmp, Address(str2));
5056       ldr(tmp1, Address(str1));
5057       zip1(vtmp, T8B, vtmp, vtmpZ);
5058       fmovd(tmp2, vtmp);
5059     }
5060     bind(TAIL_CHECK);
5061     eor(rscratch2, tmp1, tmp2);
5062     cbz(rscratch2, DONE);
5063 
5064     // Find the first different characters in the longwords and
5065     // compute their difference.
<span class="line-modified">5066     bind(DIFF);</span>
5067     rev(rscratch2, rscratch2);
5068     clz(rscratch2, rscratch2);
5069     andr(rscratch2, rscratch2, isLL ? -8 : -16);
5070     lsrv(tmp1, tmp1, rscratch2);
5071     (this-&gt;*ext_chr)(tmp1, tmp1);
5072     lsrv(tmp2, tmp2, rscratch2);
5073     (this-&gt;*ext_chr)(tmp2, tmp2);
5074     subw(result, tmp1, tmp2);
5075     b(DONE);
5076   }
5077 
5078   bind(STUB);
5079     RuntimeAddress stub = NULL;
5080     switch(ae) {
5081       case StrIntrinsicNode::LL:
5082         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_LL());
5083         break;
5084       case StrIntrinsicNode::UU:
5085         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_UU());
5086         break;
</pre>
<hr />
<pre>
5525   for (int i = zero_words_block_size &gt;&gt; 1; i &gt; 1; i &gt;&gt;= 1) {
5526     Label l;
5527     tbz(cnt, exact_log2(i), l);
5528     for (int j = 0; j &lt; i; j += 2) {
5529       stp(zr, zr, post(ptr, 16));
5530     }
5531     bind(l);
5532   }
5533   {
5534     Label l;
5535     tbz(cnt, 0, l);
5536     str(zr, Address(ptr));
5537     bind(l);
5538   }
5539   BLOCK_COMMENT(&quot;} zero_words&quot;);
5540 }
5541 
5542 // base:         Address of a buffer to be zeroed, 8 bytes aligned.
5543 // cnt:          Immediate count in HeapWords.
5544 #define SmallArraySize (18 * BytesPerLong)
<span class="line-modified">5545 void MacroAssembler::zero_words(Register base, uint64_t cnt)</span>
5546 {
5547   BLOCK_COMMENT(&quot;zero_words {&quot;);
5548   int i = cnt &amp; 1;  // store any odd word to start
5549   if (i) str(zr, Address(base));
5550 
5551   if (cnt &lt;= SmallArraySize / BytesPerLong) {
5552     for (; i &lt; (int)cnt; i += 2)
5553       stp(zr, zr, Address(base, i * wordSize));
5554   } else {
5555     const int unroll = 4; // Number of stp(zr, zr) instructions we&#39;ll unroll
5556     int remainder = cnt % (2 * unroll);
5557     for (; i &lt; remainder; i += 2)
5558       stp(zr, zr, Address(base, i * wordSize));
5559 
5560     Label loop;
5561     Register cnt_reg = rscratch1;
5562     Register loop_base = rscratch2;
5563     cnt = cnt - remainder;
5564     mov(cnt_reg, cnt);
5565     // adjust base and prebias by -2 * wordSize so we can pre-increment
</pre>
</td>
</tr>
</table>
<center><a href="jvmciCodeInstaller_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="..\..\..\..\index.html" target="_top">index</a> <a href="macroAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>