<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src\hotspot\cpu\aarch64\stubGenerator_aarch64.cpp</title>
    <link rel="stylesheet" href="..\..\..\..\style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2003, 2019, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.hpp&quot;
  28 #include &quot;asm/macroAssembler.inline.hpp&quot;
  29 #include &quot;gc/shared/barrierSet.hpp&quot;
  30 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  31 #include &quot;interpreter/interpreter.hpp&quot;
  32 #include &quot;memory/universe.hpp&quot;
  33 #include &quot;nativeInst_aarch64.hpp&quot;
  34 #include &quot;oops/instanceOop.hpp&quot;
  35 #include &quot;oops/method.hpp&quot;
  36 #include &quot;oops/objArrayKlass.hpp&quot;
  37 #include &quot;oops/oop.inline.hpp&quot;
  38 #include &quot;prims/methodHandles.hpp&quot;
  39 #include &quot;runtime/frame.inline.hpp&quot;
  40 #include &quot;runtime/handles.inline.hpp&quot;
  41 #include &quot;runtime/sharedRuntime.hpp&quot;
  42 #include &quot;runtime/stubCodeGenerator.hpp&quot;
  43 #include &quot;runtime/stubRoutines.hpp&quot;
  44 #include &quot;runtime/thread.inline.hpp&quot;
  45 #include &quot;utilities/align.hpp&quot;
  46 #ifdef COMPILER2
  47 #include &quot;opto/runtime.hpp&quot;
  48 #endif
  49 #if INCLUDE_ZGC
  50 #include &quot;gc/z/zThreadLocalData.hpp&quot;
  51 #endif
  52 
  53 // Declaration and definition of StubGenerator (no .hpp file).
  54 // For a more detailed description of the stub routine structure
  55 // see the comment in stubRoutines.hpp
  56 
  57 #undef __
  58 #define __ _masm-&gt;
  59 #define TIMES_OOP Address::sxtw(exact_log2(UseCompressedOops ? 4 : 8))
  60 
  61 #ifdef PRODUCT
  62 #define BLOCK_COMMENT(str) /* nothing */
  63 #else
  64 #define BLOCK_COMMENT(str) __ block_comment(str)
  65 #endif
  66 
  67 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  68 
  69 // Stub Code definitions
  70 
  71 class StubGenerator: public StubCodeGenerator {
  72  private:
  73 
  74 #ifdef PRODUCT
  75 #define inc_counter_np(counter) ((void)0)
  76 #else
  77   void inc_counter_np_(int&amp; counter) {
  78     __ lea(rscratch2, ExternalAddress((address)&amp;counter));
  79     __ ldrw(rscratch1, Address(rscratch2));
  80     __ addw(rscratch1, rscratch1, 1);
  81     __ strw(rscratch1, Address(rscratch2));
  82   }
  83 #define inc_counter_np(counter) \
  84   BLOCK_COMMENT(&quot;inc_counter &quot; #counter); \
  85   inc_counter_np_(counter);
  86 #endif
  87 
  88   // Call stubs are used to call Java from C
  89   //
  90   // Arguments:
  91   //    c_rarg0:   call wrapper address                   address
  92   //    c_rarg1:   result                                 address
  93   //    c_rarg2:   result type                            BasicType
  94   //    c_rarg3:   method                                 Method*
  95   //    c_rarg4:   (interpreter) entry point              address
  96   //    c_rarg5:   parameters                             intptr_t*
  97   //    c_rarg6:   parameter size (in words)              int
  98   //    c_rarg7:   thread                                 Thread*
  99   //
 100   // There is no return from the stub itself as any Java result
 101   // is written to result
 102   //
 103   // we save r30 (lr) as the return PC at the base of the frame and
 104   // link r29 (fp) below it as the frame pointer installing sp (r31)
 105   // into fp.
 106   //
 107   // we save r0-r7, which accounts for all the c arguments.
 108   //
 109   // TODO: strictly do we need to save them all? they are treated as
 110   // volatile by C so could we omit saving the ones we are going to
 111   // place in global registers (thread? method?) or those we only use
 112   // during setup of the Java call?
 113   //
 114   // we don&#39;t need to save r8 which C uses as an indirect result location
 115   // return register.
 116   //
 117   // we don&#39;t need to save r9-r15 which both C and Java treat as
 118   // volatile
 119   //
 120   // we don&#39;t need to save r16-18 because Java does not use them
 121   //
 122   // we save r19-r28 which Java uses as scratch registers and C
 123   // expects to be callee-save
 124   //
 125   // we save the bottom 64 bits of each value stored in v8-v15; it is
 126   // the responsibility of the caller to preserve larger values.
 127   //
 128   // so the stub frame looks like this when we enter Java code
 129   //
 130   //     [ return_from_Java     ] &lt;--- sp
 131   //     [ argument word n      ]
 132   //      ...
 133   // -27 [ argument word 1      ]
 134   // -26 [ saved v15            ] &lt;--- sp_after_call
 135   // -25 [ saved v14            ]
 136   // -24 [ saved v13            ]
 137   // -23 [ saved v12            ]
 138   // -22 [ saved v11            ]
 139   // -21 [ saved v10            ]
 140   // -20 [ saved v9             ]
 141   // -19 [ saved v8             ]
 142   // -18 [ saved r28            ]
 143   // -17 [ saved r27            ]
 144   // -16 [ saved r26            ]
 145   // -15 [ saved r25            ]
 146   // -14 [ saved r24            ]
 147   // -13 [ saved r23            ]
 148   // -12 [ saved r22            ]
 149   // -11 [ saved r21            ]
 150   // -10 [ saved r20            ]
 151   //  -9 [ saved r19            ]
 152   //  -8 [ call wrapper    (r0) ]
 153   //  -7 [ result          (r1) ]
 154   //  -6 [ result type     (r2) ]
 155   //  -5 [ method          (r3) ]
 156   //  -4 [ entry point     (r4) ]
 157   //  -3 [ parameters      (r5) ]
 158   //  -2 [ parameter size  (r6) ]
 159   //  -1 [ thread (r7)          ]
 160   //   0 [ saved fp       (r29) ] &lt;--- fp == saved sp (r31)
 161   //   1 [ saved lr       (r30) ]
 162 
 163   // Call stub stack layout word offsets from fp
 164   enum call_stub_layout {
 165     sp_after_call_off = -26,
 166 
 167     d15_off            = -26,
 168     d13_off            = -24,
 169     d11_off            = -22,
 170     d9_off             = -20,
 171 
 172     r28_off            = -18,
 173     r26_off            = -16,
 174     r24_off            = -14,
 175     r22_off            = -12,
 176     r20_off            = -10,
 177     call_wrapper_off   =  -8,
 178     result_off         =  -7,
 179     result_type_off    =  -6,
 180     method_off         =  -5,
 181     entry_point_off    =  -4,
 182     parameter_size_off =  -2,
 183     thread_off         =  -1,
 184     fp_f               =   0,
 185     retaddr_off        =   1,
 186   };
 187 
 188   address generate_call_stub(address&amp; return_address) {
 189     assert((int)frame::entry_frame_after_call_words == -(int)sp_after_call_off + 1 &amp;&amp;
 190            (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,
 191            &quot;adjust this code&quot;);
 192 
 193     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;call_stub&quot;);
 194     address start = __ pc();
 195 
 196     const Address sp_after_call(rfp, sp_after_call_off * wordSize);
 197 
 198     const Address call_wrapper  (rfp, call_wrapper_off   * wordSize);
 199     const Address result        (rfp, result_off         * wordSize);
 200     const Address result_type   (rfp, result_type_off    * wordSize);
 201     const Address method        (rfp, method_off         * wordSize);
 202     const Address entry_point   (rfp, entry_point_off    * wordSize);
 203     const Address parameter_size(rfp, parameter_size_off * wordSize);
 204 
 205     const Address thread        (rfp, thread_off         * wordSize);
 206 
 207     const Address d15_save      (rfp, d15_off * wordSize);
 208     const Address d13_save      (rfp, d13_off * wordSize);
 209     const Address d11_save      (rfp, d11_off * wordSize);
 210     const Address d9_save       (rfp, d9_off * wordSize);
 211 
 212     const Address r28_save      (rfp, r28_off * wordSize);
 213     const Address r26_save      (rfp, r26_off * wordSize);
 214     const Address r24_save      (rfp, r24_off * wordSize);
 215     const Address r22_save      (rfp, r22_off * wordSize);
 216     const Address r20_save      (rfp, r20_off * wordSize);
 217 
 218     // stub code
 219 
 220     address aarch64_entry = __ pc();
 221 
 222     // set up frame and move sp to end of save area
 223     __ enter();
 224     __ sub(sp, rfp, -sp_after_call_off * wordSize);
 225 
 226     // save register parameters and Java scratch/global registers
 227     // n.b. we save thread even though it gets installed in
 228     // rthread because we want to sanity check rthread later
 229     __ str(c_rarg7,  thread);
 230     __ strw(c_rarg6, parameter_size);
 231     __ stp(c_rarg4, c_rarg5,  entry_point);
 232     __ stp(c_rarg2, c_rarg3,  result_type);
 233     __ stp(c_rarg0, c_rarg1,  call_wrapper);
 234 
 235     __ stp(r20, r19,   r20_save);
 236     __ stp(r22, r21,   r22_save);
 237     __ stp(r24, r23,   r24_save);
 238     __ stp(r26, r25,   r26_save);
 239     __ stp(r28, r27,   r28_save);
 240 
 241     __ stpd(v9,  v8,   d9_save);
 242     __ stpd(v11, v10,  d11_save);
 243     __ stpd(v13, v12,  d13_save);
 244     __ stpd(v15, v14,  d15_save);
 245 
 246     // install Java thread in global register now we have saved
 247     // whatever value it held
 248     __ mov(rthread, c_rarg7);
 249     // And method
 250     __ mov(rmethod, c_rarg3);
 251 
 252     // set up the heapbase register
 253     __ reinit_heapbase();
 254 
 255 #ifdef ASSERT
 256     // make sure we have no pending exceptions
 257     {
 258       Label L;
 259       __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));
 260       __ cmp(rscratch1, (u1)NULL_WORD);
 261       __ br(Assembler::EQ, L);
 262       __ stop(&quot;StubRoutines::call_stub: entered with pending exception&quot;);
 263       __ BIND(L);
 264     }
 265 #endif
 266     // pass parameters if any
 267     __ mov(esp, sp);
 268     __ sub(rscratch1, sp, c_rarg6, ext::uxtw, LogBytesPerWord); // Move SP out of the way
 269     __ andr(sp, rscratch1, -2 * wordSize);
 270 
 271     BLOCK_COMMENT(&quot;pass parameters if any&quot;);
 272     Label parameters_done;
 273     // parameter count is still in c_rarg6
 274     // and parameter pointer identifying param 1 is in c_rarg5
 275     __ cbzw(c_rarg6, parameters_done);
 276 
 277     address loop = __ pc();
 278     __ ldr(rscratch1, Address(__ post(c_rarg5, wordSize)));
 279     __ subsw(c_rarg6, c_rarg6, 1);
 280     __ push(rscratch1);
 281     __ br(Assembler::GT, loop);
 282 
 283     __ BIND(parameters_done);
 284 
 285     // call Java entry -- passing methdoOop, and current sp
 286     //      rmethod: Method*
 287     //      r13: sender sp
 288     BLOCK_COMMENT(&quot;call Java function&quot;);
 289     __ mov(r13, sp);
 290     __ blr(c_rarg4);
 291 
 292     // we do this here because the notify will already have been done
 293     // if we get to the next instruction via an exception
 294     //
 295     // n.b. adding this instruction here affects the calculation of
 296     // whether or not a routine returns to the call stub (used when
 297     // doing stack walks) since the normal test is to check the return
 298     // pc against the address saved below. so we may need to allow for
 299     // this extra instruction in the check.
 300 
 301     // save current address for use by exception handling code
 302 
 303     return_address = __ pc();
 304 
 305     // store result depending on type (everything that is not
 306     // T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)
 307     // n.b. this assumes Java returns an integral result in r0
 308     // and a floating result in j_farg0
 309     __ ldr(j_rarg2, result);
 310     Label is_long, is_float, is_double, exit;
 311     __ ldr(j_rarg1, result_type);
 312     __ cmp(j_rarg1, (u1)T_OBJECT);
 313     __ br(Assembler::EQ, is_long);
 314     __ cmp(j_rarg1, (u1)T_LONG);
 315     __ br(Assembler::EQ, is_long);
 316     __ cmp(j_rarg1, (u1)T_FLOAT);
 317     __ br(Assembler::EQ, is_float);
 318     __ cmp(j_rarg1, (u1)T_DOUBLE);
 319     __ br(Assembler::EQ, is_double);
 320 
 321     // handle T_INT case
 322     __ strw(r0, Address(j_rarg2));
 323 
 324     __ BIND(exit);
 325 
 326     // pop parameters
 327     __ sub(esp, rfp, -sp_after_call_off * wordSize);
 328 
 329 #ifdef ASSERT
 330     // verify that threads correspond
 331     {
 332       Label L, S;
 333       __ ldr(rscratch1, thread);
 334       __ cmp(rthread, rscratch1);
 335       __ br(Assembler::NE, S);
 336       __ get_thread(rscratch1);
 337       __ cmp(rthread, rscratch1);
 338       __ br(Assembler::EQ, L);
 339       __ BIND(S);
 340       __ stop(&quot;StubRoutines::call_stub: threads must correspond&quot;);
 341       __ BIND(L);
 342     }
 343 #endif
 344 
 345     // restore callee-save registers
 346     __ ldpd(v15, v14,  d15_save);
 347     __ ldpd(v13, v12,  d13_save);
 348     __ ldpd(v11, v10,  d11_save);
 349     __ ldpd(v9,  v8,   d9_save);
 350 
 351     __ ldp(r28, r27,   r28_save);
 352     __ ldp(r26, r25,   r26_save);
 353     __ ldp(r24, r23,   r24_save);
 354     __ ldp(r22, r21,   r22_save);
 355     __ ldp(r20, r19,   r20_save);
 356 
 357     __ ldp(c_rarg0, c_rarg1,  call_wrapper);
 358     __ ldrw(c_rarg2, result_type);
 359     __ ldr(c_rarg3,  method);
 360     __ ldp(c_rarg4, c_rarg5,  entry_point);
 361     __ ldp(c_rarg6, c_rarg7,  parameter_size);
 362 
 363     // leave frame and return to caller
 364     __ leave();
 365     __ ret(lr);
 366 
 367     // handle return types different from T_INT
 368 
 369     __ BIND(is_long);
 370     __ str(r0, Address(j_rarg2, 0));
 371     __ br(Assembler::AL, exit);
 372 
 373     __ BIND(is_float);
 374     __ strs(j_farg0, Address(j_rarg2, 0));
 375     __ br(Assembler::AL, exit);
 376 
 377     __ BIND(is_double);
 378     __ strd(j_farg0, Address(j_rarg2, 0));
 379     __ br(Assembler::AL, exit);
 380 
 381     return start;
 382   }
 383 
 384   // Return point for a Java call if there&#39;s an exception thrown in
 385   // Java code.  The exception is caught and transformed into a
 386   // pending exception stored in JavaThread that can be tested from
 387   // within the VM.
 388   //
 389   // Note: Usually the parameters are removed by the callee. In case
 390   // of an exception crossing an activation frame boundary, that is
 391   // not the case if the callee is compiled code =&gt; need to setup the
 392   // rsp.
 393   //
 394   // r0: exception oop
 395 
 396   address generate_catch_exception() {
 397     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;catch_exception&quot;);
 398     address start = __ pc();
 399 
 400     // same as in generate_call_stub():
 401     const Address sp_after_call(rfp, sp_after_call_off * wordSize);
 402     const Address thread        (rfp, thread_off         * wordSize);
 403 
 404 #ifdef ASSERT
 405     // verify that threads correspond
 406     {
 407       Label L, S;
 408       __ ldr(rscratch1, thread);
 409       __ cmp(rthread, rscratch1);
 410       __ br(Assembler::NE, S);
 411       __ get_thread(rscratch1);
 412       __ cmp(rthread, rscratch1);
 413       __ br(Assembler::EQ, L);
 414       __ bind(S);
 415       __ stop(&quot;StubRoutines::catch_exception: threads must correspond&quot;);
 416       __ bind(L);
 417     }
 418 #endif
 419 
 420     // set pending exception
 421     __ verify_oop(r0);
 422 
 423     __ str(r0, Address(rthread, Thread::pending_exception_offset()));
 424     __ mov(rscratch1, (address)__FILE__);
 425     __ str(rscratch1, Address(rthread, Thread::exception_file_offset()));
 426     __ movw(rscratch1, (int)__LINE__);
 427     __ strw(rscratch1, Address(rthread, Thread::exception_line_offset()));
 428 
 429     // complete return to VM
 430     assert(StubRoutines::_call_stub_return_address != NULL,
 431            &quot;_call_stub_return_address must have been generated before&quot;);
 432     __ b(StubRoutines::_call_stub_return_address);
 433 
 434     return start;
 435   }
 436 
 437   // Continuation point for runtime calls returning with a pending
 438   // exception.  The pending exception check happened in the runtime
 439   // or native call stub.  The pending exception in Thread is
 440   // converted into a Java-level exception.
 441   //
 442   // Contract with Java-level exception handlers:
 443   // r0: exception
 444   // r3: throwing pc
 445   //
 446   // NOTE: At entry of this stub, exception-pc must be in LR !!
 447 
 448   // NOTE: this is always used as a jump target within generated code
 449   // so it just needs to be generated code wiht no x86 prolog
 450 
 451   address generate_forward_exception() {
 452     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;forward exception&quot;);
 453     address start = __ pc();
 454 
 455     // Upon entry, LR points to the return address returning into
 456     // Java (interpreted or compiled) code; i.e., the return address
 457     // becomes the throwing pc.
 458     //
 459     // Arguments pushed before the runtime call are still on the stack
 460     // but the exception handler will reset the stack pointer -&gt;
 461     // ignore them.  A potential result in registers can be ignored as
 462     // well.
 463 
 464 #ifdef ASSERT
 465     // make sure this code is only executed if there is a pending exception
 466     {
 467       Label L;
 468       __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
 469       __ cbnz(rscratch1, L);
 470       __ stop(&quot;StubRoutines::forward exception: no pending exception (1)&quot;);
 471       __ bind(L);
 472     }
 473 #endif
 474 
 475     // compute exception handler into r19
 476 
 477     // call the VM to find the handler address associated with the
 478     // caller address. pass thread in r0 and caller pc (ret address)
 479     // in r1. n.b. the caller pc is in lr, unlike x86 where it is on
 480     // the stack.
 481     __ mov(c_rarg1, lr);
 482     // lr will be trashed by the VM call so we move it to R19
 483     // (callee-saved) because we also need to pass it to the handler
 484     // returned by this call.
 485     __ mov(r19, lr);
 486     BLOCK_COMMENT(&quot;call exception_handler_for_return_address&quot;);
 487     __ call_VM_leaf(CAST_FROM_FN_PTR(address,
 488                          SharedRuntime::exception_handler_for_return_address),
 489                     rthread, c_rarg1);
 490     // we should not really care that lr is no longer the callee
 491     // address. we saved the value the handler needs in r19 so we can
 492     // just copy it to r3. however, the C2 handler will push its own
 493     // frame and then calls into the VM and the VM code asserts that
 494     // the PC for the frame above the handler belongs to a compiled
 495     // Java method. So, we restore lr here to satisfy that assert.
 496     __ mov(lr, r19);
 497     // setup r0 &amp; r3 &amp; clear pending exception
 498     __ mov(r3, r19);
 499     __ mov(r19, r0);
 500     __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));
 501     __ str(zr, Address(rthread, Thread::pending_exception_offset()));
 502 
 503 #ifdef ASSERT
 504     // make sure exception is set
 505     {
 506       Label L;
 507       __ cbnz(r0, L);
 508       __ stop(&quot;StubRoutines::forward exception: no pending exception (2)&quot;);
 509       __ bind(L);
 510     }
 511 #endif
 512 
 513     // continue at exception handler
 514     // r0: exception
 515     // r3: throwing pc
 516     // r19: exception handler
 517     __ verify_oop(r0);
 518     __ br(r19);
 519 
 520     return start;
 521   }
 522 
 523   // Non-destructive plausibility checks for oops
 524   //
 525   // Arguments:
 526   //    r0: oop to verify
 527   //    rscratch1: error message
 528   //
 529   // Stack after saving c_rarg3:
 530   //    [tos + 0]: saved c_rarg3
 531   //    [tos + 1]: saved c_rarg2
 532   //    [tos + 2]: saved lr
 533   //    [tos + 3]: saved rscratch2
 534   //    [tos + 4]: saved r0
 535   //    [tos + 5]: saved rscratch1
 536   address generate_verify_oop() {
 537 
 538     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;verify_oop&quot;);
 539     address start = __ pc();
 540 
 541     Label exit, error;
 542 
 543     // save c_rarg2 and c_rarg3
 544     __ stp(c_rarg3, c_rarg2, Address(__ pre(sp, -16)));
 545 
 546     // __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()));
 547     __ lea(c_rarg2, ExternalAddress((address) StubRoutines::verify_oop_count_addr()));
 548     __ ldr(c_rarg3, Address(c_rarg2));
 549     __ add(c_rarg3, c_rarg3, 1);
 550     __ str(c_rarg3, Address(c_rarg2));
 551 
 552     // object is in r0
 553     // make sure object is &#39;reasonable&#39;
 554     __ cbz(r0, exit); // if obj is NULL it is OK
 555 
 556 #if INCLUDE_ZGC
 557     if (UseZGC) {
 558       // Check if mask is good.
 559       // verifies that ZAddressBadMask &amp; r0 == 0
 560       __ ldr(c_rarg3, Address(rthread, ZThreadLocalData::address_bad_mask_offset()));
 561       __ andr(c_rarg2, r0, c_rarg3);
 562       __ cbnz(c_rarg2, error);
 563     }
 564 #endif
 565 
 566     // Check if the oop is in the right area of memory
 567     __ mov(c_rarg3, (intptr_t) Universe::verify_oop_mask());
 568     __ andr(c_rarg2, r0, c_rarg3);
 569     __ mov(c_rarg3, (intptr_t) Universe::verify_oop_bits());
 570 
 571     // Compare c_rarg2 and c_rarg3.  We don&#39;t use a compare
 572     // instruction here because the flags register is live.
 573     __ eor(c_rarg2, c_rarg2, c_rarg3);
 574     __ cbnz(c_rarg2, error);
 575 
 576     // make sure klass is &#39;reasonable&#39;, which is not zero.
 577     __ load_klass(r0, r0);  // get klass
 578     __ cbz(r0, error);      // if klass is NULL it is broken
 579 
 580     // return if everything seems ok
 581     __ bind(exit);
 582 
 583     __ ldp(c_rarg3, c_rarg2, Address(__ post(sp, 16)));
 584     __ ret(lr);
 585 
 586     // handle errors
 587     __ bind(error);
 588     __ ldp(c_rarg3, c_rarg2, Address(__ post(sp, 16)));
 589 
 590     __ push(RegSet::range(r0, r29), sp);
 591     // debug(char* msg, int64_t pc, int64_t regs[])
 592     __ mov(c_rarg0, rscratch1);      // pass address of error message
 593     __ mov(c_rarg1, lr);             // pass return address
 594     __ mov(c_rarg2, sp);             // pass address of regs on stack
 595 #ifndef PRODUCT
 596     assert(frame::arg_reg_save_area_bytes == 0, &quot;not expecting frame reg save area&quot;);
 597 #endif
 598     BLOCK_COMMENT(&quot;call MacroAssembler::debug&quot;);
 599     __ mov(rscratch1, CAST_FROM_FN_PTR(address, MacroAssembler::debug64));
 600     __ blr(rscratch1);
 601     __ hlt(0);
 602 
 603     return start;
 604   }
 605 
 606   void array_overlap_test(Label&amp; L_no_overlap, Address::sxtw sf) { __ b(L_no_overlap); }
 607 
 608   // The inner part of zero_words().  This is the bulk operation,
 609   // zeroing words in blocks, possibly using DC ZVA to do it.  The
 610   // caller is responsible for zeroing the last few words.
 611   //
 612   // Inputs:
 613   // r10: the HeapWord-aligned base address of an array to zero.
 614   // r11: the count in HeapWords, r11 &gt; 0.
 615   //
 616   // Returns r10 and r11, adjusted for the caller to clear.
 617   // r10: the base address of the tail of words left to clear.
 618   // r11: the number of words in the tail.
 619   //      r11 &lt; MacroAssembler::zero_words_block_size.
 620 
 621   address generate_zero_blocks() {
 622     Label done;
 623     Label base_aligned;
 624 
 625     Register base = r10, cnt = r11;
 626 
 627     __ align(CodeEntryAlignment);
 628     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;zero_blocks&quot;);
 629     address start = __ pc();
 630 
 631     if (UseBlockZeroing) {
 632       int zva_length = VM_Version::zva_length();
 633 
 634       // Ensure ZVA length can be divided by 16. This is required by
 635       // the subsequent operations.
 636       assert (zva_length % 16 == 0, &quot;Unexpected ZVA Length&quot;);
 637 
 638       __ tbz(base, 3, base_aligned);
 639       __ str(zr, Address(__ post(base, 8)));
 640       __ sub(cnt, cnt, 1);
 641       __ bind(base_aligned);
 642 
 643       // Ensure count &gt;= zva_length * 2 so that it still deserves a zva after
 644       // alignment.
 645       Label small;
 646       int low_limit = MAX2(zva_length * 2, (int)BlockZeroingLowLimit);
 647       __ subs(rscratch1, cnt, low_limit &gt;&gt; 3);
 648       __ br(Assembler::LT, small);
 649       __ zero_dcache_blocks(base, cnt);
 650       __ bind(small);
 651     }
 652 
 653     {
 654       // Number of stp instructions we&#39;ll unroll
 655       const int unroll =
 656         MacroAssembler::zero_words_block_size / 2;
 657       // Clear the remaining blocks.
 658       Label loop;
 659       __ subs(cnt, cnt, unroll * 2);
 660       __ br(Assembler::LT, done);
 661       __ bind(loop);
 662       for (int i = 0; i &lt; unroll; i++)
 663         __ stp(zr, zr, __ post(base, 16));
 664       __ subs(cnt, cnt, unroll * 2);
 665       __ br(Assembler::GE, loop);
 666       __ bind(done);
 667       __ add(cnt, cnt, unroll * 2);
 668     }
 669 
 670     __ ret(lr);
 671 
 672     return start;
 673   }
 674 
 675 
 676   typedef enum {
 677     copy_forwards = 1,
 678     copy_backwards = -1
 679   } copy_direction;
 680 
 681   // Bulk copy of blocks of 8 words.
 682   //
 683   // count is a count of words.
 684   //
 685   // Precondition: count &gt;= 8
 686   //
 687   // Postconditions:
 688   //
 689   // The least significant bit of count contains the remaining count
 690   // of words to copy.  The rest of count is trash.
 691   //
 692   // s and d are adjusted to point to the remaining words to copy
 693   //
 694   void generate_copy_longs(Label &amp;start, Register s, Register d, Register count,
 695                            copy_direction direction) {
 696     int unit = wordSize * direction;
 697     int bias = (UseSIMDForMemoryOps ? 4:2) * wordSize;
 698 
 699     int offset;
 700     const Register t0 = r3, t1 = r4, t2 = r5, t3 = r6,
 701       t4 = r7, t5 = r10, t6 = r11, t7 = r12;
 702     const Register stride = r13;
 703 
 704     assert_different_registers(rscratch1, t0, t1, t2, t3, t4, t5, t6, t7);
 705     assert_different_registers(s, d, count, rscratch1);
 706 
 707     Label again, drain;
 708     const char *stub_name;
 709     if (direction == copy_forwards)
 710       stub_name = &quot;forward_copy_longs&quot;;
 711     else
 712       stub_name = &quot;backward_copy_longs&quot;;
 713 
 714     __ align(CodeEntryAlignment);
 715 
 716     StubCodeMark mark(this, &quot;StubRoutines&quot;, stub_name);
 717 
 718     __ bind(start);
 719 
 720     Label unaligned_copy_long;
 721     if (AvoidUnalignedAccesses) {
 722       __ tbnz(d, 3, unaligned_copy_long);
 723     }
 724 
 725     if (direction == copy_forwards) {
 726       __ sub(s, s, bias);
 727       __ sub(d, d, bias);
 728     }
 729 
 730 #ifdef ASSERT
 731     // Make sure we are never given &lt; 8 words
 732     {
 733       Label L;
 734       __ cmp(count, (u1)8);
 735       __ br(Assembler::GE, L);
 736       __ stop(&quot;genrate_copy_longs called with &lt; 8 words&quot;);
 737       __ bind(L);
 738     }
 739 #endif
 740 
 741     // Fill 8 registers
 742     if (UseSIMDForMemoryOps) {
 743       __ ldpq(v0, v1, Address(s, 4 * unit));
 744       __ ldpq(v2, v3, Address(__ pre(s, 8 * unit)));
 745     } else {
 746       __ ldp(t0, t1, Address(s, 2 * unit));
 747       __ ldp(t2, t3, Address(s, 4 * unit));
 748       __ ldp(t4, t5, Address(s, 6 * unit));
 749       __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 750     }
 751 
 752     __ subs(count, count, 16);
 753     __ br(Assembler::LO, drain);
 754 
 755     int prefetch = PrefetchCopyIntervalInBytes;
 756     bool use_stride = false;
 757     if (direction == copy_backwards) {
 758        use_stride = prefetch &gt; 256;
 759        prefetch = -prefetch;
 760        if (use_stride) __ mov(stride, prefetch);
 761     }
 762 
 763     __ bind(again);
 764 
 765     if (PrefetchCopyIntervalInBytes &gt; 0)
 766       __ prfm(use_stride ? Address(s, stride) : Address(s, prefetch), PLDL1KEEP);
 767 
 768     if (UseSIMDForMemoryOps) {
 769       __ stpq(v0, v1, Address(d, 4 * unit));
 770       __ ldpq(v0, v1, Address(s, 4 * unit));
 771       __ stpq(v2, v3, Address(__ pre(d, 8 * unit)));
 772       __ ldpq(v2, v3, Address(__ pre(s, 8 * unit)));
 773     } else {
 774       __ stp(t0, t1, Address(d, 2 * unit));
 775       __ ldp(t0, t1, Address(s, 2 * unit));
 776       __ stp(t2, t3, Address(d, 4 * unit));
 777       __ ldp(t2, t3, Address(s, 4 * unit));
 778       __ stp(t4, t5, Address(d, 6 * unit));
 779       __ ldp(t4, t5, Address(s, 6 * unit));
 780       __ stp(t6, t7, Address(__ pre(d, 8 * unit)));
 781       __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 782     }
 783 
 784     __ subs(count, count, 8);
 785     __ br(Assembler::HS, again);
 786 
 787     // Drain
 788     __ bind(drain);
 789     if (UseSIMDForMemoryOps) {
 790       __ stpq(v0, v1, Address(d, 4 * unit));
 791       __ stpq(v2, v3, Address(__ pre(d, 8 * unit)));
 792     } else {
 793       __ stp(t0, t1, Address(d, 2 * unit));
 794       __ stp(t2, t3, Address(d, 4 * unit));
 795       __ stp(t4, t5, Address(d, 6 * unit));
 796       __ stp(t6, t7, Address(__ pre(d, 8 * unit)));
 797     }
 798 
 799     {
 800       Label L1, L2;
 801       __ tbz(count, exact_log2(4), L1);
 802       if (UseSIMDForMemoryOps) {
 803         __ ldpq(v0, v1, Address(__ pre(s, 4 * unit)));
 804         __ stpq(v0, v1, Address(__ pre(d, 4 * unit)));
 805       } else {
 806         __ ldp(t0, t1, Address(s, 2 * unit));
 807         __ ldp(t2, t3, Address(__ pre(s, 4 * unit)));
 808         __ stp(t0, t1, Address(d, 2 * unit));
 809         __ stp(t2, t3, Address(__ pre(d, 4 * unit)));
 810       }
 811       __ bind(L1);
 812 
 813       if (direction == copy_forwards) {
 814         __ add(s, s, bias);
 815         __ add(d, d, bias);
 816       }
 817 
 818       __ tbz(count, 1, L2);
 819       __ ldp(t0, t1, Address(__ adjust(s, 2 * unit, direction == copy_backwards)));
 820       __ stp(t0, t1, Address(__ adjust(d, 2 * unit, direction == copy_backwards)));
 821       __ bind(L2);
 822     }
 823 
 824     __ ret(lr);
 825 
 826     if (AvoidUnalignedAccesses) {
 827       Label drain, again;
 828       // Register order for storing. Order is different for backward copy.
 829 
 830       __ bind(unaligned_copy_long);
 831 
 832       // source address is even aligned, target odd aligned
 833       //
 834       // when forward copying word pairs we read long pairs at offsets
 835       // {0, 2, 4, 6} (in long words). when backwards copying we read
 836       // long pairs at offsets {-2, -4, -6, -8}. We adjust the source
 837       // address by -2 in the forwards case so we can compute the
 838       // source offsets for both as {2, 4, 6, 8} * unit where unit = 1
 839       // or -1.
 840       //
 841       // when forward copying we need to store 1 word, 3 pairs and
 842       // then 1 word at offsets {0, 1, 3, 5, 7}. Rather thna use a
 843       // zero offset We adjust the destination by -1 which means we
 844       // have to use offsets { 1, 2, 4, 6, 8} * unit for the stores.
 845       //
 846       // When backwards copyng we need to store 1 word, 3 pairs and
 847       // then 1 word at offsets {-1, -3, -5, -7, -8} i.e. we use
 848       // offsets {1, 3, 5, 7, 8} * unit.
 849 
 850       if (direction == copy_forwards) {
 851         __ sub(s, s, 16);
 852         __ sub(d, d, 8);
 853       }
 854 
 855       // Fill 8 registers
 856       //
 857       // for forwards copy s was offset by -16 from the original input
 858       // value of s so the register contents are at these offsets
 859       // relative to the 64 bit block addressed by that original input
 860       // and so on for each successive 64 byte block when s is updated
 861       //
 862       // t0 at offset 0,  t1 at offset 8
 863       // t2 at offset 16, t3 at offset 24
 864       // t4 at offset 32, t5 at offset 40
 865       // t6 at offset 48, t7 at offset 56
 866 
 867       // for backwards copy s was not offset so the register contents
 868       // are at these offsets into the preceding 64 byte block
 869       // relative to that original input and so on for each successive
 870       // preceding 64 byte block when s is updated. this explains the
 871       // slightly counter-intuitive looking pattern of register usage
 872       // in the stp instructions for backwards copy.
 873       //
 874       // t0 at offset -16, t1 at offset -8
 875       // t2 at offset -32, t3 at offset -24
 876       // t4 at offset -48, t5 at offset -40
 877       // t6 at offset -64, t7 at offset -56
 878 
 879       __ ldp(t0, t1, Address(s, 2 * unit));
 880       __ ldp(t2, t3, Address(s, 4 * unit));
 881       __ ldp(t4, t5, Address(s, 6 * unit));
 882       __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 883 
 884       __ subs(count, count, 16);
 885       __ br(Assembler::LO, drain);
 886 
 887       int prefetch = PrefetchCopyIntervalInBytes;
 888       bool use_stride = false;
 889       if (direction == copy_backwards) {
 890          use_stride = prefetch &gt; 256;
 891          prefetch = -prefetch;
 892          if (use_stride) __ mov(stride, prefetch);
 893       }
 894 
 895       __ bind(again);
 896 
 897       if (PrefetchCopyIntervalInBytes &gt; 0)
 898         __ prfm(use_stride ? Address(s, stride) : Address(s, prefetch), PLDL1KEEP);
 899 
 900       if (direction == copy_forwards) {
 901        // allowing for the offset of -8 the store instructions place
 902        // registers into the target 64 bit block at the following
 903        // offsets
 904        //
 905        // t0 at offset 0
 906        // t1 at offset 8,  t2 at offset 16
 907        // t3 at offset 24, t4 at offset 32
 908        // t5 at offset 40, t6 at offset 48
 909        // t7 at offset 56
 910 
 911         __ str(t0, Address(d, 1 * unit));
 912         __ stp(t1, t2, Address(d, 2 * unit));
 913         __ ldp(t0, t1, Address(s, 2 * unit));
 914         __ stp(t3, t4, Address(d, 4 * unit));
 915         __ ldp(t2, t3, Address(s, 4 * unit));
 916         __ stp(t5, t6, Address(d, 6 * unit));
 917         __ ldp(t4, t5, Address(s, 6 * unit));
 918         __ str(t7, Address(__ pre(d, 8 * unit)));
 919         __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 920       } else {
 921        // d was not offset when we started so the registers are
 922        // written into the 64 bit block preceding d with the following
 923        // offsets
 924        //
 925        // t1 at offset -8
 926        // t3 at offset -24, t0 at offset -16
 927        // t5 at offset -48, t2 at offset -32
 928        // t7 at offset -56, t4 at offset -48
 929        //                   t6 at offset -64
 930        //
 931        // note that this matches the offsets previously noted for the
 932        // loads
 933 
 934         __ str(t1, Address(d, 1 * unit));
 935         __ stp(t3, t0, Address(d, 3 * unit));
 936         __ ldp(t0, t1, Address(s, 2 * unit));
 937         __ stp(t5, t2, Address(d, 5 * unit));
 938         __ ldp(t2, t3, Address(s, 4 * unit));
 939         __ stp(t7, t4, Address(d, 7 * unit));
 940         __ ldp(t4, t5, Address(s, 6 * unit));
 941         __ str(t6, Address(__ pre(d, 8 * unit)));
 942         __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 943       }
 944 
 945       __ subs(count, count, 8);
 946       __ br(Assembler::HS, again);
 947 
 948       // Drain
 949       //
 950       // this uses the same pattern of offsets and register arguments
 951       // as above
 952       __ bind(drain);
 953       if (direction == copy_forwards) {
 954         __ str(t0, Address(d, 1 * unit));
 955         __ stp(t1, t2, Address(d, 2 * unit));
 956         __ stp(t3, t4, Address(d, 4 * unit));
 957         __ stp(t5, t6, Address(d, 6 * unit));
 958         __ str(t7, Address(__ pre(d, 8 * unit)));
 959       } else {
 960         __ str(t1, Address(d, 1 * unit));
 961         __ stp(t3, t0, Address(d, 3 * unit));
 962         __ stp(t5, t2, Address(d, 5 * unit));
 963         __ stp(t7, t4, Address(d, 7 * unit));
 964         __ str(t6, Address(__ pre(d, 8 * unit)));
 965       }
 966       // now we need to copy any remaining part block which may
 967       // include a 4 word block subblock and/or a 2 word subblock.
 968       // bits 2 and 1 in the count are the tell-tale for whetehr we
 969       // have each such subblock
 970       {
 971         Label L1, L2;
 972         __ tbz(count, exact_log2(4), L1);
 973        // this is the same as above but copying only 4 longs hence
 974        // with ony one intervening stp between the str instructions
 975        // but note that the offsets and registers still follow the
 976        // same pattern
 977         __ ldp(t0, t1, Address(s, 2 * unit));
 978         __ ldp(t2, t3, Address(__ pre(s, 4 * unit)));
 979         if (direction == copy_forwards) {
 980           __ str(t0, Address(d, 1 * unit));
 981           __ stp(t1, t2, Address(d, 2 * unit));
 982           __ str(t3, Address(__ pre(d, 4 * unit)));
 983         } else {
 984           __ str(t1, Address(d, 1 * unit));
 985           __ stp(t3, t0, Address(d, 3 * unit));
 986           __ str(t2, Address(__ pre(d, 4 * unit)));
 987         }
 988         __ bind(L1);
 989 
 990         __ tbz(count, 1, L2);
 991        // this is the same as above but copying only 2 longs hence
 992        // there is no intervening stp between the str instructions
 993        // but note that the offset and register patterns are still
 994        // the same
 995         __ ldp(t0, t1, Address(__ pre(s, 2 * unit)));
 996         if (direction == copy_forwards) {
 997           __ str(t0, Address(d, 1 * unit));
 998           __ str(t1, Address(__ pre(d, 2 * unit)));
 999         } else {
1000           __ str(t1, Address(d, 1 * unit));
1001           __ str(t0, Address(__ pre(d, 2 * unit)));
1002         }
1003         __ bind(L2);
1004 
1005        // for forwards copy we need to re-adjust the offsets we
1006        // applied so that s and d are follow the last words written
1007 
1008        if (direction == copy_forwards) {
1009          __ add(s, s, 16);
1010          __ add(d, d, 8);
1011        }
1012 
1013       }
1014 
1015       __ ret(lr);
1016       }
1017   }
1018 
1019   // Small copy: less than 16 bytes.
1020   //
1021   // NB: Ignores all of the bits of count which represent more than 15
1022   // bytes, so a caller doesn&#39;t have to mask them.
1023 
1024   void copy_memory_small(Register s, Register d, Register count, Register tmp, int step) {
1025     bool is_backwards = step &lt; 0;
1026     size_t granularity = uabs(step);
1027     int direction = is_backwards ? -1 : 1;
1028     int unit = wordSize * direction;
1029 
1030     Label Lword, Lint, Lshort, Lbyte;
1031 
1032     assert(granularity
1033            &amp;&amp; granularity &lt;= sizeof (jlong), &quot;Impossible granularity in copy_memory_small&quot;);
1034 
1035     const Register t0 = r3, t1 = r4, t2 = r5, t3 = r6;
1036 
1037     // ??? I don&#39;t know if this bit-test-and-branch is the right thing
1038     // to do.  It does a lot of jumping, resulting in several
1039     // mispredicted branches.  It might make more sense to do this
1040     // with something like Duff&#39;s device with a single computed branch.
1041 
1042     __ tbz(count, 3 - exact_log2(granularity), Lword);
1043     __ ldr(tmp, Address(__ adjust(s, unit, is_backwards)));
1044     __ str(tmp, Address(__ adjust(d, unit, is_backwards)));
1045     __ bind(Lword);
1046 
1047     if (granularity &lt;= sizeof (jint)) {
1048       __ tbz(count, 2 - exact_log2(granularity), Lint);
1049       __ ldrw(tmp, Address(__ adjust(s, sizeof (jint) * direction, is_backwards)));
1050       __ strw(tmp, Address(__ adjust(d, sizeof (jint) * direction, is_backwards)));
1051       __ bind(Lint);
1052     }
1053 
1054     if (granularity &lt;= sizeof (jshort)) {
1055       __ tbz(count, 1 - exact_log2(granularity), Lshort);
1056       __ ldrh(tmp, Address(__ adjust(s, sizeof (jshort) * direction, is_backwards)));
1057       __ strh(tmp, Address(__ adjust(d, sizeof (jshort) * direction, is_backwards)));
1058       __ bind(Lshort);
1059     }
1060 
1061     if (granularity &lt;= sizeof (jbyte)) {
1062       __ tbz(count, 0, Lbyte);
1063       __ ldrb(tmp, Address(__ adjust(s, sizeof (jbyte) * direction, is_backwards)));
1064       __ strb(tmp, Address(__ adjust(d, sizeof (jbyte) * direction, is_backwards)));
1065       __ bind(Lbyte);
1066     }
1067   }
1068 
1069   Label copy_f, copy_b;
1070 
1071   // All-singing all-dancing memory copy.
1072   //
1073   // Copy count units of memory from s to d.  The size of a unit is
1074   // step, which can be positive or negative depending on the direction
1075   // of copy.  If is_aligned is false, we align the source address.
1076   //
1077 
1078   void copy_memory(bool is_aligned, Register s, Register d,
1079                    Register count, Register tmp, int step) {
1080     copy_direction direction = step &lt; 0 ? copy_backwards : copy_forwards;
1081     bool is_backwards = step &lt; 0;
1082     int granularity = uabs(step);
1083     const Register t0 = r3, t1 = r4;
1084 
1085     // &lt;= 96 bytes do inline. Direction doesn&#39;t matter because we always
1086     // load all the data before writing anything
1087     Label copy4, copy8, copy16, copy32, copy80, copy_big, finish;
1088     const Register t2 = r5, t3 = r6, t4 = r7, t5 = r8;
1089     const Register t6 = r9, t7 = r10, t8 = r11, t9 = r12;
1090     const Register send = r17, dend = r18;
1091 
1092     if (PrefetchCopyIntervalInBytes &gt; 0)
1093       __ prfm(Address(s, 0), PLDL1KEEP);
1094     __ cmp(count, u1((UseSIMDForMemoryOps ? 96:80)/granularity));
1095     __ br(Assembler::HI, copy_big);
1096 
1097     __ lea(send, Address(s, count, Address::lsl(exact_log2(granularity))));
1098     __ lea(dend, Address(d, count, Address::lsl(exact_log2(granularity))));
1099 
1100     __ cmp(count, u1(16/granularity));
1101     __ br(Assembler::LS, copy16);
1102 
1103     __ cmp(count, u1(64/granularity));
1104     __ br(Assembler::HI, copy80);
1105 
1106     __ cmp(count, u1(32/granularity));
1107     __ br(Assembler::LS, copy32);
1108 
1109     // 33..64 bytes
1110     if (UseSIMDForMemoryOps) {
1111       __ ldpq(v0, v1, Address(s, 0));
1112       __ ldpq(v2, v3, Address(send, -32));
1113       __ stpq(v0, v1, Address(d, 0));
1114       __ stpq(v2, v3, Address(dend, -32));
1115     } else {
1116       __ ldp(t0, t1, Address(s, 0));
1117       __ ldp(t2, t3, Address(s, 16));
1118       __ ldp(t4, t5, Address(send, -32));
1119       __ ldp(t6, t7, Address(send, -16));
1120 
1121       __ stp(t0, t1, Address(d, 0));
1122       __ stp(t2, t3, Address(d, 16));
1123       __ stp(t4, t5, Address(dend, -32));
1124       __ stp(t6, t7, Address(dend, -16));
1125     }
1126     __ b(finish);
1127 
1128     // 17..32 bytes
1129     __ bind(copy32);
1130     __ ldp(t0, t1, Address(s, 0));
1131     __ ldp(t2, t3, Address(send, -16));
1132     __ stp(t0, t1, Address(d, 0));
1133     __ stp(t2, t3, Address(dend, -16));
1134     __ b(finish);
1135 
1136     // 65..80/96 bytes
1137     // (96 bytes if SIMD because we do 32 byes per instruction)
1138     __ bind(copy80);
1139     if (UseSIMDForMemoryOps) {
1140       __ ld4(v0, v1, v2, v3, __ T16B, Address(s, 0));
1141       __ ldpq(v4, v5, Address(send, -32));
1142       __ st4(v0, v1, v2, v3, __ T16B, Address(d, 0));
1143       __ stpq(v4, v5, Address(dend, -32));
1144     } else {
1145       __ ldp(t0, t1, Address(s, 0));
1146       __ ldp(t2, t3, Address(s, 16));
1147       __ ldp(t4, t5, Address(s, 32));
1148       __ ldp(t6, t7, Address(s, 48));
1149       __ ldp(t8, t9, Address(send, -16));
1150 
1151       __ stp(t0, t1, Address(d, 0));
1152       __ stp(t2, t3, Address(d, 16));
1153       __ stp(t4, t5, Address(d, 32));
1154       __ stp(t6, t7, Address(d, 48));
1155       __ stp(t8, t9, Address(dend, -16));
1156     }
1157     __ b(finish);
1158 
1159     // 0..16 bytes
1160     __ bind(copy16);
1161     __ cmp(count, u1(8/granularity));
1162     __ br(Assembler::LO, copy8);
1163 
1164     // 8..16 bytes
1165     __ ldr(t0, Address(s, 0));
1166     __ ldr(t1, Address(send, -8));
1167     __ str(t0, Address(d, 0));
1168     __ str(t1, Address(dend, -8));
1169     __ b(finish);
1170 
1171     if (granularity &lt; 8) {
1172       // 4..7 bytes
1173       __ bind(copy8);
1174       __ tbz(count, 2 - exact_log2(granularity), copy4);
1175       __ ldrw(t0, Address(s, 0));
1176       __ ldrw(t1, Address(send, -4));
1177       __ strw(t0, Address(d, 0));
1178       __ strw(t1, Address(dend, -4));
1179       __ b(finish);
1180       if (granularity &lt; 4) {
1181         // 0..3 bytes
1182         __ bind(copy4);
1183         __ cbz(count, finish); // get rid of 0 case
1184         if (granularity == 2) {
1185           __ ldrh(t0, Address(s, 0));
1186           __ strh(t0, Address(d, 0));
1187         } else { // granularity == 1
1188           // Now 1..3 bytes. Handle the 1 and 2 byte case by copying
1189           // the first and last byte.
1190           // Handle the 3 byte case by loading and storing base + count/2
1191           // (count == 1 (s+0)-&gt;(d+0), count == 2,3 (s+1) -&gt; (d+1))
1192           // This does means in the 1 byte case we load/store the same
1193           // byte 3 times.
1194           __ lsr(count, count, 1);
1195           __ ldrb(t0, Address(s, 0));
1196           __ ldrb(t1, Address(send, -1));
1197           __ ldrb(t2, Address(s, count));
1198           __ strb(t0, Address(d, 0));
1199           __ strb(t1, Address(dend, -1));
1200           __ strb(t2, Address(d, count));
1201         }
1202         __ b(finish);
1203       }
1204     }
1205 
1206     __ bind(copy_big);
1207     if (is_backwards) {
1208       __ lea(s, Address(s, count, Address::lsl(exact_log2(-step))));
1209       __ lea(d, Address(d, count, Address::lsl(exact_log2(-step))));
1210     }
1211 
1212     // Now we&#39;ve got the small case out of the way we can align the
1213     // source address on a 2-word boundary.
1214 
1215     Label aligned;
1216 
1217     if (is_aligned) {
1218       // We may have to adjust by 1 word to get s 2-word-aligned.
1219       __ tbz(s, exact_log2(wordSize), aligned);
1220       __ ldr(tmp, Address(__ adjust(s, direction * wordSize, is_backwards)));
1221       __ str(tmp, Address(__ adjust(d, direction * wordSize, is_backwards)));
1222       __ sub(count, count, wordSize/granularity);
1223     } else {
1224       if (is_backwards) {
1225         __ andr(rscratch2, s, 2 * wordSize - 1);
1226       } else {
1227         __ neg(rscratch2, s);
1228         __ andr(rscratch2, rscratch2, 2 * wordSize - 1);
1229       }
1230       // rscratch2 is the byte adjustment needed to align s.
1231       __ cbz(rscratch2, aligned);
1232       int shift = exact_log2(granularity);
1233       if (shift)  __ lsr(rscratch2, rscratch2, shift);
1234       __ sub(count, count, rscratch2);
1235 
1236 #if 0
1237       // ?? This code is only correct for a disjoint copy.  It may or
1238       // may not make sense to use it in that case.
1239 
1240       // Copy the first pair; s and d may not be aligned.
1241       __ ldp(t0, t1, Address(s, is_backwards ? -2 * wordSize : 0));
1242       __ stp(t0, t1, Address(d, is_backwards ? -2 * wordSize : 0));
1243 
1244       // Align s and d, adjust count
1245       if (is_backwards) {
1246         __ sub(s, s, rscratch2);
1247         __ sub(d, d, rscratch2);
1248       } else {
1249         __ add(s, s, rscratch2);
1250         __ add(d, d, rscratch2);
1251       }
1252 #else
1253       copy_memory_small(s, d, rscratch2, rscratch1, step);
1254 #endif
1255     }
1256 
1257     __ bind(aligned);
1258 
1259     // s is now 2-word-aligned.
1260 
1261     // We have a count of units and some trailing bytes.  Adjust the
1262     // count and do a bulk copy of words.
1263     __ lsr(rscratch2, count, exact_log2(wordSize/granularity));
1264     if (direction == copy_forwards)
1265       __ bl(copy_f);
1266     else
1267       __ bl(copy_b);
1268 
1269     // And the tail.
1270     copy_memory_small(s, d, count, tmp, step);
1271 
1272     if (granularity &gt;= 8) __ bind(copy8);
1273     if (granularity &gt;= 4) __ bind(copy4);
1274     __ bind(finish);
1275   }
1276 
1277 
1278   void clobber_registers() {
1279 #ifdef ASSERT
1280     __ mov(rscratch1, (uint64_t)0xdeadbeef);
1281     __ orr(rscratch1, rscratch1, rscratch1, Assembler::LSL, 32);
1282     for (Register r = r3; r &lt;= r18; r++)
1283       if (r != rscratch1) __ mov(r, rscratch1);
1284 #endif
1285   }
1286 
1287   // Scan over array at a for count oops, verifying each one.
1288   // Preserves a and count, clobbers rscratch1 and rscratch2.
1289   void verify_oop_array (size_t size, Register a, Register count, Register temp) {
1290     Label loop, end;
1291     __ mov(rscratch1, a);
1292     __ mov(rscratch2, zr);
1293     __ bind(loop);
1294     __ cmp(rscratch2, count);
1295     __ br(Assembler::HS, end);
1296     if (size == (size_t)wordSize) {
1297       __ ldr(temp, Address(a, rscratch2, Address::lsl(exact_log2(size))));
1298       __ verify_oop(temp);
1299     } else {
1300       __ ldrw(r16, Address(a, rscratch2, Address::lsl(exact_log2(size))));
1301       __ decode_heap_oop(temp); // calls verify_oop
1302     }
1303     __ add(rscratch2, rscratch2, size);
1304     __ b(loop);
1305     __ bind(end);
1306   }
1307 
1308   // Arguments:
1309   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1310   //             ignored
1311   //   is_oop  - true =&gt; oop array, so generate store check code
1312   //   name    - stub name string
1313   //
1314   // Inputs:
1315   //   c_rarg0   - source array address
1316   //   c_rarg1   - destination array address
1317   //   c_rarg2   - element count, treated as ssize_t, can be zero
1318   //
1319   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1320   // the hardware handle it.  The two dwords within qwords that span
1321   // cache line boundaries will still be loaded and stored atomicly.
1322   //
1323   // Side Effects:
1324   //   disjoint_int_copy_entry is set to the no-overlap entry point
1325   //   used by generate_conjoint_int_oop_copy().
1326   //
1327   address generate_disjoint_copy(size_t size, bool aligned, bool is_oop, address *entry,
1328                                   const char *name, bool dest_uninitialized = false) {
1329     Register s = c_rarg0, d = c_rarg1, count = c_rarg2;
1330     RegSet saved_reg = RegSet::of(s, d, count);
1331     __ align(CodeEntryAlignment);
1332     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1333     address start = __ pc();
1334     __ enter();
1335 
1336     if (entry != NULL) {
1337       *entry = __ pc();
1338       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1339       BLOCK_COMMENT(&quot;Entry:&quot;);
1340     }
1341 
1342     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;
1343     if (dest_uninitialized) {
1344       decorators |= IS_DEST_UNINITIALIZED;
1345     }
1346     if (aligned) {
1347       decorators |= ARRAYCOPY_ALIGNED;
1348     }
1349 
1350     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1351     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, s, d, count, saved_reg);
1352 
1353     if (is_oop) {
1354       // save regs before copy_memory
1355       __ push(RegSet::of(d, count), sp);
1356     }
1357     {
1358       // UnsafeCopyMemory page error: continue after ucm
1359       bool add_entry = !is_oop &amp;&amp; (!aligned || sizeof(jlong) == size);
1360       UnsafeCopyMemoryMark ucmm(this, add_entry, true);
1361       copy_memory(aligned, s, d, count, rscratch1, size);
1362     }
1363 
1364     if (is_oop) {
1365       __ pop(RegSet::of(d, count), sp);
1366       if (VerifyOops)
1367         verify_oop_array(size, d, count, r16);
1368     }
1369 
1370     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1, RegSet());
1371 
1372     __ leave();
1373     __ mov(r0, zr); // return 0
1374     __ ret(lr);
1375     return start;
1376   }
1377 
1378   // Arguments:
1379   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1380   //             ignored
1381   //   is_oop  - true =&gt; oop array, so generate store check code
1382   //   name    - stub name string
1383   //
1384   // Inputs:
1385   //   c_rarg0   - source array address
1386   //   c_rarg1   - destination array address
1387   //   c_rarg2   - element count, treated as ssize_t, can be zero
1388   //
1389   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1390   // the hardware handle it.  The two dwords within qwords that span
1391   // cache line boundaries will still be loaded and stored atomicly.
1392   //
1393   address generate_conjoint_copy(size_t size, bool aligned, bool is_oop, address nooverlap_target,
1394                                  address *entry, const char *name,
1395                                  bool dest_uninitialized = false) {
1396     Register s = c_rarg0, d = c_rarg1, count = c_rarg2;
1397     RegSet saved_regs = RegSet::of(s, d, count);
1398     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1399     address start = __ pc();
1400     __ enter();
1401 
1402     if (entry != NULL) {
1403       *entry = __ pc();
1404       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1405       BLOCK_COMMENT(&quot;Entry:&quot;);
1406     }
1407 
1408     // use fwd copy when (d-s) above_equal (count*size)
1409     __ sub(rscratch1, d, s);
1410     __ cmp(rscratch1, count, Assembler::LSL, exact_log2(size));
1411     __ br(Assembler::HS, nooverlap_target);
1412 
1413     DecoratorSet decorators = IN_HEAP | IS_ARRAY;
1414     if (dest_uninitialized) {
1415       decorators |= IS_DEST_UNINITIALIZED;
1416     }
1417     if (aligned) {
1418       decorators |= ARRAYCOPY_ALIGNED;
1419     }
1420 
1421     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1422     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, s, d, count, saved_regs);
1423 
1424     if (is_oop) {
1425       // save regs before copy_memory
1426       __ push(RegSet::of(d, count), sp);
1427     }
1428     {
1429       // UnsafeCopyMemory page error: continue after ucm
1430       bool add_entry = !is_oop &amp;&amp; (!aligned || sizeof(jlong) == size);
1431       UnsafeCopyMemoryMark ucmm(this, add_entry, true);
1432       copy_memory(aligned, s, d, count, rscratch1, -size);
1433     }
1434     if (is_oop) {
1435       __ pop(RegSet::of(d, count), sp);
1436       if (VerifyOops)
1437         verify_oop_array(size, d, count, r16);
1438     }
1439     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1, RegSet());
1440     __ leave();
1441     __ mov(r0, zr); // return 0
1442     __ ret(lr);
1443     return start;
1444 }
1445 
1446   // Arguments:
1447   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1448   //             ignored
1449   //   name    - stub name string
1450   //
1451   // Inputs:
1452   //   c_rarg0   - source array address
1453   //   c_rarg1   - destination array address
1454   //   c_rarg2   - element count, treated as ssize_t, can be zero
1455   //
1456   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1457   // we let the hardware handle it.  The one to eight bytes within words,
1458   // dwords or qwords that span cache line boundaries will still be loaded
1459   // and stored atomically.
1460   //
1461   // Side Effects:
1462   //   disjoint_byte_copy_entry is set to the no-overlap entry point  //
1463   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1464   // we let the hardware handle it.  The one to eight bytes within words,
1465   // dwords or qwords that span cache line boundaries will still be loaded
1466   // and stored atomically.
1467   //
1468   // Side Effects:
1469   //   disjoint_byte_copy_entry is set to the no-overlap entry point
1470   //   used by generate_conjoint_byte_copy().
1471   //
1472   address generate_disjoint_byte_copy(bool aligned, address* entry, const char *name) {
1473     const bool not_oop = false;
1474     return generate_disjoint_copy(sizeof (jbyte), aligned, not_oop, entry, name);
1475   }
1476 
1477   // Arguments:
1478   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1479   //             ignored
1480   //   name    - stub name string
1481   //
1482   // Inputs:
1483   //   c_rarg0   - source array address
1484   //   c_rarg1   - destination array address
1485   //   c_rarg2   - element count, treated as ssize_t, can be zero
1486   //
1487   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1488   // we let the hardware handle it.  The one to eight bytes within words,
1489   // dwords or qwords that span cache line boundaries will still be loaded
1490   // and stored atomically.
1491   //
1492   address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,
1493                                       address* entry, const char *name) {
1494     const bool not_oop = false;
1495     return generate_conjoint_copy(sizeof (jbyte), aligned, not_oop, nooverlap_target, entry, name);
1496   }
1497 
1498   // Arguments:
1499   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1500   //             ignored
1501   //   name    - stub name string
1502   //
1503   // Inputs:
1504   //   c_rarg0   - source array address
1505   //   c_rarg1   - destination array address
1506   //   c_rarg2   - element count, treated as ssize_t, can be zero
1507   //
1508   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4- or 2-byte boundaries, we
1509   // let the hardware handle it.  The two or four words within dwords
1510   // or qwords that span cache line boundaries will still be loaded
1511   // and stored atomically.
1512   //
1513   // Side Effects:
1514   //   disjoint_short_copy_entry is set to the no-overlap entry point
1515   //   used by generate_conjoint_short_copy().
1516   //
1517   address generate_disjoint_short_copy(bool aligned,
1518                                        address* entry, const char *name) {
1519     const bool not_oop = false;
1520     return generate_disjoint_copy(sizeof (jshort), aligned, not_oop, entry, name);
1521   }
1522 
1523   // Arguments:
1524   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1525   //             ignored
1526   //   name    - stub name string
1527   //
1528   // Inputs:
1529   //   c_rarg0   - source array address
1530   //   c_rarg1   - destination array address
1531   //   c_rarg2   - element count, treated as ssize_t, can be zero
1532   //
1533   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4- or 2-byte boundaries, we
1534   // let the hardware handle it.  The two or four words within dwords
1535   // or qwords that span cache line boundaries will still be loaded
1536   // and stored atomically.
1537   //
1538   address generate_conjoint_short_copy(bool aligned, address nooverlap_target,
1539                                        address *entry, const char *name) {
1540     const bool not_oop = false;
1541     return generate_conjoint_copy(sizeof (jshort), aligned, not_oop, nooverlap_target, entry, name);
1542 
1543   }
1544   // Arguments:
1545   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1546   //             ignored
1547   //   name    - stub name string
1548   //
1549   // Inputs:
1550   //   c_rarg0   - source array address
1551   //   c_rarg1   - destination array address
1552   //   c_rarg2   - element count, treated as ssize_t, can be zero
1553   //
1554   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1555   // the hardware handle it.  The two dwords within qwords that span
1556   // cache line boundaries will still be loaded and stored atomicly.
1557   //
1558   // Side Effects:
1559   //   disjoint_int_copy_entry is set to the no-overlap entry point
1560   //   used by generate_conjoint_int_oop_copy().
1561   //
1562   address generate_disjoint_int_copy(bool aligned, address *entry,
1563                                          const char *name, bool dest_uninitialized = false) {
1564     const bool not_oop = false;
1565     return generate_disjoint_copy(sizeof (jint), aligned, not_oop, entry, name);
1566   }
1567 
1568   // Arguments:
1569   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1570   //             ignored
1571   //   name    - stub name string
1572   //
1573   // Inputs:
1574   //   c_rarg0   - source array address
1575   //   c_rarg1   - destination array address
1576   //   c_rarg2   - element count, treated as ssize_t, can be zero
1577   //
1578   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1579   // the hardware handle it.  The two dwords within qwords that span
1580   // cache line boundaries will still be loaded and stored atomicly.
1581   //
1582   address generate_conjoint_int_copy(bool aligned, address nooverlap_target,
1583                                      address *entry, const char *name,
1584                                      bool dest_uninitialized = false) {
1585     const bool not_oop = false;
1586     return generate_conjoint_copy(sizeof (jint), aligned, not_oop, nooverlap_target, entry, name);
1587   }
1588 
1589 
1590   // Arguments:
1591   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1592   //             ignored
1593   //   name    - stub name string
1594   //
1595   // Inputs:
1596   //   c_rarg0   - source array address
1597   //   c_rarg1   - destination array address
1598   //   c_rarg2   - element count, treated as size_t, can be zero
1599   //
1600   // Side Effects:
1601   //   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the
1602   //   no-overlap entry point used by generate_conjoint_long_oop_copy().
1603   //
1604   address generate_disjoint_long_copy(bool aligned, address *entry,
1605                                           const char *name, bool dest_uninitialized = false) {
1606     const bool not_oop = false;
1607     return generate_disjoint_copy(sizeof (jlong), aligned, not_oop, entry, name);
1608   }
1609 
1610   // Arguments:
1611   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1612   //             ignored
1613   //   name    - stub name string
1614   //
1615   // Inputs:
1616   //   c_rarg0   - source array address
1617   //   c_rarg1   - destination array address
1618   //   c_rarg2   - element count, treated as size_t, can be zero
1619   //
1620   address generate_conjoint_long_copy(bool aligned,
1621                                       address nooverlap_target, address *entry,
1622                                       const char *name, bool dest_uninitialized = false) {
1623     const bool not_oop = false;
1624     return generate_conjoint_copy(sizeof (jlong), aligned, not_oop, nooverlap_target, entry, name);
1625   }
1626 
1627   // Arguments:
1628   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1629   //             ignored
1630   //   name    - stub name string
1631   //
1632   // Inputs:
1633   //   c_rarg0   - source array address
1634   //   c_rarg1   - destination array address
1635   //   c_rarg2   - element count, treated as size_t, can be zero
1636   //
1637   // Side Effects:
1638   //   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the
1639   //   no-overlap entry point used by generate_conjoint_long_oop_copy().
1640   //
1641   address generate_disjoint_oop_copy(bool aligned, address *entry,
1642                                      const char *name, bool dest_uninitialized) {
1643     const bool is_oop = true;
1644     const size_t size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);
1645     return generate_disjoint_copy(size, aligned, is_oop, entry, name, dest_uninitialized);
1646   }
1647 
1648   // Arguments:
1649   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1650   //             ignored
1651   //   name    - stub name string
1652   //
1653   // Inputs:
1654   //   c_rarg0   - source array address
1655   //   c_rarg1   - destination array address
1656   //   c_rarg2   - element count, treated as size_t, can be zero
1657   //
1658   address generate_conjoint_oop_copy(bool aligned,
1659                                      address nooverlap_target, address *entry,
1660                                      const char *name, bool dest_uninitialized) {
1661     const bool is_oop = true;
1662     const size_t size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);
1663     return generate_conjoint_copy(size, aligned, is_oop, nooverlap_target, entry,
1664                                   name, dest_uninitialized);
1665   }
1666 
1667 
1668   // Helper for generating a dynamic type check.
1669   // Smashes rscratch1, rscratch2.
1670   void generate_type_check(Register sub_klass,
1671                            Register super_check_offset,
1672                            Register super_klass,
1673                            Label&amp; L_success) {
1674     assert_different_registers(sub_klass, super_check_offset, super_klass);
1675 
1676     BLOCK_COMMENT(&quot;type_check:&quot;);
1677 
1678     Label L_miss;
1679 
1680     __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &amp;L_success, &amp;L_miss, NULL,
1681                                      super_check_offset);
1682     __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &amp;L_success, NULL);
1683 
1684     // Fall through on failure!
1685     __ BIND(L_miss);
1686   }
1687 
1688   //
1689   //  Generate checkcasting array copy stub
1690   //
1691   //  Input:
1692   //    c_rarg0   - source array address
1693   //    c_rarg1   - destination array address
1694   //    c_rarg2   - element count, treated as ssize_t, can be zero
1695   //    c_rarg3   - size_t ckoff (super_check_offset)
1696   //    c_rarg4   - oop ckval (super_klass)
1697   //
1698   //  Output:
1699   //    r0 ==  0  -  success
1700   //    r0 == -1^K - failure, where K is partial transfer count
1701   //
1702   address generate_checkcast_copy(const char *name, address *entry,
1703                                   bool dest_uninitialized = false) {
1704 
1705     Label L_load_element, L_store_element, L_do_card_marks, L_done, L_done_pop;
1706 
1707     // Input registers (after setup_arg_regs)
1708     const Register from        = c_rarg0;   // source array address
1709     const Register to          = c_rarg1;   // destination array address
1710     const Register count       = c_rarg2;   // elementscount
1711     const Register ckoff       = c_rarg3;   // super_check_offset
1712     const Register ckval       = c_rarg4;   // super_klass
1713 
1714     RegSet wb_pre_saved_regs = RegSet::range(c_rarg0, c_rarg4);
1715     RegSet wb_post_saved_regs = RegSet::of(count);
1716 
1717     // Registers used as temps (r18, r19, r20 are save-on-entry)
1718     const Register count_save  = r21;       // orig elementscount
1719     const Register start_to    = r20;       // destination array start address
1720     const Register copied_oop  = r18;       // actual oop copied
1721     const Register r19_klass   = r19;       // oop._klass
1722 
1723     //---------------------------------------------------------------
1724     // Assembler stub will be used for this call to arraycopy
1725     // if the two arrays are subtypes of Object[] but the
1726     // destination array type is not equal to or a supertype
1727     // of the source type.  Each element must be separately
1728     // checked.
1729 
1730     assert_different_registers(from, to, count, ckoff, ckval, start_to,
1731                                copied_oop, r19_klass, count_save);
1732 
1733     __ align(CodeEntryAlignment);
1734     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1735     address start = __ pc();
1736 
1737     __ enter(); // required for proper stackwalking of RuntimeStub frame
1738 
1739 #ifdef ASSERT
1740     // caller guarantees that the arrays really are different
1741     // otherwise, we would have to make conjoint checks
1742     { Label L;
1743       array_overlap_test(L, TIMES_OOP);
1744       __ stop(&quot;checkcast_copy within a single array&quot;);
1745       __ bind(L);
1746     }
1747 #endif //ASSERT
1748 
1749     // Caller of this entry point must set up the argument registers.
1750     if (entry != NULL) {
1751       *entry = __ pc();
1752       BLOCK_COMMENT(&quot;Entry:&quot;);
1753     }
1754 
1755      // Empty array:  Nothing to do.
1756     __ cbz(count, L_done);
1757 
1758     __ push(RegSet::of(r18, r19, r20, r21), sp);
1759 
1760 #ifdef ASSERT
1761     BLOCK_COMMENT(&quot;assert consistent ckoff/ckval&quot;);
1762     // The ckoff and ckval must be mutually consistent,
1763     // even though caller generates both.
1764     { Label L;
1765       int sco_offset = in_bytes(Klass::super_check_offset_offset());
1766       __ ldrw(start_to, Address(ckval, sco_offset));
1767       __ cmpw(ckoff, start_to);
1768       __ br(Assembler::EQ, L);
1769       __ stop(&quot;super_check_offset inconsistent&quot;);
1770       __ bind(L);
1771     }
1772 #endif //ASSERT
1773 
1774     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;
1775     bool is_oop = true;
1776     if (dest_uninitialized) {
1777       decorators |= IS_DEST_UNINITIALIZED;
1778     }
1779 
1780     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1781     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, from, to, count, wb_pre_saved_regs);
1782 
1783     // save the original count
1784     __ mov(count_save, count);
1785 
1786     // Copy from low to high addresses
1787     __ mov(start_to, to);              // Save destination array start address
1788     __ b(L_load_element);
1789 
1790     // ======== begin loop ========
1791     // (Loop is rotated; its entry is L_load_element.)
1792     // Loop control:
1793     //   for (; count != 0; count--) {
1794     //     copied_oop = load_heap_oop(from++);
1795     //     ... generate_type_check ...;
1796     //     store_heap_oop(to++, copied_oop);
1797     //   }
1798     __ align(OptoLoopAlignment);
1799 
1800     __ BIND(L_store_element);
1801     __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, AS_RAW);  // store the oop
1802     __ sub(count, count, 1);
1803     __ cbz(count, L_do_card_marks);
1804 
1805     // ======== loop entry is here ========
1806     __ BIND(L_load_element);
1807     __ load_heap_oop(copied_oop, __ post(from, UseCompressedOops ? 4 : 8), noreg, noreg, AS_RAW); // load the oop
1808     __ cbz(copied_oop, L_store_element);
1809 
1810     __ load_klass(r19_klass, copied_oop);// query the object klass
1811     generate_type_check(r19_klass, ckoff, ckval, L_store_element);
1812     // ======== end loop ========
1813 
1814     // It was a real error; we must depend on the caller to finish the job.
1815     // Register count = remaining oops, count_orig = total oops.
1816     // Emit GC store barriers for the oops we have copied and report
1817     // their number to the caller.
1818 
1819     __ subs(count, count_save, count);     // K = partially copied oop count
1820     __ eon(count, count, zr);                   // report (-1^K) to caller
1821     __ br(Assembler::EQ, L_done_pop);
1822 
1823     __ BIND(L_do_card_marks);
1824     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, start_to, count_save, rscratch1, wb_post_saved_regs);
1825 
1826     __ bind(L_done_pop);
1827     __ pop(RegSet::of(r18, r19, r20, r21), sp);
1828     inc_counter_np(SharedRuntime::_checkcast_array_copy_ctr);
1829 
1830     __ bind(L_done);
1831     __ mov(r0, count);
1832     __ leave();
1833     __ ret(lr);
1834 
1835     return start;
1836   }
1837 
1838   // Perform range checks on the proposed arraycopy.
1839   // Kills temp, but nothing else.
1840   // Also, clean the sign bits of src_pos and dst_pos.
1841   void arraycopy_range_checks(Register src,     // source array oop (c_rarg0)
1842                               Register src_pos, // source position (c_rarg1)
1843                               Register dst,     // destination array oo (c_rarg2)
1844                               Register dst_pos, // destination position (c_rarg3)
1845                               Register length,
1846                               Register temp,
1847                               Label&amp; L_failed) {
1848     BLOCK_COMMENT(&quot;arraycopy_range_checks:&quot;);
1849 
1850     assert_different_registers(rscratch1, temp);
1851 
1852     //  if (src_pos + length &gt; arrayOop(src)-&gt;length())  FAIL;
1853     __ ldrw(rscratch1, Address(src, arrayOopDesc::length_offset_in_bytes()));
1854     __ addw(temp, length, src_pos);
1855     __ cmpw(temp, rscratch1);
1856     __ br(Assembler::HI, L_failed);
1857 
1858     //  if (dst_pos + length &gt; arrayOop(dst)-&gt;length())  FAIL;
1859     __ ldrw(rscratch1, Address(dst, arrayOopDesc::length_offset_in_bytes()));
1860     __ addw(temp, length, dst_pos);
1861     __ cmpw(temp, rscratch1);
1862     __ br(Assembler::HI, L_failed);
1863 
1864     // Have to clean up high 32 bits of &#39;src_pos&#39; and &#39;dst_pos&#39;.
1865     __ movw(src_pos, src_pos);
1866     __ movw(dst_pos, dst_pos);
1867 
1868     BLOCK_COMMENT(&quot;arraycopy_range_checks done&quot;);
1869   }
1870 
1871   // These stubs get called from some dumb test routine.
1872   // I&#39;ll write them properly when they&#39;re called from
1873   // something that&#39;s actually doing something.
1874   static void fake_arraycopy_stub(address src, address dst, int count) {
1875     assert(count == 0, &quot;huh?&quot;);
1876   }
1877 
1878 
1879   //
1880   //  Generate &#39;unsafe&#39; array copy stub
1881   //  Though just as safe as the other stubs, it takes an unscaled
1882   //  size_t argument instead of an element count.
1883   //
1884   //  Input:
1885   //    c_rarg0   - source array address
1886   //    c_rarg1   - destination array address
1887   //    c_rarg2   - byte count, treated as ssize_t, can be zero
1888   //
1889   // Examines the alignment of the operands and dispatches
1890   // to a long, int, short, or byte copy loop.
1891   //
1892   address generate_unsafe_copy(const char *name,
1893                                address byte_copy_entry,
1894                                address short_copy_entry,
1895                                address int_copy_entry,
1896                                address long_copy_entry) {
1897     Label L_long_aligned, L_int_aligned, L_short_aligned;
1898     Register s = c_rarg0, d = c_rarg1, count = c_rarg2;
1899 
1900     __ align(CodeEntryAlignment);
1901     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1902     address start = __ pc();
1903     __ enter(); // required for proper stackwalking of RuntimeStub frame
1904 
1905     // bump this on entry, not on exit:
1906     inc_counter_np(SharedRuntime::_unsafe_array_copy_ctr);
1907 
1908     __ orr(rscratch1, s, d);
1909     __ orr(rscratch1, rscratch1, count);
1910 
1911     __ andr(rscratch1, rscratch1, BytesPerLong-1);
1912     __ cbz(rscratch1, L_long_aligned);
1913     __ andr(rscratch1, rscratch1, BytesPerInt-1);
1914     __ cbz(rscratch1, L_int_aligned);
1915     __ tbz(rscratch1, 0, L_short_aligned);
1916     __ b(RuntimeAddress(byte_copy_entry));
1917 
1918     __ BIND(L_short_aligned);
1919     __ lsr(count, count, LogBytesPerShort);  // size =&gt; short_count
1920     __ b(RuntimeAddress(short_copy_entry));
1921     __ BIND(L_int_aligned);
1922     __ lsr(count, count, LogBytesPerInt);    // size =&gt; int_count
1923     __ b(RuntimeAddress(int_copy_entry));
1924     __ BIND(L_long_aligned);
1925     __ lsr(count, count, LogBytesPerLong);   // size =&gt; long_count
1926     __ b(RuntimeAddress(long_copy_entry));
1927 
1928     return start;
1929   }
1930 
1931   //
1932   //  Generate generic array copy stubs
1933   //
1934   //  Input:
1935   //    c_rarg0    -  src oop
1936   //    c_rarg1    -  src_pos (32-bits)
1937   //    c_rarg2    -  dst oop
1938   //    c_rarg3    -  dst_pos (32-bits)
1939   //    c_rarg4    -  element count (32-bits)
1940   //
1941   //  Output:
1942   //    r0 ==  0  -  success
1943   //    r0 == -1^K - failure, where K is partial transfer count
1944   //
1945   address generate_generic_copy(const char *name,
1946                                 address byte_copy_entry, address short_copy_entry,
1947                                 address int_copy_entry, address oop_copy_entry,
1948                                 address long_copy_entry, address checkcast_copy_entry) {
1949 
1950     Label L_failed, L_objArray;
1951     Label L_copy_bytes, L_copy_shorts, L_copy_ints, L_copy_longs;
1952 
1953     // Input registers
1954     const Register src        = c_rarg0;  // source array oop
1955     const Register src_pos    = c_rarg1;  // source position
1956     const Register dst        = c_rarg2;  // destination array oop
1957     const Register dst_pos    = c_rarg3;  // destination position
1958     const Register length     = c_rarg4;
1959 
1960 
1961     // Registers used as temps
1962     const Register dst_klass  = c_rarg5;
1963 
1964     __ align(CodeEntryAlignment);
1965 
1966     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1967 
1968     address start = __ pc();
1969 
1970     __ enter(); // required for proper stackwalking of RuntimeStub frame
1971 
1972     // bump this on entry, not on exit:
1973     inc_counter_np(SharedRuntime::_generic_array_copy_ctr);
1974 
1975     //-----------------------------------------------------------------------
1976     // Assembler stub will be used for this call to arraycopy
1977     // if the following conditions are met:
1978     //
1979     // (1) src and dst must not be null.
1980     // (2) src_pos must not be negative.
1981     // (3) dst_pos must not be negative.
1982     // (4) length  must not be negative.
1983     // (5) src klass and dst klass should be the same and not NULL.
1984     // (6) src and dst should be arrays.
1985     // (7) src_pos + length must not exceed length of src.
1986     // (8) dst_pos + length must not exceed length of dst.
1987     //
1988 
1989     //  if (src == NULL) return -1;
1990     __ cbz(src, L_failed);
1991 
1992     //  if (src_pos &lt; 0) return -1;
1993     __ tbnz(src_pos, 31, L_failed);  // i.e. sign bit set
1994 
1995     //  if (dst == NULL) return -1;
1996     __ cbz(dst, L_failed);
1997 
1998     //  if (dst_pos &lt; 0) return -1;
1999     __ tbnz(dst_pos, 31, L_failed);  // i.e. sign bit set
2000 
2001     // registers used as temp
2002     const Register scratch_length    = r16; // elements count to copy
2003     const Register scratch_src_klass = r17; // array klass
2004     const Register lh                = r18; // layout helper
2005 
2006     //  if (length &lt; 0) return -1;
2007     __ movw(scratch_length, length);        // length (elements count, 32-bits value)
2008     __ tbnz(scratch_length, 31, L_failed);  // i.e. sign bit set
2009 
2010     __ load_klass(scratch_src_klass, src);
2011 #ifdef ASSERT
2012     //  assert(src-&gt;klass() != NULL);
2013     {
2014       BLOCK_COMMENT(&quot;assert klasses not null {&quot;);
2015       Label L1, L2;
2016       __ cbnz(scratch_src_klass, L2);   // it is broken if klass is NULL
2017       __ bind(L1);
2018       __ stop(&quot;broken null klass&quot;);
2019       __ bind(L2);
2020       __ load_klass(rscratch1, dst);
2021       __ cbz(rscratch1, L1);     // this would be broken also
2022       BLOCK_COMMENT(&quot;} assert klasses not null done&quot;);
2023     }
2024 #endif
2025 
2026     // Load layout helper (32-bits)
2027     //
2028     //  |array_tag|     | header_size | element_type |     |log2_element_size|
2029     // 32        30    24            16              8     2                 0
2030     //
2031     //   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0
2032     //
2033 
2034     const int lh_offset = in_bytes(Klass::layout_helper_offset());
2035 
2036     // Handle objArrays completely differently...
2037     const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2038     __ ldrw(lh, Address(scratch_src_klass, lh_offset));
2039     __ movw(rscratch1, objArray_lh);
2040     __ eorw(rscratch2, lh, rscratch1);
2041     __ cbzw(rscratch2, L_objArray);
2042 
2043     //  if (src-&gt;klass() != dst-&gt;klass()) return -1;
2044     __ load_klass(rscratch2, dst);
2045     __ eor(rscratch2, rscratch2, scratch_src_klass);
2046     __ cbnz(rscratch2, L_failed);
2047 
2048     //  if (!src-&gt;is_Array()) return -1;
2049     __ tbz(lh, 31, L_failed);  // i.e. (lh &gt;= 0)
2050 
2051     // At this point, it is known to be a typeArray (array_tag 0x3).
2052 #ifdef ASSERT
2053     {
2054       BLOCK_COMMENT(&quot;assert primitive array {&quot;);
2055       Label L;
2056       __ movw(rscratch2, Klass::_lh_array_tag_type_value &lt;&lt; Klass::_lh_array_tag_shift);
2057       __ cmpw(lh, rscratch2);
2058       __ br(Assembler::GE, L);
2059       __ stop(&quot;must be a primitive array&quot;);
2060       __ bind(L);
2061       BLOCK_COMMENT(&quot;} assert primitive array done&quot;);
2062     }
2063 #endif
2064 
2065     arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,
2066                            rscratch2, L_failed);
2067 
2068     // TypeArrayKlass
2069     //
2070     // src_addr = (src + array_header_in_bytes()) + (src_pos &lt;&lt; log2elemsize);
2071     // dst_addr = (dst + array_header_in_bytes()) + (dst_pos &lt;&lt; log2elemsize);
2072     //
2073 
2074     const Register rscratch1_offset = rscratch1;    // array offset
2075     const Register r18_elsize = lh; // element size
2076 
2077     __ ubfx(rscratch1_offset, lh, Klass::_lh_header_size_shift,
2078            exact_log2(Klass::_lh_header_size_mask+1));   // array_offset
2079     __ add(src, src, rscratch1_offset);           // src array offset
2080     __ add(dst, dst, rscratch1_offset);           // dst array offset
2081     BLOCK_COMMENT(&quot;choose copy loop based on element size&quot;);
2082 
2083     // next registers should be set before the jump to corresponding stub
2084     const Register from     = c_rarg0;  // source array address
2085     const Register to       = c_rarg1;  // destination array address
2086     const Register count    = c_rarg2;  // elements count
2087 
2088     // &#39;from&#39;, &#39;to&#39;, &#39;count&#39; registers should be set in such order
2089     // since they are the same as &#39;src&#39;, &#39;src_pos&#39;, &#39;dst&#39;.
2090 
2091     assert(Klass::_lh_log2_element_size_shift == 0, &quot;fix this code&quot;);
2092 
2093     // The possible values of elsize are 0-3, i.e. exact_log2(element
2094     // size in bytes).  We do a simple bitwise binary search.
2095   __ BIND(L_copy_bytes);
2096     __ tbnz(r18_elsize, 1, L_copy_ints);
2097     __ tbnz(r18_elsize, 0, L_copy_shorts);
2098     __ lea(from, Address(src, src_pos));// src_addr
2099     __ lea(to,   Address(dst, dst_pos));// dst_addr
2100     __ movw(count, scratch_length); // length
2101     __ b(RuntimeAddress(byte_copy_entry));
2102 
2103   __ BIND(L_copy_shorts);
2104     __ lea(from, Address(src, src_pos, Address::lsl(1)));// src_addr
2105     __ lea(to,   Address(dst, dst_pos, Address::lsl(1)));// dst_addr
2106     __ movw(count, scratch_length); // length
2107     __ b(RuntimeAddress(short_copy_entry));
2108 
2109   __ BIND(L_copy_ints);
2110     __ tbnz(r18_elsize, 0, L_copy_longs);
2111     __ lea(from, Address(src, src_pos, Address::lsl(2)));// src_addr
2112     __ lea(to,   Address(dst, dst_pos, Address::lsl(2)));// dst_addr
2113     __ movw(count, scratch_length); // length
2114     __ b(RuntimeAddress(int_copy_entry));
2115 
2116   __ BIND(L_copy_longs);
2117 #ifdef ASSERT
2118     {
2119       BLOCK_COMMENT(&quot;assert long copy {&quot;);
2120       Label L;
2121       __ andw(lh, lh, Klass::_lh_log2_element_size_mask); // lh -&gt; r18_elsize
2122       __ cmpw(r18_elsize, LogBytesPerLong);
2123       __ br(Assembler::EQ, L);
2124       __ stop(&quot;must be long copy, but elsize is wrong&quot;);
2125       __ bind(L);
2126       BLOCK_COMMENT(&quot;} assert long copy done&quot;);
2127     }
2128 #endif
2129     __ lea(from, Address(src, src_pos, Address::lsl(3)));// src_addr
2130     __ lea(to,   Address(dst, dst_pos, Address::lsl(3)));// dst_addr
2131     __ movw(count, scratch_length); // length
2132     __ b(RuntimeAddress(long_copy_entry));
2133 
2134     // ObjArrayKlass
2135   __ BIND(L_objArray);
2136     // live at this point:  scratch_src_klass, scratch_length, src[_pos], dst[_pos]
2137 
2138     Label L_plain_copy, L_checkcast_copy;
2139     //  test array classes for subtyping
2140     __ load_klass(r18, dst);
2141     __ cmp(scratch_src_klass, r18); // usual case is exact equality
2142     __ br(Assembler::NE, L_checkcast_copy);
2143 
2144     // Identically typed arrays can be copied without element-wise checks.
2145     arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,
2146                            rscratch2, L_failed);
2147 
2148     __ lea(from, Address(src, src_pos, Address::lsl(LogBytesPerHeapOop)));
2149     __ add(from, from, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2150     __ lea(to, Address(dst, dst_pos, Address::lsl(LogBytesPerHeapOop)));
2151     __ add(to, to, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2152     __ movw(count, scratch_length); // length
2153   __ BIND(L_plain_copy);
2154     __ b(RuntimeAddress(oop_copy_entry));
2155 
2156   __ BIND(L_checkcast_copy);
2157     // live at this point:  scratch_src_klass, scratch_length, r18 (dst_klass)
2158     {
2159       // Before looking at dst.length, make sure dst is also an objArray.
2160       __ ldrw(rscratch1, Address(r18, lh_offset));
2161       __ movw(rscratch2, objArray_lh);
2162       __ eorw(rscratch1, rscratch1, rscratch2);
2163       __ cbnzw(rscratch1, L_failed);
2164 
2165       // It is safe to examine both src.length and dst.length.
2166       arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,
2167                              r18, L_failed);
2168 
2169       __ load_klass(dst_klass, dst); // reload
2170 
2171       // Marshal the base address arguments now, freeing registers.
2172       __ lea(from, Address(src, src_pos, Address::lsl(LogBytesPerHeapOop)));
2173       __ add(from, from, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2174       __ lea(to, Address(dst, dst_pos, Address::lsl(LogBytesPerHeapOop)));
2175       __ add(to, to, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2176       __ movw(count, length);           // length (reloaded)
2177       Register sco_temp = c_rarg3;      // this register is free now
2178       assert_different_registers(from, to, count, sco_temp,
2179                                  dst_klass, scratch_src_klass);
2180       // assert_clean_int(count, sco_temp);
2181 
2182       // Generate the type check.
2183       const int sco_offset = in_bytes(Klass::super_check_offset_offset());
2184       __ ldrw(sco_temp, Address(dst_klass, sco_offset));
2185 
2186       // Smashes rscratch1, rscratch2
2187       generate_type_check(scratch_src_klass, sco_temp, dst_klass, L_plain_copy);
2188 
2189       // Fetch destination element klass from the ObjArrayKlass header.
2190       int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
2191       __ ldr(dst_klass, Address(dst_klass, ek_offset));
2192       __ ldrw(sco_temp, Address(dst_klass, sco_offset));
2193 
2194       // the checkcast_copy loop needs two extra arguments:
2195       assert(c_rarg3 == sco_temp, &quot;#3 already in place&quot;);
2196       // Set up arguments for checkcast_copy_entry.
2197       __ mov(c_rarg4, dst_klass);  // dst.klass.element_klass
2198       __ b(RuntimeAddress(checkcast_copy_entry));
2199     }
2200 
2201   __ BIND(L_failed);
2202     __ mov(r0, -1);
2203     __ leave();   // required for proper stackwalking of RuntimeStub frame
2204     __ ret(lr);
2205 
2206     return start;
2207   }
2208 
2209   //
2210   // Generate stub for array fill. If &quot;aligned&quot; is true, the
2211   // &quot;to&quot; address is assumed to be heapword aligned.
2212   //
2213   // Arguments for generated stub:
2214   //   to:    c_rarg0
2215   //   value: c_rarg1
2216   //   count: c_rarg2 treated as signed
2217   //
2218   address generate_fill(BasicType t, bool aligned, const char *name) {
2219     __ align(CodeEntryAlignment);
2220     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2221     address start = __ pc();
2222 
2223     BLOCK_COMMENT(&quot;Entry:&quot;);
2224 
2225     const Register to        = c_rarg0;  // source array address
2226     const Register value     = c_rarg1;  // value
2227     const Register count     = c_rarg2;  // elements count
2228 
2229     const Register bz_base = r10;        // base for block_zero routine
2230     const Register cnt_words = r11;      // temp register
2231 
2232     __ enter();
2233 
2234     Label L_fill_elements, L_exit1;
2235 
2236     int shift = -1;
2237     switch (t) {
2238       case T_BYTE:
2239         shift = 0;
2240         __ cmpw(count, 8 &gt;&gt; shift); // Short arrays (&lt; 8 bytes) fill by element
2241         __ bfi(value, value, 8, 8);   // 8 bit -&gt; 16 bit
2242         __ bfi(value, value, 16, 16); // 16 bit -&gt; 32 bit
2243         __ br(Assembler::LO, L_fill_elements);
2244         break;
2245       case T_SHORT:
2246         shift = 1;
2247         __ cmpw(count, 8 &gt;&gt; shift); // Short arrays (&lt; 8 bytes) fill by element
2248         __ bfi(value, value, 16, 16); // 16 bit -&gt; 32 bit
2249         __ br(Assembler::LO, L_fill_elements);
2250         break;
2251       case T_INT:
2252         shift = 2;
2253         __ cmpw(count, 8 &gt;&gt; shift); // Short arrays (&lt; 8 bytes) fill by element
2254         __ br(Assembler::LO, L_fill_elements);
2255         break;
2256       default: ShouldNotReachHere();
2257     }
2258 
2259     // Align source address at 8 bytes address boundary.
2260     Label L_skip_align1, L_skip_align2, L_skip_align4;
2261     if (!aligned) {
2262       switch (t) {
2263         case T_BYTE:
2264           // One byte misalignment happens only for byte arrays.
2265           __ tbz(to, 0, L_skip_align1);
2266           __ strb(value, Address(__ post(to, 1)));
2267           __ subw(count, count, 1);
2268           __ bind(L_skip_align1);
2269           // Fallthrough
2270         case T_SHORT:
2271           // Two bytes misalignment happens only for byte and short (char) arrays.
2272           __ tbz(to, 1, L_skip_align2);
2273           __ strh(value, Address(__ post(to, 2)));
2274           __ subw(count, count, 2 &gt;&gt; shift);
2275           __ bind(L_skip_align2);
2276           // Fallthrough
2277         case T_INT:
2278           // Align to 8 bytes, we know we are 4 byte aligned to start.
2279           __ tbz(to, 2, L_skip_align4);
2280           __ strw(value, Address(__ post(to, 4)));
2281           __ subw(count, count, 4 &gt;&gt; shift);
2282           __ bind(L_skip_align4);
2283           break;
2284         default: ShouldNotReachHere();
2285       }
2286     }
2287 
2288     //
2289     //  Fill large chunks
2290     //
2291     __ lsrw(cnt_words, count, 3 - shift); // number of words
2292     __ bfi(value, value, 32, 32);         // 32 bit -&gt; 64 bit
2293     __ subw(count, count, cnt_words, Assembler::LSL, 3 - shift);
2294     if (UseBlockZeroing) {
2295       Label non_block_zeroing, rest;
2296       // If the fill value is zero we can use the fast zero_words().
2297       __ cbnz(value, non_block_zeroing);
2298       __ mov(bz_base, to);
2299       __ add(to, to, cnt_words, Assembler::LSL, LogBytesPerWord);
2300       __ zero_words(bz_base, cnt_words);
2301       __ b(rest);
2302       __ bind(non_block_zeroing);
2303       __ fill_words(to, cnt_words, value);
2304       __ bind(rest);
2305     } else {
2306       __ fill_words(to, cnt_words, value);
2307     }
2308 
2309     // Remaining count is less than 8 bytes. Fill it by a single store.
2310     // Note that the total length is no less than 8 bytes.
2311     if (t == T_BYTE || t == T_SHORT) {
2312       Label L_exit1;
2313       __ cbzw(count, L_exit1);
2314       __ add(to, to, count, Assembler::LSL, shift); // points to the end
2315       __ str(value, Address(to, -8));    // overwrite some elements
2316       __ bind(L_exit1);
2317       __ leave();
2318       __ ret(lr);
2319     }
2320 
2321     // Handle copies less than 8 bytes.
2322     Label L_fill_2, L_fill_4, L_exit2;
2323     __ bind(L_fill_elements);
2324     switch (t) {
2325       case T_BYTE:
2326         __ tbz(count, 0, L_fill_2);
2327         __ strb(value, Address(__ post(to, 1)));
2328         __ bind(L_fill_2);
2329         __ tbz(count, 1, L_fill_4);
2330         __ strh(value, Address(__ post(to, 2)));
2331         __ bind(L_fill_4);
2332         __ tbz(count, 2, L_exit2);
2333         __ strw(value, Address(to));
2334         break;
2335       case T_SHORT:
2336         __ tbz(count, 0, L_fill_4);
2337         __ strh(value, Address(__ post(to, 2)));
2338         __ bind(L_fill_4);
2339         __ tbz(count, 1, L_exit2);
2340         __ strw(value, Address(to));
2341         break;
2342       case T_INT:
2343         __ cbzw(count, L_exit2);
2344         __ strw(value, Address(to));
2345         break;
2346       default: ShouldNotReachHere();
2347     }
2348     __ bind(L_exit2);
2349     __ leave();
2350     __ ret(lr);
2351     return start;
2352   }
2353 
2354   address generate_data_cache_writeback() {
2355     const Register line        = c_rarg0;  // address of line to write back
2356 
2357     __ align(CodeEntryAlignment);
2358 
2359     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback&quot;);
2360 
2361     address start = __ pc();
2362     __ enter();
2363     __ cache_wb(Address(line, 0));
2364     __ leave();
2365     __ ret(lr);
2366 
2367     return start;
2368   }
2369 
2370   address generate_data_cache_writeback_sync() {
2371     const Register is_pre     = c_rarg0;  // pre or post sync
2372 
2373     __ align(CodeEntryAlignment);
2374 
2375     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback_sync&quot;);
2376 
2377     // pre wbsync is a no-op
2378     // post wbsync translates to an sfence
2379 
2380     Label skip;
2381     address start = __ pc();
2382     __ enter();
2383     __ cbnz(is_pre, skip);
2384     __ cache_wbsync(false);
2385     __ bind(skip);
2386     __ leave();
2387     __ ret(lr);
2388 
2389     return start;
2390   }
2391 
2392   void generate_arraycopy_stubs() {
2393     address entry;
2394     address entry_jbyte_arraycopy;
2395     address entry_jshort_arraycopy;
2396     address entry_jint_arraycopy;
2397     address entry_oop_arraycopy;
2398     address entry_jlong_arraycopy;
2399     address entry_checkcast_arraycopy;
2400 
2401     generate_copy_longs(copy_f, r0, r1, rscratch2, copy_forwards);
2402     generate_copy_longs(copy_b, r0, r1, rscratch2, copy_backwards);
2403 
2404     StubRoutines::aarch64::_zero_blocks = generate_zero_blocks();
2405 
2406     //*** jbyte
2407     // Always need aligned and unaligned versions
2408     StubRoutines::_jbyte_disjoint_arraycopy         = generate_disjoint_byte_copy(false, &amp;entry,
2409                                                                                   &quot;jbyte_disjoint_arraycopy&quot;);
2410     StubRoutines::_jbyte_arraycopy                  = generate_conjoint_byte_copy(false, entry,
2411                                                                                   &amp;entry_jbyte_arraycopy,
2412                                                                                   &quot;jbyte_arraycopy&quot;);
2413     StubRoutines::_arrayof_jbyte_disjoint_arraycopy = generate_disjoint_byte_copy(true, &amp;entry,
2414                                                                                   &quot;arrayof_jbyte_disjoint_arraycopy&quot;);
2415     StubRoutines::_arrayof_jbyte_arraycopy          = generate_conjoint_byte_copy(true, entry, NULL,
2416                                                                                   &quot;arrayof_jbyte_arraycopy&quot;);
2417 
2418     //*** jshort
2419     // Always need aligned and unaligned versions
2420     StubRoutines::_jshort_disjoint_arraycopy         = generate_disjoint_short_copy(false, &amp;entry,
2421                                                                                     &quot;jshort_disjoint_arraycopy&quot;);
2422     StubRoutines::_jshort_arraycopy                  = generate_conjoint_short_copy(false, entry,
2423                                                                                     &amp;entry_jshort_arraycopy,
2424                                                                                     &quot;jshort_arraycopy&quot;);
2425     StubRoutines::_arrayof_jshort_disjoint_arraycopy = generate_disjoint_short_copy(true, &amp;entry,
2426                                                                                     &quot;arrayof_jshort_disjoint_arraycopy&quot;);
2427     StubRoutines::_arrayof_jshort_arraycopy          = generate_conjoint_short_copy(true, entry, NULL,
2428                                                                                     &quot;arrayof_jshort_arraycopy&quot;);
2429 
2430     //*** jint
2431     // Aligned versions
2432     StubRoutines::_arrayof_jint_disjoint_arraycopy = generate_disjoint_int_copy(true, &amp;entry,
2433                                                                                 &quot;arrayof_jint_disjoint_arraycopy&quot;);
2434     StubRoutines::_arrayof_jint_arraycopy          = generate_conjoint_int_copy(true, entry, &amp;entry_jint_arraycopy,
2435                                                                                 &quot;arrayof_jint_arraycopy&quot;);
2436     // In 64 bit we need both aligned and unaligned versions of jint arraycopy.
2437     // entry_jint_arraycopy always points to the unaligned version
2438     StubRoutines::_jint_disjoint_arraycopy         = generate_disjoint_int_copy(false, &amp;entry,
2439                                                                                 &quot;jint_disjoint_arraycopy&quot;);
2440     StubRoutines::_jint_arraycopy                  = generate_conjoint_int_copy(false, entry,
2441                                                                                 &amp;entry_jint_arraycopy,
2442                                                                                 &quot;jint_arraycopy&quot;);
2443 
2444     //*** jlong
2445     // It is always aligned
2446     StubRoutines::_arrayof_jlong_disjoint_arraycopy = generate_disjoint_long_copy(true, &amp;entry,
2447                                                                                   &quot;arrayof_jlong_disjoint_arraycopy&quot;);
2448     StubRoutines::_arrayof_jlong_arraycopy          = generate_conjoint_long_copy(true, entry, &amp;entry_jlong_arraycopy,
2449                                                                                   &quot;arrayof_jlong_arraycopy&quot;);
2450     StubRoutines::_jlong_disjoint_arraycopy         = StubRoutines::_arrayof_jlong_disjoint_arraycopy;
2451     StubRoutines::_jlong_arraycopy                  = StubRoutines::_arrayof_jlong_arraycopy;
2452 
2453     //*** oops
2454     {
2455       // With compressed oops we need unaligned versions; notice that
2456       // we overwrite entry_oop_arraycopy.
2457       bool aligned = !UseCompressedOops;
2458 
2459       StubRoutines::_arrayof_oop_disjoint_arraycopy
2460         = generate_disjoint_oop_copy(aligned, &amp;entry, &quot;arrayof_oop_disjoint_arraycopy&quot;,
2461                                      /*dest_uninitialized*/false);
2462       StubRoutines::_arrayof_oop_arraycopy
2463         = generate_conjoint_oop_copy(aligned, entry, &amp;entry_oop_arraycopy, &quot;arrayof_oop_arraycopy&quot;,
2464                                      /*dest_uninitialized*/false);
2465       // Aligned versions without pre-barriers
2466       StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit
2467         = generate_disjoint_oop_copy(aligned, &amp;entry, &quot;arrayof_oop_disjoint_arraycopy_uninit&quot;,
2468                                      /*dest_uninitialized*/true);
2469       StubRoutines::_arrayof_oop_arraycopy_uninit
2470         = generate_conjoint_oop_copy(aligned, entry, NULL, &quot;arrayof_oop_arraycopy_uninit&quot;,
2471                                      /*dest_uninitialized*/true);
2472     }
2473 
2474     StubRoutines::_oop_disjoint_arraycopy            = StubRoutines::_arrayof_oop_disjoint_arraycopy;
2475     StubRoutines::_oop_arraycopy                     = StubRoutines::_arrayof_oop_arraycopy;
2476     StubRoutines::_oop_disjoint_arraycopy_uninit     = StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit;
2477     StubRoutines::_oop_arraycopy_uninit              = StubRoutines::_arrayof_oop_arraycopy_uninit;
2478 
2479     StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(&quot;checkcast_arraycopy&quot;, &amp;entry_checkcast_arraycopy);
2480     StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(&quot;checkcast_arraycopy_uninit&quot;, NULL,
2481                                                                         /*dest_uninitialized*/true);
2482 
2483     StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(&quot;unsafe_arraycopy&quot;,
2484                                                               entry_jbyte_arraycopy,
2485                                                               entry_jshort_arraycopy,
2486                                                               entry_jint_arraycopy,
2487                                                               entry_jlong_arraycopy);
2488 
2489     StubRoutines::_generic_arraycopy   = generate_generic_copy(&quot;generic_arraycopy&quot;,
2490                                                                entry_jbyte_arraycopy,
2491                                                                entry_jshort_arraycopy,
2492                                                                entry_jint_arraycopy,
2493                                                                entry_oop_arraycopy,
2494                                                                entry_jlong_arraycopy,
2495                                                                entry_checkcast_arraycopy);
2496 
2497     StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, &quot;jbyte_fill&quot;);
2498     StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, &quot;jshort_fill&quot;);
2499     StubRoutines::_jint_fill = generate_fill(T_INT, false, &quot;jint_fill&quot;);
2500     StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, &quot;arrayof_jbyte_fill&quot;);
2501     StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, &quot;arrayof_jshort_fill&quot;);
2502     StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, &quot;arrayof_jint_fill&quot;);
2503   }
2504 
2505   void generate_math_stubs() { Unimplemented(); }
2506 
2507   // Arguments:
2508   //
2509   // Inputs:
2510   //   c_rarg0   - source byte array address
2511   //   c_rarg1   - destination byte array address
2512   //   c_rarg2   - K (key) in little endian int array
2513   //
2514   address generate_aescrypt_encryptBlock() {
2515     __ align(CodeEntryAlignment);
2516     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;aescrypt_encryptBlock&quot;);
2517 
2518     Label L_doLast;
2519 
2520     const Register from        = c_rarg0;  // source array address
2521     const Register to          = c_rarg1;  // destination array address
2522     const Register key         = c_rarg2;  // key array address
2523     const Register keylen      = rscratch1;
2524 
2525     address start = __ pc();
2526     __ enter();
2527 
2528     __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2529 
2530     __ ld1(v0, __ T16B, from); // get 16 bytes of input
2531 
2532     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2533     __ rev32(v1, __ T16B, v1);
2534     __ rev32(v2, __ T16B, v2);
2535     __ rev32(v3, __ T16B, v3);
2536     __ rev32(v4, __ T16B, v4);
2537     __ aese(v0, v1);
2538     __ aesmc(v0, v0);
2539     __ aese(v0, v2);
2540     __ aesmc(v0, v0);
2541     __ aese(v0, v3);
2542     __ aesmc(v0, v0);
2543     __ aese(v0, v4);
2544     __ aesmc(v0, v0);
2545 
2546     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2547     __ rev32(v1, __ T16B, v1);
2548     __ rev32(v2, __ T16B, v2);
2549     __ rev32(v3, __ T16B, v3);
2550     __ rev32(v4, __ T16B, v4);
2551     __ aese(v0, v1);
2552     __ aesmc(v0, v0);
2553     __ aese(v0, v2);
2554     __ aesmc(v0, v0);
2555     __ aese(v0, v3);
2556     __ aesmc(v0, v0);
2557     __ aese(v0, v4);
2558     __ aesmc(v0, v0);
2559 
2560     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2561     __ rev32(v1, __ T16B, v1);
2562     __ rev32(v2, __ T16B, v2);
2563 
2564     __ cmpw(keylen, 44);
2565     __ br(Assembler::EQ, L_doLast);
2566 
2567     __ aese(v0, v1);
2568     __ aesmc(v0, v0);
2569     __ aese(v0, v2);
2570     __ aesmc(v0, v0);
2571 
2572     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2573     __ rev32(v1, __ T16B, v1);
2574     __ rev32(v2, __ T16B, v2);
2575 
2576     __ cmpw(keylen, 52);
2577     __ br(Assembler::EQ, L_doLast);
2578 
2579     __ aese(v0, v1);
2580     __ aesmc(v0, v0);
2581     __ aese(v0, v2);
2582     __ aesmc(v0, v0);
2583 
2584     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2585     __ rev32(v1, __ T16B, v1);
2586     __ rev32(v2, __ T16B, v2);
2587 
2588     __ BIND(L_doLast);
2589 
2590     __ aese(v0, v1);
2591     __ aesmc(v0, v0);
2592     __ aese(v0, v2);
2593 
2594     __ ld1(v1, __ T16B, key);
2595     __ rev32(v1, __ T16B, v1);
2596     __ eor(v0, __ T16B, v0, v1);
2597 
2598     __ st1(v0, __ T16B, to);
2599 
2600     __ mov(r0, 0);
2601 
2602     __ leave();
2603     __ ret(lr);
2604 
2605     return start;
2606   }
2607 
2608   // Arguments:
2609   //
2610   // Inputs:
2611   //   c_rarg0   - source byte array address
2612   //   c_rarg1   - destination byte array address
2613   //   c_rarg2   - K (key) in little endian int array
2614   //
2615   address generate_aescrypt_decryptBlock() {
2616     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
2617     __ align(CodeEntryAlignment);
2618     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;aescrypt_decryptBlock&quot;);
2619     Label L_doLast;
2620 
2621     const Register from        = c_rarg0;  // source array address
2622     const Register to          = c_rarg1;  // destination array address
2623     const Register key         = c_rarg2;  // key array address
2624     const Register keylen      = rscratch1;
2625 
2626     address start = __ pc();
2627     __ enter(); // required for proper stackwalking of RuntimeStub frame
2628 
2629     __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2630 
2631     __ ld1(v0, __ T16B, from); // get 16 bytes of input
2632 
2633     __ ld1(v5, __ T16B, __ post(key, 16));
2634     __ rev32(v5, __ T16B, v5);
2635 
2636     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2637     __ rev32(v1, __ T16B, v1);
2638     __ rev32(v2, __ T16B, v2);
2639     __ rev32(v3, __ T16B, v3);
2640     __ rev32(v4, __ T16B, v4);
2641     __ aesd(v0, v1);
2642     __ aesimc(v0, v0);
2643     __ aesd(v0, v2);
2644     __ aesimc(v0, v0);
2645     __ aesd(v0, v3);
2646     __ aesimc(v0, v0);
2647     __ aesd(v0, v4);
2648     __ aesimc(v0, v0);
2649 
2650     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2651     __ rev32(v1, __ T16B, v1);
2652     __ rev32(v2, __ T16B, v2);
2653     __ rev32(v3, __ T16B, v3);
2654     __ rev32(v4, __ T16B, v4);
2655     __ aesd(v0, v1);
2656     __ aesimc(v0, v0);
2657     __ aesd(v0, v2);
2658     __ aesimc(v0, v0);
2659     __ aesd(v0, v3);
2660     __ aesimc(v0, v0);
2661     __ aesd(v0, v4);
2662     __ aesimc(v0, v0);
2663 
2664     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2665     __ rev32(v1, __ T16B, v1);
2666     __ rev32(v2, __ T16B, v2);
2667 
2668     __ cmpw(keylen, 44);
2669     __ br(Assembler::EQ, L_doLast);
2670 
2671     __ aesd(v0, v1);
2672     __ aesimc(v0, v0);
2673     __ aesd(v0, v2);
2674     __ aesimc(v0, v0);
2675 
2676     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2677     __ rev32(v1, __ T16B, v1);
2678     __ rev32(v2, __ T16B, v2);
2679 
2680     __ cmpw(keylen, 52);
2681     __ br(Assembler::EQ, L_doLast);
2682 
2683     __ aesd(v0, v1);
2684     __ aesimc(v0, v0);
2685     __ aesd(v0, v2);
2686     __ aesimc(v0, v0);
2687 
2688     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2689     __ rev32(v1, __ T16B, v1);
2690     __ rev32(v2, __ T16B, v2);
2691 
2692     __ BIND(L_doLast);
2693 
2694     __ aesd(v0, v1);
2695     __ aesimc(v0, v0);
2696     __ aesd(v0, v2);
2697 
2698     __ eor(v0, __ T16B, v0, v5);
2699 
2700     __ st1(v0, __ T16B, to);
2701 
2702     __ mov(r0, 0);
2703 
2704     __ leave();
2705     __ ret(lr);
2706 
2707     return start;
2708   }
2709 
2710   // Arguments:
2711   //
2712   // Inputs:
2713   //   c_rarg0   - source byte array address
2714   //   c_rarg1   - destination byte array address
2715   //   c_rarg2   - K (key) in little endian int array
2716   //   c_rarg3   - r vector byte array address
2717   //   c_rarg4   - input length
2718   //
2719   // Output:
2720   //   x0        - input length
2721   //
2722   address generate_cipherBlockChaining_encryptAESCrypt() {
2723     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
2724     __ align(CodeEntryAlignment);
2725     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_encryptAESCrypt&quot;);
2726 
2727     Label L_loadkeys_44, L_loadkeys_52, L_aes_loop, L_rounds_44, L_rounds_52;
2728 
2729     const Register from        = c_rarg0;  // source array address
2730     const Register to          = c_rarg1;  // destination array address
2731     const Register key         = c_rarg2;  // key array address
2732     const Register rvec        = c_rarg3;  // r byte array initialized from initvector array address
2733                                            // and left with the results of the last encryption block
2734     const Register len_reg     = c_rarg4;  // src len (must be multiple of blocksize 16)
2735     const Register keylen      = rscratch1;
2736 
2737     address start = __ pc();
2738 
2739       __ enter();
2740 
2741       __ movw(rscratch2, len_reg);
2742 
2743       __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2744 
2745       __ ld1(v0, __ T16B, rvec);
2746 
2747       __ cmpw(keylen, 52);
2748       __ br(Assembler::CC, L_loadkeys_44);
2749       __ br(Assembler::EQ, L_loadkeys_52);
2750 
2751       __ ld1(v17, v18, __ T16B, __ post(key, 32));
2752       __ rev32(v17, __ T16B, v17);
2753       __ rev32(v18, __ T16B, v18);
2754     __ BIND(L_loadkeys_52);
2755       __ ld1(v19, v20, __ T16B, __ post(key, 32));
2756       __ rev32(v19, __ T16B, v19);
2757       __ rev32(v20, __ T16B, v20);
2758     __ BIND(L_loadkeys_44);
2759       __ ld1(v21, v22, v23, v24, __ T16B, __ post(key, 64));
2760       __ rev32(v21, __ T16B, v21);
2761       __ rev32(v22, __ T16B, v22);
2762       __ rev32(v23, __ T16B, v23);
2763       __ rev32(v24, __ T16B, v24);
2764       __ ld1(v25, v26, v27, v28, __ T16B, __ post(key, 64));
2765       __ rev32(v25, __ T16B, v25);
2766       __ rev32(v26, __ T16B, v26);
2767       __ rev32(v27, __ T16B, v27);
2768       __ rev32(v28, __ T16B, v28);
2769       __ ld1(v29, v30, v31, __ T16B, key);
2770       __ rev32(v29, __ T16B, v29);
2771       __ rev32(v30, __ T16B, v30);
2772       __ rev32(v31, __ T16B, v31);
2773 
2774     __ BIND(L_aes_loop);
2775       __ ld1(v1, __ T16B, __ post(from, 16));
2776       __ eor(v0, __ T16B, v0, v1);
2777 
2778       __ br(Assembler::CC, L_rounds_44);
2779       __ br(Assembler::EQ, L_rounds_52);
2780 
2781       __ aese(v0, v17); __ aesmc(v0, v0);
2782       __ aese(v0, v18); __ aesmc(v0, v0);
2783     __ BIND(L_rounds_52);
2784       __ aese(v0, v19); __ aesmc(v0, v0);
2785       __ aese(v0, v20); __ aesmc(v0, v0);
2786     __ BIND(L_rounds_44);
2787       __ aese(v0, v21); __ aesmc(v0, v0);
2788       __ aese(v0, v22); __ aesmc(v0, v0);
2789       __ aese(v0, v23); __ aesmc(v0, v0);
2790       __ aese(v0, v24); __ aesmc(v0, v0);
2791       __ aese(v0, v25); __ aesmc(v0, v0);
2792       __ aese(v0, v26); __ aesmc(v0, v0);
2793       __ aese(v0, v27); __ aesmc(v0, v0);
2794       __ aese(v0, v28); __ aesmc(v0, v0);
2795       __ aese(v0, v29); __ aesmc(v0, v0);
2796       __ aese(v0, v30);
2797       __ eor(v0, __ T16B, v0, v31);
2798 
2799       __ st1(v0, __ T16B, __ post(to, 16));
2800 
2801       __ subw(len_reg, len_reg, 16);
2802       __ cbnzw(len_reg, L_aes_loop);
2803 
2804       __ st1(v0, __ T16B, rvec);
2805 
2806       __ mov(r0, rscratch2);
2807 
2808       __ leave();
2809       __ ret(lr);
2810 
2811       return start;
2812   }
2813 
2814   // Arguments:
2815   //
2816   // Inputs:
2817   //   c_rarg0   - source byte array address
2818   //   c_rarg1   - destination byte array address
2819   //   c_rarg2   - K (key) in little endian int array
2820   //   c_rarg3   - r vector byte array address
2821   //   c_rarg4   - input length
2822   //
2823   // Output:
2824   //   r0        - input length
2825   //
2826   address generate_cipherBlockChaining_decryptAESCrypt() {
2827     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
2828     __ align(CodeEntryAlignment);
2829     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_decryptAESCrypt&quot;);
2830 
2831     Label L_loadkeys_44, L_loadkeys_52, L_aes_loop, L_rounds_44, L_rounds_52;
2832 
2833     const Register from        = c_rarg0;  // source array address
2834     const Register to          = c_rarg1;  // destination array address
2835     const Register key         = c_rarg2;  // key array address
2836     const Register rvec        = c_rarg3;  // r byte array initialized from initvector array address
2837                                            // and left with the results of the last encryption block
2838     const Register len_reg     = c_rarg4;  // src len (must be multiple of blocksize 16)
2839     const Register keylen      = rscratch1;
2840 
2841     address start = __ pc();
2842 
2843       __ enter();
2844 
2845       __ movw(rscratch2, len_reg);
2846 
2847       __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2848 
2849       __ ld1(v2, __ T16B, rvec);
2850 
2851       __ ld1(v31, __ T16B, __ post(key, 16));
2852       __ rev32(v31, __ T16B, v31);
2853 
2854       __ cmpw(keylen, 52);
2855       __ br(Assembler::CC, L_loadkeys_44);
2856       __ br(Assembler::EQ, L_loadkeys_52);
2857 
2858       __ ld1(v17, v18, __ T16B, __ post(key, 32));
2859       __ rev32(v17, __ T16B, v17);
2860       __ rev32(v18, __ T16B, v18);
2861     __ BIND(L_loadkeys_52);
2862       __ ld1(v19, v20, __ T16B, __ post(key, 32));
2863       __ rev32(v19, __ T16B, v19);
2864       __ rev32(v20, __ T16B, v20);
2865     __ BIND(L_loadkeys_44);
2866       __ ld1(v21, v22, v23, v24, __ T16B, __ post(key, 64));
2867       __ rev32(v21, __ T16B, v21);
2868       __ rev32(v22, __ T16B, v22);
2869       __ rev32(v23, __ T16B, v23);
2870       __ rev32(v24, __ T16B, v24);
2871       __ ld1(v25, v26, v27, v28, __ T16B, __ post(key, 64));
2872       __ rev32(v25, __ T16B, v25);
2873       __ rev32(v26, __ T16B, v26);
2874       __ rev32(v27, __ T16B, v27);
2875       __ rev32(v28, __ T16B, v28);
2876       __ ld1(v29, v30, __ T16B, key);
2877       __ rev32(v29, __ T16B, v29);
2878       __ rev32(v30, __ T16B, v30);
2879 
2880     __ BIND(L_aes_loop);
2881       __ ld1(v0, __ T16B, __ post(from, 16));
2882       __ orr(v1, __ T16B, v0, v0);
2883 
2884       __ br(Assembler::CC, L_rounds_44);
2885       __ br(Assembler::EQ, L_rounds_52);
2886 
2887       __ aesd(v0, v17); __ aesimc(v0, v0);
2888       __ aesd(v0, v18); __ aesimc(v0, v0);
2889     __ BIND(L_rounds_52);
2890       __ aesd(v0, v19); __ aesimc(v0, v0);
2891       __ aesd(v0, v20); __ aesimc(v0, v0);
2892     __ BIND(L_rounds_44);
2893       __ aesd(v0, v21); __ aesimc(v0, v0);
2894       __ aesd(v0, v22); __ aesimc(v0, v0);
2895       __ aesd(v0, v23); __ aesimc(v0, v0);
2896       __ aesd(v0, v24); __ aesimc(v0, v0);
2897       __ aesd(v0, v25); __ aesimc(v0, v0);
2898       __ aesd(v0, v26); __ aesimc(v0, v0);
2899       __ aesd(v0, v27); __ aesimc(v0, v0);
2900       __ aesd(v0, v28); __ aesimc(v0, v0);
2901       __ aesd(v0, v29); __ aesimc(v0, v0);
2902       __ aesd(v0, v30);
2903       __ eor(v0, __ T16B, v0, v31);
2904       __ eor(v0, __ T16B, v0, v2);
2905 
2906       __ st1(v0, __ T16B, __ post(to, 16));
2907       __ orr(v2, __ T16B, v1, v1);
2908 
2909       __ subw(len_reg, len_reg, 16);
2910       __ cbnzw(len_reg, L_aes_loop);
2911 
2912       __ st1(v2, __ T16B, rvec);
2913 
2914       __ mov(r0, rscratch2);
2915 
2916       __ leave();
2917       __ ret(lr);
2918 
2919     return start;
2920   }
2921 
2922   // Arguments:
2923   //
2924   // Inputs:
2925   //   c_rarg0   - byte[]  source+offset
2926   //   c_rarg1   - int[]   SHA.state
2927   //   c_rarg2   - int     offset
2928   //   c_rarg3   - int     limit
2929   //
2930   address generate_sha1_implCompress(bool multi_block, const char *name) {
2931     __ align(CodeEntryAlignment);
2932     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2933     address start = __ pc();
2934 
2935     Register buf   = c_rarg0;
2936     Register state = c_rarg1;
2937     Register ofs   = c_rarg2;
2938     Register limit = c_rarg3;
2939 
2940     Label keys;
2941     Label sha1_loop;
2942 
2943     // load the keys into v0..v3
2944     __ adr(rscratch1, keys);
2945     __ ld4r(v0, v1, v2, v3, __ T4S, Address(rscratch1));
2946     // load 5 words state into v6, v7
2947     __ ldrq(v6, Address(state, 0));
2948     __ ldrs(v7, Address(state, 16));
2949 
2950 
2951     __ BIND(sha1_loop);
2952     // load 64 bytes of data into v16..v19
2953     __ ld1(v16, v17, v18, v19, __ T4S, multi_block ? __ post(buf, 64) : buf);
2954     __ rev32(v16, __ T16B, v16);
2955     __ rev32(v17, __ T16B, v17);
2956     __ rev32(v18, __ T16B, v18);
2957     __ rev32(v19, __ T16B, v19);
2958 
2959     // do the sha1
2960     __ addv(v4, __ T4S, v16, v0);
2961     __ orr(v20, __ T16B, v6, v6);
2962 
2963     FloatRegister d0 = v16;
2964     FloatRegister d1 = v17;
2965     FloatRegister d2 = v18;
2966     FloatRegister d3 = v19;
2967 
2968     for (int round = 0; round &lt; 20; round++) {
2969       FloatRegister tmp1 = (round &amp; 1) ? v4 : v5;
2970       FloatRegister tmp2 = (round &amp; 1) ? v21 : v22;
2971       FloatRegister tmp3 = round ? ((round &amp; 1) ? v22 : v21) : v7;
2972       FloatRegister tmp4 = (round &amp; 1) ? v5 : v4;
2973       FloatRegister key = (round &lt; 4) ? v0 : ((round &lt; 9) ? v1 : ((round &lt; 14) ? v2 : v3));
2974 
2975       if (round &lt; 16) __ sha1su0(d0, __ T4S, d1, d2);
2976       if (round &lt; 19) __ addv(tmp1, __ T4S, d1, key);
2977       __ sha1h(tmp2, __ T4S, v20);
2978       if (round &lt; 5)
2979         __ sha1c(v20, __ T4S, tmp3, tmp4);
2980       else if (round &lt; 10 || round &gt;= 15)
2981         __ sha1p(v20, __ T4S, tmp3, tmp4);
2982       else
2983         __ sha1m(v20, __ T4S, tmp3, tmp4);
2984       if (round &lt; 16) __ sha1su1(d0, __ T4S, d3);
2985 
2986       tmp1 = d0; d0 = d1; d1 = d2; d2 = d3; d3 = tmp1;
2987     }
2988 
2989     __ addv(v7, __ T2S, v7, v21);
2990     __ addv(v6, __ T4S, v6, v20);
2991 
2992     if (multi_block) {
2993       __ add(ofs, ofs, 64);
2994       __ cmp(ofs, limit);
2995       __ br(Assembler::LE, sha1_loop);
2996       __ mov(c_rarg0, ofs); // return ofs
2997     }
2998 
2999     __ strq(v6, Address(state, 0));
3000     __ strs(v7, Address(state, 16));
3001 
3002     __ ret(lr);
3003 
3004     __ bind(keys);
3005     __ emit_int32(0x5a827999);
3006     __ emit_int32(0x6ed9eba1);
3007     __ emit_int32(0x8f1bbcdc);
3008     __ emit_int32(0xca62c1d6);
3009 
3010     return start;
3011   }
3012 
3013 
3014   // Arguments:
3015   //
3016   // Inputs:
3017   //   c_rarg0   - byte[]  source+offset
3018   //   c_rarg1   - int[]   SHA.state
3019   //   c_rarg2   - int     offset
3020   //   c_rarg3   - int     limit
3021   //
3022   address generate_sha256_implCompress(bool multi_block, const char *name) {
3023     static const uint32_t round_consts[64] = {
3024       0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5,
3025       0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,
3026       0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,
3027       0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,
3028       0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc,
3029       0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,
3030       0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7,
3031       0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,
3032       0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,
3033       0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,
3034       0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3,
3035       0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,
3036       0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5,
3037       0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,
3038       0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,
3039       0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2,
3040     };
3041     __ align(CodeEntryAlignment);
3042     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3043     address start = __ pc();
3044 
3045     Register buf   = c_rarg0;
3046     Register state = c_rarg1;
3047     Register ofs   = c_rarg2;
3048     Register limit = c_rarg3;
3049 
3050     Label sha1_loop;
3051 
3052     __ stpd(v8, v9, __ pre(sp, -32));
3053     __ stpd(v10, v11, Address(sp, 16));
3054 
3055 // dga == v0
3056 // dgb == v1
3057 // dg0 == v2
3058 // dg1 == v3
3059 // dg2 == v4
3060 // t0 == v6
3061 // t1 == v7
3062 
3063     // load 16 keys to v16..v31
3064     __ lea(rscratch1, ExternalAddress((address)round_consts));
3065     __ ld1(v16, v17, v18, v19, __ T4S, __ post(rscratch1, 64));
3066     __ ld1(v20, v21, v22, v23, __ T4S, __ post(rscratch1, 64));
3067     __ ld1(v24, v25, v26, v27, __ T4S, __ post(rscratch1, 64));
3068     __ ld1(v28, v29, v30, v31, __ T4S, rscratch1);
3069 
3070     // load 8 words (256 bits) state
3071     __ ldpq(v0, v1, state);
3072 
3073     __ BIND(sha1_loop);
3074     // load 64 bytes of data into v8..v11
3075     __ ld1(v8, v9, v10, v11, __ T4S, multi_block ? __ post(buf, 64) : buf);
3076     __ rev32(v8, __ T16B, v8);
3077     __ rev32(v9, __ T16B, v9);
3078     __ rev32(v10, __ T16B, v10);
3079     __ rev32(v11, __ T16B, v11);
3080 
3081     __ addv(v6, __ T4S, v8, v16);
3082     __ orr(v2, __ T16B, v0, v0);
3083     __ orr(v3, __ T16B, v1, v1);
3084 
3085     FloatRegister d0 = v8;
3086     FloatRegister d1 = v9;
3087     FloatRegister d2 = v10;
3088     FloatRegister d3 = v11;
3089 
3090 
3091     for (int round = 0; round &lt; 16; round++) {
3092       FloatRegister tmp1 = (round &amp; 1) ? v6 : v7;
3093       FloatRegister tmp2 = (round &amp; 1) ? v7 : v6;
3094       FloatRegister tmp3 = (round &amp; 1) ? v2 : v4;
3095       FloatRegister tmp4 = (round &amp; 1) ? v4 : v2;
3096 
3097       if (round &lt; 12) __ sha256su0(d0, __ T4S, d1);
3098        __ orr(v4, __ T16B, v2, v2);
3099       if (round &lt; 15)
3100         __ addv(tmp1, __ T4S, d1, as_FloatRegister(round + 17));
3101       __ sha256h(v2, __ T4S, v3, tmp2);
3102       __ sha256h2(v3, __ T4S, v4, tmp2);
3103       if (round &lt; 12) __ sha256su1(d0, __ T4S, d2, d3);
3104 
3105       tmp1 = d0; d0 = d1; d1 = d2; d2 = d3; d3 = tmp1;
3106     }
3107 
3108     __ addv(v0, __ T4S, v0, v2);
3109     __ addv(v1, __ T4S, v1, v3);
3110 
3111     if (multi_block) {
3112       __ add(ofs, ofs, 64);
3113       __ cmp(ofs, limit);
3114       __ br(Assembler::LE, sha1_loop);
3115       __ mov(c_rarg0, ofs); // return ofs
3116     }
3117 
3118     __ ldpd(v10, v11, Address(sp, 16));
3119     __ ldpd(v8, v9, __ post(sp, 32));
3120 
3121     __ stpq(v0, v1, state);
3122 
3123     __ ret(lr);
3124 
3125     return start;
3126   }
3127 
3128   // Safefetch stubs.
3129   void generate_safefetch(const char* name, int size, address* entry,
3130                           address* fault_pc, address* continuation_pc) {
3131     // safefetch signatures:
3132     //   int      SafeFetch32(int*      adr, int      errValue);
3133     //   intptr_t SafeFetchN (intptr_t* adr, intptr_t errValue);
3134     //
3135     // arguments:
3136     //   c_rarg0 = adr
3137     //   c_rarg1 = errValue
3138     //
3139     // result:
3140     //   PPC_RET  = *adr or errValue
3141 
3142     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3143 
3144     // Entry point, pc or function descriptor.
3145     *entry = __ pc();
3146 
3147     // Load *adr into c_rarg1, may fault.
3148     *fault_pc = __ pc();
3149     switch (size) {
3150       case 4:
3151         // int32_t
3152         __ ldrw(c_rarg1, Address(c_rarg0, 0));
3153         break;
3154       case 8:
3155         // int64_t
3156         __ ldr(c_rarg1, Address(c_rarg0, 0));
3157         break;
3158       default:
3159         ShouldNotReachHere();
3160     }
3161 
3162     // return errValue or *adr
3163     *continuation_pc = __ pc();
3164     __ mov(r0, c_rarg1);
3165     __ ret(lr);
3166   }
3167 
3168   /**
3169    *  Arguments:
3170    *
3171    * Inputs:
3172    *   c_rarg0   - int crc
3173    *   c_rarg1   - byte* buf
3174    *   c_rarg2   - int length
3175    *
3176    * Ouput:
3177    *       rax   - int crc result
3178    */
3179   address generate_updateBytesCRC32() {
3180     assert(UseCRC32Intrinsics, &quot;what are we doing here?&quot;);
3181 
3182     __ align(CodeEntryAlignment);
3183     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesCRC32&quot;);
3184 
3185     address start = __ pc();
3186 
3187     const Register crc   = c_rarg0;  // crc
3188     const Register buf   = c_rarg1;  // source java byte array address
3189     const Register len   = c_rarg2;  // length
3190     const Register table0 = c_rarg3; // crc_table address
3191     const Register table1 = c_rarg4;
3192     const Register table2 = c_rarg5;
3193     const Register table3 = c_rarg6;
3194     const Register tmp3 = c_rarg7;
3195 
3196     BLOCK_COMMENT(&quot;Entry:&quot;);
3197     __ enter(); // required for proper stackwalking of RuntimeStub frame
3198 
3199     __ kernel_crc32(crc, buf, len,
3200               table0, table1, table2, table3, rscratch1, rscratch2, tmp3);
3201 
3202     __ leave(); // required for proper stackwalking of RuntimeStub frame
3203     __ ret(lr);
3204 
3205     return start;
3206   }
3207 
3208   /**
3209    *  Arguments:
3210    *
3211    * Inputs:
3212    *   c_rarg0   - int crc
3213    *   c_rarg1   - byte* buf
3214    *   c_rarg2   - int length
3215    *   c_rarg3   - int* table
3216    *
3217    * Ouput:
3218    *       r0   - int crc result
3219    */
3220   address generate_updateBytesCRC32C() {
3221     assert(UseCRC32CIntrinsics, &quot;what are we doing here?&quot;);
3222 
3223     __ align(CodeEntryAlignment);
3224     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesCRC32C&quot;);
3225 
3226     address start = __ pc();
3227 
3228     const Register crc   = c_rarg0;  // crc
3229     const Register buf   = c_rarg1;  // source java byte array address
3230     const Register len   = c_rarg2;  // length
3231     const Register table0 = c_rarg3; // crc_table address
3232     const Register table1 = c_rarg4;
3233     const Register table2 = c_rarg5;
3234     const Register table3 = c_rarg6;
3235     const Register tmp3 = c_rarg7;
3236 
3237     BLOCK_COMMENT(&quot;Entry:&quot;);
3238     __ enter(); // required for proper stackwalking of RuntimeStub frame
3239 
3240     __ kernel_crc32c(crc, buf, len,
3241               table0, table1, table2, table3, rscratch1, rscratch2, tmp3);
3242 
3243     __ leave(); // required for proper stackwalking of RuntimeStub frame
3244     __ ret(lr);
3245 
3246     return start;
3247   }
3248 
3249   /***
3250    *  Arguments:
3251    *
3252    *  Inputs:
3253    *   c_rarg0   - int   adler
3254    *   c_rarg1   - byte* buff
3255    *   c_rarg2   - int   len
3256    *
3257    * Output:
3258    *   c_rarg0   - int adler result
3259    */
3260   address generate_updateBytesAdler32() {
3261     __ align(CodeEntryAlignment);
3262     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesAdler32&quot;);
3263     address start = __ pc();
3264 
3265     Label L_simple_by1_loop, L_nmax, L_nmax_loop, L_by16, L_by16_loop, L_by1_loop, L_do_mod, L_combine, L_by1;
3266 
3267     // Aliases
3268     Register adler  = c_rarg0;
3269     Register s1     = c_rarg0;
3270     Register s2     = c_rarg3;
3271     Register buff   = c_rarg1;
3272     Register len    = c_rarg2;
3273     Register nmax  = r4;
3274     Register base  = r5;
3275     Register count = r6;
3276     Register temp0 = rscratch1;
3277     Register temp1 = rscratch2;
3278     FloatRegister vbytes = v0;
3279     FloatRegister vs1acc = v1;
3280     FloatRegister vs2acc = v2;
3281     FloatRegister vtable = v3;
3282 
3283     // Max number of bytes we can process before having to take the mod
3284     // 0x15B0 is 5552 in decimal, the largest n such that 255n(n+1)/2 + (n+1)(BASE-1) &lt;= 2^32-1
3285     unsigned long BASE = 0xfff1;
3286     unsigned long NMAX = 0x15B0;
3287 
3288     __ mov(base, BASE);
3289     __ mov(nmax, NMAX);
3290 
3291     // Load accumulation coefficients for the upper 16 bits
3292     __ lea(temp0, ExternalAddress((address) StubRoutines::aarch64::_adler_table));
3293     __ ld1(vtable, __ T16B, Address(temp0));
3294 
3295     // s1 is initialized to the lower 16 bits of adler
3296     // s2 is initialized to the upper 16 bits of adler
3297     __ ubfx(s2, adler, 16, 16);  // s2 = ((adler &gt;&gt; 16) &amp; 0xffff)
3298     __ uxth(s1, adler);          // s1 = (adler &amp; 0xffff)
3299 
3300     // The pipelined loop needs at least 16 elements for 1 iteration
3301     // It does check this, but it is more effective to skip to the cleanup loop
3302     __ cmp(len, (u1)16);
3303     __ br(Assembler::HS, L_nmax);
3304     __ cbz(len, L_combine);
3305 
3306     __ bind(L_simple_by1_loop);
3307     __ ldrb(temp0, Address(__ post(buff, 1)));
3308     __ add(s1, s1, temp0);
3309     __ add(s2, s2, s1);
3310     __ subs(len, len, 1);
3311     __ br(Assembler::HI, L_simple_by1_loop);
3312 
3313     // s1 = s1 % BASE
3314     __ subs(temp0, s1, base);
3315     __ csel(s1, temp0, s1, Assembler::HS);
3316 
3317     // s2 = s2 % BASE
3318     __ lsr(temp0, s2, 16);
3319     __ lsl(temp1, temp0, 4);
3320     __ sub(temp1, temp1, temp0);
3321     __ add(s2, temp1, s2, ext::uxth);
3322 
3323     __ subs(temp0, s2, base);
3324     __ csel(s2, temp0, s2, Assembler::HS);
3325 
3326     __ b(L_combine);
3327 
3328     __ bind(L_nmax);
3329     __ subs(len, len, nmax);
3330     __ sub(count, nmax, 16);
3331     __ br(Assembler::LO, L_by16);
3332 
3333     __ bind(L_nmax_loop);
3334 
3335     generate_updateBytesAdler32_accum(s1, s2, buff, temp0, temp1,
3336                                       vbytes, vs1acc, vs2acc, vtable);
3337 
3338     __ subs(count, count, 16);
3339     __ br(Assembler::HS, L_nmax_loop);
3340 
3341     // s1 = s1 % BASE
3342     __ lsr(temp0, s1, 16);
3343     __ lsl(temp1, temp0, 4);
3344     __ sub(temp1, temp1, temp0);
3345     __ add(temp1, temp1, s1, ext::uxth);
3346 
3347     __ lsr(temp0, temp1, 16);
3348     __ lsl(s1, temp0, 4);
3349     __ sub(s1, s1, temp0);
3350     __ add(s1, s1, temp1, ext:: uxth);
3351 
3352     __ subs(temp0, s1, base);
3353     __ csel(s1, temp0, s1, Assembler::HS);
3354 
3355     // s2 = s2 % BASE
3356     __ lsr(temp0, s2, 16);
3357     __ lsl(temp1, temp0, 4);
3358     __ sub(temp1, temp1, temp0);
3359     __ add(temp1, temp1, s2, ext::uxth);
3360 
3361     __ lsr(temp0, temp1, 16);
3362     __ lsl(s2, temp0, 4);
3363     __ sub(s2, s2, temp0);
3364     __ add(s2, s2, temp1, ext:: uxth);
3365 
3366     __ subs(temp0, s2, base);
3367     __ csel(s2, temp0, s2, Assembler::HS);
3368 
3369     __ subs(len, len, nmax);
3370     __ sub(count, nmax, 16);
3371     __ br(Assembler::HS, L_nmax_loop);
3372 
3373     __ bind(L_by16);
3374     __ adds(len, len, count);
3375     __ br(Assembler::LO, L_by1);
3376 
3377     __ bind(L_by16_loop);
3378 
3379     generate_updateBytesAdler32_accum(s1, s2, buff, temp0, temp1,
3380                                       vbytes, vs1acc, vs2acc, vtable);
3381 
3382     __ subs(len, len, 16);
3383     __ br(Assembler::HS, L_by16_loop);
3384 
3385     __ bind(L_by1);
3386     __ adds(len, len, 15);
3387     __ br(Assembler::LO, L_do_mod);
3388 
3389     __ bind(L_by1_loop);
3390     __ ldrb(temp0, Address(__ post(buff, 1)));
3391     __ add(s1, temp0, s1);
3392     __ add(s2, s2, s1);
3393     __ subs(len, len, 1);
3394     __ br(Assembler::HS, L_by1_loop);
3395 
3396     __ bind(L_do_mod);
3397     // s1 = s1 % BASE
3398     __ lsr(temp0, s1, 16);
3399     __ lsl(temp1, temp0, 4);
3400     __ sub(temp1, temp1, temp0);
3401     __ add(temp1, temp1, s1, ext::uxth);
3402 
3403     __ lsr(temp0, temp1, 16);
3404     __ lsl(s1, temp0, 4);
3405     __ sub(s1, s1, temp0);
3406     __ add(s1, s1, temp1, ext:: uxth);
3407 
3408     __ subs(temp0, s1, base);
3409     __ csel(s1, temp0, s1, Assembler::HS);
3410 
3411     // s2 = s2 % BASE
3412     __ lsr(temp0, s2, 16);
3413     __ lsl(temp1, temp0, 4);
3414     __ sub(temp1, temp1, temp0);
3415     __ add(temp1, temp1, s2, ext::uxth);
3416 
3417     __ lsr(temp0, temp1, 16);
3418     __ lsl(s2, temp0, 4);
3419     __ sub(s2, s2, temp0);
3420     __ add(s2, s2, temp1, ext:: uxth);
3421 
3422     __ subs(temp0, s2, base);
3423     __ csel(s2, temp0, s2, Assembler::HS);
3424 
3425     // Combine lower bits and higher bits
3426     __ bind(L_combine);
3427     __ orr(s1, s1, s2, Assembler::LSL, 16); // adler = s1 | (s2 &lt;&lt; 16)
3428 
3429     __ ret(lr);
3430 
3431     return start;
3432   }
3433 
3434   void generate_updateBytesAdler32_accum(Register s1, Register s2, Register buff,
3435           Register temp0, Register temp1, FloatRegister vbytes,
3436           FloatRegister vs1acc, FloatRegister vs2acc, FloatRegister vtable) {
3437     // Below is a vectorized implementation of updating s1 and s2 for 16 bytes.
3438     // We use b1, b2, ..., b16 to denote the 16 bytes loaded in each iteration.
3439     // In non-vectorized code, we update s1 and s2 as:
3440     //   s1 &lt;- s1 + b1
3441     //   s2 &lt;- s2 + s1
3442     //   s1 &lt;- s1 + b2
3443     //   s2 &lt;- s2 + b1
3444     //   ...
3445     //   s1 &lt;- s1 + b16
3446     //   s2 &lt;- s2 + s1
3447     // Putting above assignments together, we have:
3448     //   s1_new = s1 + b1 + b2 + ... + b16
3449     //   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b16)
3450     //          = s2 + s1 * 16 + (b1 * 16 + b2 * 15 + ... + b16 * 1)
3451     //          = s2 + s1 * 16 + (b1, b2, ... b16) dot (16, 15, ... 1)
3452     __ ld1(vbytes, __ T16B, Address(__ post(buff, 16)));
3453 
3454     // s2 = s2 + s1 * 16
3455     __ add(s2, s2, s1, Assembler::LSL, 4);
3456 
3457     // vs1acc = b1 + b2 + b3 + ... + b16
3458     // vs2acc = (b1 * 16) + (b2 * 15) + (b3 * 14) + ... + (b16 * 1)
3459     __ umullv(vs2acc, __ T8B, vtable, vbytes);
3460     __ umlalv(vs2acc, __ T16B, vtable, vbytes);
3461     __ uaddlv(vs1acc, __ T16B, vbytes);
3462     __ uaddlv(vs2acc, __ T8H, vs2acc);
3463 
3464     // s1 = s1 + vs1acc, s2 = s2 + vs2acc
3465     __ fmovd(temp0, vs1acc);
3466     __ fmovd(temp1, vs2acc);
3467     __ add(s1, s1, temp0);
3468     __ add(s2, s2, temp1);
3469   }
3470 
3471   /**
3472    *  Arguments:
3473    *
3474    *  Input:
3475    *    c_rarg0   - x address
3476    *    c_rarg1   - x length
3477    *    c_rarg2   - y address
3478    *    c_rarg3   - y lenth
3479    *    c_rarg4   - z address
3480    *    c_rarg5   - z length
3481    */
3482   address generate_multiplyToLen() {
3483     __ align(CodeEntryAlignment);
3484     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;multiplyToLen&quot;);
3485 
3486     address start = __ pc();
3487     const Register x     = r0;
3488     const Register xlen  = r1;
3489     const Register y     = r2;
3490     const Register ylen  = r3;
3491     const Register z     = r4;
3492     const Register zlen  = r5;
3493 
3494     const Register tmp1  = r10;
3495     const Register tmp2  = r11;
3496     const Register tmp3  = r12;
3497     const Register tmp4  = r13;
3498     const Register tmp5  = r14;
3499     const Register tmp6  = r15;
3500     const Register tmp7  = r16;
3501 
3502     BLOCK_COMMENT(&quot;Entry:&quot;);
3503     __ enter(); // required for proper stackwalking of RuntimeStub frame
3504     __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
3505     __ leave(); // required for proper stackwalking of RuntimeStub frame
3506     __ ret(lr);
3507 
3508     return start;
3509   }
3510 
3511   address generate_squareToLen() {
3512     // squareToLen algorithm for sizes 1..127 described in java code works
3513     // faster than multiply_to_len on some CPUs and slower on others, but
3514     // multiply_to_len shows a bit better overall results
3515     __ align(CodeEntryAlignment);
3516     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;squareToLen&quot;);
3517     address start = __ pc();
3518 
3519     const Register x     = r0;
3520     const Register xlen  = r1;
3521     const Register z     = r2;
3522     const Register zlen  = r3;
3523     const Register y     = r4; // == x
3524     const Register ylen  = r5; // == xlen
3525 
3526     const Register tmp1  = r10;
3527     const Register tmp2  = r11;
3528     const Register tmp3  = r12;
3529     const Register tmp4  = r13;
3530     const Register tmp5  = r14;
3531     const Register tmp6  = r15;
3532     const Register tmp7  = r16;
3533 
3534     RegSet spilled_regs = RegSet::of(y, ylen);
3535     BLOCK_COMMENT(&quot;Entry:&quot;);
3536     __ enter();
3537     __ push(spilled_regs, sp);
3538     __ mov(y, x);
3539     __ mov(ylen, xlen);
3540     __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
3541     __ pop(spilled_regs, sp);
3542     __ leave();
3543     __ ret(lr);
3544     return start;
3545   }
3546 
3547   address generate_mulAdd() {
3548     __ align(CodeEntryAlignment);
3549     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;mulAdd&quot;);
3550 
3551     address start = __ pc();
3552 
3553     const Register out     = r0;
3554     const Register in      = r1;
3555     const Register offset  = r2;
3556     const Register len     = r3;
3557     const Register k       = r4;
3558 
3559     BLOCK_COMMENT(&quot;Entry:&quot;);
3560     __ enter();
3561     __ mul_add(out, in, offset, len, k);
3562     __ leave();
3563     __ ret(lr);
3564 
3565     return start;
3566   }
3567 
3568   void ghash_multiply(FloatRegister result_lo, FloatRegister result_hi,
3569                       FloatRegister a, FloatRegister b, FloatRegister a1_xor_a0,
3570                       FloatRegister tmp1, FloatRegister tmp2, FloatRegister tmp3, FloatRegister tmp4) {
3571     // Karatsuba multiplication performs a 128*128 -&gt; 256-bit
3572     // multiplication in three 128-bit multiplications and a few
3573     // additions.
3574     //
3575     // (C1:C0) = A1*B1, (D1:D0) = A0*B0, (E1:E0) = (A0+A1)(B0+B1)
3576     // (A1:A0)(B1:B0) = C1:(C0+C1+D1+E1):(D1+C0+D0+E0):D0
3577     //
3578     // Inputs:
3579     //
3580     // A0 in a.d[0]     (subkey)
3581     // A1 in a.d[1]
3582     // (A1+A0) in a1_xor_a0.d[0]
3583     //
3584     // B0 in b.d[0]     (state)
3585     // B1 in b.d[1]
3586 
3587     __ ext(tmp1, __ T16B, b, b, 0x08);
3588     __ pmull2(result_hi, __ T1Q, b, a, __ T2D);  // A1*B1
3589     __ eor(tmp1, __ T16B, tmp1, b);            // (B1+B0)
3590     __ pmull(result_lo,  __ T1Q, b, a, __ T1D);  // A0*B0
3591     __ pmull(tmp2, __ T1Q, tmp1, a1_xor_a0, __ T1D); // (A1+A0)(B1+B0)
3592 
3593     __ ext(tmp4, __ T16B, result_lo, result_hi, 0x08);
3594     __ eor(tmp3, __ T16B, result_hi, result_lo); // A1*B1+A0*B0
3595     __ eor(tmp2, __ T16B, tmp2, tmp4);
3596     __ eor(tmp2, __ T16B, tmp2, tmp3);
3597 
3598     // Register pair &lt;result_hi:result_lo&gt; holds the result of carry-less multiplication
3599     __ ins(result_hi, __ D, tmp2, 0, 1);
3600     __ ins(result_lo, __ D, tmp2, 1, 0);
3601   }
3602 
3603   void ghash_reduce(FloatRegister result, FloatRegister lo, FloatRegister hi,
3604                     FloatRegister p, FloatRegister z, FloatRegister t1) {
3605     const FloatRegister t0 = result;
3606 
3607     // The GCM field polynomial f is z^128 + p(z), where p =
3608     // z^7+z^2+z+1.
3609     //
3610     //    z^128 === -p(z)  (mod (z^128 + p(z)))
3611     //
3612     // so, given that the product we&#39;re reducing is
3613     //    a == lo + hi * z^128
3614     // substituting,
3615     //      === lo - hi * p(z)  (mod (z^128 + p(z)))
3616     //
3617     // we reduce by multiplying hi by p(z) and subtracting the result
3618     // from (i.e. XORing it with) lo.  Because p has no nonzero high
3619     // bits we can do this with two 64-bit multiplications, lo*p and
3620     // hi*p.
3621 
3622     __ pmull2(t0, __ T1Q, hi, p, __ T2D);
3623     __ ext(t1, __ T16B, t0, z, 8);
3624     __ eor(hi, __ T16B, hi, t1);
3625     __ ext(t1, __ T16B, z, t0, 8);
3626     __ eor(lo, __ T16B, lo, t1);
3627     __ pmull(t0, __ T1Q, hi, p, __ T1D);
3628     __ eor(result, __ T16B, lo, t0);
3629   }
3630 
3631   address generate_has_negatives(address &amp;has_negatives_long) {
3632     const u1 large_loop_size = 64;
3633     const uint64_t UPPER_BIT_MASK=0x8080808080808080;
3634     int dcache_line = VM_Version::dcache_line_size();
3635 
3636     Register ary1 = r1, len = r2, result = r0;
3637 
3638     __ align(CodeEntryAlignment);
3639 
3640     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;has_negatives&quot;);
3641 
3642     address entry = __ pc();
3643 
3644     __ enter();
3645 
3646   Label RET_TRUE, RET_TRUE_NO_POP, RET_FALSE, ALIGNED, LOOP16, CHECK_16, DONE,
3647         LARGE_LOOP, POST_LOOP16, LEN_OVER_15, LEN_OVER_8, POST_LOOP16_LOAD_TAIL;
3648 
3649   __ cmp(len, (u1)15);
3650   __ br(Assembler::GT, LEN_OVER_15);
3651   // The only case when execution falls into this code is when pointer is near
3652   // the end of memory page and we have to avoid reading next page
3653   __ add(ary1, ary1, len);
3654   __ subs(len, len, 8);
3655   __ br(Assembler::GT, LEN_OVER_8);
3656   __ ldr(rscratch2, Address(ary1, -8));
3657   __ sub(rscratch1, zr, len, __ LSL, 3);  // LSL 3 is to get bits from bytes.
3658   __ lsrv(rscratch2, rscratch2, rscratch1);
3659   __ tst(rscratch2, UPPER_BIT_MASK);
3660   __ cset(result, Assembler::NE);
3661   __ leave();
3662   __ ret(lr);
3663   __ bind(LEN_OVER_8);
3664   __ ldp(rscratch1, rscratch2, Address(ary1, -16));
3665   __ sub(len, len, 8); // no data dep., then sub can be executed while loading
3666   __ tst(rscratch2, UPPER_BIT_MASK);
3667   __ br(Assembler::NE, RET_TRUE_NO_POP);
3668   __ sub(rscratch2, zr, len, __ LSL, 3); // LSL 3 is to get bits from bytes
3669   __ lsrv(rscratch1, rscratch1, rscratch2);
3670   __ tst(rscratch1, UPPER_BIT_MASK);
3671   __ cset(result, Assembler::NE);
3672   __ leave();
3673   __ ret(lr);
3674 
3675   Register tmp1 = r3, tmp2 = r4, tmp3 = r5, tmp4 = r6, tmp5 = r7, tmp6 = r10;
3676   const RegSet spilled_regs = RegSet::range(tmp1, tmp5) + tmp6;
3677 
3678   has_negatives_long = __ pc(); // 2nd entry point
3679 
3680   __ enter();
3681 
3682   __ bind(LEN_OVER_15);
3683     __ push(spilled_regs, sp);
3684     __ andr(rscratch2, ary1, 15); // check pointer for 16-byte alignment
3685     __ cbz(rscratch2, ALIGNED);
3686     __ ldp(tmp6, tmp1, Address(ary1));
3687     __ mov(tmp5, 16);
3688     __ sub(rscratch1, tmp5, rscratch2); // amount of bytes until aligned address
3689     __ add(ary1, ary1, rscratch1);
3690     __ sub(len, len, rscratch1);
3691     __ orr(tmp6, tmp6, tmp1);
3692     __ tst(tmp6, UPPER_BIT_MASK);
3693     __ br(Assembler::NE, RET_TRUE);
3694 
3695   __ bind(ALIGNED);
3696     __ cmp(len, large_loop_size);
3697     __ br(Assembler::LT, CHECK_16);
3698     // Perform 16-byte load as early return in pre-loop to handle situation
3699     // when initially aligned large array has negative values at starting bytes,
3700     // so LARGE_LOOP would do 4 reads instead of 1 (in worst case), which is
3701     // slower. Cases with negative bytes further ahead won&#39;t be affected that
3702     // much. In fact, it&#39;ll be faster due to early loads, less instructions and
3703     // less branches in LARGE_LOOP.
3704     __ ldp(tmp6, tmp1, Address(__ post(ary1, 16)));
3705     __ sub(len, len, 16);
3706     __ orr(tmp6, tmp6, tmp1);
3707     __ tst(tmp6, UPPER_BIT_MASK);
3708     __ br(Assembler::NE, RET_TRUE);
3709     __ cmp(len, large_loop_size);
3710     __ br(Assembler::LT, CHECK_16);
3711 
3712     if (SoftwarePrefetchHintDistance &gt;= 0
3713         &amp;&amp; SoftwarePrefetchHintDistance &gt;= dcache_line) {
3714       // initial prefetch
3715       __ prfm(Address(ary1, SoftwarePrefetchHintDistance - dcache_line));
3716     }
3717   __ bind(LARGE_LOOP);
3718     if (SoftwarePrefetchHintDistance &gt;= 0) {
3719       __ prfm(Address(ary1, SoftwarePrefetchHintDistance));
3720     }
3721     // Issue load instructions first, since it can save few CPU/MEM cycles, also
3722     // instead of 4 triples of &quot;orr(...), addr(...);cbnz(...);&quot; (for each ldp)
3723     // better generate 7 * orr(...) + 1 andr(...) + 1 cbnz(...) which saves 3
3724     // instructions per cycle and have less branches, but this approach disables
3725     // early return, thus, all 64 bytes are loaded and checked every time.
3726     __ ldp(tmp2, tmp3, Address(ary1));
3727     __ ldp(tmp4, tmp5, Address(ary1, 16));
3728     __ ldp(rscratch1, rscratch2, Address(ary1, 32));
3729     __ ldp(tmp6, tmp1, Address(ary1, 48));
3730     __ add(ary1, ary1, large_loop_size);
3731     __ sub(len, len, large_loop_size);
3732     __ orr(tmp2, tmp2, tmp3);
3733     __ orr(tmp4, tmp4, tmp5);
3734     __ orr(rscratch1, rscratch1, rscratch2);
3735     __ orr(tmp6, tmp6, tmp1);
3736     __ orr(tmp2, tmp2, tmp4);
3737     __ orr(rscratch1, rscratch1, tmp6);
3738     __ orr(tmp2, tmp2, rscratch1);
3739     __ tst(tmp2, UPPER_BIT_MASK);
3740     __ br(Assembler::NE, RET_TRUE);
3741     __ cmp(len, large_loop_size);
3742     __ br(Assembler::GE, LARGE_LOOP);
3743 
3744   __ bind(CHECK_16); // small 16-byte load pre-loop
3745     __ cmp(len, (u1)16);
3746     __ br(Assembler::LT, POST_LOOP16);
3747 
3748   __ bind(LOOP16); // small 16-byte load loop
3749     __ ldp(tmp2, tmp3, Address(__ post(ary1, 16)));
3750     __ sub(len, len, 16);
3751     __ orr(tmp2, tmp2, tmp3);
3752     __ tst(tmp2, UPPER_BIT_MASK);
3753     __ br(Assembler::NE, RET_TRUE);
3754     __ cmp(len, (u1)16);
3755     __ br(Assembler::GE, LOOP16); // 16-byte load loop end
3756 
3757   __ bind(POST_LOOP16); // 16-byte aligned, so we can read unconditionally
3758     __ cmp(len, (u1)8);
3759     __ br(Assembler::LE, POST_LOOP16_LOAD_TAIL);
3760     __ ldr(tmp3, Address(__ post(ary1, 8)));
3761     __ sub(len, len, 8);
3762     __ tst(tmp3, UPPER_BIT_MASK);
3763     __ br(Assembler::NE, RET_TRUE);
3764 
3765   __ bind(POST_LOOP16_LOAD_TAIL);
3766     __ cbz(len, RET_FALSE); // Can&#39;t shift left by 64 when len==0
3767     __ ldr(tmp1, Address(ary1));
3768     __ mov(tmp2, 64);
3769     __ sub(tmp4, tmp2, len, __ LSL, 3);
3770     __ lslv(tmp1, tmp1, tmp4);
3771     __ tst(tmp1, UPPER_BIT_MASK);
3772     __ br(Assembler::NE, RET_TRUE);
3773     // Fallthrough
3774 
3775   __ bind(RET_FALSE);
3776     __ pop(spilled_regs, sp);
3777     __ leave();
3778     __ mov(result, zr);
3779     __ ret(lr);
3780 
3781   __ bind(RET_TRUE);
3782     __ pop(spilled_regs, sp);
3783   __ bind(RET_TRUE_NO_POP);
3784     __ leave();
3785     __ mov(result, 1);
3786     __ ret(lr);
3787 
3788   __ bind(DONE);
3789     __ pop(spilled_regs, sp);
3790     __ leave();
3791     __ ret(lr);
3792     return entry;
3793   }
3794 
3795   void generate_large_array_equals_loop_nonsimd(int loopThreshold,
3796         bool usePrefetch, Label &amp;NOT_EQUAL) {
3797     Register a1 = r1, a2 = r2, result = r0, cnt1 = r10, tmp1 = rscratch1,
3798         tmp2 = rscratch2, tmp3 = r3, tmp4 = r4, tmp5 = r5, tmp6 = r11,
3799         tmp7 = r12, tmp8 = r13;
3800     Label LOOP;
3801 
3802     __ ldp(tmp1, tmp3, Address(__ post(a1, 2 * wordSize)));
3803     __ ldp(tmp2, tmp4, Address(__ post(a2, 2 * wordSize)));
3804     __ bind(LOOP);
3805     if (usePrefetch) {
3806       __ prfm(Address(a1, SoftwarePrefetchHintDistance));
3807       __ prfm(Address(a2, SoftwarePrefetchHintDistance));
3808     }
3809     __ ldp(tmp5, tmp7, Address(__ post(a1, 2 * wordSize)));
3810     __ eor(tmp1, tmp1, tmp2);
3811     __ eor(tmp3, tmp3, tmp4);
3812     __ ldp(tmp6, tmp8, Address(__ post(a2, 2 * wordSize)));
3813     __ orr(tmp1, tmp1, tmp3);
3814     __ cbnz(tmp1, NOT_EQUAL);
3815     __ ldp(tmp1, tmp3, Address(__ post(a1, 2 * wordSize)));
3816     __ eor(tmp5, tmp5, tmp6);
3817     __ eor(tmp7, tmp7, tmp8);
3818     __ ldp(tmp2, tmp4, Address(__ post(a2, 2 * wordSize)));
3819     __ orr(tmp5, tmp5, tmp7);
3820     __ cbnz(tmp5, NOT_EQUAL);
3821     __ ldp(tmp5, tmp7, Address(__ post(a1, 2 * wordSize)));
3822     __ eor(tmp1, tmp1, tmp2);
3823     __ eor(tmp3, tmp3, tmp4);
3824     __ ldp(tmp6, tmp8, Address(__ post(a2, 2 * wordSize)));
3825     __ orr(tmp1, tmp1, tmp3);
3826     __ cbnz(tmp1, NOT_EQUAL);
3827     __ ldp(tmp1, tmp3, Address(__ post(a1, 2 * wordSize)));
3828     __ eor(tmp5, tmp5, tmp6);
3829     __ sub(cnt1, cnt1, 8 * wordSize);
3830     __ eor(tmp7, tmp7, tmp8);
3831     __ ldp(tmp2, tmp4, Address(__ post(a2, 2 * wordSize)));
3832     // tmp6 is not used. MacroAssembler::subs is used here (rather than
3833     // cmp) because subs allows an unlimited range of immediate operand.
3834     __ subs(tmp6, cnt1, loopThreshold);
3835     __ orr(tmp5, tmp5, tmp7);
3836     __ cbnz(tmp5, NOT_EQUAL);
3837     __ br(__ GE, LOOP);
3838     // post-loop
3839     __ eor(tmp1, tmp1, tmp2);
3840     __ eor(tmp3, tmp3, tmp4);
3841     __ orr(tmp1, tmp1, tmp3);
3842     __ sub(cnt1, cnt1, 2 * wordSize);
3843     __ cbnz(tmp1, NOT_EQUAL);
3844   }
3845 
3846   void generate_large_array_equals_loop_simd(int loopThreshold,
3847         bool usePrefetch, Label &amp;NOT_EQUAL) {
3848     Register a1 = r1, a2 = r2, result = r0, cnt1 = r10, tmp1 = rscratch1,
3849         tmp2 = rscratch2;
3850     Label LOOP;
3851 
3852     __ bind(LOOP);
3853     if (usePrefetch) {
3854       __ prfm(Address(a1, SoftwarePrefetchHintDistance));
3855       __ prfm(Address(a2, SoftwarePrefetchHintDistance));
3856     }
3857     __ ld1(v0, v1, v2, v3, __ T2D, Address(__ post(a1, 4 * 2 * wordSize)));
3858     __ sub(cnt1, cnt1, 8 * wordSize);
3859     __ ld1(v4, v5, v6, v7, __ T2D, Address(__ post(a2, 4 * 2 * wordSize)));
3860     __ subs(tmp1, cnt1, loopThreshold);
3861     __ eor(v0, __ T16B, v0, v4);
3862     __ eor(v1, __ T16B, v1, v5);
3863     __ eor(v2, __ T16B, v2, v6);
3864     __ eor(v3, __ T16B, v3, v7);
3865     __ orr(v0, __ T16B, v0, v1);
3866     __ orr(v1, __ T16B, v2, v3);
3867     __ orr(v0, __ T16B, v0, v1);
3868     __ umov(tmp1, v0, __ D, 0);
3869     __ umov(tmp2, v0, __ D, 1);
3870     __ orr(tmp1, tmp1, tmp2);
3871     __ cbnz(tmp1, NOT_EQUAL);
3872     __ br(__ GE, LOOP);
3873   }
3874 
3875   // a1 = r1 - array1 address
3876   // a2 = r2 - array2 address
3877   // result = r0 - return value. Already contains &quot;false&quot;
3878   // cnt1 = r10 - amount of elements left to check, reduced by wordSize
3879   // r3-r5 are reserved temporary registers
3880   address generate_large_array_equals() {
3881     Register a1 = r1, a2 = r2, result = r0, cnt1 = r10, tmp1 = rscratch1,
3882         tmp2 = rscratch2, tmp3 = r3, tmp4 = r4, tmp5 = r5, tmp6 = r11,
3883         tmp7 = r12, tmp8 = r13;
3884     Label TAIL, NOT_EQUAL, EQUAL, NOT_EQUAL_NO_POP, NO_PREFETCH_LARGE_LOOP,
3885         SMALL_LOOP, POST_LOOP;
3886     const int PRE_LOOP_SIZE = UseSIMDForArrayEquals ? 0 : 16;
3887     // calculate if at least 32 prefetched bytes are used
3888     int prefetchLoopThreshold = SoftwarePrefetchHintDistance + 32;
3889     int nonPrefetchLoopThreshold = (64 + PRE_LOOP_SIZE);
3890     RegSet spilled_regs = RegSet::range(tmp6, tmp8);
3891     assert_different_registers(a1, a2, result, cnt1, tmp1, tmp2, tmp3, tmp4,
3892         tmp5, tmp6, tmp7, tmp8);
3893 
3894     __ align(CodeEntryAlignment);
3895 
3896     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;large_array_equals&quot;);
3897 
3898     address entry = __ pc();
3899     __ enter();
3900     __ sub(cnt1, cnt1, wordSize);  // first 8 bytes were loaded outside of stub
3901     // also advance pointers to use post-increment instead of pre-increment
3902     __ add(a1, a1, wordSize);
3903     __ add(a2, a2, wordSize);
3904     if (AvoidUnalignedAccesses) {
3905       // both implementations (SIMD/nonSIMD) are using relatively large load
3906       // instructions (ld1/ldp), which has huge penalty (up to x2 exec time)
3907       // on some CPUs in case of address is not at least 16-byte aligned.
3908       // Arrays are 8-byte aligned currently, so, we can make additional 8-byte
3909       // load if needed at least for 1st address and make if 16-byte aligned.
3910       Label ALIGNED16;
3911       __ tbz(a1, 3, ALIGNED16);
3912       __ ldr(tmp1, Address(__ post(a1, wordSize)));
3913       __ ldr(tmp2, Address(__ post(a2, wordSize)));
3914       __ sub(cnt1, cnt1, wordSize);
3915       __ eor(tmp1, tmp1, tmp2);
3916       __ cbnz(tmp1, NOT_EQUAL_NO_POP);
3917       __ bind(ALIGNED16);
3918     }
3919     if (UseSIMDForArrayEquals) {
3920       if (SoftwarePrefetchHintDistance &gt;= 0) {
3921         __ subs(tmp1, cnt1, prefetchLoopThreshold);
3922         __ br(__ LE, NO_PREFETCH_LARGE_LOOP);
3923         generate_large_array_equals_loop_simd(prefetchLoopThreshold,
3924             /* prfm = */ true, NOT_EQUAL);
3925         __ subs(zr, cnt1, nonPrefetchLoopThreshold);
3926         __ br(__ LT, TAIL);
3927       }
3928       __ bind(NO_PREFETCH_LARGE_LOOP);
3929       generate_large_array_equals_loop_simd(nonPrefetchLoopThreshold,
3930           /* prfm = */ false, NOT_EQUAL);
3931     } else {
3932       __ push(spilled_regs, sp);
3933       if (SoftwarePrefetchHintDistance &gt;= 0) {
3934         __ subs(tmp1, cnt1, prefetchLoopThreshold);
3935         __ br(__ LE, NO_PREFETCH_LARGE_LOOP);
3936         generate_large_array_equals_loop_nonsimd(prefetchLoopThreshold,
3937             /* prfm = */ true, NOT_EQUAL);
3938         __ subs(zr, cnt1, nonPrefetchLoopThreshold);
3939         __ br(__ LT, TAIL);
3940       }
3941       __ bind(NO_PREFETCH_LARGE_LOOP);
3942       generate_large_array_equals_loop_nonsimd(nonPrefetchLoopThreshold,
3943           /* prfm = */ false, NOT_EQUAL);
3944     }
3945     __ bind(TAIL);
3946       __ cbz(cnt1, EQUAL);
3947       __ subs(cnt1, cnt1, wordSize);
3948       __ br(__ LE, POST_LOOP);
3949     __ bind(SMALL_LOOP);
3950       __ ldr(tmp1, Address(__ post(a1, wordSize)));
3951       __ ldr(tmp2, Address(__ post(a2, wordSize)));
3952       __ subs(cnt1, cnt1, wordSize);
3953       __ eor(tmp1, tmp1, tmp2);
3954       __ cbnz(tmp1, NOT_EQUAL);
3955       __ br(__ GT, SMALL_LOOP);
3956     __ bind(POST_LOOP);
3957       __ ldr(tmp1, Address(a1, cnt1));
3958       __ ldr(tmp2, Address(a2, cnt1));
3959       __ eor(tmp1, tmp1, tmp2);
3960       __ cbnz(tmp1, NOT_EQUAL);
3961     __ bind(EQUAL);
3962       __ mov(result, true);
3963     __ bind(NOT_EQUAL);
3964       if (!UseSIMDForArrayEquals) {
3965         __ pop(spilled_regs, sp);
3966       }
3967     __ bind(NOT_EQUAL_NO_POP);
3968     __ leave();
3969     __ ret(lr);
3970     return entry;
3971   }
3972 
3973   address generate_dsin_dcos(bool isCos) {
3974     __ align(CodeEntryAlignment);
3975     StubCodeMark mark(this, &quot;StubRoutines&quot;, isCos ? &quot;libmDcos&quot; : &quot;libmDsin&quot;);
3976     address start = __ pc();
3977     __ generate_dsin_dcos(isCos, (address)StubRoutines::aarch64::_npio2_hw,
3978         (address)StubRoutines::aarch64::_two_over_pi,
3979         (address)StubRoutines::aarch64::_pio2,
3980         (address)StubRoutines::aarch64::_dsin_coef,
3981         (address)StubRoutines::aarch64::_dcos_coef);
3982     return start;
3983   }
3984 
3985   address generate_dlog() {
3986     __ align(CodeEntryAlignment);
3987     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;dlog&quot;);
3988     address entry = __ pc();
3989     FloatRegister vtmp0 = v0, vtmp1 = v1, vtmp2 = v2, vtmp3 = v3, vtmp4 = v4,
3990         vtmp5 = v5, tmpC1 = v16, tmpC2 = v17, tmpC3 = v18, tmpC4 = v19;
3991     Register tmp1 = r0, tmp2 = r1, tmp3 = r2, tmp4 = r3, tmp5 = r4;
3992     __ fast_log(vtmp0, vtmp1, vtmp2, vtmp3, vtmp4, vtmp5, tmpC1, tmpC2, tmpC3,
3993         tmpC4, tmp1, tmp2, tmp3, tmp4, tmp5);
3994     return entry;
3995   }
3996 
3997   // code for comparing 16 bytes of strings with same encoding
3998   void compare_string_16_bytes_same(Label &amp;DIFF1, Label &amp;DIFF2) {
3999     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, tmp1 = r10, tmp2 = r11;
4000     __ ldr(rscratch1, Address(__ post(str1, 8)));
4001     __ eor(rscratch2, tmp1, tmp2);
4002     __ ldr(cnt1, Address(__ post(str2, 8)));
4003     __ cbnz(rscratch2, DIFF1);
4004     __ ldr(tmp1, Address(__ post(str1, 8)));
4005     __ eor(rscratch2, rscratch1, cnt1);
4006     __ ldr(tmp2, Address(__ post(str2, 8)));
4007     __ cbnz(rscratch2, DIFF2);
4008   }
4009 
4010   // code for comparing 16 characters of strings with Latin1 and Utf16 encoding
4011   void compare_string_16_x_LU(Register tmpL, Register tmpU, Label &amp;DIFF1,
4012       Label &amp;DIFF2) {
4013     Register cnt1 = r2, tmp2 = r11, tmp3 = r12;
4014     FloatRegister vtmp = v1, vtmpZ = v0, vtmp3 = v2;
4015 
4016     __ ldrq(vtmp, Address(__ post(tmp2, 16)));
4017     __ ldr(tmpU, Address(__ post(cnt1, 8)));
4018     __ zip1(vtmp3, __ T16B, vtmp, vtmpZ);
4019     // now we have 32 bytes of characters (converted to U) in vtmp:vtmp3
4020 
4021     __ fmovd(tmpL, vtmp3);
4022     __ eor(rscratch2, tmp3, tmpL);
4023     __ cbnz(rscratch2, DIFF2);
4024 
4025     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4026     __ umov(tmpL, vtmp3, __ D, 1);
4027     __ eor(rscratch2, tmpU, tmpL);
4028     __ cbnz(rscratch2, DIFF1);
4029 
4030     __ zip2(vtmp, __ T16B, vtmp, vtmpZ);
4031     __ ldr(tmpU, Address(__ post(cnt1, 8)));
4032     __ fmovd(tmpL, vtmp);
4033     __ eor(rscratch2, tmp3, tmpL);
4034     __ cbnz(rscratch2, DIFF2);
4035 
4036     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4037     __ umov(tmpL, vtmp, __ D, 1);
4038     __ eor(rscratch2, tmpU, tmpL);
4039     __ cbnz(rscratch2, DIFF1);
4040   }
4041 
4042   // r0  = result
4043   // r1  = str1
4044   // r2  = cnt1
4045   // r3  = str2
4046   // r4  = cnt2
4047   // r10 = tmp1
4048   // r11 = tmp2
4049   address generate_compare_long_string_different_encoding(bool isLU) {
4050     __ align(CodeEntryAlignment);
4051     StubCodeMark mark(this, &quot;StubRoutines&quot;, isLU
4052         ? &quot;compare_long_string_different_encoding LU&quot;
4053         : &quot;compare_long_string_different_encoding UL&quot;);
4054     address entry = __ pc();
4055     Label SMALL_LOOP, TAIL, TAIL_LOAD_16, LOAD_LAST, DIFF1, DIFF2,
4056         DONE, CALCULATE_DIFFERENCE, LARGE_LOOP_PREFETCH, NO_PREFETCH,
4057         LARGE_LOOP_PREFETCH_REPEAT1, LARGE_LOOP_PREFETCH_REPEAT2;
4058     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, cnt2 = r4,
4059         tmp1 = r10, tmp2 = r11, tmp3 = r12, tmp4 = r14;
4060     FloatRegister vtmpZ = v0, vtmp = v1, vtmp3 = v2;
4061     RegSet spilled_regs = RegSet::of(tmp3, tmp4);
4062 
4063     int prefetchLoopExitCondition = MAX(64, SoftwarePrefetchHintDistance/2);
4064 
4065     __ eor(vtmpZ, __ T16B, vtmpZ, vtmpZ);
4066     // cnt2 == amount of characters left to compare
4067     // Check already loaded first 4 symbols(vtmp and tmp2(LU)/tmp1(UL))
4068     __ zip1(vtmp, __ T8B, vtmp, vtmpZ);
4069     __ add(str1, str1, isLU ? wordSize/2 : wordSize);
4070     __ add(str2, str2, isLU ? wordSize : wordSize/2);
4071     __ fmovd(isLU ? tmp1 : tmp2, vtmp);
4072     __ subw(cnt2, cnt2, 8); // Already loaded 4 symbols. Last 4 is special case.
4073     __ eor(rscratch2, tmp1, tmp2);
4074     __ mov(rscratch1, tmp2);
4075     __ cbnz(rscratch2, CALCULATE_DIFFERENCE);
4076     Register tmpU = isLU ? rscratch1 : tmp1, // where to keep U for comparison
4077              tmpL = isLU ? tmp1 : rscratch1; // where to keep L for comparison
4078     __ push(spilled_regs, sp);
4079     __ mov(tmp2, isLU ? str1 : str2); // init the pointer to L next load
4080     __ mov(cnt1, isLU ? str2 : str1); // init the pointer to U next load
4081 
4082     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4083 
4084     if (SoftwarePrefetchHintDistance &gt;= 0) {
4085       __ subs(rscratch2, cnt2, prefetchLoopExitCondition);
4086       __ br(__ LT, NO_PREFETCH);
4087       __ bind(LARGE_LOOP_PREFETCH);
4088         __ prfm(Address(tmp2, SoftwarePrefetchHintDistance));
4089         __ mov(tmp4, 2);
4090         __ prfm(Address(cnt1, SoftwarePrefetchHintDistance));
4091         __ bind(LARGE_LOOP_PREFETCH_REPEAT1);
4092           compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4093           __ subs(tmp4, tmp4, 1);
4094           __ br(__ GT, LARGE_LOOP_PREFETCH_REPEAT1);
4095           __ prfm(Address(cnt1, SoftwarePrefetchHintDistance));
4096           __ mov(tmp4, 2);
4097         __ bind(LARGE_LOOP_PREFETCH_REPEAT2);
4098           compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4099           __ subs(tmp4, tmp4, 1);
4100           __ br(__ GT, LARGE_LOOP_PREFETCH_REPEAT2);
4101           __ sub(cnt2, cnt2, 64);
4102           __ subs(rscratch2, cnt2, prefetchLoopExitCondition);
4103           __ br(__ GE, LARGE_LOOP_PREFETCH);
4104     }
4105     __ cbz(cnt2, LOAD_LAST); // no characters left except last load
4106     __ bind(NO_PREFETCH);
4107     __ subs(cnt2, cnt2, 16);
4108     __ br(__ LT, TAIL);
4109     __ align(OptoLoopAlignment);
4110     __ bind(SMALL_LOOP); // smaller loop
4111       __ subs(cnt2, cnt2, 16);
4112       compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4113       __ br(__ GE, SMALL_LOOP);
4114       __ cmn(cnt2, (u1)16);
4115       __ br(__ EQ, LOAD_LAST);
4116     __ bind(TAIL); // 1..15 characters left until last load (last 4 characters)
4117       __ add(cnt1, cnt1, cnt2, __ LSL, 1); // Address of 32 bytes before last 4 characters in UTF-16 string
4118       __ add(tmp2, tmp2, cnt2); // Address of 16 bytes before last 4 characters in Latin1 string
4119       __ ldr(tmp3, Address(cnt1, -8));
4120       compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2); // last 16 characters before last load
4121       __ b(LOAD_LAST);
4122     __ bind(DIFF2);
4123       __ mov(tmpU, tmp3);
4124     __ bind(DIFF1);
4125       __ pop(spilled_regs, sp);
4126       __ b(CALCULATE_DIFFERENCE);
4127     __ bind(LOAD_LAST);
4128       // Last 4 UTF-16 characters are already pre-loaded into tmp3 by compare_string_16_x_LU.
4129       // No need to load it again
4130       __ mov(tmpU, tmp3);
4131       __ pop(spilled_regs, sp);
4132 
4133       // tmp2 points to the address of the last 4 Latin1 characters right now
4134       __ ldrs(vtmp, Address(tmp2));
4135       __ zip1(vtmp, __ T8B, vtmp, vtmpZ);
4136       __ fmovd(tmpL, vtmp);
4137 
4138       __ eor(rscratch2, tmpU, tmpL);
4139       __ cbz(rscratch2, DONE);
4140 
4141     // Find the first different characters in the longwords and
4142     // compute their difference.
4143     __ bind(CALCULATE_DIFFERENCE);
4144       __ rev(rscratch2, rscratch2);
4145       __ clz(rscratch2, rscratch2);
4146       __ andr(rscratch2, rscratch2, -16);
4147       __ lsrv(tmp1, tmp1, rscratch2);
4148       __ uxthw(tmp1, tmp1);
4149       __ lsrv(rscratch1, rscratch1, rscratch2);
4150       __ uxthw(rscratch1, rscratch1);
4151       __ subw(result, tmp1, rscratch1);
4152     __ bind(DONE);
4153       __ ret(lr);
4154     return entry;
4155   }
4156 
4157   // r0  = result
4158   // r1  = str1
4159   // r2  = cnt1
4160   // r3  = str2
4161   // r4  = cnt2
4162   // r10 = tmp1
4163   // r11 = tmp2
4164   address generate_compare_long_string_same_encoding(bool isLL) {
4165     __ align(CodeEntryAlignment);
4166     StubCodeMark mark(this, &quot;StubRoutines&quot;, isLL
4167         ? &quot;compare_long_string_same_encoding LL&quot;
4168         : &quot;compare_long_string_same_encoding UU&quot;);
4169     address entry = __ pc();
4170     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, cnt2 = r4,
4171         tmp1 = r10, tmp2 = r11;
4172     Label SMALL_LOOP, LARGE_LOOP_PREFETCH, CHECK_LAST, DIFF2, TAIL,
4173         LENGTH_DIFF, DIFF, LAST_CHECK_AND_LENGTH_DIFF,
4174         DIFF_LAST_POSITION, DIFF_LAST_POSITION2;
4175     // exit from large loop when less than 64 bytes left to read or we&#39;re about
4176     // to prefetch memory behind array border
4177     int largeLoopExitCondition = MAX(64, SoftwarePrefetchHintDistance)/(isLL ? 1 : 2);
4178     // cnt1/cnt2 contains amount of characters to compare. cnt1 can be re-used
4179     // update cnt2 counter with already loaded 8 bytes
4180     __ sub(cnt2, cnt2, wordSize/(isLL ? 1 : 2));
4181     // update pointers, because of previous read
4182     __ add(str1, str1, wordSize);
4183     __ add(str2, str2, wordSize);
4184     if (SoftwarePrefetchHintDistance &gt;= 0) {
4185       __ bind(LARGE_LOOP_PREFETCH);
4186         __ prfm(Address(str1, SoftwarePrefetchHintDistance));
4187         __ prfm(Address(str2, SoftwarePrefetchHintDistance));
4188         compare_string_16_bytes_same(DIFF, DIFF2);
4189         compare_string_16_bytes_same(DIFF, DIFF2);
4190         __ sub(cnt2, cnt2, isLL ? 64 : 32);
4191         compare_string_16_bytes_same(DIFF, DIFF2);
4192         __ subs(rscratch2, cnt2, largeLoopExitCondition);
4193         compare_string_16_bytes_same(DIFF, DIFF2);
4194         __ br(__ GT, LARGE_LOOP_PREFETCH);
4195         __ cbz(cnt2, LAST_CHECK_AND_LENGTH_DIFF); // no more chars left?
4196     }
4197     // less than 16 bytes left?
4198     __ subs(cnt2, cnt2, isLL ? 16 : 8);
4199     __ br(__ LT, TAIL);
4200     __ align(OptoLoopAlignment);
4201     __ bind(SMALL_LOOP);
4202       compare_string_16_bytes_same(DIFF, DIFF2);
4203       __ subs(cnt2, cnt2, isLL ? 16 : 8);
4204       __ br(__ GE, SMALL_LOOP);
4205     __ bind(TAIL);
4206       __ adds(cnt2, cnt2, isLL ? 16 : 8);
4207       __ br(__ EQ, LAST_CHECK_AND_LENGTH_DIFF);
4208       __ subs(cnt2, cnt2, isLL ? 8 : 4);
4209       __ br(__ LE, CHECK_LAST);
4210       __ eor(rscratch2, tmp1, tmp2);
4211       __ cbnz(rscratch2, DIFF);
4212       __ ldr(tmp1, Address(__ post(str1, 8)));
4213       __ ldr(tmp2, Address(__ post(str2, 8)));
4214       __ sub(cnt2, cnt2, isLL ? 8 : 4);
4215     __ bind(CHECK_LAST);
4216       if (!isLL) {
4217         __ add(cnt2, cnt2, cnt2); // now in bytes
4218       }
4219       __ eor(rscratch2, tmp1, tmp2);
4220       __ cbnz(rscratch2, DIFF);
4221       __ ldr(rscratch1, Address(str1, cnt2));
4222       __ ldr(cnt1, Address(str2, cnt2));
4223       __ eor(rscratch2, rscratch1, cnt1);
4224       __ cbz(rscratch2, LENGTH_DIFF);
4225       // Find the first different characters in the longwords and
4226       // compute their difference.
4227     __ bind(DIFF2);
4228       __ rev(rscratch2, rscratch2);
4229       __ clz(rscratch2, rscratch2);
4230       __ andr(rscratch2, rscratch2, isLL ? -8 : -16);
4231       __ lsrv(rscratch1, rscratch1, rscratch2);
4232       if (isLL) {
4233         __ lsrv(cnt1, cnt1, rscratch2);
4234         __ uxtbw(rscratch1, rscratch1);
4235         __ uxtbw(cnt1, cnt1);
4236       } else {
4237         __ lsrv(cnt1, cnt1, rscratch2);
4238         __ uxthw(rscratch1, rscratch1);
4239         __ uxthw(cnt1, cnt1);
4240       }
4241       __ subw(result, rscratch1, cnt1);
4242       __ b(LENGTH_DIFF);
4243     __ bind(DIFF);
4244       __ rev(rscratch2, rscratch2);
4245       __ clz(rscratch2, rscratch2);
4246       __ andr(rscratch2, rscratch2, isLL ? -8 : -16);
4247       __ lsrv(tmp1, tmp1, rscratch2);
4248       if (isLL) {
4249         __ lsrv(tmp2, tmp2, rscratch2);
4250         __ uxtbw(tmp1, tmp1);
4251         __ uxtbw(tmp2, tmp2);
4252       } else {
4253         __ lsrv(tmp2, tmp2, rscratch2);
4254         __ uxthw(tmp1, tmp1);
4255         __ uxthw(tmp2, tmp2);
4256       }
4257       __ subw(result, tmp1, tmp2);
4258       __ b(LENGTH_DIFF);
4259     __ bind(LAST_CHECK_AND_LENGTH_DIFF);
4260       __ eor(rscratch2, tmp1, tmp2);
4261       __ cbnz(rscratch2, DIFF);
4262     __ bind(LENGTH_DIFF);
4263       __ ret(lr);
4264     return entry;
4265   }
4266 
4267   void generate_compare_long_strings() {
4268       StubRoutines::aarch64::_compare_long_string_LL
4269           = generate_compare_long_string_same_encoding(true);
4270       StubRoutines::aarch64::_compare_long_string_UU
4271           = generate_compare_long_string_same_encoding(false);
4272       StubRoutines::aarch64::_compare_long_string_LU
4273           = generate_compare_long_string_different_encoding(true);
4274       StubRoutines::aarch64::_compare_long_string_UL
4275           = generate_compare_long_string_different_encoding(false);
4276   }
4277 
4278   // R0 = result
4279   // R1 = str2
4280   // R2 = cnt1
4281   // R3 = str1
4282   // R4 = cnt2
4283   // This generic linear code use few additional ideas, which makes it faster:
4284   // 1) we can safely keep at least 1st register of pattern(since length &gt;= 8)
4285   // in order to skip initial loading(help in systems with 1 ld pipeline)
4286   // 2) we can use &quot;fast&quot; algorithm of finding single character to search for
4287   // first symbol with less branches(1 branch per each loaded register instead
4288   // of branch for each symbol), so, this is where constants like
4289   // 0x0101...01, 0x00010001...0001, 0x7f7f...7f, 0x7fff7fff...7fff comes from
4290   // 3) after loading and analyzing 1st register of source string, it can be
4291   // used to search for every 1st character entry, saving few loads in
4292   // comparison with &quot;simplier-but-slower&quot; implementation
4293   // 4) in order to avoid lots of push/pop operations, code below is heavily
4294   // re-using/re-initializing/compressing register values, which makes code
4295   // larger and a bit less readable, however, most of extra operations are
4296   // issued during loads or branches, so, penalty is minimal
4297   address generate_string_indexof_linear(bool str1_isL, bool str2_isL) {
4298     const char* stubName = str1_isL
4299         ? (str2_isL ? &quot;indexof_linear_ll&quot; : &quot;indexof_linear_ul&quot;)
4300         : &quot;indexof_linear_uu&quot;;
4301     __ align(CodeEntryAlignment);
4302     StubCodeMark mark(this, &quot;StubRoutines&quot;, stubName);
4303     address entry = __ pc();
4304 
4305     int str1_chr_size = str1_isL ? 1 : 2;
4306     int str2_chr_size = str2_isL ? 1 : 2;
4307     int str1_chr_shift = str1_isL ? 0 : 1;
4308     int str2_chr_shift = str2_isL ? 0 : 1;
4309     bool isL = str1_isL &amp;&amp; str2_isL;
4310    // parameters
4311     Register result = r0, str2 = r1, cnt1 = r2, str1 = r3, cnt2 = r4;
4312     // temporary registers
4313     Register tmp1 = r20, tmp2 = r21, tmp3 = r22, tmp4 = r23;
4314     RegSet spilled_regs = RegSet::range(tmp1, tmp4);
4315     // redefinitions
4316     Register ch1 = rscratch1, ch2 = rscratch2, first = tmp3;
4317 
4318     __ push(spilled_regs, sp);
4319     Label L_LOOP, L_LOOP_PROCEED, L_SMALL, L_HAS_ZERO,
4320         L_HAS_ZERO_LOOP, L_CMP_LOOP, L_CMP_LOOP_NOMATCH, L_SMALL_PROCEED,
4321         L_SMALL_HAS_ZERO_LOOP, L_SMALL_CMP_LOOP_NOMATCH, L_SMALL_CMP_LOOP,
4322         L_POST_LOOP, L_CMP_LOOP_LAST_CMP, L_HAS_ZERO_LOOP_NOMATCH,
4323         L_SMALL_CMP_LOOP_LAST_CMP, L_SMALL_CMP_LOOP_LAST_CMP2,
4324         L_CMP_LOOP_LAST_CMP2, DONE, NOMATCH;
4325     // Read whole register from str1. It is safe, because length &gt;=8 here
4326     __ ldr(ch1, Address(str1));
4327     // Read whole register from str2. It is safe, because length &gt;=8 here
4328     __ ldr(ch2, Address(str2));
4329     __ sub(cnt2, cnt2, cnt1);
4330     __ andr(first, ch1, str1_isL ? 0xFF : 0xFFFF);
4331     if (str1_isL != str2_isL) {
4332       __ eor(v0, __ T16B, v0, v0);
4333     }
4334     __ mov(tmp1, str2_isL ? 0x0101010101010101 : 0x0001000100010001);
4335     __ mul(first, first, tmp1);
4336     // check if we have less than 1 register to check
4337     __ subs(cnt2, cnt2, wordSize/str2_chr_size - 1);
4338     if (str1_isL != str2_isL) {
4339       __ fmovd(v1, ch1);
4340     }
4341     __ br(__ LE, L_SMALL);
4342     __ eor(ch2, first, ch2);
4343     if (str1_isL != str2_isL) {
4344       __ zip1(v1, __ T16B, v1, v0);
4345     }
4346     __ sub(tmp2, ch2, tmp1);
4347     __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4348     __ bics(tmp2, tmp2, ch2);
4349     if (str1_isL != str2_isL) {
4350       __ fmovd(ch1, v1);
4351     }
4352     __ br(__ NE, L_HAS_ZERO);
4353     __ subs(cnt2, cnt2, wordSize/str2_chr_size);
4354     __ add(result, result, wordSize/str2_chr_size);
4355     __ add(str2, str2, wordSize);
4356     __ br(__ LT, L_POST_LOOP);
4357     __ BIND(L_LOOP);
4358       __ ldr(ch2, Address(str2));
4359       __ eor(ch2, first, ch2);
4360       __ sub(tmp2, ch2, tmp1);
4361       __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4362       __ bics(tmp2, tmp2, ch2);
4363       __ br(__ NE, L_HAS_ZERO);
4364     __ BIND(L_LOOP_PROCEED);
4365       __ subs(cnt2, cnt2, wordSize/str2_chr_size);
4366       __ add(str2, str2, wordSize);
4367       __ add(result, result, wordSize/str2_chr_size);
4368       __ br(__ GE, L_LOOP);
4369     __ BIND(L_POST_LOOP);
4370       __ subs(zr, cnt2, -wordSize/str2_chr_size); // no extra characters to check
4371       __ br(__ LE, NOMATCH);
4372       __ ldr(ch2, Address(str2));
4373       __ sub(cnt2, zr, cnt2, __ LSL, LogBitsPerByte + str2_chr_shift);
4374       __ eor(ch2, first, ch2);
4375       __ sub(tmp2, ch2, tmp1);
4376       __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4377       __ mov(tmp4, -1); // all bits set
4378       __ b(L_SMALL_PROCEED);
4379     __ align(OptoLoopAlignment);
4380     __ BIND(L_SMALL);
4381       __ sub(cnt2, zr, cnt2, __ LSL, LogBitsPerByte + str2_chr_shift);
4382       __ eor(ch2, first, ch2);
4383       if (str1_isL != str2_isL) {
4384         __ zip1(v1, __ T16B, v1, v0);
4385       }
4386       __ sub(tmp2, ch2, tmp1);
4387       __ mov(tmp4, -1); // all bits set
4388       __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4389       if (str1_isL != str2_isL) {
4390         __ fmovd(ch1, v1); // move converted 4 symbols
4391       }
4392     __ BIND(L_SMALL_PROCEED);
4393       __ lsrv(tmp4, tmp4, cnt2); // mask. zeroes on useless bits.
4394       __ bic(tmp2, tmp2, ch2);
4395       __ ands(tmp2, tmp2, tmp4); // clear useless bits and check
4396       __ rbit(tmp2, tmp2);
4397       __ br(__ EQ, NOMATCH);
4398     __ BIND(L_SMALL_HAS_ZERO_LOOP);
4399       __ clz(tmp4, tmp2); // potentially long. Up to 4 cycles on some cpu&#39;s
4400       __ cmp(cnt1, u1(wordSize/str2_chr_size));
4401       __ br(__ LE, L_SMALL_CMP_LOOP_LAST_CMP2);
4402       if (str2_isL) { // LL
4403         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte); // address of &quot;index&quot;
4404         __ ldr(ch2, Address(str2)); // read whole register of str2. Safe.
4405         __ lslv(tmp2, tmp2, tmp4); // shift off leading zeroes from match info
4406         __ add(result, result, tmp4, __ LSR, LogBitsPerByte);
4407         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4408       } else {
4409         __ mov(ch2, 0xE); // all bits in byte set except last one
4410         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4411         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4412         __ lslv(tmp2, tmp2, tmp4);
4413         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4414         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4415         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4416         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4417       }
4418       __ cmp(ch1, ch2);
4419       __ mov(tmp4, wordSize/str2_chr_size);
4420       __ br(__ NE, L_SMALL_CMP_LOOP_NOMATCH);
4421     __ BIND(L_SMALL_CMP_LOOP);
4422       str1_isL ? __ ldrb(first, Address(str1, tmp4, Address::lsl(str1_chr_shift)))
4423                : __ ldrh(first, Address(str1, tmp4, Address::lsl(str1_chr_shift)));
4424       str2_isL ? __ ldrb(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)))
4425                : __ ldrh(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)));
4426       __ add(tmp4, tmp4, 1);
4427       __ cmp(tmp4, cnt1);
4428       __ br(__ GE, L_SMALL_CMP_LOOP_LAST_CMP);
4429       __ cmp(first, ch2);
4430       __ br(__ EQ, L_SMALL_CMP_LOOP);
4431     __ BIND(L_SMALL_CMP_LOOP_NOMATCH);
4432       __ cbz(tmp2, NOMATCH); // no more matches. exit
4433       __ clz(tmp4, tmp2);
4434       __ add(result, result, 1); // advance index
4435       __ add(str2, str2, str2_chr_size); // advance pointer
4436       __ b(L_SMALL_HAS_ZERO_LOOP);
4437     __ align(OptoLoopAlignment);
4438     __ BIND(L_SMALL_CMP_LOOP_LAST_CMP);
4439       __ cmp(first, ch2);
4440       __ br(__ NE, L_SMALL_CMP_LOOP_NOMATCH);
4441       __ b(DONE);
4442     __ align(OptoLoopAlignment);
4443     __ BIND(L_SMALL_CMP_LOOP_LAST_CMP2);
4444       if (str2_isL) { // LL
4445         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte); // address of &quot;index&quot;
4446         __ ldr(ch2, Address(str2)); // read whole register of str2. Safe.
4447         __ lslv(tmp2, tmp2, tmp4); // shift off leading zeroes from match info
4448         __ add(result, result, tmp4, __ LSR, LogBitsPerByte);
4449         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4450       } else {
4451         __ mov(ch2, 0xE); // all bits in byte set except last one
4452         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4453         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4454         __ lslv(tmp2, tmp2, tmp4);
4455         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4456         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4457         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4458         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4459       }
4460       __ cmp(ch1, ch2);
4461       __ br(__ NE, L_SMALL_CMP_LOOP_NOMATCH);
4462       __ b(DONE);
4463     __ align(OptoLoopAlignment);
4464     __ BIND(L_HAS_ZERO);
4465       __ rbit(tmp2, tmp2);
4466       __ clz(tmp4, tmp2); // potentially long. Up to 4 cycles on some CPU&#39;s
4467       // Now, perform compression of counters(cnt2 and cnt1) into one register.
4468       // It&#39;s fine because both counters are 32bit and are not changed in this
4469       // loop. Just restore it on exit. So, cnt1 can be re-used in this loop.
4470       __ orr(cnt2, cnt2, cnt1, __ LSL, BitsPerByte * wordSize / 2);
4471       __ sub(result, result, 1);
4472     __ BIND(L_HAS_ZERO_LOOP);
4473       __ mov(cnt1, wordSize/str2_chr_size);
4474       __ cmp(cnt1, cnt2, __ LSR, BitsPerByte * wordSize / 2);
4475       __ br(__ GE, L_CMP_LOOP_LAST_CMP2); // case of 8 bytes only to compare
4476       if (str2_isL) {
4477         __ lsr(ch2, tmp4, LogBitsPerByte + str2_chr_shift); // char index
4478         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4479         __ lslv(tmp2, tmp2, tmp4);
4480         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4481         __ add(tmp4, tmp4, 1);
4482         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4483         __ lsl(tmp2, tmp2, 1);
4484         __ mov(tmp4, wordSize/str2_chr_size);
4485       } else {
4486         __ mov(ch2, 0xE);
4487         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4488         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4489         __ lslv(tmp2, tmp2, tmp4);
4490         __ add(tmp4, tmp4, 1);
4491         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4492         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte);
4493         __ lsl(tmp2, tmp2, 1);
4494         __ mov(tmp4, wordSize/str2_chr_size);
4495         __ sub(str2, str2, str2_chr_size);
4496       }
4497       __ cmp(ch1, ch2);
4498       __ mov(tmp4, wordSize/str2_chr_size);
4499       __ br(__ NE, L_CMP_LOOP_NOMATCH);
4500     __ BIND(L_CMP_LOOP);
4501       str1_isL ? __ ldrb(cnt1, Address(str1, tmp4, Address::lsl(str1_chr_shift)))
4502                : __ ldrh(cnt1, Address(str1, tmp4, Address::lsl(str1_chr_shift)));
4503       str2_isL ? __ ldrb(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)))
4504                : __ ldrh(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)));
4505       __ add(tmp4, tmp4, 1);
4506       __ cmp(tmp4, cnt2, __ LSR, BitsPerByte * wordSize / 2);
4507       __ br(__ GE, L_CMP_LOOP_LAST_CMP);
4508       __ cmp(cnt1, ch2);
4509       __ br(__ EQ, L_CMP_LOOP);
4510     __ BIND(L_CMP_LOOP_NOMATCH);
4511       // here we&#39;re not matched
4512       __ cbz(tmp2, L_HAS_ZERO_LOOP_NOMATCH); // no more matches. Proceed to main loop
4513       __ clz(tmp4, tmp2);
4514       __ add(str2, str2, str2_chr_size); // advance pointer
4515       __ b(L_HAS_ZERO_LOOP);
4516     __ align(OptoLoopAlignment);
4517     __ BIND(L_CMP_LOOP_LAST_CMP);
4518       __ cmp(cnt1, ch2);
4519       __ br(__ NE, L_CMP_LOOP_NOMATCH);
4520       __ b(DONE);
4521     __ align(OptoLoopAlignment);
4522     __ BIND(L_CMP_LOOP_LAST_CMP2);
4523       if (str2_isL) {
4524         __ lsr(ch2, tmp4, LogBitsPerByte + str2_chr_shift); // char index
4525         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4526         __ lslv(tmp2, tmp2, tmp4);
4527         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4528         __ add(tmp4, tmp4, 1);
4529         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4530         __ lsl(tmp2, tmp2, 1);
4531       } else {
4532         __ mov(ch2, 0xE);
4533         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4534         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4535         __ lslv(tmp2, tmp2, tmp4);
4536         __ add(tmp4, tmp4, 1);
4537         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4538         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte);
4539         __ lsl(tmp2, tmp2, 1);
4540         __ sub(str2, str2, str2_chr_size);
4541       }
4542       __ cmp(ch1, ch2);
4543       __ br(__ NE, L_CMP_LOOP_NOMATCH);
4544       __ b(DONE);
4545     __ align(OptoLoopAlignment);
4546     __ BIND(L_HAS_ZERO_LOOP_NOMATCH);
4547       // 1) Restore &quot;result&quot; index. Index was wordSize/str2_chr_size * N until
4548       // L_HAS_ZERO block. Byte octet was analyzed in L_HAS_ZERO_LOOP,
4549       // so, result was increased at max by wordSize/str2_chr_size - 1, so,
4550       // respective high bit wasn&#39;t changed. L_LOOP_PROCEED will increase
4551       // result by analyzed characters value, so, we can just reset lower bits
4552       // in result here. Clear 2 lower bits for UU/UL and 3 bits for LL
4553       // 2) restore cnt1 and cnt2 values from &quot;compressed&quot; cnt2
4554       // 3) advance str2 value to represent next str2 octet. result &amp; 7/3 is
4555       // index of last analyzed substring inside current octet. So, str2 in at
4556       // respective start address. We need to advance it to next octet
4557       __ andr(tmp2, result, wordSize/str2_chr_size - 1); // symbols analyzed
4558       __ lsr(cnt1, cnt2, BitsPerByte * wordSize / 2);
4559       __ bfm(result, zr, 0, 2 - str2_chr_shift);
4560       __ sub(str2, str2, tmp2, __ LSL, str2_chr_shift); // restore str2
4561       __ movw(cnt2, cnt2);
4562       __ b(L_LOOP_PROCEED);
4563     __ align(OptoLoopAlignment);
4564     __ BIND(NOMATCH);
4565       __ mov(result, -1);
4566     __ BIND(DONE);
4567       __ pop(spilled_regs, sp);
4568       __ ret(lr);
4569     return entry;
4570   }
4571 
4572   void generate_string_indexof_stubs() {
4573     StubRoutines::aarch64::_string_indexof_linear_ll = generate_string_indexof_linear(true, true);
4574     StubRoutines::aarch64::_string_indexof_linear_uu = generate_string_indexof_linear(false, false);
4575     StubRoutines::aarch64::_string_indexof_linear_ul = generate_string_indexof_linear(true, false);
4576   }
4577 
4578   void inflate_and_store_2_fp_registers(bool generatePrfm,
4579       FloatRegister src1, FloatRegister src2) {
4580     Register dst = r1;
4581     __ zip1(v1, __ T16B, src1, v0);
4582     __ zip2(v2, __ T16B, src1, v0);
4583     if (generatePrfm) {
4584       __ prfm(Address(dst, SoftwarePrefetchHintDistance), PSTL1STRM);
4585     }
4586     __ zip1(v3, __ T16B, src2, v0);
4587     __ zip2(v4, __ T16B, src2, v0);
4588     __ st1(v1, v2, v3, v4, __ T16B, Address(__ post(dst, 64)));
4589   }
4590 
4591   // R0 = src
4592   // R1 = dst
4593   // R2 = len
4594   // R3 = len &gt;&gt; 3
4595   // V0 = 0
4596   // v1 = loaded 8 bytes
4597   address generate_large_byte_array_inflate() {
4598     __ align(CodeEntryAlignment);
4599     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;large_byte_array_inflate&quot;);
4600     address entry = __ pc();
4601     Label LOOP, LOOP_START, LOOP_PRFM, LOOP_PRFM_START, DONE;
4602     Register src = r0, dst = r1, len = r2, octetCounter = r3;
4603     const int large_loop_threshold = MAX(64, SoftwarePrefetchHintDistance)/8 + 4;
4604 
4605     // do one more 8-byte read to have address 16-byte aligned in most cases
4606     // also use single store instruction
4607     __ ldrd(v2, __ post(src, 8));
4608     __ sub(octetCounter, octetCounter, 2);
4609     __ zip1(v1, __ T16B, v1, v0);
4610     __ zip1(v2, __ T16B, v2, v0);
4611     __ st1(v1, v2, __ T16B, __ post(dst, 32));
4612     __ ld1(v3, v4, v5, v6, __ T16B, Address(__ post(src, 64)));
4613     __ subs(rscratch1, octetCounter, large_loop_threshold);
4614     __ br(__ LE, LOOP_START);
4615     __ b(LOOP_PRFM_START);
4616     __ bind(LOOP_PRFM);
4617       __ ld1(v3, v4, v5, v6, __ T16B, Address(__ post(src, 64)));
4618     __ bind(LOOP_PRFM_START);
4619       __ prfm(Address(src, SoftwarePrefetchHintDistance));
4620       __ sub(octetCounter, octetCounter, 8);
4621       __ subs(rscratch1, octetCounter, large_loop_threshold);
4622       inflate_and_store_2_fp_registers(true, v3, v4);
4623       inflate_and_store_2_fp_registers(true, v5, v6);
4624       __ br(__ GT, LOOP_PRFM);
4625       __ cmp(octetCounter, (u1)8);
4626       __ br(__ LT, DONE);
4627     __ bind(LOOP);
4628       __ ld1(v3, v4, v5, v6, __ T16B, Address(__ post(src, 64)));
4629       __ bind(LOOP_START);
4630       __ sub(octetCounter, octetCounter, 8);
4631       __ cmp(octetCounter, (u1)8);
4632       inflate_and_store_2_fp_registers(false, v3, v4);
4633       inflate_and_store_2_fp_registers(false, v5, v6);
4634       __ br(__ GE, LOOP);
4635     __ bind(DONE);
4636       __ ret(lr);
4637     return entry;
4638   }
4639 
4640   /**
4641    *  Arguments:
4642    *
4643    *  Input:
4644    *  c_rarg0   - current state address
4645    *  c_rarg1   - H key address
4646    *  c_rarg2   - data address
4647    *  c_rarg3   - number of blocks
4648    *
4649    *  Output:
4650    *  Updated state at c_rarg0
4651    */
4652   address generate_ghash_processBlocks() {
4653     // Bafflingly, GCM uses little-endian for the byte order, but
4654     // big-endian for the bit order.  For example, the polynomial 1 is
4655     // represented as the 16-byte string 80 00 00 00 | 12 bytes of 00.
4656     //
4657     // So, we must either reverse the bytes in each word and do
4658     // everything big-endian or reverse the bits in each byte and do
4659     // it little-endian.  On AArch64 it&#39;s more idiomatic to reverse
4660     // the bits in each byte (we have an instruction, RBIT, to do
4661     // that) and keep the data in little-endian bit order throught the
4662     // calculation, bit-reversing the inputs and outputs.
4663 
4664     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;ghash_processBlocks&quot;);
4665     __ align(wordSize * 2);
4666     address p = __ pc();
4667     __ emit_int64(0x87);  // The low-order bits of the field
4668                           // polynomial (i.e. p = z^7+z^2+z+1)
4669                           // repeated in the low and high parts of a
4670                           // 128-bit vector
4671     __ emit_int64(0x87);
4672 
4673     __ align(CodeEntryAlignment);
4674     address start = __ pc();
4675 
4676     Register state   = c_rarg0;
4677     Register subkeyH = c_rarg1;
4678     Register data    = c_rarg2;
4679     Register blocks  = c_rarg3;
4680 
4681     FloatRegister vzr = v30;
4682     __ eor(vzr, __ T16B, vzr, vzr); // zero register
4683 
4684     __ ldrq(v0, Address(state));
4685     __ ldrq(v1, Address(subkeyH));
4686 
4687     __ rev64(v0, __ T16B, v0);          // Bit-reverse words in state and subkeyH
4688     __ rbit(v0, __ T16B, v0);
4689     __ rev64(v1, __ T16B, v1);
4690     __ rbit(v1, __ T16B, v1);
4691 
4692     __ ldrq(v26, p);
4693 
4694     __ ext(v16, __ T16B, v1, v1, 0x08); // long-swap subkeyH into v1
4695     __ eor(v16, __ T16B, v16, v1);      // xor subkeyH into subkeyL (Karatsuba: (A1+A0))
4696 
4697     {
4698       Label L_ghash_loop;
4699       __ bind(L_ghash_loop);
4700 
4701       __ ldrq(v2, Address(__ post(data, 0x10))); // Load the data, bit
4702                                                  // reversing each byte
4703       __ rbit(v2, __ T16B, v2);
4704       __ eor(v2, __ T16B, v0, v2);   // bit-swapped data ^ bit-swapped state
4705 
4706       // Multiply state in v2 by subkey in v1
4707       ghash_multiply(/*result_lo*/v5, /*result_hi*/v7,
4708                      /*a*/v1, /*b*/v2, /*a1_xor_a0*/v16,
4709                      /*temps*/v6, v20, v18, v21);
4710       // Reduce v7:v5 by the field polynomial
4711       ghash_reduce(v0, v5, v7, v26, vzr, v20);
4712 
4713       __ sub(blocks, blocks, 1);
4714       __ cbnz(blocks, L_ghash_loop);
4715     }
4716 
4717     // The bit-reversed result is at this point in v0
4718     __ rev64(v1, __ T16B, v0);
4719     __ rbit(v1, __ T16B, v1);
4720 
4721     __ st1(v1, __ T16B, state);
4722     __ ret(lr);
4723 
4724     return start;
4725   }
4726 
4727   // Continuation point for throwing of implicit exceptions that are
4728   // not handled in the current activation. Fabricates an exception
4729   // oop and initiates normal exception dispatching in this
4730   // frame. Since we need to preserve callee-saved values (currently
4731   // only for C2, but done for C1 as well) we need a callee-saved oop
4732   // map and therefore have to make these stubs into RuntimeStubs
4733   // rather than BufferBlobs.  If the compiler needs all registers to
4734   // be preserved between the fault point and the exception handler
4735   // then it must assume responsibility for that in
4736   // AbstractCompiler::continuation_for_implicit_null_exception or
4737   // continuation_for_implicit_division_by_zero_exception. All other
4738   // implicit exceptions (e.g., NullPointerException or
4739   // AbstractMethodError on entry) are either at call sites or
4740   // otherwise assume that stack unwinding will be initiated, so
4741   // caller saved registers were assumed volatile in the compiler.
4742 
4743 #undef __
4744 #define __ masm-&gt;
4745 
4746   address generate_throw_exception(const char* name,
4747                                    address runtime_entry,
4748                                    Register arg1 = noreg,
4749                                    Register arg2 = noreg) {
4750     // Information about frame layout at time of blocking runtime call.
4751     // Note that we only have to preserve callee-saved registers since
4752     // the compilers are responsible for supplying a continuation point
4753     // if they expect all registers to be preserved.
4754     // n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0
4755     enum layout {
4756       rfp_off = 0,
4757       rfp_off2,
4758       return_off,
4759       return_off2,
4760       framesize // inclusive of return address
4761     };
4762 
4763     int insts_size = 512;
4764     int locs_size  = 64;
4765 
4766     CodeBuffer code(name, insts_size, locs_size);
4767     OopMapSet* oop_maps  = new OopMapSet();
4768     MacroAssembler* masm = new MacroAssembler(&amp;code);
4769 
4770     address start = __ pc();
4771 
4772     // This is an inlined and slightly modified version of call_VM
4773     // which has the ability to fetch the return PC out of
4774     // thread-local storage and also sets up last_Java_sp slightly
4775     // differently than the real call_VM
4776 
4777     __ enter(); // Save FP and LR before call
4778 
4779     assert(is_even(framesize/2), &quot;sp not 16-byte aligned&quot;);
4780 
4781     // lr and fp are already in place
4782     __ sub(sp, rfp, ((unsigned)framesize-4) &lt;&lt; LogBytesPerInt); // prolog
4783 
4784     int frame_complete = __ pc() - start;
4785 
4786     // Set up last_Java_sp and last_Java_fp
4787     address the_pc = __ pc();
4788     __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);
4789 
4790     // Call runtime
4791     if (arg1 != noreg) {
4792       assert(arg2 != c_rarg1, &quot;clobbered&quot;);
4793       __ mov(c_rarg1, arg1);
4794     }
4795     if (arg2 != noreg) {
4796       __ mov(c_rarg2, arg2);
4797     }
4798     __ mov(c_rarg0, rthread);
4799     BLOCK_COMMENT(&quot;call runtime_entry&quot;);
4800     __ mov(rscratch1, runtime_entry);
4801     __ blr(rscratch1);
4802 
4803     // Generate oop map
4804     OopMap* map = new OopMap(framesize, 0);
4805 
4806     oop_maps-&gt;add_gc_map(the_pc - start, map);
4807 
4808     __ reset_last_Java_frame(true);
4809     __ maybe_isb();
4810 
4811     __ leave();
4812 
4813     // check for pending exceptions
4814 #ifdef ASSERT
4815     Label L;
4816     __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
4817     __ cbnz(rscratch1, L);
4818     __ should_not_reach_here();
4819     __ bind(L);
4820 #endif // ASSERT
4821     __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
4822 
4823 
4824     // codeBlob framesize is in words (not VMRegImpl::slot_size)
4825     RuntimeStub* stub =
4826       RuntimeStub::new_runtime_stub(name,
4827                                     &amp;code,
4828                                     frame_complete,
4829                                     (framesize &gt;&gt; (LogBytesPerWord - LogBytesPerInt)),
4830                                     oop_maps, false);
4831     return stub-&gt;entry_point();
4832   }
4833 
4834   class MontgomeryMultiplyGenerator : public MacroAssembler {
4835 
4836     Register Pa_base, Pb_base, Pn_base, Pm_base, inv, Rlen, Ra, Rb, Rm, Rn,
4837       Pa, Pb, Pn, Pm, Rhi_ab, Rlo_ab, Rhi_mn, Rlo_mn, t0, t1, t2, Ri, Rj;
4838 
4839     RegSet _toSave;
4840     bool _squaring;
4841 
4842   public:
4843     MontgomeryMultiplyGenerator (Assembler *as, bool squaring)
4844       : MacroAssembler(as-&gt;code()), _squaring(squaring) {
4845 
4846       // Register allocation
4847 
4848       Register reg = c_rarg0;
4849       Pa_base = reg;       // Argument registers
4850       if (squaring)
4851         Pb_base = Pa_base;
4852       else
4853         Pb_base = ++reg;
4854       Pn_base = ++reg;
4855       Rlen= ++reg;
4856       inv = ++reg;
4857       Pm_base = ++reg;
4858 
4859                           // Working registers:
4860       Ra =  ++reg;        // The current digit of a, b, n, and m.
4861       Rb =  ++reg;
4862       Rm =  ++reg;
4863       Rn =  ++reg;
4864 
4865       Pa =  ++reg;        // Pointers to the current/next digit of a, b, n, and m.
4866       Pb =  ++reg;
4867       Pm =  ++reg;
4868       Pn =  ++reg;
4869 
4870       t0 =  ++reg;        // Three registers which form a
4871       t1 =  ++reg;        // triple-precision accumuator.
4872       t2 =  ++reg;
4873 
4874       Ri =  ++reg;        // Inner and outer loop indexes.
4875       Rj =  ++reg;
4876 
4877       Rhi_ab = ++reg;     // Product registers: low and high parts
4878       Rlo_ab = ++reg;     // of a*b and m*n.
4879       Rhi_mn = ++reg;
4880       Rlo_mn = ++reg;
4881 
4882       // r19 and up are callee-saved.
4883       _toSave = RegSet::range(r19, reg) + Pm_base;
4884     }
4885 
4886   private:
4887     void save_regs() {
4888       push(_toSave, sp);
4889     }
4890 
4891     void restore_regs() {
4892       pop(_toSave, sp);
4893     }
4894 
4895     template &lt;typename T&gt;
4896     void unroll_2(Register count, T block) {
4897       Label loop, end, odd;
4898       tbnz(count, 0, odd);
4899       cbz(count, end);
4900       align(16);
4901       bind(loop);
4902       (this-&gt;*block)();
4903       bind(odd);
4904       (this-&gt;*block)();
4905       subs(count, count, 2);
4906       br(Assembler::GT, loop);
4907       bind(end);
4908     }
4909 
4910     template &lt;typename T&gt;
4911     void unroll_2(Register count, T block, Register d, Register s, Register tmp) {
4912       Label loop, end, odd;
4913       tbnz(count, 0, odd);
4914       cbz(count, end);
4915       align(16);
4916       bind(loop);
4917       (this-&gt;*block)(d, s, tmp);
4918       bind(odd);
4919       (this-&gt;*block)(d, s, tmp);
4920       subs(count, count, 2);
4921       br(Assembler::GT, loop);
4922       bind(end);
4923     }
4924 
4925     void pre1(RegisterOrConstant i) {
4926       block_comment(&quot;pre1&quot;);
4927       // Pa = Pa_base;
4928       // Pb = Pb_base + i;
4929       // Pm = Pm_base;
4930       // Pn = Pn_base + i;
4931       // Ra = *Pa;
4932       // Rb = *Pb;
4933       // Rm = *Pm;
4934       // Rn = *Pn;
4935       ldr(Ra, Address(Pa_base));
4936       ldr(Rb, Address(Pb_base, i, Address::uxtw(LogBytesPerWord)));
4937       ldr(Rm, Address(Pm_base));
4938       ldr(Rn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
4939       lea(Pa, Address(Pa_base));
4940       lea(Pb, Address(Pb_base, i, Address::uxtw(LogBytesPerWord)));
4941       lea(Pm, Address(Pm_base));
4942       lea(Pn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
4943 
4944       // Zero the m*n result.
4945       mov(Rhi_mn, zr);
4946       mov(Rlo_mn, zr);
4947     }
4948 
4949     // The core multiply-accumulate step of a Montgomery
4950     // multiplication.  The idea is to schedule operations as a
4951     // pipeline so that instructions with long latencies (loads and
4952     // multiplies) have time to complete before their results are
4953     // used.  This most benefits in-order implementations of the
4954     // architecture but out-of-order ones also benefit.
4955     void step() {
4956       block_comment(&quot;step&quot;);
4957       // MACC(Ra, Rb, t0, t1, t2);
4958       // Ra = *++Pa;
4959       // Rb = *--Pb;
4960       umulh(Rhi_ab, Ra, Rb);
4961       mul(Rlo_ab, Ra, Rb);
4962       ldr(Ra, pre(Pa, wordSize));
4963       ldr(Rb, pre(Pb, -wordSize));
4964       acc(Rhi_mn, Rlo_mn, t0, t1, t2); // The pending m*n from the
4965                                        // previous iteration.
4966       // MACC(Rm, Rn, t0, t1, t2);
4967       // Rm = *++Pm;
4968       // Rn = *--Pn;
4969       umulh(Rhi_mn, Rm, Rn);
4970       mul(Rlo_mn, Rm, Rn);
4971       ldr(Rm, pre(Pm, wordSize));
4972       ldr(Rn, pre(Pn, -wordSize));
4973       acc(Rhi_ab, Rlo_ab, t0, t1, t2);
4974     }
4975 
4976     void post1() {
4977       block_comment(&quot;post1&quot;);
4978 
4979       // MACC(Ra, Rb, t0, t1, t2);
4980       // Ra = *++Pa;
4981       // Rb = *--Pb;
4982       umulh(Rhi_ab, Ra, Rb);
4983       mul(Rlo_ab, Ra, Rb);
4984       acc(Rhi_mn, Rlo_mn, t0, t1, t2);  // The pending m*n
4985       acc(Rhi_ab, Rlo_ab, t0, t1, t2);
4986 
4987       // *Pm = Rm = t0 * inv;
4988       mul(Rm, t0, inv);
4989       str(Rm, Address(Pm));
4990 
4991       // MACC(Rm, Rn, t0, t1, t2);
4992       // t0 = t1; t1 = t2; t2 = 0;
4993       umulh(Rhi_mn, Rm, Rn);
4994 
4995 #ifndef PRODUCT
4996       // assert(m[i] * n[0] + t0 == 0, &quot;broken Montgomery multiply&quot;);
4997       {
4998         mul(Rlo_mn, Rm, Rn);
4999         add(Rlo_mn, t0, Rlo_mn);
5000         Label ok;
5001         cbz(Rlo_mn, ok); {
5002           stop(&quot;broken Montgomery multiply&quot;);
5003         } bind(ok);
5004       }
5005 #endif
5006       // We have very carefully set things up so that
5007       // m[i]*n[0] + t0 == 0 (mod b), so we don&#39;t have to calculate
5008       // the lower half of Rm * Rn because we know the result already:
5009       // it must be -t0.  t0 + (-t0) must generate a carry iff
5010       // t0 != 0.  So, rather than do a mul and an adds we just set
5011       // the carry flag iff t0 is nonzero.
5012       //
5013       // mul(Rlo_mn, Rm, Rn);
5014       // adds(zr, t0, Rlo_mn);
5015       subs(zr, t0, 1); // Set carry iff t0 is nonzero
5016       adcs(t0, t1, Rhi_mn);
5017       adc(t1, t2, zr);
5018       mov(t2, zr);
5019     }
5020 
5021     void pre2(RegisterOrConstant i, RegisterOrConstant len) {
5022       block_comment(&quot;pre2&quot;);
5023       // Pa = Pa_base + i-len;
5024       // Pb = Pb_base + len;
5025       // Pm = Pm_base + i-len;
5026       // Pn = Pn_base + len;
5027 
5028       if (i.is_register()) {
5029         sub(Rj, i.as_register(), len);
5030       } else {
5031         mov(Rj, i.as_constant());
5032         sub(Rj, Rj, len);
5033       }
5034       // Rj == i-len
5035 
5036       lea(Pa, Address(Pa_base, Rj, Address::uxtw(LogBytesPerWord)));
5037       lea(Pb, Address(Pb_base, len, Address::uxtw(LogBytesPerWord)));
5038       lea(Pm, Address(Pm_base, Rj, Address::uxtw(LogBytesPerWord)));
5039       lea(Pn, Address(Pn_base, len, Address::uxtw(LogBytesPerWord)));
5040 
5041       // Ra = *++Pa;
5042       // Rb = *--Pb;
5043       // Rm = *++Pm;
5044       // Rn = *--Pn;
5045       ldr(Ra, pre(Pa, wordSize));
5046       ldr(Rb, pre(Pb, -wordSize));
5047       ldr(Rm, pre(Pm, wordSize));
5048       ldr(Rn, pre(Pn, -wordSize));
5049 
5050       mov(Rhi_mn, zr);
5051       mov(Rlo_mn, zr);
5052     }
5053 
5054     void post2(RegisterOrConstant i, RegisterOrConstant len) {
5055       block_comment(&quot;post2&quot;);
5056       if (i.is_constant()) {
5057         mov(Rj, i.as_constant()-len.as_constant());
5058       } else {
5059         sub(Rj, i.as_register(), len);
5060       }
5061 
5062       adds(t0, t0, Rlo_mn); // The pending m*n, low part
5063 
5064       // As soon as we know the least significant digit of our result,
5065       // store it.
5066       // Pm_base[i-len] = t0;
5067       str(t0, Address(Pm_base, Rj, Address::uxtw(LogBytesPerWord)));
5068 
5069       // t0 = t1; t1 = t2; t2 = 0;
5070       adcs(t0, t1, Rhi_mn); // The pending m*n, high part
5071       adc(t1, t2, zr);
5072       mov(t2, zr);
5073     }
5074 
5075     // A carry in t0 after Montgomery multiplication means that we
5076     // should subtract multiples of n from our result in m.  We&#39;ll
5077     // keep doing that until there is no carry.
5078     void normalize(RegisterOrConstant len) {
5079       block_comment(&quot;normalize&quot;);
5080       // while (t0)
5081       //   t0 = sub(Pm_base, Pn_base, t0, len);
5082       Label loop, post, again;
5083       Register cnt = t1, i = t2; // Re-use registers; we&#39;re done with them now
5084       cbz(t0, post); {
5085         bind(again); {
5086           mov(i, zr);
5087           mov(cnt, len);
5088           ldr(Rm, Address(Pm_base, i, Address::uxtw(LogBytesPerWord)));
5089           ldr(Rn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
5090           subs(zr, zr, zr); // set carry flag, i.e. no borrow
5091           align(16);
5092           bind(loop); {
5093             sbcs(Rm, Rm, Rn);
5094             str(Rm, Address(Pm_base, i, Address::uxtw(LogBytesPerWord)));
5095             add(i, i, 1);
5096             ldr(Rm, Address(Pm_base, i, Address::uxtw(LogBytesPerWord)));
5097             ldr(Rn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
5098             sub(cnt, cnt, 1);
5099           } cbnz(cnt, loop);
5100           sbc(t0, t0, zr);
5101         } cbnz(t0, again);
5102       } bind(post);
5103     }
5104 
5105     // Move memory at s to d, reversing words.
5106     //    Increments d to end of copied memory
5107     //    Destroys tmp1, tmp2
5108     //    Preserves len
5109     //    Leaves s pointing to the address which was in d at start
5110     void reverse(Register d, Register s, Register len, Register tmp1, Register tmp2) {
5111       assert(tmp1 &lt; r19 &amp;&amp; tmp2 &lt; r19, &quot;register corruption&quot;);
5112 
5113       lea(s, Address(s, len, Address::uxtw(LogBytesPerWord)));
5114       mov(tmp1, len);
5115       unroll_2(tmp1, &amp;MontgomeryMultiplyGenerator::reverse1, d, s, tmp2);
5116       sub(s, d, len, ext::uxtw, LogBytesPerWord);
5117     }
5118     // where
5119     void reverse1(Register d, Register s, Register tmp) {
5120       ldr(tmp, pre(s, -wordSize));
5121       ror(tmp, tmp, 32);
5122       str(tmp, post(d, wordSize));
5123     }
5124 
5125     void step_squaring() {
5126       // An extra ACC
5127       step();
5128       acc(Rhi_ab, Rlo_ab, t0, t1, t2);
5129     }
5130 
5131     void last_squaring(RegisterOrConstant i) {
5132       Label dont;
5133       // if ((i &amp; 1) == 0) {
5134       tbnz(i.as_register(), 0, dont); {
5135         // MACC(Ra, Rb, t0, t1, t2);
5136         // Ra = *++Pa;
5137         // Rb = *--Pb;
5138         umulh(Rhi_ab, Ra, Rb);
5139         mul(Rlo_ab, Ra, Rb);
5140         acc(Rhi_ab, Rlo_ab, t0, t1, t2);
5141       } bind(dont);
5142     }
5143 
5144     void extra_step_squaring() {
5145       acc(Rhi_mn, Rlo_mn, t0, t1, t2);  // The pending m*n
5146 
5147       // MACC(Rm, Rn, t0, t1, t2);
5148       // Rm = *++Pm;
5149       // Rn = *--Pn;
5150       umulh(Rhi_mn, Rm, Rn);
5151       mul(Rlo_mn, Rm, Rn);
5152       ldr(Rm, pre(Pm, wordSize));
5153       ldr(Rn, pre(Pn, -wordSize));
5154     }
5155 
5156     void post1_squaring() {
5157       acc(Rhi_mn, Rlo_mn, t0, t1, t2);  // The pending m*n
5158 
5159       // *Pm = Rm = t0 * inv;
5160       mul(Rm, t0, inv);
5161       str(Rm, Address(Pm));
5162 
5163       // MACC(Rm, Rn, t0, t1, t2);
5164       // t0 = t1; t1 = t2; t2 = 0;
5165       umulh(Rhi_mn, Rm, Rn);
5166 
5167 #ifndef PRODUCT
5168       // assert(m[i] * n[0] + t0 == 0, &quot;broken Montgomery multiply&quot;);
5169       {
5170         mul(Rlo_mn, Rm, Rn);
5171         add(Rlo_mn, t0, Rlo_mn);
5172         Label ok;
5173         cbz(Rlo_mn, ok); {
5174           stop(&quot;broken Montgomery multiply&quot;);
5175         } bind(ok);
5176       }
5177 #endif
5178       // We have very carefully set things up so that
5179       // m[i]*n[0] + t0 == 0 (mod b), so we don&#39;t have to calculate
5180       // the lower half of Rm * Rn because we know the result already:
5181       // it must be -t0.  t0 + (-t0) must generate a carry iff
5182       // t0 != 0.  So, rather than do a mul and an adds we just set
5183       // the carry flag iff t0 is nonzero.
5184       //
5185       // mul(Rlo_mn, Rm, Rn);
5186       // adds(zr, t0, Rlo_mn);
5187       subs(zr, t0, 1); // Set carry iff t0 is nonzero
5188       adcs(t0, t1, Rhi_mn);
5189       adc(t1, t2, zr);
5190       mov(t2, zr);
5191     }
5192 
5193     void acc(Register Rhi, Register Rlo,
5194              Register t0, Register t1, Register t2) {
5195       adds(t0, t0, Rlo);
5196       adcs(t1, t1, Rhi);
5197       adc(t2, t2, zr);
5198     }
5199 
5200   public:
5201     /**
5202      * Fast Montgomery multiplication.  The derivation of the
5203      * algorithm is in A Cryptographic Library for the Motorola
5204      * DSP56000, Dusse and Kaliski, Proc. EUROCRYPT 90, pp. 230-237.
5205      *
5206      * Arguments:
5207      *
5208      * Inputs for multiplication:
5209      *   c_rarg0   - int array elements a
5210      *   c_rarg1   - int array elements b
5211      *   c_rarg2   - int array elements n (the modulus)
5212      *   c_rarg3   - int length
5213      *   c_rarg4   - int inv
5214      *   c_rarg5   - int array elements m (the result)
5215      *
5216      * Inputs for squaring:
5217      *   c_rarg0   - int array elements a
5218      *   c_rarg1   - int array elements n (the modulus)
5219      *   c_rarg2   - int length
5220      *   c_rarg3   - int inv
5221      *   c_rarg4   - int array elements m (the result)
5222      *
5223      */
5224     address generate_multiply() {
5225       Label argh, nothing;
5226       bind(argh);
5227       stop(&quot;MontgomeryMultiply total_allocation must be &lt;= 8192&quot;);
5228 
5229       align(CodeEntryAlignment);
5230       address entry = pc();
5231 
5232       cbzw(Rlen, nothing);
5233 
5234       enter();
5235 
5236       // Make room.
5237       cmpw(Rlen, 512);
5238       br(Assembler::HI, argh);
5239       sub(Ra, sp, Rlen, ext::uxtw, exact_log2(4 * sizeof (jint)));
5240       andr(sp, Ra, -2 * wordSize);
5241 
5242       lsrw(Rlen, Rlen, 1);  // length in longwords = len/2
5243 
5244       {
5245         // Copy input args, reversing as we go.  We use Ra as a
5246         // temporary variable.
5247         reverse(Ra, Pa_base, Rlen, t0, t1);
5248         if (!_squaring)
5249           reverse(Ra, Pb_base, Rlen, t0, t1);
5250         reverse(Ra, Pn_base, Rlen, t0, t1);
5251       }
5252 
5253       // Push all call-saved registers and also Pm_base which we&#39;ll need
5254       // at the end.
5255       save_regs();
5256 
5257 #ifndef PRODUCT
5258       // assert(inv * n[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
5259       {
5260         ldr(Rn, Address(Pn_base, 0));
5261         mul(Rlo_mn, Rn, inv);
5262         subs(zr, Rlo_mn, -1);
5263         Label ok;
5264         br(EQ, ok); {
5265           stop(&quot;broken inverse in Montgomery multiply&quot;);
5266         } bind(ok);
5267       }
5268 #endif
5269 
5270       mov(Pm_base, Ra);
5271 
5272       mov(t0, zr);
5273       mov(t1, zr);
5274       mov(t2, zr);
5275 
5276       block_comment(&quot;for (int i = 0; i &lt; len; i++) {&quot;);
5277       mov(Ri, zr); {
5278         Label loop, end;
5279         cmpw(Ri, Rlen);
5280         br(Assembler::GE, end);
5281 
5282         bind(loop);
5283         pre1(Ri);
5284 
5285         block_comment(&quot;  for (j = i; j; j--) {&quot;); {
5286           movw(Rj, Ri);
5287           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step);
5288         } block_comment(&quot;  } // j&quot;);
5289 
5290         post1();
5291         addw(Ri, Ri, 1);
5292         cmpw(Ri, Rlen);
5293         br(Assembler::LT, loop);
5294         bind(end);
5295         block_comment(&quot;} // i&quot;);
5296       }
5297 
5298       block_comment(&quot;for (int i = len; i &lt; 2*len; i++) {&quot;);
5299       mov(Ri, Rlen); {
5300         Label loop, end;
5301         cmpw(Ri, Rlen, Assembler::LSL, 1);
5302         br(Assembler::GE, end);
5303 
5304         bind(loop);
5305         pre2(Ri, Rlen);
5306 
5307         block_comment(&quot;  for (j = len*2-i-1; j; j--) {&quot;); {
5308           lslw(Rj, Rlen, 1);
5309           subw(Rj, Rj, Ri);
5310           subw(Rj, Rj, 1);
5311           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step);
5312         } block_comment(&quot;  } // j&quot;);
5313 
5314         post2(Ri, Rlen);
5315         addw(Ri, Ri, 1);
5316         cmpw(Ri, Rlen, Assembler::LSL, 1);
5317         br(Assembler::LT, loop);
5318         bind(end);
5319       }
5320       block_comment(&quot;} // i&quot;);
5321 
5322       normalize(Rlen);
5323 
5324       mov(Ra, Pm_base);  // Save Pm_base in Ra
5325       restore_regs();  // Restore caller&#39;s Pm_base
5326 
5327       // Copy our result into caller&#39;s Pm_base
5328       reverse(Pm_base, Ra, Rlen, t0, t1);
5329 
5330       leave();
5331       bind(nothing);
5332       ret(lr);
5333 
5334       return entry;
5335     }
5336     // In C, approximately:
5337 
5338     // void
5339     // montgomery_multiply(unsigned long Pa_base[], unsigned long Pb_base[],
5340     //                     unsigned long Pn_base[], unsigned long Pm_base[],
5341     //                     unsigned long inv, int len) {
5342     //   unsigned long t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
5343     //   unsigned long *Pa, *Pb, *Pn, *Pm;
5344     //   unsigned long Ra, Rb, Rn, Rm;
5345 
5346     //   int i;
5347 
5348     //   assert(inv * Pn_base[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
5349 
5350     //   for (i = 0; i &lt; len; i++) {
5351     //     int j;
5352 
5353     //     Pa = Pa_base;
5354     //     Pb = Pb_base + i;
5355     //     Pm = Pm_base;
5356     //     Pn = Pn_base + i;
5357 
5358     //     Ra = *Pa;
5359     //     Rb = *Pb;
5360     //     Rm = *Pm;
5361     //     Rn = *Pn;
5362 
5363     //     int iters = i;
5364     //     for (j = 0; iters--; j++) {
5365     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pb_base[i-j], &quot;must be&quot;);
5366     //       MACC(Ra, Rb, t0, t1, t2);
5367     //       Ra = *++Pa;
5368     //       Rb = *--Pb;
5369     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5370     //       MACC(Rm, Rn, t0, t1, t2);
5371     //       Rm = *++Pm;
5372     //       Rn = *--Pn;
5373     //     }
5374 
5375     //     assert(Ra == Pa_base[i] &amp;&amp; Rb == Pb_base[0], &quot;must be&quot;);
5376     //     MACC(Ra, Rb, t0, t1, t2);
5377     //     *Pm = Rm = t0 * inv;
5378     //     assert(Rm == Pm_base[i] &amp;&amp; Rn == Pn_base[0], &quot;must be&quot;);
5379     //     MACC(Rm, Rn, t0, t1, t2);
5380 
5381     //     assert(t0 == 0, &quot;broken Montgomery multiply&quot;);
5382 
5383     //     t0 = t1; t1 = t2; t2 = 0;
5384     //   }
5385 
5386     //   for (i = len; i &lt; 2*len; i++) {
5387     //     int j;
5388 
5389     //     Pa = Pa_base + i-len;
5390     //     Pb = Pb_base + len;
5391     //     Pm = Pm_base + i-len;
5392     //     Pn = Pn_base + len;
5393 
5394     //     Ra = *++Pa;
5395     //     Rb = *--Pb;
5396     //     Rm = *++Pm;
5397     //     Rn = *--Pn;
5398 
5399     //     int iters = len*2-i-1;
5400     //     for (j = i-len+1; iters--; j++) {
5401     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pb_base[i-j], &quot;must be&quot;);
5402     //       MACC(Ra, Rb, t0, t1, t2);
5403     //       Ra = *++Pa;
5404     //       Rb = *--Pb;
5405     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5406     //       MACC(Rm, Rn, t0, t1, t2);
5407     //       Rm = *++Pm;
5408     //       Rn = *--Pn;
5409     //     }
5410 
5411     //     Pm_base[i-len] = t0;
5412     //     t0 = t1; t1 = t2; t2 = 0;
5413     //   }
5414 
5415     //   while (t0)
5416     //     t0 = sub(Pm_base, Pn_base, t0, len);
5417     // }
5418 
5419     /**
5420      * Fast Montgomery squaring.  This uses asymptotically 25% fewer
5421      * multiplies than Montgomery multiplication so it should be up to
5422      * 25% faster.  However, its loop control is more complex and it
5423      * may actually run slower on some machines.
5424      *
5425      * Arguments:
5426      *
5427      * Inputs:
5428      *   c_rarg0   - int array elements a
5429      *   c_rarg1   - int array elements n (the modulus)
5430      *   c_rarg2   - int length
5431      *   c_rarg3   - int inv
5432      *   c_rarg4   - int array elements m (the result)
5433      *
5434      */
5435     address generate_square() {
5436       Label argh;
5437       bind(argh);
5438       stop(&quot;MontgomeryMultiply total_allocation must be &lt;= 8192&quot;);
5439 
5440       align(CodeEntryAlignment);
5441       address entry = pc();
5442 
5443       enter();
5444 
5445       // Make room.
5446       cmpw(Rlen, 512);
5447       br(Assembler::HI, argh);
5448       sub(Ra, sp, Rlen, ext::uxtw, exact_log2(4 * sizeof (jint)));
5449       andr(sp, Ra, -2 * wordSize);
5450 
5451       lsrw(Rlen, Rlen, 1);  // length in longwords = len/2
5452 
5453       {
5454         // Copy input args, reversing as we go.  We use Ra as a
5455         // temporary variable.
5456         reverse(Ra, Pa_base, Rlen, t0, t1);
5457         reverse(Ra, Pn_base, Rlen, t0, t1);
5458       }
5459 
5460       // Push all call-saved registers and also Pm_base which we&#39;ll need
5461       // at the end.
5462       save_regs();
5463 
5464       mov(Pm_base, Ra);
5465 
5466       mov(t0, zr);
5467       mov(t1, zr);
5468       mov(t2, zr);
5469 
5470       block_comment(&quot;for (int i = 0; i &lt; len; i++) {&quot;);
5471       mov(Ri, zr); {
5472         Label loop, end;
5473         bind(loop);
5474         cmp(Ri, Rlen);
5475         br(Assembler::GE, end);
5476 
5477         pre1(Ri);
5478 
5479         block_comment(&quot;for (j = (i+1)/2; j; j--) {&quot;); {
5480           add(Rj, Ri, 1);
5481           lsr(Rj, Rj, 1);
5482           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step_squaring);
5483         } block_comment(&quot;  } // j&quot;);
5484 
5485         last_squaring(Ri);
5486 
5487         block_comment(&quot;  for (j = i/2; j; j--) {&quot;); {
5488           lsr(Rj, Ri, 1);
5489           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::extra_step_squaring);
5490         } block_comment(&quot;  } // j&quot;);
5491 
5492         post1_squaring();
5493         add(Ri, Ri, 1);
5494         cmp(Ri, Rlen);
5495         br(Assembler::LT, loop);
5496 
5497         bind(end);
5498         block_comment(&quot;} // i&quot;);
5499       }
5500 
5501       block_comment(&quot;for (int i = len; i &lt; 2*len; i++) {&quot;);
5502       mov(Ri, Rlen); {
5503         Label loop, end;
5504         bind(loop);
5505         cmp(Ri, Rlen, Assembler::LSL, 1);
5506         br(Assembler::GE, end);
5507 
5508         pre2(Ri, Rlen);
5509 
5510         block_comment(&quot;  for (j = (2*len-i-1)/2; j; j--) {&quot;); {
5511           lsl(Rj, Rlen, 1);
5512           sub(Rj, Rj, Ri);
5513           sub(Rj, Rj, 1);
5514           lsr(Rj, Rj, 1);
5515           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step_squaring);
5516         } block_comment(&quot;  } // j&quot;);
5517 
5518         last_squaring(Ri);
5519 
5520         block_comment(&quot;  for (j = (2*len-i)/2; j; j--) {&quot;); {
5521           lsl(Rj, Rlen, 1);
5522           sub(Rj, Rj, Ri);
5523           lsr(Rj, Rj, 1);
5524           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::extra_step_squaring);
5525         } block_comment(&quot;  } // j&quot;);
5526 
5527         post2(Ri, Rlen);
5528         add(Ri, Ri, 1);
5529         cmp(Ri, Rlen, Assembler::LSL, 1);
5530 
5531         br(Assembler::LT, loop);
5532         bind(end);
5533         block_comment(&quot;} // i&quot;);
5534       }
5535 
5536       normalize(Rlen);
5537 
5538       mov(Ra, Pm_base);  // Save Pm_base in Ra
5539       restore_regs();  // Restore caller&#39;s Pm_base
5540 
5541       // Copy our result into caller&#39;s Pm_base
5542       reverse(Pm_base, Ra, Rlen, t0, t1);
5543 
5544       leave();
5545       ret(lr);
5546 
5547       return entry;
5548     }
5549     // In C, approximately:
5550 
5551     // void
5552     // montgomery_square(unsigned long Pa_base[], unsigned long Pn_base[],
5553     //                   unsigned long Pm_base[], unsigned long inv, int len) {
5554     //   unsigned long t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
5555     //   unsigned long *Pa, *Pb, *Pn, *Pm;
5556     //   unsigned long Ra, Rb, Rn, Rm;
5557 
5558     //   int i;
5559 
5560     //   assert(inv * Pn_base[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
5561 
5562     //   for (i = 0; i &lt; len; i++) {
5563     //     int j;
5564 
5565     //     Pa = Pa_base;
5566     //     Pb = Pa_base + i;
5567     //     Pm = Pm_base;
5568     //     Pn = Pn_base + i;
5569 
5570     //     Ra = *Pa;
5571     //     Rb = *Pb;
5572     //     Rm = *Pm;
5573     //     Rn = *Pn;
5574 
5575     //     int iters = (i+1)/2;
5576     //     for (j = 0; iters--; j++) {
5577     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pa_base[i-j], &quot;must be&quot;);
5578     //       MACC2(Ra, Rb, t0, t1, t2);
5579     //       Ra = *++Pa;
5580     //       Rb = *--Pb;
5581     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5582     //       MACC(Rm, Rn, t0, t1, t2);
5583     //       Rm = *++Pm;
5584     //       Rn = *--Pn;
5585     //     }
5586     //     if ((i &amp; 1) == 0) {
5587     //       assert(Ra == Pa_base[j], &quot;must be&quot;);
5588     //       MACC(Ra, Ra, t0, t1, t2);
5589     //     }
5590     //     iters = i/2;
5591     //     assert(iters == i-j, &quot;must be&quot;);
5592     //     for (; iters--; j++) {
5593     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5594     //       MACC(Rm, Rn, t0, t1, t2);
5595     //       Rm = *++Pm;
5596     //       Rn = *--Pn;
5597     //     }
5598 
5599     //     *Pm = Rm = t0 * inv;
5600     //     assert(Rm == Pm_base[i] &amp;&amp; Rn == Pn_base[0], &quot;must be&quot;);
5601     //     MACC(Rm, Rn, t0, t1, t2);
5602 
5603     //     assert(t0 == 0, &quot;broken Montgomery multiply&quot;);
5604 
5605     //     t0 = t1; t1 = t2; t2 = 0;
5606     //   }
5607 
5608     //   for (i = len; i &lt; 2*len; i++) {
5609     //     int start = i-len+1;
5610     //     int end = start + (len - start)/2;
5611     //     int j;
5612 
5613     //     Pa = Pa_base + i-len;
5614     //     Pb = Pa_base + len;
5615     //     Pm = Pm_base + i-len;
5616     //     Pn = Pn_base + len;
5617 
5618     //     Ra = *++Pa;
5619     //     Rb = *--Pb;
5620     //     Rm = *++Pm;
5621     //     Rn = *--Pn;
5622 
5623     //     int iters = (2*len-i-1)/2;
5624     //     assert(iters == end-start, &quot;must be&quot;);
5625     //     for (j = start; iters--; j++) {
5626     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pa_base[i-j], &quot;must be&quot;);
5627     //       MACC2(Ra, Rb, t0, t1, t2);
5628     //       Ra = *++Pa;
5629     //       Rb = *--Pb;
5630     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5631     //       MACC(Rm, Rn, t0, t1, t2);
5632     //       Rm = *++Pm;
5633     //       Rn = *--Pn;
5634     //     }
5635     //     if ((i &amp; 1) == 0) {
5636     //       assert(Ra == Pa_base[j], &quot;must be&quot;);
5637     //       MACC(Ra, Ra, t0, t1, t2);
5638     //     }
5639     //     iters =  (2*len-i)/2;
5640     //     assert(iters == len-j, &quot;must be&quot;);
5641     //     for (; iters--; j++) {
5642     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5643     //       MACC(Rm, Rn, t0, t1, t2);
5644     //       Rm = *++Pm;
5645     //       Rn = *--Pn;
5646     //     }
5647     //     Pm_base[i-len] = t0;
5648     //     t0 = t1; t1 = t2; t2 = 0;
5649     //   }
5650 
5651     //   while (t0)
5652     //     t0 = sub(Pm_base, Pn_base, t0, len);
5653     // }
5654   };
5655 
5656 
5657   // Initialization
5658   void generate_initial() {
5659     // Generate initial stubs and initializes the entry points
5660 
5661     // entry points that exist in all platforms Note: This is code
5662     // that could be shared among different platforms - however the
5663     // benefit seems to be smaller than the disadvantage of having a
5664     // much more complicated generator structure. See also comment in
5665     // stubRoutines.hpp.
5666 
5667     StubRoutines::_forward_exception_entry = generate_forward_exception();
5668 
5669     StubRoutines::_call_stub_entry =
5670       generate_call_stub(StubRoutines::_call_stub_return_address);
5671 
5672     // is referenced by megamorphic call
5673     StubRoutines::_catch_exception_entry = generate_catch_exception();
5674 
5675     // Build this early so it&#39;s available for the interpreter.
5676     StubRoutines::_throw_StackOverflowError_entry =
5677       generate_throw_exception(&quot;StackOverflowError throw_exception&quot;,
5678                                CAST_FROM_FN_PTR(address,
5679                                                 SharedRuntime::throw_StackOverflowError));
5680     StubRoutines::_throw_delayed_StackOverflowError_entry =
5681       generate_throw_exception(&quot;delayed StackOverflowError throw_exception&quot;,
5682                                CAST_FROM_FN_PTR(address,
5683                                                 SharedRuntime::throw_delayed_StackOverflowError));
5684     if (UseCRC32Intrinsics) {
5685       // set table address before stub generation which use it
5686       StubRoutines::_crc_table_adr = (address)StubRoutines::aarch64::_crc_table;
5687       StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();
5688     }
5689 
5690     if (UseCRC32CIntrinsics) {
5691       StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C();
5692     }
5693 
5694     // Disabled until JDK-8210858 is fixed
5695     // if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {
5696     //   StubRoutines::_dlog = generate_dlog();
5697     // }
5698 
5699     if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {
5700       StubRoutines::_dsin = generate_dsin_dcos(/* isCos = */ false);
5701     }
5702 
5703     if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {
5704       StubRoutines::_dcos = generate_dsin_dcos(/* isCos = */ true);
5705     }
5706   }
5707 
5708   void generate_all() {
5709     // support for verify_oop (must happen after universe_init)
5710     StubRoutines::_verify_oop_subroutine_entry     = generate_verify_oop();
5711     StubRoutines::_throw_AbstractMethodError_entry =
5712       generate_throw_exception(&quot;AbstractMethodError throw_exception&quot;,
5713                                CAST_FROM_FN_PTR(address,
5714                                                 SharedRuntime::
5715                                                 throw_AbstractMethodError));
5716 
5717     StubRoutines::_throw_IncompatibleClassChangeError_entry =
5718       generate_throw_exception(&quot;IncompatibleClassChangeError throw_exception&quot;,
5719                                CAST_FROM_FN_PTR(address,
5720                                                 SharedRuntime::
5721                                                 throw_IncompatibleClassChangeError));
5722 
5723     StubRoutines::_throw_NullPointerException_at_call_entry =
5724       generate_throw_exception(&quot;NullPointerException at call throw_exception&quot;,
5725                                CAST_FROM_FN_PTR(address,
5726                                                 SharedRuntime::
5727                                                 throw_NullPointerException_at_call));
5728 
5729     // arraycopy stubs used by compilers
5730     generate_arraycopy_stubs();
5731 
5732     // has negatives stub for large arrays.
5733     StubRoutines::aarch64::_has_negatives = generate_has_negatives(StubRoutines::aarch64::_has_negatives_long);
5734 
5735     // array equals stub for large arrays.
5736     if (!UseSimpleArrayEquals) {
5737       StubRoutines::aarch64::_large_array_equals = generate_large_array_equals();
5738     }
5739 
5740     generate_compare_long_strings();
5741 
5742     generate_string_indexof_stubs();
5743 
5744     // byte_array_inflate stub for large arrays.
5745     StubRoutines::aarch64::_large_byte_array_inflate = generate_large_byte_array_inflate();
5746 
5747 #ifdef COMPILER2
5748     if (UseMultiplyToLenIntrinsic) {
5749       StubRoutines::_multiplyToLen = generate_multiplyToLen();
5750     }
5751 
5752     if (UseSquareToLenIntrinsic) {
5753       StubRoutines::_squareToLen = generate_squareToLen();
5754     }
5755 
5756     if (UseMulAddIntrinsic) {
5757       StubRoutines::_mulAdd = generate_mulAdd();
5758     }
5759 
5760     if (UseMontgomeryMultiplyIntrinsic) {
5761       StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;montgomeryMultiply&quot;);
5762       MontgomeryMultiplyGenerator g(_masm, /*squaring*/false);
5763       StubRoutines::_montgomeryMultiply = g.generate_multiply();
5764     }
5765 
5766     if (UseMontgomerySquareIntrinsic) {
5767       StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;montgomerySquare&quot;);
5768       MontgomeryMultiplyGenerator g(_masm, /*squaring*/true);
5769       // We use generate_multiply() rather than generate_square()
5770       // because it&#39;s faster for the sizes of modulus we care about.
5771       StubRoutines::_montgomerySquare = g.generate_multiply();
5772     }
5773 #endif // COMPILER2
5774 
5775     // generate GHASH intrinsics code
5776     if (UseGHASHIntrinsics) {
5777       StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();
5778     }
5779 
5780     // data cache line writeback
5781     StubRoutines::_data_cache_writeback = generate_data_cache_writeback();
5782     StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();
5783 
5784     if (UseAESIntrinsics) {
5785       StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();
5786       StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();
5787       StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();
5788       StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt();
5789     }
5790 
5791     if (UseSHA1Intrinsics) {
5792       StubRoutines::_sha1_implCompress     = generate_sha1_implCompress(false,   &quot;sha1_implCompress&quot;);
5793       StubRoutines::_sha1_implCompressMB   = generate_sha1_implCompress(true,    &quot;sha1_implCompressMB&quot;);
5794     }
5795     if (UseSHA256Intrinsics) {
5796       StubRoutines::_sha256_implCompress   = generate_sha256_implCompress(false, &quot;sha256_implCompress&quot;);
5797       StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true,  &quot;sha256_implCompressMB&quot;);
5798     }
5799 
5800     // generate Adler32 intrinsics code
5801     if (UseAdler32Intrinsics) {
5802       StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();
5803     }
5804 
5805     // Safefetch stubs.
5806     generate_safefetch(&quot;SafeFetch32&quot;, sizeof(int),     &amp;StubRoutines::_safefetch32_entry,
5807                                                        &amp;StubRoutines::_safefetch32_fault_pc,
5808                                                        &amp;StubRoutines::_safefetch32_continuation_pc);
5809     generate_safefetch(&quot;SafeFetchN&quot;, sizeof(intptr_t), &amp;StubRoutines::_safefetchN_entry,
5810                                                        &amp;StubRoutines::_safefetchN_fault_pc,
5811                                                        &amp;StubRoutines::_safefetchN_continuation_pc);
5812     StubRoutines::aarch64::set_completed();
5813   }
5814 
5815  public:
5816   StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {
5817     if (all) {
5818       generate_all();
5819     } else {
5820       generate_initial();
5821     }
5822   }
5823 }; // end class declaration
5824 
5825 #define UCM_TABLE_MAX_ENTRIES 8
5826 void StubGenerator_generate(CodeBuffer* code, bool all) {
5827   if (UnsafeCopyMemory::_table == NULL) {
5828     UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);
5829   }
5830   StubGenerator g(code, all);
5831 }
    </pre>
  </body>
</html>